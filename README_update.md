# 小男孩项目更新说明

## 问题诊断与修复

我们诊断出模型存在"拾取强迫症"的核心原因是：

1. **奖励设计失衡**：拾取奖励(+3.0)远高于击杀奖励(+2.0)，导致模型倾向于拾取行为。
2. **动作掩码过宽**：原先的掩码逻辑只在is_dead=0时禁用拾取，允许在非选中状态下尝试拾取。
3. **BC数据增强问题**：过度增强拾取动作(]键)，导致模型偏好拾取。
4. **奖励归因不合理**：gae_lambda=0.95过高，导致击杀奖励被过度分配给拾取动作。

## 修复方案

### 1. 奖励函数重新平衡

我们对奖励函数进行了以下调整：

- 击杀奖励：从+2.0提升到+3.0
- 拾取成功：从+3.0降低到+1.0
- G键选中尸体：从+0.5提升到+1.5
- 无效拾取惩罚：从-0.1增加到-0.2
- 基础惩罚：从-0.02增加到-0.05

这样重新分配奖励，将更多价值归因于击杀和尸体选择行为，减轻拾取动作的权重。

### 2. 更严格的动作掩码

实现了更严格的动作掩码规则：

```python
# 更严格的拾取逻辑：只有选中且死亡才允许拾取
if not (sel_flag and is_dead):
    mask[2] = 0  # 禁用]键

# 尸体阶段禁止攻击
if is_dead:
    mask[0] = 0  # 禁用F键

# 完全禁用Tab键
mask[7] = 0
```

这确保了模型只能在正确的状态下执行正确的动作，减少试错。

### 3. 局部奖励归因

调整PPO的参数以实现更合理的奖励归因：

- 将gae_lambda从0.95降低到0.8，使奖励更加局部化
- 保持gamma=0.99，确保长期奖励仍能传播
- 保持ent_coef=0.02和clip_range=0.25，鼓励适当探索

### 4. 新增功能：在线学习

创建了新的在线学习功能，可以在玩游戏的同时持续改进模型：

- `online_bc_update.py`：边玩边学习的工具
- 自动周期性更新模型（默认每5分钟）
- 支持手动触发保存和更新

## 使用指南

### 运行修复后的模型

1. 重新初始化PPO模型：
```
python scripts/reset_ppo_with_bc.py
```

2. 使用新的奖励设置训练PPO：
```
python scripts/train_ppo_wow.py
```

3. 测试训练后的模型：
```
python scripts/debug_model_actions.py
```

### 使用在线学习功能

1. 启动在线学习工具：
```
python scripts/online_bc_update.py
```

2. 使用功能键控制：
   - F8：开始记录玩家操作
   - F9：暂停记录
   - F10：手动保存数据并更新模型
   - Esc：退出程序

3. 工作流程：
   - 白天：运行`online_bc_update.py`，边玩边学习
   - 晚上：使用收集的数据运行PPO训练微调模型

## 未来改进方向

1. 实现更智能的动作掩码，动态适应不同状态
2. 开发更复杂的奖励结构，区分不同类型的击杀和拾取
3. 结合在线学习和强化学习，形成混合学习系统
4. 添加场景理解能力，让模型能识别不同区域和敌人类型

希望这些修改能解决"拾取强迫症"问题，让AI在魔兽世界中表现得更加自然和高效！ 