

# ===== 文件: D:\wow_ai\docker\cvat\manage.py =====
#!/usr/bin/env python3

# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os
import sys

if __name__ == "__main__":
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "cvat.settings.development")
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)


# ===== 文件: D:\wow_ai\docker\cvat\rqscheduler.py =====
#!/usr/bin/env python

# This script adds access to the Django env and settings in the default rqscheduler
# implementation. This is required for correct work with CVAT queue settings and
# their access options such as login and password.

from rq_scheduler.scripts import rqscheduler

# Required to initialize Django settings correctly
from cvat.asgi import application  # pylint: disable=unused-import

if __name__ == "__main__":
    rqscheduler.main()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\asgi.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

"""
ASGI config for CVAT project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/3.2/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application
from django.core.handlers.asgi import ASGIHandler

import cvat.utils.remote_debugger as debug

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "cvat.settings.development")

application = get_asgi_application()


if debug.is_debugging_enabled():

    class DebuggerApp(ASGIHandler):
        """
        Support for VS code debugger
        """

        def __init__(self) -> None:
            super().__init__()
            self.__debugger = debug.RemoteDebugger()

        async def handle(self, *args, **kwargs):
            self.__debugger.attach_current_thread()
            return await super().handle(*args, **kwargs)

    application = DebuggerApp()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\rqworker.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os

from rq import Worker

import cvat.utils.remote_debugger as debug

DefaultWorker = Worker


class BaseDeathPenalty:
    def __init__(self, timeout, exception, **kwargs):
        pass

    def __enter__(self):
        pass

    def __exit__(self, exc_type, exc_value, traceback):
        pass


class SimpleWorker(Worker):
    """
    Allows to work with at most 1 worker thread. Useful for debugging.
    """

    death_penalty_class = BaseDeathPenalty

    def main_work_horse(self, *args, **kwargs):
        raise NotImplementedError("Test worker does not implement this method")

    def execute_job(self, *args, **kwargs):
        """Execute job in same thread/process, do not fork()"""

        # Resolves problems with
        # django.db.utils.OperationalError: server closed the connection unexpectedly
        # errors during debugging
        # https://stackoverflow.com/questions/8242837/django-multiprocessing-and-database-connections/10684672#10684672
        from django import db

        db.connections.close_all()

        return self.perform_job(*args, **kwargs)


if debug.is_debugging_enabled():

    class RemoteDebugWorker(SimpleWorker):
        """
        Support for VS code debugger
        """

        def __init__(self, *args, **kwargs):
            self.__debugger = debug.RemoteDebugger()
            super().__init__(*args, **kwargs)

        def execute_job(self, *args, **kwargs):
            """Execute job in same thread/process, do not fork()"""
            self.__debugger.attach_current_thread()

            return super().execute_job(*args, **kwargs)

    DefaultWorker = RemoteDebugWorker


if os.environ.get("COVERAGE_PROCESS_START"):
    import coverage

    default_exit = os._exit

    def coverage_exit(*args, **kwargs):
        cov = coverage.Coverage.current()
        if cov:
            cov.stop()
            cov.save()
        default_exit(*args, **kwargs)

    os._exit = coverage_exit


# ===== 文件: D:\wow_ai\docker\cvat\cvat\rq_patching.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import traceback
from datetime import datetime
from typing import Optional

import rq.registry
from rq.exceptions import AbandonedJobError, NoSuchJobError
from rq.job import JobStatus
from rq.utils import current_timestamp
from rq.version import VERSION


# NOTE: we should patch implementation of original method because
# there is no enqueuing dependent jobs in original function
# https://github.com/rq/rq/issues/2006
# Link to PR: https://github.com/rq/rq/pull/2008
def custom_started_job_registry_cleanup(self, timestamp: Optional[float] = None):
    """Remove abandoned jobs from registry and add them to FailedJobRegistry.

    Removes jobs with an expiry time earlier than timestamp, specified as
    seconds since the Unix epoch. timestamp defaults to call time if
    unspecified. Removed jobs are added to the global failed job queue.

    Args:
        timestamp (datetime): The datetime to use as the limit.
    """

    score = timestamp if timestamp is not None else current_timestamp()
    job_ids = self.get_expired_job_ids(score)

    if job_ids:
        failed_job_registry = rq.registry.FailedJobRegistry(
            self.name, self.connection, serializer=self.serializer
        )
        queue = self.get_queue()

        with self.connection.pipeline() as pipeline:
            for job_id in job_ids:
                try:
                    job = self.job_class.fetch(
                        job_id, connection=self.connection, serializer=self.serializer
                    )
                except NoSuchJobError:
                    continue

                job.execute_failure_callback(
                    self.death_penalty_class,
                    AbandonedJobError,
                    AbandonedJobError(),
                    traceback.extract_stack(),
                )

                retry = job.retries_left and job.retries_left > 0

                if retry:
                    job.retry(queue, pipeline)

                else:
                    exc_string = f"due to {AbandonedJobError.__name__}"
                    rq.registry.logger.warning(
                        f"{self.__class__.__name__} cleanup: Moving job to {rq.registry.FailedJobRegistry.__name__} "
                        f"({exc_string})"
                    )
                    job.set_status(JobStatus.FAILED)
                    job._exc_info = f"Moved to {rq.registry.FailedJobRegistry.__name__}, {exc_string}, at {datetime.now()}"
                    job.save(pipeline=pipeline, include_meta=False)
                    job.cleanup(ttl=-1, pipeline=pipeline)
                    failed_job_registry.add(job, job.failure_ttl)
                    queue.enqueue_dependents(job)

            pipeline.zremrangebyscore(self.key, 0, score)
            pipeline.execute()

    return job_ids


def update_started_job_registry_cleanup() -> None:
    # don't forget to check if the issue https://github.com/rq/rq/issues/2006 has been resolved in upstream
    assert VERSION == "1.16.0"
    rq.registry.StartedJobRegistry.cleanup = custom_started_job_registry_cleanup


# ===== 文件: D:\wow_ai\docker\cvat\cvat\urls.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

"""CVAT URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/2.0/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
"""

from django.apps import apps
from django.contrib import admin
from django.urls import include, path

urlpatterns = [
    path("admin/", admin.site.urls),
    path("", include("cvat.apps.engine.urls")),
    path("django-rq/", include("django_rq.urls")),
]

if apps.is_installed("cvat.apps.log_viewer"):
    urlpatterns.append(path("", include("cvat.apps.log_viewer.urls")))

if apps.is_installed("cvat.apps.events"):
    urlpatterns.append(path("api/", include("cvat.apps.events.urls")))

if apps.is_installed("cvat.apps.lambda_manager"):
    urlpatterns.append(path("", include("cvat.apps.lambda_manager.urls")))

if apps.is_installed("cvat.apps.webhooks"):
    urlpatterns.append(path("api/", include("cvat.apps.webhooks.urls")))

if apps.is_installed("cvat.apps.quality_control"):
    urlpatterns.append(path("api/", include("cvat.apps.quality_control.urls")))

if apps.is_installed("silk"):
    urlpatterns.append(path("profiler/", include("silk.urls")))

if apps.is_installed("health_check"):
    urlpatterns.append(path("api/server/health/", include("health_check.urls")))

if apps.is_installed("cvat.apps.consensus"):
    urlpatterns.append(path("api/", include("cvat.apps.consensus.urls")))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from cvat.utils.version import get_version

VERSION = (2, 35, 1, "alpha", 0)

__version__ = get_version(VERSION)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\profiler.py =====
from django.apps import apps

if apps.is_installed("silk"):
    from silk.profiling.profiler import silk_profile  # pylint: disable=unused-import
else:
    from functools import wraps

    def silk_profile(name=None):
        def profile(f):
            @wraps(f)
            def wrapped(*args, **kwargs):
                return f(*args, **kwargs)

            return wrapped

        return profile


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class ConsensusConfig(AppConfig):
    name = "cvat.apps.consensus"

    def ready(self) -> None:

        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)

        # Required to define signals in the application
        from . import signals  # pylint: disable=unused-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\intersect_merge.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import itertools
from abc import ABCMeta, abstractmethod
from collections.abc import Collection
from typing import ClassVar, Iterable, Sequence

import attrs
import datumaro as dm
import datumaro.components.merge.intersect_merge
from datumaro.components.errors import FailedLabelVotingError
from datumaro.util.annotation_util import mean_bbox
from datumaro.util.attrs_util import ensure_cls

from cvat.apps.quality_control.quality_reports import (
    ComparisonParameters,
    DistanceComparator,
    segment_iou,
)


@attrs.define(kw_only=True, slots=False)
class IntersectMerge(datumaro.components.merge.intersect_merge.IntersectMerge):
    @attrs.define(kw_only=True, slots=False)
    class Conf:
        pairwise_dist: float = 0.5
        sigma: float = 0.1

        output_conf_thresh: float = 0
        quorum: int = 1
        ignored_attributes: Collection[str] = attrs.field(factory=tuple)
        torso_r: float = 0.01

        groups: Collection[Collection[str]] = attrs.field(factory=tuple)
        close_distance: float = 0  # disabled

        included_annotation_types: Collection[dm.AnnotationType] = None

        def __attrs_post_init__(self):
            if self.included_annotation_types is None:
                self.included_annotation_types = ComparisonParameters.included_annotation_types

    conf: Conf = attrs.field(converter=ensure_cls(Conf), factory=Conf)

    def __call__(self, *datasets):
        return dm.Dataset(super().__call__(*datasets))

    def _find_cluster_attrs(self, cluster, ann):
        merged_attributes = super()._find_cluster_attrs(cluster, ann)
        merged_attributes["source"] = "consensus"
        return merged_attributes

    def _check_annotation_distance(self, t, merged_clusters):
        return  # disabled, need to clarify how to compare merged instances correctly

    def get_ann_dataset_id(self, ann_id: int) -> int:
        return self._dataset_map[self.get_ann_source(ann_id)][1]

    def get_ann_source_item(self, ann_id: int) -> dm.DatasetItem:
        return self._item_map[self._ann_map[ann_id][1]][0]

    def get_item_media_dims(self, ann_id: int) -> tuple[int, int]:
        return self.get_ann_source_item(ann_id).media_as(dm.Image).size

    def get_label_id(self, label: str) -> int:
        return self._get_label_id(label)

    def get_src_label_name(self, ann: dm.Annotation, label_id: int) -> str:
        return self._get_src_label_name(ann, label_id)

    def get_dataset_source_id(self, dataset_id: int) -> int:
        return self._dataset_map[dataset_id][1]

    def dataset_count(self) -> int:
        return len(self._dataset_map)

    def _make_mergers(self, sources):
        def _make(c, **kwargs):
            kwargs.update(attrs.asdict(self.conf))
            fields = attrs.fields_dict(c)
            return c(**{k: v for k, v in kwargs.items() if k in fields}, context=self)

        def _for_type(t: dm.AnnotationType, **kwargs) -> AnnotationMatcher:
            if t is dm.AnnotationType.label:
                return _make(LabelMerger, **kwargs)
            elif t is dm.AnnotationType.bbox:
                return _make(BboxMerger, **kwargs)
            elif t is dm.AnnotationType.mask:
                return _make(MaskMerger, **kwargs)
            elif t is dm.AnnotationType.polygon or t is dm.AnnotationType.mask:
                return _make(PolygonMerger, **kwargs)
            elif t is dm.AnnotationType.polyline:
                return _make(LineMerger, **kwargs)
            elif t is dm.AnnotationType.points:
                return _make(PointsMerger, **kwargs)
            elif t is dm.AnnotationType.skeleton:
                return _make(SkeletonMerger, **kwargs)
            else:
                raise AssertionError(f"Annotation type {t} is not supported")

        self._mergers = {
            t: _for_type(t, categories=self._categories)
            for t in self.conf.included_annotation_types
        }


@attrs.define(kw_only=True, slots=False)
class AnnotationMatcher(metaclass=ABCMeta):
    _context: IntersectMerge

    @abstractmethod
    def match_annotations(
        self, sources: Sequence[Sequence[dm.Annotation]]
    ) -> Sequence[Sequence[dm.Annotation]]:
        "Matches annotations from different sources and produces annotation groups"

    @abstractmethod
    def distance(self, a: dm.Annotation, b: dm.Annotation) -> float:
        """
        Computes distance (actually similarity) between 2 annotations.
        The output value is in the range [0; 1].

        distance(a, b) == 0 => annotations are different

        distance(a, b) == 1 => annotations are same
        """


@attrs.define(kw_only=True, slots=False)
class LabelMatcher(AnnotationMatcher):
    def distance(self, a: dm.Label, b: dm.Label) -> bool:
        a_label = self._context.get_any_label_name(a, a.label)
        b_label = self._context.get_any_label_name(b, b.label)
        return a_label == b_label

    def match_annotations(self, sources):
        return [list(itertools.chain.from_iterable(sources))]


CacheKey = tuple[int, int]


@attrs.define
class CachedSimilarityFunction:
    cache: dict[CacheKey, float] = attrs.field(factory=dict, kw_only=True)

    def __call__(self, a_ann: dm.Annotation, b_ann: dm.Annotation) -> float:
        a_ann_id = id(a_ann)
        b_ann_id = id(b_ann)

        if a_ann_id == b_ann_id:
            return 1

        key = (
            a_ann_id,
            b_ann_id,
        )  # make sure the annotations have stable ids before calling this
        key = self._sort_key(key)
        return self.cache[key]

    @staticmethod
    def _sort_key(key: CacheKey) -> CacheKey:
        return tuple(sorted(key))

    def pop(self, key: CacheKey) -> float:
        return self.cache.pop(self._sort_key(key), None)

    def set(self, key: CacheKey, value: float):
        self.cache[self._sort_key(key)] = value

    def keys(self) -> Iterable[CacheKey]:
        return self.cache.keys()

    def clear_cache(self):
        self.cache.clear()


@attrs.define(kw_only=True, slots=False)
class ShapeMatcher(AnnotationMatcher, metaclass=ABCMeta):
    pairwise_dist: float = 0.9
    cluster_dist: float | None = None
    categories: dm.CategoriesInfo
    _comparator: DistanceComparator = attrs.field(init=False)
    _distance: CachedSimilarityFunction = attrs.field(init=False)

    def __attrs_post_init__(self):
        if self.cluster_dist is None:
            self.cluster_dist = self.pairwise_dist

        self._comparator = DistanceComparator(
            categories=self.categories,
            return_distances=True,
            iou_threshold=self.pairwise_dist,
            oks_sigma=self._context.conf.sigma,
            line_torso_radius=self._context.conf.torso_r,
            panoptic_comparison=False,
            # allow_groups=True is not supported. Requires significant logic changes in
            # the whole merging algorithm, as it's likely to produce clusters with annotations
            # from the same source or some new annotations (instances).
            allow_groups=False,
        )

        self._distance = CachedSimilarityFunction()

    def distance(self, a, b):
        return self._distance(a, b)

    def _match_annotations_between_two_sources(
        self, source_a: list[dm.Annotation], source_b: list[dm.Annotation]
    ) -> list[tuple[dm.Annotation, dm.Annotation]]:
        if not source_a and not source_b:
            return []

        item_a = self._context.get_ann_source_item(id(source_a[0]))
        item_b = self._context.get_ann_source_item(id(source_b[0]))
        return self._match_annotations_between_two_items(item_a, item_b)

    def _match_annotations_between_two_items(
        self, item_a: dm.DatasetItem, item_b: dm.DatasetItem
    ) -> list[tuple[dm.Annotation, dm.Annotation]]:
        matches, distances = self.match_annotations_between_two_items(item_a, item_b)

        # Remember distances
        for (p_a_id, p_b_id), dist in distances.items():
            self._distance.set((p_a_id, p_b_id), dist)

        return matches

    @abstractmethod
    def match_annotations_between_two_items(
        self, item_a: dm.DatasetItem, item_b: dm.DatasetItem
    ) -> tuple[list[tuple[dm.Annotation, dm.Annotation]], dict[CacheKey, float]]: ...

    def match_annotations(self, sources):
        distance = self.distance
        cluster_dist = self.cluster_dist

        id_segm = {id(ann): (ann, id(source)) for source in sources for ann in source}

        def _is_close_enough(cluster, extra_id):
            # check if whole cluster IoU will not be broken
            # when this segment is added
            b = id_segm[extra_id][0]
            for a_id in cluster:
                a = id_segm[a_id][0]
                if distance(a, b) < cluster_dist:
                    return False
            return True

        def _has_same_source(cluster, extra_id):
            b = id_segm[extra_id][1]
            for a_id in cluster:
                a = id_segm[a_id][1]
                if a == b:
                    return True
            return False

        # match segments in sources, pairwise
        adjacent = {i: [] for i in id_segm}  # id(sgm) -> [id(adj_sgm1), ...]
        for a_idx, src_a in enumerate(sources):
            # matches further sources of same frame for matching annotations
            for src_b in sources[a_idx + 1 :]:
                # an annotation can be adjacent to multiple annotations
                matches = self._match_annotations_between_two_sources(src_a, src_b)
                for a, b in matches:
                    adjacent[id(a)].append(id(b))

        # join all segments into matching clusters
        clusters = []
        visited = set()
        for cluster_idx in adjacent:
            if cluster_idx in visited:
                continue

            cluster = set()
            to_visit = {cluster_idx}
            while to_visit:
                c = to_visit.pop()
                cluster.add(c)
                visited.add(c)

                for i in adjacent[c]:
                    if i in visited:
                        continue

                    if _has_same_source(cluster, i):
                        continue

                    if 0 < cluster_dist and not _is_close_enough(cluster, i):
                        # if positive, cluster_dist and this annotation isn't close enough
                        # with other annotations in the cluster
                        continue

                    to_visit.add(i)  # check annotations adjacent to the new one in the cluster

            clusters.append([id_segm[i][0] for i in cluster])

        return clusters


@attrs.define(kw_only=True, slots=False)
class BboxMatcher(ShapeMatcher):
    def match_annotations_between_two_items(self, item_a, item_b):
        matches, _, _, _, distances = self._comparator.match_boxes(item_a, item_b)
        return matches, distances


@attrs.define(kw_only=True, slots=False)
class PolygonMatcher(ShapeMatcher):
    _annotation_type: ClassVar[dm.AnnotationType] = dm.AnnotationType.polygon

    def match_annotations_between_two_items(self, item_a, item_b):
        item_a = item_a.wrap(
            annotations=[a for a in item_a.annotations if a.type == self._annotation_type]
        )
        item_b = item_b.wrap(
            annotations=[a for a in item_b.annotations if a.type == self._annotation_type]
        )
        matches, _, _, _, distances = self._comparator.match_segmentations(item_a, item_b)
        return matches, distances


@attrs.define(kw_only=True, slots=False)
class MaskMatcher(PolygonMatcher):
    _annotation_type: ClassVar[dm.AnnotationType] = dm.AnnotationType.mask


@attrs.define(kw_only=True, slots=False)
class PointsMatcher(ShapeMatcher):
    def match_annotations_between_two_items(self, item_a, item_b):
        matches, _, _, _, distances = self._comparator.match_points(item_a, item_b)
        return matches, distances


@attrs.define(kw_only=True, slots=False)
class SkeletonMatcher(ShapeMatcher):
    def match_annotations_between_two_items(self, item_a, item_b):
        matches, _, _, _, distances = self._comparator.match_skeletons(item_a, item_b)
        return matches, distances


@attrs.define(kw_only=True, slots=False)
class LineMatcher(ShapeMatcher):
    def match_annotations_between_two_items(self, item_a, item_b):
        matches, _, _, _, distances = self._comparator.match_lines(item_a, item_b)
        return matches, distances


@attrs.define(kw_only=True, slots=False)
class AnnotationMerger(AnnotationMatcher, metaclass=ABCMeta):
    @abstractmethod
    def merge_clusters(
        self, clusters: Sequence[Sequence[dm.Annotation]]
    ) -> Sequence[dm.Annotation]:
        "Merges annotations in each cluster into a single annotation"


@attrs.define(kw_only=True, slots=False)
class LabelMerger(AnnotationMerger, LabelMatcher):
    quorum: int = 0

    def merge_clusters(self, clusters):
        assert len(clusters) <= 1
        if len(clusters) == 0:
            return []

        votes = {}  # label -> score
        for ann in clusters[0]:
            label = self._context.get_src_label_name(ann, ann.label)
            votes[label] = 1 + votes.get(label, 0)

        merged = []
        for label, count in votes.items():
            if count < self.quorum:
                sources = set(
                    self._context.get_ann_source(id(a))
                    for a in clusters[0]
                    if label not in [self._context.get_src_label_name(l, l.label) for l in a]
                )
                sources = [self._context.get_dataset_source_id(s) for s in sources]
                self._context.add_item_error(FailedLabelVotingError, votes, sources=sources)
                continue

            merged.append(
                dm.Label(
                    self._context.get_label_id(label),
                    attributes={"score": count / self._context.dataset_count()},
                )
            )

        return merged


@attrs.define(kw_only=True, slots=False)
class ShapeMerger(AnnotationMerger, ShapeMatcher):
    quorum = attrs.field(converter=int, default=0)

    def merge_clusters(self, clusters):
        return list(filter(lambda x: x is not None, map(self.merge_cluster, clusters)))

    def find_cluster_label(self, cluster: Sequence[dm.Annotation]) -> tuple[int | None, float]:
        votes = {}
        for s in cluster:
            label = self._context.get_src_label_name(s, s.label)
            state = votes.setdefault(label, [0, 0])
            state[0] += s.attributes.get("score", 1.0)
            state[1] += 1

        label, (score, count) = max(votes.items(), key=lambda e: e[1][0])
        if count < self.quorum:
            self._context.add_item_error(FailedLabelVotingError, votes)
            label = None
        else:
            label = self._context.get_label_id(label)

        score = score / self._context.dataset_count()
        return label, score

    def _merge_cluster_shape_mean_box_nearest(
        self, cluster: Sequence[dm.Annotation]
    ) -> dm.Annotation:
        mbbox = dm.Bbox(*mean_bbox(cluster))
        img_h, img_w = self._context.get_item_media_dims(id(cluster[0]))

        dist = []
        for s in cluster:
            if isinstance(s, (dm.Points, dm.PolyLine)):
                s = self._comparator.to_polygon(dm.Bbox(*s.get_bbox()))
            elif isinstance(s, dm.Bbox):
                s = self._comparator.to_polygon(s)
            dist.append(
                segment_iou(self._comparator.to_polygon(mbbox), s, img_h=img_h, img_w=img_w)
            )
        nearest_pos, _ = max(enumerate(dist), key=lambda e: e[1])
        return cluster[nearest_pos]

    def merge_cluster_shape_mean_nearest(self, cluster: Sequence[dm.Annotation]) -> dm.Annotation:
        return self._merge_cluster_shape_mean_box_nearest(cluster)

    def merge_cluster_shape(self, cluster: Sequence[dm.Annotation]) -> tuple[dm.Annotation, float]:
        shape = self.merge_cluster_shape_mean_nearest(cluster)
        shape_score = sum(max(0, self.distance(shape, s)) for s in cluster) / len(cluster)
        return shape, shape_score

    def merge_cluster(self, cluster):
        label, label_score = self.find_cluster_label(cluster)

        # when the merged annotation is rejected due to quorum constraint
        if label is None:
            return None

        shape, shape_score = self.merge_cluster_shape(cluster)
        shape.z_order = max(cluster, key=lambda a: a.z_order).z_order
        shape.label = label
        shape.attributes["score"] = label_score * shape_score

        return shape


@attrs.define(kw_only=True, slots=False)
class BboxMerger(ShapeMerger, BboxMatcher):
    pass


@attrs.define(kw_only=True, slots=False)
class PolygonMerger(ShapeMerger, PolygonMatcher):
    pass


@attrs.define(kw_only=True, slots=False)
class MaskMerger(ShapeMerger, MaskMatcher):
    pass


@attrs.define(kw_only=True, slots=False)
class PointsMerger(ShapeMerger, PointsMatcher):
    pass


@attrs.define(kw_only=True, slots=False)
class LineMerger(ShapeMerger, LineMatcher):
    pass


@attrs.define(kw_only=True, slots=False)
class SkeletonMerger(ShapeMerger, SkeletonMatcher):
    def merge_cluster_shape_mean_nearest(self, cluster):
        dist = {}
        for idx, a in enumerate(cluster):
            a_cluster_distance = 0
            for b in cluster:
                # (1 - x) because it's actually a similarity function
                a_cluster_distance += 1 - self.distance(a, b)

            dist[idx] = a_cluster_distance / len(cluster)

        return cluster[min(dist, key=dist.get)]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\merging_manager.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
from typing import Type

import datumaro as dm
import django_rq
from django.conf import settings
from django.db import transaction
from django_rq.queues import DjangoRQ as RqQueue
from rq.job import Job as RqJob
from rq.job import JobStatus as RqJobStatus

from cvat.apps.consensus.intersect_merge import IntersectMerge
from cvat.apps.consensus.models import ConsensusSettings
from cvat.apps.dataset_manager.bindings import import_dm_annotations
from cvat.apps.dataset_manager.task import PatchAction, patch_job_data
from cvat.apps.engine.models import (
    DimensionType,
    Job,
    JobType,
    StageChoice,
    StateChoice,
    Task,
    User,
    clear_annotations_in_jobs,
)
from cvat.apps.engine.rq import BaseRQMeta, define_dependent_job
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import get_rq_lock_by_user
from cvat.apps.profiler import silk_profile
from cvat.apps.quality_control.quality_reports import ComparisonParameters, JobDataProvider


class _TaskMerger:
    _task: Task
    _jobs: dict[int, list[tuple[int, User]]]
    _parent_jobs: list[Job]
    _settings: ConsensusSettings

    def check_merging_available(self, *, parent_job_id: int | None = None):
        if not self._task.consensus_replicas:
            raise MergingNotAvailable("Consensus is not enabled in this task")

        if self._task.dimension != DimensionType.DIM_2D:
            raise MergingNotAvailable("Merging is only supported in 2d tasks")

        if self._jobs is None:
            self._init_jobs()

        if not self._jobs:
            raise MergingNotAvailable(
                f"No {JobType.ANNOTATION} jobs in the {StageChoice.ANNOTATION} stage or "
                f"no {JobType.CONSENSUS_REPLICA} jobs "
                f"not in the {StageChoice.ANNOTATION} - {StateChoice.NEW} state found"
            )

        if parent_job_id:
            parent_job_info = self._jobs.get(parent_job_id)
            if not parent_job_info:
                raise MergingNotAvailable(
                    f"No annotated consensus jobs found for parent job {parent_job_id}. "
                    f"Make sure at least one consensus job is not "
                    f"in the {StageChoice.ANNOTATION} - {StateChoice.NEW} state"
                )

    def __init__(self, task: int | Task) -> None:
        if not isinstance(task, Task):
            task = Task.objects.get(pk=task)
        self._task = task

        self._init_jobs()

        self._settings = ConsensusSettings.objects.get_or_create(task=task)[0]

    def _init_jobs(self) -> None:
        job_map = {}  # parent_job_id -> [(consensus_job_id, assignee)]
        parent_jobs: dict[int, Job] = {}
        for job in (
            Job.objects.prefetch_related("segment", "parent_job", "assignee")
            .filter(
                segment__task=self._task,
                type=JobType.CONSENSUS_REPLICA,
                parent_job__stage=StageChoice.ANNOTATION,
                parent_job__isnull=False,
            )
            .exclude(stage=StageChoice.ANNOTATION, state=StateChoice.NEW)
        ):
            job_map.setdefault(job.parent_job_id, []).append((job.id, job.assignee))
            parent_jobs.setdefault(job.parent_job_id, job.parent_job)

        self._jobs = job_map
        self._parent_jobs = list(parent_jobs.values())

    @staticmethod
    def _get_annotations(job_id: int) -> dm.Dataset:
        return JobDataProvider(job_id).dm_dataset

    def _merge_consensus_jobs(self, parent_job_id: int):
        self.check_merging_available(parent_job_id=parent_job_id)

        consensus_job_info = self._jobs[parent_job_id]

        consensus_job_ids = [consensus_job_id for consensus_job_id, _ in consensus_job_info]

        consensus_job_data_providers = list(map(JobDataProvider, consensus_job_ids))
        consensus_datasets = [
            consensus_job_data_provider.dm_dataset
            for consensus_job_data_provider in consensus_job_data_providers
        ]

        comparison_parameters = ComparisonParameters()
        merger = IntersectMerge(
            conf=IntersectMerge.Conf(
                pairwise_dist=self._settings.iou_threshold,
                quorum=math.ceil(self._settings.quorum * len(consensus_datasets)),
                sigma=comparison_parameters.oks_sigma,
                torso_r=comparison_parameters.line_thickness,
                included_annotation_types=comparison_parameters.included_annotation_types,
            )
        )
        merged_dataset = merger(*consensus_datasets)

        # Delete the existing annotations in the job.
        # If we don't delete existing annotations, the imported annotations
        # will be appended to the existing annotations, and thus updated annotation
        # would have both existing + imported annotations, but we only want the
        # imported annotations
        clear_annotations_in_jobs([parent_job_id])

        parent_job_data_provider = JobDataProvider(parent_job_id)

        # imports the annotations in the `parent_job.job_data` instance
        import_dm_annotations(merged_dataset, parent_job_data_provider.job_data)

        # updates the annotations in the job
        patch_job_data(
            parent_job_id, parent_job_data_provider.job_data.data.serialize(), PatchAction.UPDATE
        )

        for parent_job in self._parent_jobs:
            if parent_job.id == parent_job_id and parent_job.type == JobType.ANNOTATION.value:
                parent_job.state = StateChoice.COMPLETED.value
                parent_job.save()

    @transaction.atomic
    def merge_all_consensus_jobs(self) -> None:
        for parent_job_id in self._jobs.keys():
            self._merge_consensus_jobs(parent_job_id)

    @transaction.atomic
    def merge_single_consensus_job(self, parent_job_id: int) -> None:
        self._merge_consensus_jobs(parent_job_id)


class MergingNotAvailable(Exception):
    pass


class JobAlreadyExists(MergingNotAvailable):
    def __init__(self, instance: Task | Job):
        super().__init__()
        self.instance = instance

    def __str__(self):
        return f"Merging for this {type(self.instance).__name__.lower()} already enqueued"


class MergingManager:
    _QUEUE_CUSTOM_JOB_PREFIX = "consensus-merge-"
    _JOB_RESULT_TTL = 300

    def _get_queue(self) -> RqQueue:
        return django_rq.get_queue(settings.CVAT_QUEUES.CONSENSUS.value)

    def _make_job_id(self, task_id: int, job_id: int | None, user_id: int) -> str:
        key = f"{self._QUEUE_CUSTOM_JOB_PREFIX}task-{task_id}"
        if job_id:
            key += f"-job-{job_id}"
        key += f"-user-{user_id}"  # TODO: remove user id, add support for non owners to get status
        return key

    def _check_merging_available(self, task: Task, job: Job | None):
        _TaskMerger(task=task).check_merging_available(parent_job_id=job.id if job else None)

    def schedule_merge(self, target: Task | Job, *, request: ExtendedRequest) -> str:
        if isinstance(target, Job):
            target_task = target.segment.task
            target_job = target
        else:
            target_task = target
            target_job = None

        self._check_merging_available(target_task, target_job)

        queue = self._get_queue()

        user_id = request.user.id
        with get_rq_lock_by_user(queue, user_id=user_id):
            rq_id = self._make_job_id(
                task_id=target_task.id,
                job_id=target_job.id if target_job else None,
                user_id=user_id,
            )
            rq_job = queue.fetch_job(rq_id)
            if rq_job:
                if rq_job.get_status(refresh=False) in (
                    RqJobStatus.QUEUED,
                    RqJobStatus.STARTED,
                    RqJobStatus.SCHEDULED,
                    RqJobStatus.DEFERRED,
                ):
                    raise JobAlreadyExists(target)

                rq_job.delete()

            dependency = define_dependent_job(
                queue, user_id=user_id, rq_id=rq_id, should_be_dependent=True
            )

            queue.enqueue(
                self._merge,
                target_type=type(target),
                target_id=target.id,
                job_id=rq_id,
                meta=BaseRQMeta.build(request=request, db_obj=target),
                result_ttl=self._JOB_RESULT_TTL,
                failure_ttl=self._JOB_RESULT_TTL,
                depends_on=dependency,
            )

        return rq_id

    def get_job(self, rq_id: str) -> RqJob | None:
        queue = self._get_queue()
        return queue.fetch_job(rq_id)

    @classmethod
    @silk_profile()
    def _merge(cls, *, target_type: Type[Task | Job], target_id: int) -> int:
        if issubclass(target_type, Task):
            return _TaskMerger(task=target_id).merge_all_consensus_jobs()
        elif issubclass(target_type, Job):
            job = Job.objects.get(pk=target_id)
            return _TaskMerger(task=job.get_task_id()).merge_single_consensus_job(target_id)
        else:
            assert False


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\models.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from django.db import models

import cvat.apps.quality_control.quality_reports as qc
from cvat.apps.engine.models import Task


class ConsensusSettings(models.Model):
    task = models.ForeignKey(Task, on_delete=models.CASCADE, related_name="consensus_settings")
    quorum = models.FloatField(default=0.5)
    iou_threshold = models.FloatField(default=qc.DatasetComparator.DEFAULT_SETTINGS.iou_threshold)

    @property
    def organization_id(self):
        return self.task.organization_id


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\permissions.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import cast

from django.conf import settings
from rest_framework.exceptions import PermissionDenied, ValidationError

from cvat.apps.engine.models import Job, Project, Task
from cvat.apps.engine.permissions import TaskPermission
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum, get_iam_context

from .models import ConsensusSettings


class ConsensusMergePermission(OpenPolicyAgentPermission):
    rq_job_owner_id: int | None
    task_id: int | None

    class Scopes(StrEnum):
        CREATE = "create"
        VIEW_STATUS = "view:status"

    @classmethod
    def create_scope_check_status(
        cls, request: ExtendedRequest, rq_job_owner_id: int, iam_context=None
    ):
        if not iam_context and request:
            iam_context = get_iam_context(request, None)
        return cls(**iam_context, scope=cls.Scopes.VIEW_STATUS, rq_job_owner_id=rq_job_owner_id)

    @classmethod
    def create(cls, request, view, obj, iam_context):
        Scopes = __class__.Scopes

        permissions = []
        if view.basename == "consensus_merges":
            for scope in cls.get_scopes(request, view, obj):
                if scope == Scopes.CREATE:
                    # Note: POST /api/consensus/merges is used to initiate report creation
                    # and to check the operation status
                    rq_id = request.query_params.get("rq_id")
                    task_id = request.data.get("task_id")
                    job_id = request.data.get("job_id")

                    if not (task_id or job_id or rq_id):
                        raise PermissionDenied(
                            "Either task_id or job_id or rq_id must be specified"
                        )

                    if rq_id:
                        # There will be another check for this case during request processing
                        continue

                    # merge is always at least at the task level, even for specific jobs
                    if task_id is not None or job_id is not None:
                        if job_id:
                            try:
                                job = Job.objects.select_related("segment").get(id=job_id)
                            except Job.DoesNotExist:
                                raise ValidationError("The specified job does not exist")

                            task_id = job.get_task_id()

                        # The request may have a different org or org unset
                        # Here we need to retrieve iam_context for this user, based on the task_id
                        try:
                            task = Task.objects.get(id=task_id)
                        except Task.DoesNotExist:
                            raise ValidationError("The specified task does not exist")

                        iam_context = get_iam_context(request, task)

                    permissions.append(
                        cls.create_base_perm(
                            request,
                            view,
                            scope,
                            iam_context,
                            obj,
                            task_id=task_id,
                        )
                    )

                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        if "rq_job_owner_id" in kwargs:
            self.rq_job_owner_id = int(kwargs.pop("rq_job_owner_id"))

        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/consensus_merges/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "create": Scopes.CREATE,
            }[view.action]
        ]

    def get_resource(self):
        data = None

        if self.scope == self.Scopes.CREATE:
            task: Task | None = None
            project: Project | None = None

            if self.scope == self.Scopes.CREATE and self.task_id:
                try:
                    task = Task.objects.get(id=self.task_id)
                except Task.DoesNotExist:
                    raise ValidationError("The specified task does not exist")

            if task and task.project:
                project = task.project
                organization_id = task.project.organization_id
            else:
                organization_id = task.organization_id

            data = {
                "organization": {"id": organization_id},
                "task": (
                    {
                        "owner": {"id": task.owner_id},
                        "assignee": {"id": task.assignee_id},
                    }
                    if task
                    else None
                ),
                "project": (
                    {
                        "owner": {"id": project.owner_id},
                        "assignee": {"id": project.assignee_id},
                    }
                    if project
                    else None
                ),
            }
        elif self.scope == self.Scopes.VIEW_STATUS:
            data = {"owner": {"id": self.rq_job_owner_id}}

        return data


class ConsensusSettingPermission(OpenPolicyAgentPermission):
    obj: ConsensusSettings | None

    class Scopes(StrEnum):
        LIST = "list"
        VIEW = "view"
        UPDATE = "update"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        Scopes = __class__.Scopes

        permissions = []
        if view.basename == "consensus_settings":
            for scope in cls.get_scopes(request, view, obj):
                if scope in [Scopes.VIEW, Scopes.UPDATE]:
                    obj = cast(ConsensusSettings, obj)

                    if scope == Scopes.VIEW:
                        task_scope = TaskPermission.Scopes.VIEW
                    elif scope == Scopes.UPDATE:
                        task_scope = TaskPermission.Scopes.UPDATE_DESC
                    else:
                        assert False

                    # Access rights are the same as in the owning task
                    # This component doesn't define its own rules in this case
                    permissions.append(
                        TaskPermission.create_base_perm(
                            request,
                            view,
                            iam_context=iam_context,
                            scope=task_scope,
                            obj=obj.task,
                        )
                    )
                elif scope == cls.Scopes.LIST:
                    if task_id := request.query_params.get("task_id", None):
                        permissions.append(
                            TaskPermission.create_scope_view(
                                request,
                                int(task_id),
                                iam_context=iam_context,
                            )
                        )

                    permissions.append(cls.create_scope_list(request, iam_context))
                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/consensus_settings/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
                "retrieve": Scopes.VIEW,
                "partial_update": Scopes.UPDATE,
            }.get(view.action, None)
        ]

    def get_resource(self):
        data = None

        if self.obj:
            task = self.obj.task
            if task.project:
                organization_id = task.project.organization_id
            else:
                organization_id = task.organization_id

            data = {
                "id": self.obj.id,
                "organization": {"id": organization_id},
                "task": (
                    {
                        "owner": {"id": task.owner_id},
                        "assignee": {"id": task.assignee_id},
                    }
                    if task
                    else None
                ),
                "project": (
                    {
                        "owner": {"id": task.project.owner_id},
                        "assignee": {"id": task.project.assignee_id},
                    }
                    if task.project
                    else None
                ),
            }

        return data


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\serializers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap

from rest_framework import serializers

from cvat.apps.consensus import models
from cvat.apps.engine import field_validation


class ConsensusMergeCreateSerializer(serializers.Serializer):
    task_id = serializers.IntegerField(write_only=True, required=False)
    job_id = serializers.IntegerField(write_only=True, required=False)

    def validate(self, attrs):
        field_validation.require_one_of_fields(attrs, ["task_id", "job_id"])
        return super().validate(attrs)


class ConsensusSettingsSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.ConsensusSettings
        fields = (
            "id",
            "task_id",
            "iou_threshold",
            "quorum",
        )
        read_only_fields = (
            "id",
            "task_id",
        )

        extra_kwargs = {}

        for field_name, help_text in {
            "iou_threshold": "Pairwise annotation matching IoU threshold",
            "quorum": """
                Minimum required share of sources having an annotation for it to be accepted
            """,
        }.items():
            extra_kwargs.setdefault(field_name, {}).setdefault(
                "help_text", textwrap.dedent(help_text.lstrip("\n"))
            )

        for field_name in fields:
            if field_name.endswith("_threshold") or field_name == "quorum":
                extra_kwargs.setdefault(field_name, {}).setdefault("min_value", 0)
                extra_kwargs.setdefault(field_name, {}).setdefault("max_value", 1)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\signals.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.db.models.signals import post_save
from django.dispatch import receiver

from cvat.apps.consensus.models import ConsensusSettings
from cvat.apps.engine.models import Task


@receiver(
    post_save,
    sender=Task,
    dispatch_uid=__name__ + ".save_task-initialize_consensus_settings",
)
def __save_task__initialize_consensus_settings(instance: Task, created: bool, raw: bool, **kwargs):
    # Initializes default quality settings for the task
    # this is done in a signal to decouple this component from the engine app
    if created and instance.consensus_replicas and not raw:
        ConsensusSettings.objects.get_or_create(task=instance)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\urls.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.urls import include, path
from rest_framework import routers

from cvat.apps.consensus import views

router = routers.DefaultRouter(trailing_slash=False)
router.register("merges", views.ConsensusMergesViewSet, basename="consensus_merges")
router.register("settings", views.ConsensusSettingsViewSet, basename="consensus_settings")

urlpatterns = [
    # entry point for API
    path("consensus/", include(router.urls)),
]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\views.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap

from drf_spectacular.utils import (
    OpenApiParameter,
    OpenApiResponse,
    OpenApiTypes,
    extend_schema,
    extend_schema_view,
)
from rest_framework import mixins, status, viewsets
from rest_framework.exceptions import NotFound, ValidationError
from rest_framework.response import Response
from rq.job import JobStatus as RqJobStatus

from cvat.apps.consensus import merging_manager as merging
from cvat.apps.consensus.models import ConsensusSettings
from cvat.apps.consensus.permissions import ConsensusMergePermission, ConsensusSettingPermission
from cvat.apps.consensus.serializers import (
    ConsensusMergeCreateSerializer,
    ConsensusSettingsSerializer,
)
from cvat.apps.engine.mixins import PartialUpdateModelMixin
from cvat.apps.engine.models import Job, Task
from cvat.apps.engine.rq import BaseRQMeta
from cvat.apps.engine.serializers import RqIdSerializer
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import process_failed_job


@extend_schema(tags=["consensus"])
class ConsensusMergesViewSet(viewsets.GenericViewSet):
    CREATE_MERGE_RQ_ID_PARAMETER = "rq_id"

    @extend_schema(
        operation_id="consensus_create_merge",
        summary="Create a consensus merge",
        parameters=[
            OpenApiParameter(
                CREATE_MERGE_RQ_ID_PARAMETER,
                type=str,
                description=textwrap.dedent(
                    """\
                    The consensus merge request id. Can be specified to check operation status.
                """
                ),
            )
        ],
        request=ConsensusMergeCreateSerializer(required=False),
        responses={
            "201": None,
            "202": OpenApiResponse(
                RqIdSerializer,
                description=textwrap.dedent(
                    """\
                    A consensus merge request has been enqueued, the request id is returned.
                    The request status can be checked at this endpoint by passing the {}
                    as the query parameter. If the request id is specified, this response
                    means the consensus merge request is queued or is being processed.
                """.format(
                        CREATE_MERGE_RQ_ID_PARAMETER
                    )
                ),
            ),
            "400": OpenApiResponse(
                description="Invalid or failed request, check the response data for details"
            ),
        },
    )
    def create(self, request: ExtendedRequest, *args, **kwargs):
        rq_id = request.query_params.get(self.CREATE_MERGE_RQ_ID_PARAMETER, None)

        if rq_id is None:
            input_serializer = ConsensusMergeCreateSerializer(data=request.data)
            input_serializer.is_valid(raise_exception=True)

            task_id = input_serializer.validated_data.get("task_id", None)
            job_id = input_serializer.validated_data.get("job_id", None)
            if task_id:
                try:
                    instance = Task.objects.get(pk=task_id)
                except Task.DoesNotExist as ex:
                    raise NotFound(f"Task {task_id} does not exist") from ex
            elif job_id:
                try:
                    instance = Job.objects.select_related("segment").get(pk=job_id)
                except Job.DoesNotExist as ex:
                    raise NotFound(f"Jobs {job_id} do not exist") from ex

            try:
                manager = merging.MergingManager()
                rq_id = manager.schedule_merge(instance, request=request)
                serializer = RqIdSerializer({"rq_id": rq_id})
                return Response(serializer.data, status=status.HTTP_202_ACCEPTED)
            except merging.MergingNotAvailable as ex:
                raise ValidationError(str(ex))
        else:
            serializer = RqIdSerializer(data={"rq_id": rq_id})
            serializer.is_valid(raise_exception=True)
            rq_id = serializer.validated_data["rq_id"]

            manager = merging.MergingManager()
            rq_job = manager.get_job(rq_id)
            if (
                not rq_job
                or not ConsensusMergePermission.create_scope_check_status(
                    request, rq_job_owner_id=BaseRQMeta.for_job(rq_job).user.id
                )
                .check_access()
                .allow
            ):
                # We should not provide job existence information to unauthorized users
                raise NotFound("Unknown request id")

            rq_job_status = rq_job.get_status(refresh=False)
            if rq_job_status == RqJobStatus.FAILED:
                exc_info = process_failed_job(rq_job)

                exc_name_pattern = f"{merging.MergingNotAvailable.__name__}: "
                if (exc_pos := exc_info.find(exc_name_pattern)) != -1:
                    return Response(
                        data=exc_info[exc_pos + len(exc_name_pattern) :].strip(),
                        status=status.HTTP_400_BAD_REQUEST,
                    )

                return Response(data=str(exc_info), status=status.HTTP_500_INTERNAL_SERVER_ERROR)
            elif rq_job_status in (
                RqJobStatus.QUEUED,
                RqJobStatus.STARTED,
                RqJobStatus.SCHEDULED,
                RqJobStatus.DEFERRED,
            ):
                return Response(serializer.data, status=status.HTTP_202_ACCEPTED)
            elif rq_job_status == RqJobStatus.FINISHED:
                rq_job.delete()
                return Response(status=status.HTTP_201_CREATED)

            raise AssertionError(f"Unexpected rq job '{rq_id}' status '{rq_job_status}'")


@extend_schema(tags=["consensus"])
@extend_schema_view(
    list=extend_schema(
        summary="List consensus settings instances",
        responses={
            "200": ConsensusSettingsSerializer(many=True),
        },
    ),
    retrieve=extend_schema(
        summary="Get consensus settings instance details",
        parameters=[
            OpenApiParameter(
                "id",
                type=OpenApiTypes.INT,
                location="path",
                description="An id of a consensus settings instance",
            )
        ],
        responses={
            "200": ConsensusSettingsSerializer,
        },
    ),
    partial_update=extend_schema(
        summary="Update a consensus settings instance",
        parameters=[
            OpenApiParameter(
                "id",
                type=OpenApiTypes.INT,
                location="path",
                description="An id of a consensus settings instance",
            )
        ],
        request=ConsensusSettingsSerializer(partial=True),
        responses={
            "200": ConsensusSettingsSerializer,
        },
    ),
)
class ConsensusSettingsViewSet(
    viewsets.GenericViewSet,
    mixins.ListModelMixin,
    mixins.RetrieveModelMixin,
    PartialUpdateModelMixin,
):
    queryset = ConsensusSettings.objects

    iam_organization_field = "task__organization"

    search_fields = []
    filter_fields = ["id", "task_id"]
    simple_filters = ["task_id"]
    ordering_fields = ["id"]
    ordering = "id"

    serializer_class = ConsensusSettingsSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            permissions = ConsensusSettingPermission.create_scope_list(self.request)
            queryset = permissions.filter(queryset)

        return queryset


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\migrations\0001_initial.py =====
# Generated by Django 4.2.18 on 2025-02-10 11:29

import django.db.models.deletion
from django.db import migrations, models


def init_consensus_settings_in_existing_consensus_tasks(apps, schema_editor):
    Task = apps.get_model("engine", "Task")
    ConsensusSettings = apps.get_model("consensus", "ConsensusSettings")

    tasks_with_consensus = Task.objects.filter(
        segment__job__type="consensus_replica", consensus_settings__isnull=True
    ).distinct()
    ConsensusSettings.objects.bulk_create(
        [ConsensusSettings(task=t) for t in tasks_with_consensus],
        batch_size=10000,
    )


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ("engine", "0088_consensus_jobs"),
    ]

    operations = [
        migrations.CreateModel(
            name="ConsensusSettings",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("quorum", models.FloatField(default=0.5)),
                ("iou_threshold", models.FloatField(default=0.4)),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="consensus_settings",
                        to="engine.task",
                    ),
                ),
            ],
        ),
        migrations.RunPython(
            init_consensus_settings_in_existing_consensus_tasks,
            reverse_code=migrations.RunPython.noop,
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\consensus\migrations\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\annotation.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
from collections.abc import Container, Sequence
from copy import copy, deepcopy
from itertools import chain
from typing import Optional

import numpy as np
from scipy.optimize import linear_sum_assignment
from shapely import geometry

from cvat.apps.dataset_manager.util import faster_deepcopy
from cvat.apps.engine.models import DimensionType, ShapeType
from cvat.apps.engine.serializers import LabeledDataSerializer


class AnnotationIR:
    def __init__(self, dimension, data=None):
        self.reset()
        self.dimension = dimension
        if data:
            self.tags = getattr(data, 'tags', []) or data['tags']
            self.shapes = getattr(data, 'shapes', []) or data['shapes']
            self.tracks = getattr(data, 'tracks', []) or data['tracks']

    def add_tag(self, tag):
        self.tags.append(tag)

    def add_shape(self, shape):
        self.shapes.append(shape)

    def add_track(self, track):
        self.tracks.append(track)

    @property
    def data(self):
        return {
            'version': self.version,
            'tags': self.tags,
            'shapes': self.shapes,
            'tracks': self.tracks,
        }

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        return setattr(self, key, value)

    @data.setter
    def data(self, data):
        self.version = data['version']
        self.tags = data['tags']
        self.shapes = data['shapes']
        self.tracks = data['tracks']

    def serialize(self):
        serializer = LabeledDataSerializer(data=self.data)
        if serializer.is_valid(raise_exception=True):
            return serializer.data

    @staticmethod
    def _is_shape_inside(shape, start, stop):
        return start <= int(shape['frame']) <= stop

    @staticmethod
    def _is_track_inside(track, start, stop):
        def has_overlap(a, b):
            # a <= b
            return 0 <= min(b, stop) - max(a, start)

        elements = track.get('elements', [])
        if elements:
            # skeletons usually have only one shape defined on initial frame
            # anyway, for them we decided if they are inside range based on child elements
            return any(AnnotationIR._is_track_inside(t, start, stop) for t in elements)

        prev_shape = None
        for shape in track['shapes']:
            if shape['frame'] == start and shape['outside']:
                # corner case when the only shape on segment is outside frame on start frame
                prev_shape = shape
                continue

            if prev_shape and not prev_shape['outside'] and \
                    has_overlap(prev_shape['frame'], shape['frame']):
                return True

            prev_shape = shape

        if prev_shape is not None and not prev_shape['outside'] and prev_shape['frame'] <= stop:
            return True

        return False

    @classmethod
    def _slice_track(cls, track_, start, stop, dimension):
        def filter_track_shapes(shapes):
            shapes = [s for s in shapes if cls._is_shape_inside(s, start, stop)]

            # remove leading outside shapes as they are not necessary
            drop_count = 0
            for s in shapes:
                if s['outside']:
                    drop_count += 1
                else:
                    break

            return shapes[drop_count:]

        track = deepcopy(track_)
        segment_shapes = filter_track_shapes(deepcopy(track['shapes']))

        track["elements"] = [
            cls._slice_track(element, start, stop, dimension)
            for element in track.get('elements', [])
        ]

        if len(segment_shapes) < len(track['shapes']):
            interpolated_shapes = TrackManager.get_interpolated_shapes(
                track, start, stop + 1, dimension)
            scoped_shapes = filter_track_shapes(interpolated_shapes)

            if scoped_shapes:
                last_key = max(shape['frame'] for shape in track['shapes'])
                if not scoped_shapes[0]['keyframe']:
                    segment_shapes.insert(0, scoped_shapes[0])
                if last_key >= stop and scoped_shapes[-1]['points'] != segment_shapes[-1]['points']:
                    segment_shapes.append(scoped_shapes[-1])
                elif scoped_shapes[-1]['keyframe'] and \
                        scoped_shapes[-1]['outside'] and \
                        (len(segment_shapes) == 0 or scoped_shapes[-1]['frame'] > segment_shapes[-1]['frame']):
                    segment_shapes.append(scoped_shapes[-1])
                elif stop + 1 < len(interpolated_shapes) and \
                        interpolated_shapes[stop + 1]['outside']:
                    segment_shapes.append(interpolated_shapes[stop + 1])

            for shape in segment_shapes:
                shape.pop('keyframe', None)

        track['shapes'] = segment_shapes
        if 0 < len(segment_shapes):
            track['frame'] = track['shapes'][0]['frame']
        return track

    def slice(self, start, stop):
        # makes a data copy from specified frame interval
        splitted_data = AnnotationIR(self.dimension)
        splitted_data.tags = [deepcopy(t)
            for t in self.tags if self._is_shape_inside(t, start, stop)]
        splitted_data.shapes = [deepcopy(s)
            for s in self.shapes if self._is_shape_inside(s, start, stop)]
        splitted_tracks = []
        for t in self.tracks:
            if self._is_track_inside(t, start, stop):
                track = self._slice_track(t, start, stop, self.dimension)
                if 0 < len(track['shapes']):
                    splitted_tracks.append(track)
        splitted_data.tracks = splitted_tracks

        return splitted_data

    def reset(self):
        self.version = 0
        self.tags = []
        self.shapes = []
        self.tracks = []

class AnnotationManager:
    def __init__(self, data: AnnotationIR, *, dimension: DimensionType):
        self.data = data
        self.dimension = dimension

    def merge(self, data: AnnotationIR, start_frame: int, overlap: int):
        tags = TagManager(self.data.tags, dimension=self.dimension)
        tags.merge(data.tags, start_frame, overlap)

        shapes = ShapeManager(self.data.shapes, dimension=self.dimension)
        shapes.merge(data.shapes, start_frame, overlap)

        tracks = TrackManager(self.data.tracks, dimension=self.dimension)
        tracks.merge(data.tracks, start_frame, overlap)

    def clear_frames(self, frames: Container[int]):
        if not isinstance(frames, set):
            frames = set(frames)

        tags = TagManager(self.data.tags, dimension=self.dimension)
        tags.clear_frames(frames)

        shapes = ShapeManager(self.data.shapes, dimension=self.dimension)
        shapes.clear_frames(frames)

        if self.data.tracks:
            # Tracks are not expected in the cases this function is supposed to be used
            raise AssertionError("Partial annotation cleanup is not supported for tracks")

    def to_shapes(self,
        end_frame: int,
        *,
        deleted_frames: Sequence[int] | None = None,
        included_frames: Sequence[int] | None = None,
        include_outside: bool = False,
        use_server_track_ids: bool = False,
    ) -> list:
        shapes = self.data.shapes
        tracks = TrackManager(self.data.tracks, dimension=self.dimension)

        if included_frames is not None:
            shapes = [s for s in shapes if s["frame"] in included_frames]

        if deleted_frames is not None:
            shapes = [s for s in shapes if s["frame"] not in deleted_frames]

        return shapes + tracks.to_shapes(
            end_frame,
            included_frames=included_frames,
            deleted_frames=deleted_frames,
            include_outside=include_outside,
            use_server_track_ids=use_server_track_ids,
        )

    def to_tracks(self):
        tracks = self.data.tracks
        shapes = ShapeManager(self.data.shapes, dimension=self.dimension)

        return tracks + shapes.to_tracks()

class ObjectManager:
    def __init__(self, objects, *, dimension: DimensionType):
        self.objects = objects
        self.dimension = dimension

    @staticmethod
    def _get_objects_by_frame(objects, start_frame):
        objects_by_frame = {}
        for obj in objects:
            if obj["frame"] >= start_frame:
                if obj["frame"] in objects_by_frame:
                    objects_by_frame[obj["frame"]].append(obj)
                else:
                    objects_by_frame[obj["frame"]] = [obj]

        return objects_by_frame

    @staticmethod
    def _get_cost_threshold():
        raise NotImplementedError()

    @staticmethod
    def _calc_objects_similarity(obj0, obj1, start_frame, overlap, dimension):
        raise NotImplementedError()

    @staticmethod
    def _unite_objects(obj0, obj1):
        raise NotImplementedError()

    def _modify_unmatched_object(self, obj, end_frame):
        raise NotImplementedError()

    def merge(self, objects, start_frame, overlap):
        # 1. Split objects on two parts: new and which can be intersected
        # with existing objects.
        new_objects = [obj for obj in objects
            if obj["frame"] >= start_frame + overlap]
        int_objects = [obj for obj in objects
            if obj["frame"] < start_frame + overlap]
        assert len(new_objects) + len(int_objects) == len(objects)

        # 2. Convert to more convenient data structure (objects by frame)
        int_objects_by_frame = self._get_objects_by_frame(int_objects, start_frame)
        old_objects_by_frame = self._get_objects_by_frame(self.objects, start_frame)

        # 3. Add new objects as is. It should be done only after old_objects_by_frame
        # variable is initialized.
        self.objects.extend(new_objects)

        # Nothing to merge here. Just add all int_objects if any.
        if not old_objects_by_frame or not int_objects_by_frame:
            for frame in old_objects_by_frame:
                for old_obj in old_objects_by_frame[frame]:
                    self._modify_unmatched_object(old_obj, start_frame + overlap)
            self.objects.extend(int_objects)
            return

        # 4. Build cost matrix for each frame and find correspondence using
        # Hungarian algorithm. In this case min_cost_thresh is stronger
        # because we compare only on one frame.
        min_cost_thresh = self._get_cost_threshold()
        for frame in int_objects_by_frame:
            if frame in old_objects_by_frame:
                int_objects = int_objects_by_frame[frame]
                old_objects = old_objects_by_frame[frame]
                cost_matrix = np.empty(shape=(len(int_objects), len(old_objects)),
                    dtype=float)
                # 5.1 Construct cost matrix for the frame.
                for i, int_obj in enumerate(int_objects):
                    for j, old_obj in enumerate(old_objects):
                        cost_matrix[i][j] = 1 - self._calc_objects_similarity(
                            int_obj, old_obj, start_frame, overlap, self.dimension)

                # 6. Find optimal solution using Hungarian algorithm.
                row_ind, col_ind = linear_sum_assignment(cost_matrix)
                old_objects_indexes = list(range(0, len(old_objects)))
                int_objects_indexes = list(range(0, len(int_objects)))
                for i, j in zip(row_ind, col_ind):
                    # Reject the solution if the cost is too high. Remember
                    # inside int_objects_indexes objects which were handled.
                    if cost_matrix[i][j] <= min_cost_thresh:
                        old_objects[j] = self._unite_objects(int_objects[i], old_objects[j])
                        int_objects_indexes[i] = -1
                        old_objects_indexes[j] = -1

                # 7. Add all new objects which were not processed.
                for i in int_objects_indexes:
                    if i != -1:
                        self.objects.append(int_objects[i])

                # 8. Modify all old objects which were not processed
                # (e.g. generate a shape with outside=True at the end).
                for j in old_objects_indexes:
                    if j != -1:
                        self._modify_unmatched_object(old_objects[j],
                            start_frame + overlap)
            else:
                # We don't have old objects on the frame. Let's add all new ones.
                self.objects.extend(int_objects_by_frame[frame])

    def clear_frames(self, frames: Container[int]):
        new_objects = [obj for obj in self.objects if obj["frame"] not in frames]
        self.objects.clear()
        self.objects.extend(new_objects)

class TagManager(ObjectManager):
    @staticmethod
    def _get_cost_threshold():
        return 0.25

    @staticmethod
    def _calc_objects_similarity(obj0, obj1, start_frame, overlap, dimension):
        # TODO: improve the trivial implementation, compare attributes
        return 1 if obj0["label_id"] == obj1["label_id"] else 0

    @staticmethod
    def _unite_objects(obj0, obj1):
        # TODO: improve the trivial implementation
        return obj0 if obj0["frame"] < obj1["frame"] else obj1

    def _modify_unmatched_object(self, obj, end_frame):
        pass

def pairwise(iterable):
    a = iter(iterable)
    return zip(a, a)

class ShapeManager(ObjectManager):
    def to_tracks(self):
        tracks = []
        for shape in self.objects:
            shape0 = copy(shape)
            shape0["keyframe"] = True
            shape0["outside"] = False
            # TODO: Separate attributes on mutable and unmutable
            shape0["attributes"] = []
            shape0.pop("group", None)
            shape1 = copy(shape0)
            shape1["outside"] = True
            shape1["frame"] += 1

            track = {
                "label_id": shape["label_id"],
                "frame": shape["frame"],
                "group": shape.get("group", None),
                "attributes": shape["attributes"],
                "shapes": [shape0, shape1]
            }
            tracks.append(track)

        return tracks

    @staticmethod
    def _get_cost_threshold():
        return 0.25

    @staticmethod
    def _calc_objects_similarity(obj0, obj1, start_frame, overlap, dimension):
        def _calc_polygons_similarity(p0, p1):
            if p0.is_valid and p1.is_valid: # check validity of polygons
                overlap_area = p0.intersection(p1).area
                if p0.area == 0 or p1.area == 0: # a line with many points
                    return 0
                else:
                    return overlap_area / (p0.area + p1.area - overlap_area)
            else:
                return 0 # if there's invalid polygon, assume similarity is 0

        has_same_type = obj0["type"] == obj1["type"]
        has_same_label = obj0.get("label_id") == obj1.get("label_id")
        if has_same_type and has_same_label:
            if obj0["type"] == ShapeType.RECTANGLE:
                # FIXME: need to consider rotated boxes
                p0 = geometry.box(*obj0["points"])
                p1 = geometry.box(*obj1["points"])

                return _calc_polygons_similarity(p0, p1)
            elif obj0["type"] == ShapeType.CUBOID and dimension == DimensionType.DIM_3D:
                [x_c0, y_c0, z_c0] = obj0["points"][0:3]
                [x_c1, y_c1, z_c1] = obj1["points"][0:3]

                [x_len0, y_len0, z_len0] = obj0["points"][6:9]
                [x_len1, y_len1, z_len1] = obj1["points"][6:9]

                top_view_0 = [
                    x_c0 - x_len0 / 2,
                    y_c0 - y_len0 / 2,
                    x_c0 + x_len0 / 2,
                    y_c0 + y_len0 / 2
                ]

                top_view_1 = [
                    x_c1 - x_len1 / 2,
                    y_c1 - y_len1 / 2,
                    x_c1 + x_len1 / 2,
                    y_c1 + y_len1 / 2
                ]

                p_top0 = geometry.box(*top_view_0)
                p_top1 = geometry.box(*top_view_1)
                top_similarity = _calc_polygons_similarity(p_top0, p_top1)

                side_view_0 = [
                    x_c0 - x_len0 / 2,
                    z_c0 - z_len0 / 2,
                    x_c0 + x_len0 / 2,
                    z_c0 + z_len0 / 2
                ]

                side_view_1 = [
                    x_c1 - x_len1 / 2,
                    z_c1 - z_len1 / 2,
                    x_c1 + x_len1 / 2,
                    z_c1 + z_len1 / 2
                ]
                p_side0 = geometry.box(*side_view_0)
                p_side1 = geometry.box(*side_view_1)
                side_similarity = _calc_polygons_similarity(p_side0, p_side1)

                return top_similarity * side_similarity
            elif obj0["type"] == ShapeType.POLYGON:
                p0 = geometry.Polygon(pairwise(obj0["points"]))
                p1 = geometry.Polygon(pairwise(obj1["points"]))

                return _calc_polygons_similarity(p0, p1)
            else:
                return 0 # FIXME: need some similarity for points, polylines, ellipses and 2D cuboids
        return 0

    @staticmethod
    def _unite_objects(obj0, obj1):
        # TODO: improve the trivial implementation
        return obj0 if obj0["frame"] < obj1["frame"] else obj1

    def _modify_unmatched_object(self, obj, end_frame):
        pass


class TrackManager(ObjectManager):
    def to_shapes(
        self,
        end_frame: int,
        *,
        included_frames: Sequence[int] | None = None,
        deleted_frames: Sequence[int] | None = None,
        include_outside: bool = False,
        use_server_track_ids: bool = False,
    ) -> list:
        shapes = []
        for idx, track in enumerate(self.objects):
            track_id = track["id"] if use_server_track_ids else idx
            track_shapes = {}

            for shape in TrackManager.get_interpolated_shapes(
                track,
                0,
                end_frame,
                self.dimension,
                include_outside=include_outside,
                included_frames=included_frames,
                deleted_frames=deleted_frames,
            ):
                shape["label_id"] = track["label_id"]
                shape["group"] = track["group"]
                shape["track_id"] = track_id
                shape["source"] = track["source"]
                shape["attributes"] += track["attributes"]
                shape["elements"] = []

                track_shapes[shape["frame"]] = shape

            if not track_shapes:
                # This track has no elements on the included frames
                continue

            if track.get("elements"):
                track_elements = TrackManager(track["elements"], dimension=self.dimension)
                element_included_frames = set(track_shapes.keys())
                if included_frames is not None:
                    element_included_frames = element_included_frames.intersection(included_frames)
                element_shapes = track_elements.to_shapes(
                    end_frame,
                    included_frames=element_included_frames,
                    deleted_frames=deleted_frames,
                    include_outside=True, # elements are controlled by the parent shape
                    use_server_track_ids=use_server_track_ids,
                )

                for shape in element_shapes:
                    track_shapes[shape["frame"]]["elements"].append(shape)

                # The whole shape can be filtered out, if all its elements are outside,
                # and outside shapes are not requested.
                if not include_outside:
                    track_shapes = {
                        frame_number: shape for frame_number, shape in track_shapes.items()
                        if not shape["elements"]
                        or not all(elem["outside"] for elem in shape["elements"])
                    }

            shapes.extend(track_shapes.values())
        return shapes

    @staticmethod
    def _get_objects_by_frame(objects, start_frame):
        # Just for unification. All tracks are assigned on the same frame
        objects_by_frame = {0: []}
        for obj in objects:
            if not obj["shapes"]:
                continue

            shape = obj["shapes"][-1] # optimization for old tracks
            if shape["frame"] >= start_frame or not shape["outside"]:
                objects_by_frame[0].append(obj)

        if not objects_by_frame[0]:
            objects_by_frame = {}

        return objects_by_frame

    @staticmethod
    def _get_cost_threshold():
        return 0.5

    @staticmethod
    def _calc_objects_similarity(obj0, obj1, start_frame, overlap, dimension):
        if obj0["label_id"] == obj1["label_id"]:
            # Here start_frame is the start frame of next segment
            # and stop_frame is the stop frame of current segment
            # end_frame == stop_frame + 1
            end_frame = start_frame + overlap
            obj0_shapes = TrackManager.get_interpolated_shapes(obj0, start_frame, end_frame, dimension)
            obj1_shapes = TrackManager.get_interpolated_shapes(obj1, start_frame, end_frame, dimension)
            if not obj0_shapes or not obj1_shapes:
                return 0

            obj0_shapes_by_frame = {shape["frame"]:shape for shape in obj0_shapes}
            obj1_shapes_by_frame = {shape["frame"]:shape for shape in obj1_shapes}

            count, error = 0, 0
            for frame in range(start_frame, end_frame):
                shape0 = obj0_shapes_by_frame.get(frame)
                shape1 = obj1_shapes_by_frame.get(frame)
                if shape0 and shape1:
                    if shape0["outside"] != shape1["outside"]:
                        error += 1
                    else:
                        error += 1 - ShapeManager._calc_objects_similarity(shape0, shape1, start_frame, overlap, dimension)
                    count += 1
                elif shape0 or shape1:
                    error += 1
                    count += 1

            return 1 - error / (count or 1)
        else:
            return 0

    def _modify_unmatched_object(self, obj, end_frame):
        if not obj["shapes"]:
            return
        shape = obj["shapes"][-1]
        if not shape["outside"]:
            shape = deepcopy(shape)
            shape["frame"] = end_frame
            shape["outside"] = True
            obj["shapes"].append(shape)

            for element in obj.get("elements", []):
                self._modify_unmatched_object(element, end_frame)

    @staticmethod
    def get_interpolated_shapes(
        track: dict,
        start_frame: int,
        end_frame: int,
        dimension: DimensionType | str,
        *,
        included_frames: Optional[Sequence[int]] = None,
        deleted_frames: Optional[Sequence[int]] = None,
        include_outside: bool = False,
    ):
        # If a task or job contains deleted frames that contain track keyframes,
        # these keyframes should be excluded from the interpolation.
        # In jobs having specific frames included (e.g. GT jobs),
        # deleted frames should not be confused with included frames during track interpolation.
        # Deleted frames affect existing shapes in tracks.
        # Included frames filter the resulting annotations after interpolation
        # to produce the requested track frames.
        deleted_frames = deleted_frames or []

        def copy_shape(source, frame, points=None, rotation=None):
            copied = source.copy()
            copied["attributes"] = faster_deepcopy(source["attributes"])

            copied["keyframe"] = False
            copied["frame"] = frame
            if rotation is not None:
                copied["rotation"] = rotation

            if points is None:
                points = copied["points"]

            if isinstance(points, np.ndarray):
                points = points.tolist()
            else:
                points = points.copy()

            if points is not None:
                copied["points"] = points

            return copied

        def find_angle_diff(right_angle, left_angle):
            angle_diff = right_angle - left_angle
            angle_diff = ((angle_diff + 180) % 360) - 180
            if abs(angle_diff) >= 180:
                # if the main arc is bigger than 180, go another arc
                # to find it, just subtract absolute value from 360 and inverse sign
                angle_diff = 360 - abs(angle_diff) * -1 if angle_diff > 0 else 1

            return angle_diff

        def simple_interpolation(shape0, shape1):
            shapes = []
            distance = shape1["frame"] - shape0["frame"]
            diff = np.subtract(shape1["points"], shape0["points"])

            for frame in range(shape0["frame"] + 1, shape1["frame"]):
                offset = (frame - shape0["frame"]) / distance
                rotation = (shape0["rotation"] + find_angle_diff(
                    shape1["rotation"], shape0["rotation"],
                ) * offset + 360) % 360
                points = shape0["points"] + diff * offset

                if included_frames is None or frame in included_frames:
                    shapes.append(copy_shape(shape0, frame, points, rotation))

            return shapes

        def simple_3d_interpolation(shape0, shape1):
            result = simple_interpolation(shape0, shape1)
            angles = (shape0["points"][3:6] + shape1["points"][3:6])
            distance = shape1["frame"] - shape0["frame"]

            for shape in result:
                offset = (shape["frame"] - shape0["frame"]) / distance
                for i, angle0 in enumerate(angles):
                    if i < 3:
                        angle1 = angles[i + 3]
                        angle0 = (angle0 if angle0 >= 0 else angle0 + math.pi * 2) * 180 / math.pi
                        angle1 = (angle1 if angle1 >= 0 else angle1 + math.pi * 2) * 180 / math.pi
                        angle = angle0 + find_angle_diff(angle1, angle0) * offset * math.pi / 180
                        shape["points"][i + 3] = angle if angle <= math.pi else angle - math.pi * 2

            return result

        def points_interpolation(shape0, shape1):
            if len(shape0["points"]) == 2 and len(shape1["points"]) == 2:
                return simple_interpolation(shape0, shape1)
            else:
                shapes = []
                for frame in range(shape0["frame"] + 1, shape1["frame"]):
                    if included_frames is None or frame in included_frames:
                        shapes.append(copy_shape(shape0, frame))

            return shapes

        def interpolate_position(left_position, right_position, offset):
            def to_array(points):
                return np.asarray(
                    list(map(lambda point: [point["x"], point["y"]], points))
                ).flatten()

            def to_points(array):
                return list(map(
                    lambda point: {"x": point[0], "y": point[1]}, np.asarray(array).reshape(-1, 2)
                ))

            def curve_length(points):
                length = 0
                for i in range(1, len(points)):
                    dx = points[i]["x"] - points[i - 1]["x"]
                    dy = points[i]["y"] - points[i - 1]["y"]
                    length += np.sqrt(dx ** 2 + dy ** 2)
                return length

            def curve_to_offset_vec(points, length):
                offset_vector = [0]
                accumulated_length = 0
                for i in range(1, len(points)):
                    dx = points[i]["x"] - points[i - 1]["x"]
                    dy = points[i]["y"] - points[i - 1]["y"]
                    accumulated_length += np.sqrt(dx ** 2 + dy ** 2)
                    offset_vector.append(accumulated_length / length)

                return offset_vector

            def find_nearest_pair(value, curve):
                minimum = [0, abs(value - curve[0])]
                for i in range(1, len(curve)):
                    distance = abs(value - curve[i])
                    if distance < minimum[1]:
                        minimum = [i, distance]

                return minimum[0]

            def match_left_right(left_curve, right_curve):
                matching = {}
                for i, left_curve_item in enumerate(left_curve):
                    matching[i] = [find_nearest_pair(left_curve_item, right_curve)]
                return matching

            def match_right_left(left_curve, right_curve, left_right_matching):
                matched_right_points = list(chain.from_iterable(left_right_matching.values()))
                unmatched_right_points = filter(lambda x: x not in matched_right_points, range(len(right_curve)))
                updated_matching = faster_deepcopy(left_right_matching)

                for right_point in unmatched_right_points:
                    left_point = find_nearest_pair(right_curve[right_point], left_curve)
                    updated_matching[left_point].append(right_point)

                for key, value in updated_matching.items():
                    updated_matching[key] = sorted(value)

                return updated_matching

            def reduce_interpolation(interpolated_points, matching, left_points, right_points):
                def average_point(points):
                    sumX = 0
                    sumY = 0
                    for point in points:
                        sumX += point["x"]
                        sumY += point["y"]

                    return {
                        "x": sumX / len(points),
                        "y": sumY / len(points)
                    }

                def compute_distance(point1, point2):
                    return np.sqrt(
                        ((point1["x"] - point2["x"])) ** 2
                        + ((point1["y"] - point2["y"]) ** 2)
                    )

                def minimize_segment(base_length, N, start_interpolated, stop_interpolated):
                    threshold = base_length / (2 * N)
                    minimized = [interpolated_points[start_interpolated]]
                    latest_pushed = start_interpolated
                    for i in range(start_interpolated + 1, stop_interpolated):
                        distance = compute_distance(
                            interpolated_points[latest_pushed], interpolated_points[i]
                        )

                        if distance >= threshold:
                            minimized.append(interpolated_points[i])
                            latest_pushed = i

                    minimized.append(interpolated_points[stop_interpolated])

                    if len(minimized) == 2:
                        distance = compute_distance(
                            interpolated_points[start_interpolated],
                            interpolated_points[stop_interpolated]
                        )

                        if distance < threshold:
                            return [average_point(minimized)]

                    return minimized

                reduced = []
                interpolated_indexes = {}
                accumulated = 0
                for i in range(len(left_points)):
                    interpolated_indexes[i] = []
                    for _ in range(len(matching[i])):
                        interpolated_indexes[i].append(accumulated)
                        accumulated += 1

                def left_segment(start, stop):
                    start_interpolated = interpolated_indexes[start][0]
                    stop_interpolated = interpolated_indexes[stop][0]

                    if start_interpolated == stop_interpolated:
                        reduced.append(interpolated_points[start_interpolated])
                        return

                    base_length = curve_length(left_points[start: stop + 1])
                    N = stop - start + 1

                    reduced.extend(
                        minimize_segment(base_length, N, start_interpolated, stop_interpolated)
                    )


                def right_segment(left_point):
                    start = matching[left_point][0]
                    stop = matching[left_point][-1]
                    start_interpolated = interpolated_indexes[left_point][0]
                    stop_interpolated = interpolated_indexes[left_point][-1]
                    base_length = curve_length(right_points[start: stop + 1])
                    N = stop - start + 1

                    reduced.extend(
                        minimize_segment(base_length, N, start_interpolated, stop_interpolated)
                    )

                previous_opened = None
                for i in range(len(left_points)):
                    if len(matching[i]) == 1:
                        if previous_opened is not None:
                            if matching[i][0] == matching[previous_opened][0]:
                                continue
                            else:
                                start = previous_opened
                                stop = i - 1
                                left_segment(start, stop)
                                previous_opened = i
                        else:
                            previous_opened = i
                    else:
                        if previous_opened is not None:
                            start = previous_opened
                            stop = i - 1
                            left_segment(start, stop)
                            previous_opened = None

                        right_segment(i)

                if previous_opened is not None:
                    left_segment(previous_opened, len(left_points) - 1)

                return reduced

            left_points = to_points(left_position["points"])
            right_points = to_points(right_position["points"])
            left_offset_vec = curve_to_offset_vec(left_points, curve_length(left_points))
            right_offset_vec = curve_to_offset_vec(right_points, curve_length(right_points))

            matching = match_left_right(left_offset_vec, right_offset_vec)
            completed_matching = match_right_left(
                left_offset_vec, right_offset_vec, matching
            )

            interpolated_points = []
            for left_point_index, left_point in enumerate(left_points):
                for right_point_index in completed_matching[left_point_index]:
                    right_point = right_points[right_point_index]
                    interpolated_points.append({
                        "x": left_point["x"] + (right_point["x"] - left_point["x"]) * offset,
                        "y": left_point["y"] + (right_point["y"] - left_point["y"]) * offset
                    })

            reducedPoints = reduce_interpolation(
                interpolated_points,
                completed_matching,
                left_points,
                right_points
            )

            return to_array(reducedPoints).tolist()

        def polyshape_interpolation(shape0, shape1):
            shapes = []
            is_polygon = shape0["type"] == ShapeType.POLYGON
            if is_polygon:
                # Make the polygon closed for computations
                shape0 = shape0.copy()
                shape1 = shape1.copy()
                shape0["points"] = shape0["points"] + shape0["points"][:2]
                shape1["points"] = shape1["points"] + shape1["points"][:2]

            distance = shape1["frame"] - shape0["frame"]
            for frame in range(shape0["frame"] + 1, shape1["frame"]):
                offset = (frame - shape0["frame"]) / distance
                points = interpolate_position(shape0, shape1, offset)

                if included_frames is None or frame in included_frames:
                    shapes.append(copy_shape(shape0, frame, points))

            if is_polygon:
                # Remove the extra point added
                shape0["points"] = shape0["points"][:-2]
                shape1["points"] = shape1["points"][:-2]
                for shape in shapes:
                    shape["points"] = shape["points"][:-2]

            return shapes

        def interpolate(shape0, shape1):
            is_same_type = shape0["type"] == shape1["type"]
            is_rectangle = shape0["type"] == ShapeType.RECTANGLE
            is_ellipse = shape0["type"] == ShapeType.ELLIPSE
            is_cuboid = shape0["type"] == ShapeType.CUBOID
            is_polygon = shape0["type"] == ShapeType.POLYGON
            is_polyline = shape0["type"] == ShapeType.POLYLINE
            is_points = shape0["type"] == ShapeType.POINTS
            is_skeleton = shape0["type"] == ShapeType.SKELETON

            if not is_same_type:
                raise NotImplementedError()

            shapes = []
            if dimension == DimensionType.DIM_3D:
                shapes = simple_3d_interpolation(shape0, shape1)
            if is_rectangle or is_cuboid or is_ellipse or is_skeleton:
                shapes = simple_interpolation(shape0, shape1)
            elif is_points:
                shapes = points_interpolation(shape0, shape1)
            elif is_polygon or is_polyline:
                shapes = polyshape_interpolation(shape0, shape1)
            else:
                raise NotImplementedError()

            return shapes

        def propagate(shape, end_frame, *, included_frames=None):
            return [
                copy_shape(shape, i)
                for i in range(shape["frame"] + 1, end_frame)
                if included_frames is None or i in included_frames
            ]

        shapes = []
        prev_shape = None
        for shape in sorted(track["shapes"], key=lambda shape: shape["frame"]):
            curr_frame = shape["frame"]
            if curr_frame in deleted_frames:
                continue
            if prev_shape and end_frame <= curr_frame:
                # If we exceed the end_frame and there was a previous shape,
                # we still need to interpolate up to the next keyframe,
                # but keep the results only up to the end_frame:
                #        vvvvvvv
                # ---- | ------- | ----- | ----->
                #     prev      end   cur kf
                interpolated = interpolate(prev_shape, shape)
                interpolated.append(shape)

                for shape in sorted(interpolated, key=lambda shape: shape["frame"]):
                    if shape["frame"] < end_frame:
                        shapes.append(shape)
                    else:
                        break

                # Update the last added shape
                shape["keyframe"] = True
                prev_shape = shape

                break # The track finishes here

            if prev_shape:
                if (
                    curr_frame == prev_shape["frame"]
                    and dict(shape, id=None, keyframe=None) == dict(prev_shape, id=None, keyframe=None)
                ):
                    continue
                assert curr_frame > prev_shape["frame"], f"{curr_frame} > {prev_shape['frame']}. Track id: {track['id']}" # Catch invalid tracks

                # Propagate attributes
                for attr in prev_shape["attributes"]:
                    if attr["spec_id"] not in map(lambda el: el["spec_id"], shape["attributes"]):
                        shape["attributes"].append(faster_deepcopy(attr))

                if not prev_shape["outside"] or include_outside:
                    shapes.extend(interpolate(prev_shape, shape))

            shape["keyframe"] = True
            shapes.append(shape)
            prev_shape = shape

        if prev_shape and (not prev_shape["outside"] or include_outside):
            # When the latest keyframe of a track is less than the end_frame
            # and it is not outside, need to propagate
            shapes.extend(propagate(prev_shape, end_frame, included_frames=included_frames))

        shapes = [
            shape for shape in shapes

            if shape["frame"] not in deleted_frames

            # After interpolation there can be a finishing frame
            # outside of the task boundaries. Filter it out to avoid errors.
            # https://github.com/openvinotoolkit/cvat/issues/2827
            if track["frame"] <= shape["frame"] < end_frame

            # Exclude outside shapes.
            # Keyframes should be included regardless the outside value
            # If really needed, they can be excluded on the later stages,
            # but here they represent a finishing shape in a visible sequence
            if shape["keyframe"] or not shape["outside"] or include_outside

            if included_frames is None or shape["frame"] in included_frames
        ]

        return shapes

    @staticmethod
    def _unite_objects(obj0, obj1):
        track = obj0 if obj0["frame"] < obj1["frame"] else obj1
        assert obj0["label_id"] == obj1["label_id"]
        shapes = {shape["frame"]: shape for shape in obj0["shapes"]}
        for shape in obj1["shapes"]:
            frame = shape["frame"]
            if frame in shapes:
                shapes[frame] = ShapeManager._unite_objects(shapes[frame], shape)
            else:
                shapes[frame] = shape

        track["frame"] = min(obj0["frame"], obj1["frame"])
        track["shapes"] = list(sorted(shapes.values(), key=lambda shape: shape["frame"]))

        return track

    def clear_frames(self, frames: Container[int]):
        raise AssertionError("This function is not supported for tracks")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class DatasetManagerConfig(AppConfig):
    name = "cvat.apps.dataset_manager"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\bindings.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import os.path as osp
import re
import sys
from collections import OrderedDict, defaultdict
from collections.abc import Iterable, Iterator, Mapping, Sequence
from functools import partial, reduce
from operator import add
from pathlib import Path
from types import SimpleNamespace
from typing import Any, Callable, Dict, Literal, NamedTuple, Optional, Union

import attr
import datumaro as dm
import datumaro.util
import defusedxml.ElementTree as ET
import rq
from attr import attrib, attrs
from attrs.converters import to_bool
from datumaro.components.dataset_base import IDataset, StreamingDatasetBase, StreamingSubsetBase
from datumaro.components.format_detection import RejectionReason
from django.conf import settings
from django.db.models import Prefetch, QuerySet
from django.utils import timezone

from cvat.apps.dataset_manager.formats.utils import get_label_color
from cvat.apps.engine import models
from cvat.apps.engine.frame_provider import FrameOutputType, FrameQuality, TaskFrameProvider
from cvat.apps.engine.lazy_list import LazyList
from cvat.apps.engine.model_utils import add_prefetch_fields
from cvat.apps.engine.models import (
    AttributeSpec,
    AttributeType,
    DimensionType,
    Job,
    JobType,
    Label,
    LabelType,
    Project,
    SegmentType,
    ShapeType,
    Task,
)
from cvat.apps.engine.rq import ImportRQMeta

from ..engine.log import ServerLogManager
from .annotation import AnnotationIR, AnnotationManager, TrackManager
from .formats.transformations import EllipsesToMasks, MaskConverter

slogger = ServerLogManager(__name__)

CVAT_INTERNAL_ATTRIBUTES = {'occluded', 'outside', 'keyframe', 'track_id', 'rotation'}

class InstanceLabelData:
    class Attribute(NamedTuple):
        name: str
        value: Any

    @classmethod
    def add_prefetch_info(cls, queryset: QuerySet[Label]) -> QuerySet[Label]:
        assert issubclass(queryset.model, Label)

        return add_prefetch_fields(queryset, [
            'skeleton',
            'parent',
            'attributespec_set',
            'sublabels',
        ])

    def __init__(self, instance: Union[Task, Project]) -> None:
        instance = instance.project if isinstance(instance, Task) and instance.project_id is not None else instance

        db_labels = self.add_prefetch_info(instance.label_set.all())

        # If this flag is set to true, create attribute within anntations import
        self._soft_attribute_import = False
        self._label_mapping = OrderedDict[int, Label](
            (db_label.id, db_label)
            for db_label in sorted(db_labels, key=lambda v: v.pk)
        )

        self._attribute_mapping = {db_label.id: {
            'mutable': {}, 'immutable': {}, 'spec': {}}
            for db_label in db_labels}

        for db_label in db_labels:
            for db_attribute in db_label.attributespec_set.all():
                if db_attribute.mutable:
                    self._attribute_mapping[db_label.id]['mutable'][db_attribute.id] = db_attribute.name
                else:
                    self._attribute_mapping[db_label.id]['immutable'][db_attribute.id] = db_attribute.name
                self._attribute_mapping[db_label.id]['spec'][db_attribute.id] = db_attribute

        self._attribute_mapping_merged = {}
        for label_id, attr_mapping in self._attribute_mapping.items():
            self._attribute_mapping_merged[label_id] = {
                **attr_mapping['mutable'],
                **attr_mapping['immutable'],
            }

    def _get_label_id(self, label_name, parent_id=None):
        for db_label in self._label_mapping.values():
            if label_name == db_label.name and parent_id == db_label.parent_id:
                return db_label.id
        raise ValueError("Label {!r} is not registered for this task".format(label_name))

    def _get_label_name(self, label_id):
        return self._label_mapping[label_id].name

    def _get_attribute_name(self, attribute_id):
        for attribute_mapping in self._attribute_mapping_merged.values():
            if attribute_id in attribute_mapping:
                return attribute_mapping[attribute_id]

    def _get_attribute_id(self, label_id, attribute_name, attribute_type=None):
        if attribute_type:
            container = self._attribute_mapping[label_id][attribute_type]
        else:
            container = self._attribute_mapping_merged[label_id]

        for attr_id, attr_name in container.items():
            if attribute_name == attr_name:
                return attr_id
        return None

    def _get_mutable_attribute_id(self, label_id, attribute_name):
        return self._get_attribute_id(label_id, attribute_name, 'mutable')

    def _get_immutable_attribute_id(self, label_id, attribute_name):
        return self._get_attribute_id(label_id, attribute_name, 'immutable')

    def _import_attribute(self, label_id, attribute, mutable=False):
        spec_id = self._get_attribute_id(label_id, attribute.name)
        value = attribute.value

        if spec_id:
            spec = self._attribute_mapping[label_id]['spec'][spec_id]

            try:
                if spec.input_type == AttributeType.NUMBER:
                    pass # no extra processing required
                elif spec.input_type == AttributeType.CHECKBOX:
                    if value == spec.default_value:
                        pass
                    elif isinstance(value, str):
                        value = value.lower()
                        assert value in {'true', 'false'}
                    elif isinstance(value, (bool, int, float)):
                        value = 'true' if value else 'false'
                    else:
                        raise ValueError("Unexpected attribute value")
            except Exception as e:
                raise Exception("Failed to convert attribute '%s'='%s': %s" %
                    (self._get_label_name(label_id), value, e))

        elif self._soft_attribute_import:
            if isinstance(value, (int, float)):
                attr_type = AttributeType.NUMBER
            elif isinstance(value, bool):
                attr_type = AttributeType.CHECKBOX
            else:
                value = str(value)
                if value.lower() in {'true', 'false'}:
                    value = value.lower() == 'true'
                    attr_type = AttributeType.CHECKBOX
                else:
                    attr_type = AttributeType.TEXT

            attr_spec = AttributeSpec(
                label_id=label_id,
                name=attribute.name,
                input_type=attr_type,
                mutable=mutable,
            )
            attr_spec.save()
            spec_id = attr_spec.id
            if label_id not in self._label_mapping:
                self._label_mapping[label_id] = Label.objects.get(id=label_id)
            if label_id not in self._attribute_mapping:
                self._attribute_mapping[label_id] = {'mutable': {}, 'immutable': {}, 'spec': {}}
            self._attribute_mapping[label_id]['immutable'][spec_id] = attribute.name
            self._attribute_mapping[label_id]['spec'][spec_id] = attr_spec
            self._attribute_mapping_merged[label_id] = {
                **self._attribute_mapping[label_id]['mutable'],
                **self._attribute_mapping[label_id]['immutable'],
            }


        return { 'spec_id': spec_id, 'value': value }

    def _export_attributes(self, attributes):
        exported_attributes = []
        for attr in attributes:
            attribute_name = self._get_attribute_name(attr["spec_id"])
            exported_attributes.append(InstanceLabelData.Attribute(
                name=attribute_name,
                value=attr["value"],
            ))
        return exported_attributes

class CommonData(InstanceLabelData):
    class Shape(NamedTuple):
        id: int
        label_id: int

    class LabeledShape(NamedTuple):
        type: int
        frame: int
        label: int
        points: Sequence[int]
        occluded: bool
        attributes: Sequence[CommonData.Attribute]
        source: str | None
        rotation: float = 0
        group: int = 0
        z_order: int = 0
        elements: Sequence[CommonData.LabeledShape] = ()
        outside: bool = False
        id: int | None = None

    class TrackedShape(NamedTuple):
        type: int
        frame: int
        points: Sequence[int]
        occluded: bool
        outside: bool
        keyframe: bool
        attributes: Sequence[CommonData.Attribute]
        rotation: float = 0
        source: str = "manual"
        group: int = 0
        z_order: int = 0
        label: str | None = None
        track_id: int = 0
        elements: Sequence[CommonData.TrackedShape] = ()
        id: int | None = None

    class Track(NamedTuple):
        label: int
        group: int
        source: str
        shapes: Sequence[CommonData.TrackedShape]
        elements: Sequence[int] = ()
        id: int | None = None

    class Tag(NamedTuple):
        frame: int
        label: int
        attributes: Sequence[CommonData.Attribute]
        source: str | None
        group: int | None = 0
        id: int | None = None

    class Frame(NamedTuple):
        idx: int
        id: int
        frame: int
        name: str
        width: int
        height: int
        labeled_shapes: Sequence[CommonData.LabeledShape]
        tags: Sequence[CommonData.Tag]
        shapes: Sequence[CommonData.Shape]
        labels: Mapping[int, CommonData.Label]
        subset: str
        task_id: int

    class Label(NamedTuple):
        id: int
        name: str
        color: str | None
        type: str | None

    def __init__(self,
        annotation_ir: AnnotationIR,
        db_task: Task,
        *,
        host: str = '',
        create_callback=None,
        use_server_track_ids: bool = False,
        included_frames: Optional[Sequence[int]] = None
    ) -> None:
        self._dimension = annotation_ir.dimension
        self._annotation_ir = annotation_ir
        self._host = host
        self._create_callback = create_callback
        self._MAX_ANNO_SIZE = 30000
        self._frame_info = {}
        self._frame_mapping: dict[str, int] = {}
        self._frame_step = db_task.data.get_frame_step()
        self._db_data: models.Data = db_task.data
        self._use_server_track_ids = use_server_track_ids
        self._required_frames = included_frames
        self._initialized_included_frames: Optional[set[int]] = None
        self._db_subset = db_task.subset

        super().__init__(db_task)

        self._init_frame_info()
        self._init_meta()

    @property
    def rel_range(self):
        raise NotImplementedError()

    @property
    def start(self) -> int:
        return 0

    @property
    def stop(self) -> int:
        return max(0, len(self) - 1)

    def _get_db_images(self) -> Iterator[models.Image]:
        raise NotImplementedError()

    def abs_frame_id(self, relative_id):
        # relative_id is frame index in segment for job, so it can start with more than just zero
        if relative_id not in self.rel_range:
            raise ValueError("Unknown internal frame id %s" % relative_id)
        return relative_id * self._frame_step + self._db_data.start_frame

    def rel_frame_id(self, absolute_id):
        d, m = divmod(
            absolute_id - self._db_data.start_frame, self._frame_step)
        if m or d not in self.rel_range:
            raise ValueError("Unknown frame %s" % absolute_id)
        return d

    def _init_frame_info(self):
        self._deleted_frames = { k: True for k in self._db_data.deleted_frames }

        self._excluded_frames = set()

        if hasattr(self._db_data, 'video'):
            self._frame_info = {
                frame: {
                    "path": "frame_{:06d}".format(self.abs_frame_id(frame)),
                    "width": self._db_data.video.width,
                    "height": self._db_data.video.height,
                    "subset": self._db_subset,
                } for frame in self.rel_range
            }
        else:
            self._frame_info = {
                self.rel_frame_id(db_image.frame): {
                    "id": db_image.id,
                    "path": db_image.path,
                    "width": db_image.width,
                    "height": db_image.height,
                    "subset": self._db_subset,
                } for db_image in self._get_db_images()
            }

        self._frame_mapping = {
            self._get_filename(info["path"]): frame_number
            for frame_number, info in self._frame_info.items()
        }

    @staticmethod
    def _convert_db_labels(db_labels):
        labels = []
        for db_label in db_labels:
            label = OrderedDict([
                ("name", db_label.name),
                ("color", db_label.color),
                ("type", db_label.type),
                ("attributes", [
                    ("attribute", OrderedDict([
                        ("name", db_attr.name),
                        ("mutable", str(db_attr.mutable)),
                        ("input_type", db_attr.input_type),
                        ("default_value", db_attr.default_value),
                        ("values", db_attr.values)]))
                    for db_attr in db_label.attributespec_set.all()])
            ])

            if db_label.parent:
                label["parent"] = db_label.parent.name

            if db_label.type == str(LabelType.SKELETON):
                label["svg"] = db_label.skeleton.svg
                for db_sublabel in list(db_label.sublabels.all()):
                    label["svg"] = label["svg"].replace(f'data-label-id="{db_sublabel.id}"', f'data-label-name="{db_sublabel.name}"')

            labels.append(('label', label))
        return labels

    def _init_meta(self):
        raise NotImplementedError()

    def _export_tracked_shape(self, shape):
        return CommonData.TrackedShape(
            id=shape["id"],
            type=shape["type"],
            frame=self.abs_frame_id(shape["frame"]),
            label=self._get_label_name(shape["label_id"]),
            points=shape["points"],
            rotation=shape["rotation"],
            occluded=shape["occluded"],
            z_order=shape.get("z_order", 0),
            group=shape.get("group", 0),
            outside=shape.get("outside", False),
            keyframe=shape.get("keyframe", True),
            track_id=shape["track_id"],
            source=shape.get("source", "manual"),
            attributes=self._export_attributes(shape["attributes"]),
            elements=[self._export_tracked_shape(element) for element in shape.get("elements", [])]
        )

    def _export_labeled_shape(self, shape):
        return CommonData.LabeledShape(
            id=shape["id"],
            type=shape["type"],
            label=self._get_label_name(shape["label_id"]),
            frame=self.abs_frame_id(shape["frame"]),
            points=shape["points"],
            rotation=shape["rotation"],
            occluded=shape["occluded"],
            outside=shape.get("outside", False),
            z_order=shape.get("z_order", 0),
            group=shape.get("group", 0),
            source=shape["source"],
            attributes=self._export_attributes(shape["attributes"]),
            elements=[self._export_labeled_shape(element) for element in shape.get("elements", [])]
        )

    def _export_shape(self, shape):
        return CommonData.Shape(
            id=shape["id"],
            label_id=shape["label_id"]
        )

    def _export_tag(self, tag):
        return CommonData.Tag(
            id=tag["id"],
            frame=self.abs_frame_id(tag["frame"]),
            label=self._get_label_name(tag["label_id"]),
            group=tag.get("group", 0),
            source=tag["source"],
            attributes=self._export_attributes(tag["attributes"]),
        )

    def _export_track(self, track, idx):
        track['shapes'] = list(filter(lambda x: not self._is_frame_deleted(x['frame']), track['shapes']))
        tracked_shapes = TrackManager.get_interpolated_shapes(
            track, 0, self.stop + 1, self._annotation_ir.dimension)
        for tracked_shape in tracked_shapes:
            tracked_shape["attributes"] += track["attributes"]
            tracked_shape["track_id"] = track["track_id"] if self._use_server_track_ids else idx
            tracked_shape["group"] = track["group"]
            tracked_shape["source"] = track["source"]
            tracked_shape["label_id"] = track["label_id"]

        return CommonData.Track(
            id=track["id"],
            label=self._get_label_name(track["label_id"]),
            group=track["group"],
            source=track["source"],
            shapes=[self._export_tracked_shape(shape)
                for shape in tracked_shapes if not self._is_frame_deleted(shape["frame"])],
            elements=[self._export_track(element, i) for i, element in enumerate(track.get("elements", []))]
        )

    @staticmethod
    def _export_label(label):
        return CommonData.Label(
            id=label.id,
            name=label.name,
            color=label.color,
            type=label.type
        )

    def group_by_frame(self, include_empty: bool = False):
        frames = {}
        def get_frame(idx):
            frame_info = self._frame_info[idx]
            frame = self.abs_frame_id(idx)
            if frame not in frames:
                frames[frame] = CommonData.Frame(
                    idx=idx,
                    id=frame_info.get("id", 0),
                    subset=frame_info["subset"],
                    frame=frame,
                    name=frame_info["path"],
                    height=frame_info["height"],
                    width=frame_info["width"],
                    labeled_shapes=[],
                    tags=[],
                    shapes=[],
                    labels={},
                    task_id=self._db_task.id,
                )
            return frames[frame]

        included_frames = self.get_included_frames()

        if include_empty:
            for idx in sorted(set(self._frame_info) & included_frames):
                get_frame(idx)

        anno_manager = AnnotationManager(
            self._annotation_ir, dimension=self._annotation_ir.dimension
        )
        for shape in sorted(
            anno_manager.to_shapes(
                self.stop + 1,
                # Skip outside, deleted and excluded frames
                included_frames=included_frames,
                deleted_frames=self.deleted_frames.keys(),
                include_outside=False,
                use_server_track_ids=self._use_server_track_ids,
            ),
            key=lambda shape: shape.get("z_order", 0)
        ):
            shape_data = ''

            if 'track_id' in shape:
                if shape['outside']:
                    continue
                exported_shape = self._export_tracked_shape(shape)
            else:
                exported_shape = self._export_labeled_shape(shape)
                shape_data = self._export_shape(shape)

            get_frame(shape['frame']).labeled_shapes.append(exported_shape)

            if shape_data:
                get_frame(shape['frame']).shapes.append(shape_data)
                for label in self._label_mapping.values():
                    label = self._export_label(label)
                    get_frame(shape['frame']).labels.update({label.id: label})

        for tag in self._annotation_ir.tags:
            if tag['frame'] not in included_frames:
                continue
            get_frame(tag['frame']).tags.append(self._export_tag(tag))

        return iter(frames.values())

    @property
    def shapes(self):
        for shape in self._annotation_ir.shapes:
            if not self._is_frame_deleted(shape["frame"]):
                yield self._export_labeled_shape(shape)

    def get_included_frames(self):
        if self._initialized_included_frames is None:
            self._initialized_included_frames = set(
                i for i in self.rel_range
                if not self._is_frame_deleted(i)
                and not self._is_frame_excluded(i)
                and self._is_frame_required(i)
            )
        return self._initialized_included_frames

    def _is_frame_deleted(self, frame):
        return frame in self._deleted_frames

    def _is_frame_excluded(self, frame):
        return frame in self._excluded_frames

    def _is_frame_required(self, frame):
        return self._required_frames is None or frame in self._required_frames

    @property
    def tracks(self):
        for idx, track in enumerate(self._annotation_ir.tracks):
            yield self._export_track(track, idx)

    @property
    def tags(self):
        for tag in self._annotation_ir.tags:
            if tag["frame"] not in self._deleted_frames:
                yield self._export_tag(tag)

    @property
    def meta(self):
        return self._meta

    @property
    def soft_attribute_import(self):
        return self._soft_attribute_import

    @soft_attribute_import.setter
    def soft_attribute_import(self, value: bool):
        self._soft_attribute_import = value

    def _import_tag(self, tag):
        _tag = tag._asdict()
        label_id = self._get_label_id(_tag.pop('label'))
        _tag['frame'] = self.rel_frame_id(int(_tag['frame']))
        _tag['label_id'] = label_id
        _tag['attributes'] = [self._import_attribute(label_id, attrib)
            for attrib in _tag['attributes']
            if self._get_attribute_id(label_id, attrib.name) or (
                self.soft_attribute_import and attrib.name not in CVAT_INTERNAL_ATTRIBUTES
            )
        ]
        return _tag

    def _import_shape(self, shape, parent_label_id=None):
        _shape = shape._asdict()
        label_id = self._get_label_id(_shape.pop('label'), parent_label_id)
        _shape['frame'] = self.rel_frame_id(int(_shape['frame']))
        _shape['label_id'] = label_id
        _shape['attributes'] = [self._import_attribute(label_id, attrib)
            for attrib in _shape['attributes']
            if self._get_attribute_id(label_id, attrib.name) or (
                self.soft_attribute_import and attrib.name not in CVAT_INTERNAL_ATTRIBUTES
            )
        ]

        self._ensure_points_converted_to_floats(_shape)
        _shape['elements'] = [self._import_shape(element, label_id) for element in _shape.get('elements', [])]

        return _shape

    def _import_track(self, track, parent_label_id=None):
        _track = track._asdict()
        label_id = self._get_label_id(_track.pop('label'), parent_label_id)
        _track['frame'] = self.rel_frame_id(
            min(int(shape.frame) for shape in _track['shapes']))
        _track['label_id'] = label_id
        _track['attributes'] = []
        _track['shapes'] = [shape._asdict() for shape in _track['shapes']]
        _track['elements'] = [self._import_track(element, label_id) for element in _track.get('elements', [])]
        for shape in _track['shapes']:
            shape['frame'] = self.rel_frame_id(int(shape['frame']))
            _track['attributes'] = [self._import_attribute(label_id, attrib)
                for attrib in shape['attributes']
                if self._get_immutable_attribute_id(label_id, attrib.name) or (
                    self.soft_attribute_import and attrib.name not in CVAT_INTERNAL_ATTRIBUTES
                )
            ]
            shape['attributes'] = [self._import_attribute(label_id, attrib, mutable=True)
                for attrib in shape['attributes']
                if self._get_mutable_attribute_id(label_id, attrib.name)
            ]
            self._ensure_points_converted_to_floats(shape)

        return _track

    def _ensure_points_converted_to_floats(self, shape) -> None:
        """
        Historically, there were importers that were not converting points to ints/floats.
        The only place to make sure that all points in shapes have the right type was this one.
        However, this does eat up a lot of memory for some reason.
        (see https://github.com/cvat-ai/cvat/pull/1898)

        So, before we can guarantee that all the importers are returning the right data,
        we have to have this conversion.
        """
        # if points is LazyList or tuple, we can be sure it has the right type already
        if isinstance(points := shape["points"], LazyList | tuple):
            return

        for point in points:
            if not isinstance(point, int | float):
                slogger.glob.error(
                    f"Points must be type of "
                    f"`tuple[int | float] | list[int | float] | LazyList`, "
                    f"not `{points.__class__.__name__}[{point.__class__.__name__}]`"
                    "Please, update import code to return the correct type."
                )
                shape["points"] = tuple(map(float, points))
                return

    def _call_callback(self):
        if self._len() > self._MAX_ANNO_SIZE:
            self._create_callback(self._annotation_ir.serialize())
            self._annotation_ir.reset()

    def add_tag(self, tag):
        imported_tag = self._import_tag(tag)
        if imported_tag['label_id']:
            self._annotation_ir.add_tag(imported_tag)
            self._call_callback()

    def add_shape(self, shape):
        imported_shape = self._import_shape(shape)
        if imported_shape['label_id']:
            self._annotation_ir.add_shape(imported_shape)
            self._call_callback()

    def add_track(self, track):
        imported_track = self._import_track(track)
        if imported_track['label_id']:
            self._annotation_ir.add_track(imported_track)
            self._call_callback()

    @property
    def data(self):
        return self._annotation_ir

    def _len(self):
        track_len = 0
        for track in self._annotation_ir.tracks:
            track_len += len(track['shapes'])

        return len(self._annotation_ir.tags) + len(self._annotation_ir.shapes) + track_len

    @property
    def frame_info(self):
        return self._frame_info

    @property
    def deleted_frames(self):
        return self._deleted_frames

    @property
    def frame_step(self):
        return self._frame_step

    @property
    def db_instance(self):
        raise NotImplementedError()

    @property
    def db_data(self):
        return self._db_data

    def __len__(self):
        raise NotImplementedError()

    @staticmethod
    def _get_filename(path: str) -> str:
        return osp.splitext(path)[0]

    def match_frame(self,
        path: str, root_hint: Optional[str] = None, *, path_has_ext: bool = True
    ) -> Optional[int]:
        if path_has_ext:
            path = self._get_filename(path)

        match = self._frame_mapping.get(path)

        if not match and root_hint and not path.startswith(root_hint):
            path = osp.join(root_hint, path)
            match = self._frame_mapping.get(path)

        return match

    def match_frame_fuzzy(self, path: str, *, path_has_ext: bool = True) -> Optional[int]:
        # Preconditions:
        # - The input dataset is full, i.e. all items present. Partial dataset
        # matching can't be correct for all input cases.
        # - path is the longest path of input dataset in terms of path parts

        if path_has_ext:
            path = self._get_filename(path)

        path = Path(path).parts
        for p, v in self._frame_mapping.items():
            if Path(p).parts[-len(path):] == path: # endswith() for paths
                return v

        return None

class JobData(CommonData):
    META_FIELD = "job"
    def __init__(self, annotation_ir: AnnotationIR, db_job: Job, **kwargs):
        self._db_job = db_job
        self._db_task = db_job.segment.task

        super().__init__(annotation_ir, self._db_task, **kwargs)

    def _init_meta(self):
        db_segment = self._db_job.segment
        self._meta = OrderedDict([
            (JobData.META_FIELD, OrderedDict([
                ("id", str(self._db_job.id)),
                ("size", str(len(self))),
                ("mode", self._db_task.mode),
                ("overlap", str(self._db_task.overlap)),
                ("bugtracker", self._db_task.bug_tracker),
                ("created", str(timezone.localtime(self._db_task.created_date))),
                ("updated", str(timezone.localtime(self._db_job.updated_date))),
                ("subset", self._db_task.subset or dm.DEFAULT_SUBSET_NAME),
                ("start_frame", str(self._db_data.start_frame + db_segment.start_frame * self._frame_step)),
                ("stop_frame", str(self._db_data.start_frame + db_segment.stop_frame * self._frame_step)),
                ("frame_filter", self._db_data.frame_filter),
                ("segments", [
                    ("segment", OrderedDict([
                        ("id", str(db_segment.id)),
                        ("start", str(db_segment.start_frame)),
                        ("stop", str(db_segment.stop_frame)),
                        ("url", "{}/api/jobs/{}".format(self._host, self._db_job.id))])),
                ]),
                ("owner", OrderedDict([
                    ("username", self._db_task.owner.username),
                    ("email", self._db_task.owner.email)
                ]) if self._db_task.owner else ""),

                ("assignee", OrderedDict([
                    ("username", self._db_job.assignee.username),
                    ("email", self._db_job.assignee.email)
                ]) if self._db_job.assignee else ""),
            ])),
            ("dumped", str(timezone.localtime(timezone.now()))),
        ])

        if self._label_mapping is not None:
            self._meta[JobData.META_FIELD]["labels"] = CommonData._convert_db_labels(self._label_mapping.values())

        if hasattr(self._db_data, "video"):
            self._meta["original_size"] = OrderedDict([
                ("width", str(self._db_data.video.width)),
                ("height", str(self._db_data.video.height))
            ])

    def _init_frame_info(self):
        super()._init_frame_info()

        if self.db_instance.segment.type == SegmentType.SPECIFIC_FRAMES:
            frame_set = self.db_instance.segment.frame_set
            self._excluded_frames.update(
                frame for frame in self.rel_range
                if self.abs_frame_id(frame) not in frame_set
            )

            if self.db_instance.type == JobType.GROUND_TRUTH:
                self._excluded_frames.update(self.db_data.validation_layout.disabled_frames)

        if self._required_frames:
            rel_range = self.rel_range
            self._required_frames = set(frame for frame in self._required_frames if frame in rel_range)

    def __len__(self):
        segment = self._db_job.segment
        return segment.stop_frame - segment.start_frame + 1

    def _get_db_images(self):
        return (image for image in self._db_data.images.all() if image.frame in self.abs_range)

    @property
    def abs_range(self):
        segment = self._db_job.segment
        step = self._frame_step
        start_frame = self._db_data.start_frame + segment.start_frame * step
        stop_frame = self._db_data.start_frame + segment.stop_frame * step + 1
        return range(start_frame, stop_frame, step)

    @property
    def rel_range(self):
        segment = self._db_job.segment
        return range(segment.start_frame, segment.stop_frame + 1)

    @property
    def start(self) -> int:
        segment = self._db_job.segment
        return segment.start_frame

    @property
    def stop(self) -> int:
        segment = self._db_job.segment
        return segment.stop_frame

    @property
    def db_instance(self):
        return self._db_job


class TaskData(CommonData):
    META_FIELD = "task"
    def __init__(self, annotation_ir: AnnotationIR, db_task: Task, **kwargs):
        self._db_task = db_task
        super().__init__(annotation_ir, db_task, **kwargs)

    @staticmethod
    def meta_for_task(db_task, host, label_mapping=None):
        db_segments = db_task.segment_set.all().prefetch_related(
            Prefetch('job_set', models.Job.objects.order_by("pk"))
        )

        meta = OrderedDict([
            ("id", str(db_task.id)),
            ("name", db_task.name),
            ("size", str(db_task.data.size)),
            ("mode", db_task.mode),
            ("overlap", str(db_task.overlap)),
            ("bugtracker", db_task.bug_tracker),
            ("created", str(timezone.localtime(db_task.created_date))),
            ("updated", str(timezone.localtime(db_task.updated_date))),
            ("subset", db_task.subset or dm.DEFAULT_SUBSET_NAME),
            ("start_frame", str(db_task.data.start_frame)),
            ("stop_frame", str(db_task.data.stop_frame)),
            ("frame_filter", db_task.data.frame_filter),

            ("segments", [
                ("segment", OrderedDict([
                    ("id", str(db_segment.id)),
                    ("start", str(db_segment.start_frame)),
                    ("stop", str(db_segment.stop_frame)),
                    ("url", "{}/api/jobs/{}".format(
                        host, db_segment.job_set.first().id))]
                ))
                for db_segment in db_segments
                if db_segment.job_set.first().type == JobType.ANNOTATION
            ]),

            ("owner", OrderedDict([
                ("username", db_task.owner.username),
                ("email", db_task.owner.email)
            ]) if db_task.owner else ""),

            ("assignee", OrderedDict([
                ("username", db_task.assignee.username),
                ("email", db_task.assignee.email)
            ]) if db_task.assignee else ""),
        ])

        if label_mapping is not None:
            meta['labels'] = CommonData._convert_db_labels(label_mapping.values())

        if hasattr(db_task.data, "video"):
            meta["original_size"] = OrderedDict([
                ("width", str(db_task.data.video.width)),
                ("height", str(db_task.data.video.height))
            ])

            # Add source to dumped file
            meta["source"] = str(osp.basename(db_task.data.video.path))

        return meta

    def _init_meta(self):
        self._meta = OrderedDict([
            (TaskData.META_FIELD, self.meta_for_task(self._db_task, self._host, self._label_mapping)),
            ("dumped", str(timezone.localtime(timezone.now())))
        ])

    def __len__(self):
        return self._db_data.size

    @property
    def rel_range(self):
        return range(len(self))

    @property
    def db_instance(self):
        return self._db_task

    def _get_db_images(self):
        return self._db_data.images.all()

    def _init_frame_info(self):
        super()._init_frame_info()

        if self.db_data.validation_mode == models.ValidationMode.GT_POOL:
            # For GT pool-enabled tasks, we:
            # - skip validation frames in normal jobs on annotation export
            # - load annotations for GT pool frames on annotation import

            assert not hasattr(self.db_data, 'video')

            for db_image in self._get_db_images():
                # We should not include placeholder frames in task export, so we exclude them
                if db_image.is_placeholder:
                    self._excluded_frames.add(db_image.frame)
                    continue

                # We should not match placeholder frames during task import,
                # so we update the frame matching index
                self._frame_mapping[self._get_filename(db_image.path)] = (
                    self.rel_frame_id(db_image.frame)
                )

class ProjectData(InstanceLabelData):
    META_FIELD = 'project'
    @attrs
    class LabeledShape:
        type: str = attrib()
        frame: int = attrib()
        label: str = attrib()
        points: list[float] = attrib()
        occluded: bool = attrib()
        attributes: list[InstanceLabelData.Attribute] = attrib()
        source: str = attrib(default='manual')
        group: int = attrib(default=0)
        rotation: int = attrib(default=0)
        z_order: int = attrib(default=0)
        task_id: int = attrib(default=None)
        subset: str = attrib(default=None)
        outside: bool = attrib(default=False)
        elements: list['ProjectData.LabeledShape'] = attrib(default=[])

    @attrs
    class TrackedShape:
        type: str = attrib()
        frame: int = attrib()
        points: list[float] = attrib()
        occluded: bool = attrib()
        outside: bool = attrib()
        keyframe: bool = attrib()
        attributes: list[InstanceLabelData.Attribute] = attrib()
        rotation: int = attrib(default=0)
        source: str = attrib(default='manual')
        group: int = attrib(default=0)
        z_order: int = attrib(default=0)
        label: str = attrib(default=None)
        track_id: int = attrib(default=0)
        elements: list['ProjectData.TrackedShape'] = attrib(default=[])

    @attrs
    class Track:
        label: str = attrib()
        shapes: list['ProjectData.TrackedShape'] = attrib()
        source: str = attrib(default='manual')
        group: int = attrib(default=0)
        task_id: int = attrib(default=None)
        subset: str = attrib(default=None)
        elements: list['ProjectData.Track'] = attrib(default=[])

    @attrs
    class Tag:
        frame: int = attrib()
        label: str = attrib()
        attributes: list[InstanceLabelData.Attribute] = attrib()
        source: str = attrib(default='manual')
        group: int = attrib(default=0)
        task_id: int = attrib(default=None)
        subset: str = attrib(default=None)

    @attrs
    class Frame:
        idx: int = attrib()
        id: int = attrib()
        frame: int = attrib()
        name: str = attrib()
        width: int = attrib()
        height: int = attrib()
        labeled_shapes: list[Union['ProjectData.LabeledShape', 'ProjectData.TrackedShape']] = attrib()
        tags: list['ProjectData.Tag'] = attrib()
        task_id: int = attrib(default=None)
        subset: str = attrib(default=None)

    def __init__(self,
        annotation_irs: Mapping[str, AnnotationIR],
        db_project: Project,
        host: str = '',
        task_annotations: Mapping[int, Any] = None,
        project_annotation=None,
        *,
        use_server_track_ids: bool = False
    ):
        self._annotation_irs = annotation_irs
        self._db_project = db_project
        self._task_annotations = task_annotations
        self._host = host
        self._soft_attribute_import = False
        self._project_annotation = project_annotation
        self._tasks_data: dict[int, TaskData] = {}
        self._frame_info: dict[tuple[int, int], Literal["path", "width", "height", "subset"]] = dict()
        # (subset, path): (task id, frame number)
        self._frame_mapping: dict[tuple[str, str], tuple[int, int]] = dict()
        self._frame_steps: dict[int, int] = {}
        self.new_tasks: set[int] = set()
        self._use_server_track_ids = use_server_track_ids

        InstanceLabelData.__init__(self, db_project)
        self.init()


    def abs_frame_id(self, task_id: int, relative_id: int) -> int:
        task = self._db_tasks[task_id]
        if relative_id not in range(0, task.data.size):
            raise ValueError(f"Unknown internal frame id {relative_id}")
        return relative_id * task.data.get_frame_step() + task.data.start_frame + self._task_frame_offsets[task_id]

    def rel_frame_id(self, task_id: int, absolute_id: int) -> int:
        task = self._db_tasks[task_id]
        d, m = divmod(
            absolute_id - task.data.start_frame, task.data.get_frame_step())
        if m or d not in range(0, task.data.size):
            raise ValueError(f"Unknown frame {absolute_id}")
        return d

    def init(self):
        self._init_tasks()
        self._init_task_frame_offsets()
        self._init_frame_info()
        self._init_meta()

    def _init_tasks(self):
        self._db_tasks: OrderedDict[int, Task] = OrderedDict(
            (
                (db_task.id, db_task)
                for db_task in self._db_project.tasks.exclude(data=None).order_by("subset","id").all()
            )
        )

        subsets = set()
        for task in self._db_tasks.values():
            subsets.add(task.subset)
        self._subsets: list[str] = list(subsets)

        self._frame_steps: dict[int, int] = {task.id: task.data.get_frame_step() for task in self._db_tasks.values()}

    def _init_task_frame_offsets(self):
        self._task_frame_offsets: dict[int, int] = dict()
        s = 0
        subset = None

        for task in self._db_tasks.values():
            if subset != task.subset:
                s = 0
                subset = task.subset
            self._task_frame_offsets[task.id] = s
            s += task.data.start_frame + task.data.get_frame_step() * task.data.size


    def _init_frame_info(self):
        self._frame_info = dict()
        self._deleted_frames = { (task.id, frame): True for task in self._db_tasks.values() for frame in task.data.deleted_frames }
        original_names = defaultdict[tuple[str, str], int](int)
        for task in self._db_tasks.values():
            defaulted_subset = get_defaulted_subset(task.subset, self._subsets)
            if hasattr(task.data, 'video'):
                self._frame_info.update({(task.id, frame): {
                    "path": "frame_{:06d}".format(self.abs_frame_id(task.id, frame)),
                    "width": task.data.video.width,
                    "height": task.data.video.height,
                    "subset": defaulted_subset,
                } for frame in range(task.data.size)})
            else:
                self._frame_info.update({(task.id, self.rel_frame_id(task.id, db_image.frame)): {
                    # do not modify honeypot names since they will be excluded from the dataset
                    # and their quantity should not affect the validation frame name
                    "path": mangle_image_name(db_image.path, defaulted_subset, original_names) \
                        if not db_image.is_placeholder else db_image.path,
                    "id": db_image.id,
                    "width": db_image.width,
                    "height": db_image.height,
                    "subset": defaulted_subset
                } for db_image in task.data.images.all()})

        self._frame_mapping = {
            (self._db_tasks[frame_ident[0]].subset, self._get_filename(info["path"])): frame_ident
            for frame_ident, info in self._frame_info.items()
        }

    def _init_meta(self):
        self._meta = OrderedDict([
            (ProjectData.META_FIELD, OrderedDict([
                ('id', str(self._db_project.id)),
                ('name', self._db_project.name),
                ("bugtracker", self._db_project.bug_tracker),
                ("created", str(timezone.localtime(self._db_project.created_date))),
                ("updated", str(timezone.localtime(self._db_project.updated_date))),
                ("tasks", [
                    ('task',
                        TaskData.meta_for_task(db_task, self._host)
                    ) for db_task in self._db_tasks.values()
                ]),

                ("subsets", '\n'.join([s if s else dm.DEFAULT_SUBSET_NAME for s in self._subsets])),

                ("owner", OrderedDict([
                    ("username", self._db_project.owner.username),
                    ("email", self._db_project.owner.email),
                ]) if self._db_project.owner else ""),

                ("assignee", OrderedDict([
                    ("username", self._db_project.assignee.username),
                    ("email", self._db_project.assignee.email),
                ]) if self._db_project.assignee else ""),
            ])),
            ("dumped", str(timezone.localtime(timezone.now())))
        ])

        if self._label_mapping is not None:
            labels = []
            for db_label in self._label_mapping.values():
                label = OrderedDict([
                    ("name", db_label.name),
                    ("color", db_label.color),
                    ("type", db_label.type),
                    ("attributes", [
                        ("attribute", OrderedDict([
                            ("name", db_attr.name),
                            ("mutable", str(db_attr.mutable)),
                            ("input_type", db_attr.input_type),
                            ("default_value", db_attr.default_value),
                            ("values", db_attr.values)]))
                        for db_attr in db_label.attributespec_set.all()])
                ])

                if db_label.parent:
                    label["parent"] = db_label.parent.name

                if db_label.type == str(LabelType.SKELETON):
                    label["svg"] = db_label.skeleton.svg
                    for db_sublabel in list(db_label.sublabels.all()):
                        label["svg"] = label["svg"].replace(f'data-label-id="{db_sublabel.id}"', f'data-label-name="{db_sublabel.name}"')

                labels.append(('label', label))

            self._meta[ProjectData.META_FIELD]['labels'] = labels

    def _export_tracked_shape(self, shape: dict, task_id: int):
        return ProjectData.TrackedShape(
            type=shape["type"],
            frame=self.abs_frame_id(task_id, shape["frame"]),
            label=self._get_label_name(shape["label_id"]),
            points=shape["points"],
            rotation=shape["rotation"],
            occluded=shape["occluded"],
            z_order=shape.get("z_order", 0),
            group=shape.get("group", 0),
            outside=shape.get("outside", False),
            keyframe=shape.get("keyframe", True),
            track_id=shape["track_id"],
            source=shape.get("source", "manual"),
            attributes=self._export_attributes(shape["attributes"]),
            elements=[self._export_tracked_shape(element, task_id) for element in shape.get("elements", [])],
        )

    def _export_labeled_shape(self, shape: dict, task_id: int):
        return ProjectData.LabeledShape(
            type=shape["type"],
            label=self._get_label_name(shape["label_id"]),
            frame=self.abs_frame_id(task_id, shape["frame"]),
            points=shape["points"],
            rotation=shape["rotation"],
            occluded=shape["occluded"],
            outside=shape.get("outside", False),
            z_order=shape.get("z_order", 0),
            group=shape.get("group", 0),
            source=shape["source"],
            attributes=self._export_attributes(shape["attributes"]),
            elements=[self._export_labeled_shape(element, task_id) for element in shape.get("elements", [])],
            task_id=task_id,
        )

    def _export_tag(self, tag: dict, task_id: int):
        return ProjectData.Tag(
            frame=self.abs_frame_id(task_id, tag["frame"]),
            label=self._get_label_name(tag["label_id"]),
            group=tag.get("group", 0),
            source=tag["source"],
            attributes=self._export_attributes(tag["attributes"]),
            task_id=task_id
        )

    def _export_track(self, track: dict, task_id: int, task_size: int, idx: int):
        track['shapes'] = list(filter(lambda x: (task_id, x['frame']) not in self._deleted_frames, track['shapes']))
        tracked_shapes = TrackManager.get_interpolated_shapes(
            track, 0, task_size, self._annotation_irs[task_id].dimension
        )
        for tracked_shape in tracked_shapes:
            tracked_shape["attributes"] += track["attributes"]
            tracked_shape["track_id"] = track["track_id"] if self._use_server_track_ids else idx
            tracked_shape["group"] = track["group"]
            tracked_shape["source"] = track["source"]
            tracked_shape["label_id"] = track["label_id"]

        return ProjectData.Track(
            label=self._get_label_name(track["label_id"]),
            group=track["group"],
            source=track["source"],
            shapes=[self._export_tracked_shape(shape, task_id) for shape in tracked_shapes
                if (task_id, shape["frame"]) not in self._deleted_frames],
            task_id=task_id,
            elements=[self._export_track(element, task_id, task_size, i)
                for i, element in enumerate(track.get("elements", []))]
        )

    def group_by_frame(self, include_empty: bool = False):
        frames: dict[tuple[str, int], ProjectData.Frame] = {}
        def get_frame(task_id: int, idx: int) -> ProjectData.Frame:
            frame_info = self._frame_info[(task_id, idx)]
            abs_frame = self.abs_frame_id(task_id, idx)
            if (frame_info["subset"], abs_frame) not in frames:
                frames[(frame_info["subset"], abs_frame)] = ProjectData.Frame(
                    task_id=task_id,
                    subset=frame_info["subset"],
                    idx=idx,
                    id=frame_info.get('id',0),
                    frame=abs_frame,
                    name=frame_info["path"],
                    height=frame_info["height"],
                    width=frame_info["width"],
                    labeled_shapes=[],
                    tags=[],
                )
            return frames[(frame_info["subset"], abs_frame)]

        if include_empty:
            for task_id, frame in sorted(self._frame_info):
                if not self._tasks_data.get(task_id):
                    self.init_task_data(task_id)

                task_included_frames = self._tasks_data[task_id].get_included_frames()
                if frame in task_included_frames:
                    get_frame(task_id, frame)

        for task_data in self.all_task_data:
            task: Task = task_data.db_instance

            anno_manager = AnnotationManager(
                self._annotation_irs[task.id], dimension=self._annotation_irs[task.id].dimension
            )
            task_included_frames = task_data.get_included_frames()

            for shape in sorted(
                anno_manager.to_shapes(
                    task.data.size,
                    included_frames=task_included_frames,
                    deleted_frames=task_data.deleted_frames.keys(),
                    include_outside=False,
                    use_server_track_ids=self._use_server_track_ids,
                ),
                key=lambda shape: shape.get("z_order", 0)
            ):
                assert (task.id, shape['frame']) in self._frame_info

                if 'track_id' in shape:
                    if shape['outside']:
                        continue
                    exported_shape = self._export_tracked_shape(shape, task.id)
                else:
                    exported_shape = self._export_labeled_shape(shape, task.id)
                get_frame(task.id, shape['frame']).labeled_shapes.append(exported_shape)

            for tag in self._annotation_irs[task.id].tags:
                if (task.id, tag['frame']) not in self._frame_info:
                    continue
                get_frame(task.id, tag['frame']).tags.append(self._export_tag(tag, task.id))

        return iter(frames.values())

    @property
    def shapes(self):
        for task in self._db_tasks.values():
            for shape in self._annotation_irs[task.id].shapes:
                if (task.id, shape['frame']) not in self._deleted_frames:
                    yield self._export_labeled_shape(shape, task.id)

    @property
    def tracks(self):
        idx = 0
        for task in self._db_tasks.values():
            for track in self._annotation_irs[task.id].tracks:
                yield self._export_track(track, task.id, task.data.size, idx)

    @property
    def tags(self):
        for task in self._db_tasks.values():
            for tag in self._annotation_irs[task.id].tags:
                if (task.id, tag['frame']) not in self._deleted_frames:
                    yield self._export_tag(tag, task.id)

    @property
    def meta(self):
        return self._meta

    @property
    def data(self):
        raise NotImplementedError()

    @property
    def frame_info(self):
        return self._frame_info

    @property
    def deleted_frames(self):
        return self._deleted_frames

    @property
    def frame_step(self):
        return self._frame_steps

    @property
    def db_project(self):
        return self._db_project

    @property
    def subsets(self) -> list[str]:
        return self._subsets

    @property
    def tasks(self):
        return list(self._db_tasks.values())

    @property
    def soft_attribute_import(self):
        return self._soft_attribute_import

    @soft_attribute_import.setter
    def soft_attribute_import(self, value: bool):
        self._soft_attribute_import =  value
        for task_data in self._tasks_data.values():
            task_data.soft_attribute_import = value


    def init_task_data(self, task_id: int) -> TaskData:
        try:
            task = self._db_tasks[task_id]
        except KeyError as ex:
            raise Exception("There is no such task in the project") from ex

        task_data = TaskData(
            annotation_ir=self._annotation_irs[task_id],
            db_task=task,
            host=self._host,
            create_callback=self._task_annotations[task_id].create \
                if self._task_annotations is not None else None,
        )
        task_data._MAX_ANNO_SIZE //= len(self._db_tasks)
        task_data.soft_attribute_import = self.soft_attribute_import
        self._tasks_data[task_id] = task_data

        return task_data

    def _task_data(self, task_id: int) -> TaskData:
        if task_id in self._tasks_data:
            return self._tasks_data[task_id]
        else:
            return self.init_task_data(task_id)

    @property
    def all_task_data(self):
        for task_id in self._db_tasks.keys():
            yield self._task_data(task_id)

    @staticmethod
    def _get_filename(path):
        return osp.splitext(path)[0]

    def match_frame(self,
        path: str, subset: str = dm.DEFAULT_SUBSET_NAME,
        root_hint: str = None, path_has_ext: bool = True
    ) -> Optional[int]:
        if path_has_ext:
            path = self._get_filename(path)

        match_task, match_frame = self._frame_mapping.get((subset, path), (None, None))

        if not match_frame and root_hint and not path.startswith(root_hint):
            path = osp.join(root_hint, path)
            match_task, match_frame = self._frame_mapping.get((subset, path), (None, None))

        return match_task, match_frame

    def match_frame_fuzzy(self, path: str, *, path_has_ext: bool = True) -> Optional[int]:
        if path_has_ext:
            path = self._get_filename(path)

        path = Path(path).parts
        for (_subset, _path), (_tid, frame_number) in self._frame_mapping.items():
            if Path(_path).parts[-len(path):] == path :
                return frame_number

        return None

    def split_dataset(self, dataset: dm.Dataset):
        for task_id in self._db_tasks.keys():
            if task_id not in self.new_tasks:
                continue
            task_data = self._task_data(task_id)
            subset_dataset: dm.Dataset = dataset.subsets()[task_data.db_instance.subset].as_dataset()
            yield subset_dataset, task_data

    def add_labels(self, labels: list[dict]):
        attributes = []
        _labels = []
        for label in labels:
            _attributes = label.pop('attributes')
            _labels.append(Label(**label))
            attributes += [(label['name'], AttributeSpec(**at)) for at in _attributes]
        self._project_annotation.add_labels(_labels, attributes)

    def add_task(self, task, files):
        self._project_annotation.add_task(task, files, self)

@attrs(frozen=True, auto_attribs=True)
class MediaSource:
    db_task: Task

    @property
    def is_video(self) -> bool:
        return self.db_task.mode == 'interpolation'

class MediaProvider:
    def __init__(self, sources: dict[int, MediaSource]) -> None:
        self._sources = sources

    def unload(self) -> None:
        pass

class MediaProvider2D(MediaProvider):
    def __init__(self, sources: dict[int, MediaSource]) -> None:
        super().__init__(sources)
        self._current_source_id = None
        self._frame_provider = None

    def unload(self) -> None:
        self._unload_source()

    def get_media_for_frame(self, source_id: int, frame_index: int, **image_kwargs) -> dm.Image:
        source = self._sources[source_id]

        if source.is_video:
            def video_frame_loader():
                self._load_source(source_id, source)

                # optimization for videos: use numpy arrays instead of bytes
                # some formats or transforms can require image data
                return self._frame_provider.get_frame(frame_index,
                    quality=FrameQuality.ORIGINAL,
                    out_type=FrameOutputType.NUMPY_ARRAY
                ).data

            return dm.Image.from_numpy(data=video_frame_loader, **image_kwargs)
        else:
            def image_loader():
                self._load_source(source_id, source)

                # for images use encoded data to avoid recoding
                return self._frame_provider.get_frame(frame_index,
                    quality=FrameQuality.ORIGINAL,
                    out_type=FrameOutputType.BUFFER
                ).data.getvalue()

            return dm.Image.from_bytes(data=image_loader, **image_kwargs)

    def _load_source(self, source_id: int, source: MediaSource) -> None:
        if self._current_source_id == source_id:
            return

        self._unload_source()
        self._frame_provider = TaskFrameProvider(source.db_task)
        self._current_source_id = source_id

    def _unload_source(self) -> None:
        if self._frame_provider:
            self._frame_provider.unload()
            self._frame_provider = None

        self._current_source_id = None

class MediaProvider3D(MediaProvider):
    def __init__(self, sources: dict[int, MediaSource]) -> None:
        super().__init__(sources)
        self._images_per_source = {
            source_id: {
                image.id: image
                for image in source.db_task.data.images.prefetch_related('related_files')
            }
            for source_id, source in sources.items()
        }

    def get_media_for_frame(self, source_id: int, frame_id: int, **image_kwargs) -> dm.PointCloud:
        source = self._sources[source_id]

        point_cloud_path = osp.join(
            source.db_task.data.get_upload_dirname(), image_kwargs['path'],
        )

        image = self._images_per_source[source_id][frame_id]

        related_images = [
            dm.Image.from_file(path=path)
            for rf in image.related_files.all()
            for path in [osp.realpath(str(rf.path))]
            if osp.isfile(path)
        ]

        return dm.PointCloud.from_file(point_cloud_path, extra_images=related_images)

MEDIA_PROVIDERS_BY_DIMENSION: dict[DimensionType, MediaProvider] = {
    DimensionType.DIM_3D: MediaProvider3D,
    DimensionType.DIM_2D: MediaProvider2D,
}

class CVATDataExtractorMixin:
    def __init__(self, *,
        convert_annotations: Callable = None
    ):
        self.convert_annotations = convert_annotations or convert_cvat_anno_to_dm

        self._media_provider: Optional[MediaProvider] = None

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        if self._media_provider:
            self._media_provider.unload()

    def categories(self) -> dict:
        raise NotImplementedError()

    @staticmethod
    def load_categories(labels: list):
        categories: dict[dm.AnnotationType,
            dm.Categories] = {}

        label_categories = dm.LabelCategories(attributes=['occluded'])
        point_categories = dm.PointsCategories()

        for _, label in labels:
            label_id = label_categories.add(label['name'], label.get('parent'))
            for _, attr in label['attributes']:
                label_categories.attributes.add(attr['name'])

            if label['type'] == str(LabelType.SKELETON):
                joints = []
                sublabels = []
                for el in ET.fromstring('<root>' + label.get('svg', '') + '</root>'):
                    if el.tag == 'line':
                        joints.append([int(el.attrib['data-node-from']), int(el.attrib['data-node-to'])])
                    elif el.tag == 'circle':
                        sublabels.append(el.attrib['data-label-name'])

                point_categories.add(label_id, sublabels, joints)

        categories[dm.AnnotationType.label] = label_categories
        categories[dm.AnnotationType.points] = point_categories

        return categories

    @staticmethod
    def _load_user_info(meta: dict):
        return {
            "name": meta['owner']['username'],
            "createdAt": meta['created'],
            "updatedAt": meta['updated']
        }

    def _read_cvat_anno(self, cvat_frame_anno: Union[ProjectData.Frame, CommonData.Frame], labels: list):
        categories = self.categories()
        label_cat = categories[dm.AnnotationType.label]
        def map_label(name, parent=''): return label_cat.find(name, parent)[0]
        label_attrs = {
            label.get('parent', '') + label['name']: label['attributes']
            for _, label in labels
        }

        return self.convert_annotations(cvat_frame_anno, label_attrs, map_label)


class CvatDataExtractorBase(CVATDataExtractorMixin):
    def __init__(
        self,
        instance_data: CommonData,
        *,
        include_images: bool = False,
        format_type: str = None,
        dimension: DimensionType = DimensionType.DIM_2D,
        **kwargs
    ):
        instance_meta = instance_data.meta[instance_data.META_FIELD]
        CVATDataExtractorMixin.__init__(self, **kwargs)

        self._user = self._load_user_info(instance_meta) if dimension == DimensionType.DIM_3D else {}
        self._dimension = dimension
        self._format_type = format_type
        self._include_images = include_images
        self._instance_data = instance_data
        self._instance_meta = instance_meta

        if isinstance(instance_data, TaskData):
            db_tasks = [instance_data.db_instance]
        elif isinstance(instance_data, JobData):
            db_tasks = [instance_data.db_instance.segment.task]
        elif isinstance(instance_data, ProjectData):
            db_tasks = instance_data.tasks
        else:
            assert False

        if self._dimension == DimensionType.DIM_3D or include_images:
            self._media_provider = MEDIA_PROVIDERS_BY_DIMENSION[self._dimension](
                {
                    task.id: MediaSource(task)
                    for task in db_tasks
                }
            )

        self._ext_per_task: dict[int, str] = {
            task.id: TaskFrameProvider.VIDEO_FRAME_EXT if is_video else ''
            for task in db_tasks
            for is_video in [task.mode == 'interpolation']
        }

    def _process_one_frame_data(self, frame_data: CommonData.Frame | ProjectData.Frame) -> dm.DatasetItem:
        dm_media_args = {
            'path': frame_data.name + self._ext_per_task[frame_data.task_id],
            'ext': self._ext_per_task[frame_data.task_id] or frame_data.name.rsplit(osp.extsep, maxsplit=1)[1],
        }
        if self._dimension == DimensionType.DIM_3D:
            dm_media: dm.PointCloud = self._media_provider.get_media_for_frame(
                frame_data.task_id, frame_data.id, **dm_media_args
            )

            if not self._include_images:
                dm_media_args["extra_images"] = [
                    dm.Image.from_file(path=osp.basename(image.path))
                    for image in dm_media.extra_images
                ]
                dm_media = dm.PointCloud.from_file(**dm_media_args)
        else:
            dm_media_args['size'] = (frame_data.height, frame_data.width)
            if self._include_images:
                dm_media: dm.Image = self._media_provider.get_media_for_frame(
                    frame_data.task_id, frame_data.idx, **dm_media_args
                )
            else:
                dm_media = dm.Image.from_file(**dm_media_args)

        dm_anno = partial(self._read_cvat_anno, frame_data, self._instance_meta['labels'])

        dm_attributes = {'frame': frame_data.frame}

        if self._dimension == DimensionType.DIM_2D:
            dm_item = dm.DatasetItem(
                id=osp.splitext(frame_data.name)[0],
                subset=frame_data.subset,
                annotations=dm_anno,
                media=dm_media,
                attributes=dm_attributes,
            )
        elif self._dimension == DimensionType.DIM_3D:
            if self._format_type == "sly_pointcloud":
                dm_attributes["name"] = self._user["name"]
                dm_attributes["createdAt"] = self._user["createdAt"]
                dm_attributes["updatedAt"] = self._user["updatedAt"]
                dm_attributes["labels"] = []
                for (idx, (_, label)) in enumerate(self._instance_meta['labels']):
                    dm_attributes["labels"].append({"label_id": idx, "name": label["name"], "color": label["color"], "type": label["type"]})
                    dm_attributes["track_id"] = -1

            dm_item = dm.DatasetItem(
                id=osp.splitext(osp.split(frame_data.name)[-1])[0],
                subset=frame_data.subset,
                annotations=dm_anno,
                media=dm_media,
                attributes=dm_attributes,
            )

        return dm_item


class CvatTaskOrJobDataExtractor(StreamingSubsetBase, CvatDataExtractorBase):
    def __init__(self, *args, **kwargs):
        CvatDataExtractorBase.__init__(self, *args, **kwargs)
        StreamingSubsetBase.__init__(
            self,
            media_type=dm.Image if self._dimension == DimensionType.DIM_2D else dm.PointCloud,
            subset=self._instance_meta['subset'],
        )
        self._categories = self.load_categories(self._instance_meta['labels'])

        self._grouped_by_frame = list(self._instance_data.group_by_frame(include_empty=True))

    @staticmethod
    def copy_frame_data_with_replaced_lazy_lists(frame_data: CommonData.Frame) -> CommonData.Frame:
        return frame_data._replace(
            labeled_shapes=[
                (
                    shape._replace(points=shape.points.lazy_copy())
                    if isinstance(shape.points, LazyList) and not shape.points.is_parsed
                    else shape
                )
                for shape in frame_data.labeled_shapes
            ]
        )

    def __iter__(self):
        for frame_data in self._grouped_by_frame:
            # do not keep parsed lazy list data after this iteration
            frame_data = self.copy_frame_data_with_replaced_lazy_lists(frame_data)
            yield self._process_one_frame_data(frame_data)

    def _read_cvat_anno(self, cvat_frame_anno: CommonData.Frame, labels: list):
        categories = self.categories()
        label_cat = categories[dm.AnnotationType.label]
        def map_label(name, parent=''): return label_cat.find(name, parent)[0]
        label_attrs = {
            label.get('parent', '') + label['name']: label['attributes']
            for _, label in labels
        }

        return self.convert_annotations(cvat_frame_anno,
            label_attrs, map_label, self._format_type, self._dimension)

    def __len__(self):
        return len(self._instance_data)

    def categories(self):
        return self._categories


class CVATProjectDataExtractor(StreamingDatasetBase, CvatDataExtractorBase):
    def __init__(self, *args, **kwargs):
        CvatDataExtractorBase.__init__(self, *args, **kwargs)

        self._frame_data_by_subset: Dict[str, list[ProjectData.Frame]] = {}
        for frame_data in self._instance_data.group_by_frame(include_empty=True):
            if frame_data.subset not in self._frame_data_by_subset:
                self._frame_data_by_subset[frame_data.subset] = []
            self._frame_data_by_subset[frame_data.subset].append(frame_data)

        StreamingDatasetBase.__init__(
            self,
            length=sum(len(v) for v in self._frame_data_by_subset.values()),
            subsets=list(self._frame_data_by_subset.keys()),
            media_type=dm.Image if self._dimension == DimensionType.DIM_2D else dm.PointCloud,
        )
        self._categories = self.load_categories(self._instance_meta['labels'])

    @staticmethod
    def copy_frame_data_with_replaced_lazy_lists(frame_data: ProjectData.Frame) -> ProjectData.Frame:
        return attr.evolve(
            frame_data,
            labeled_shapes=[
                (
                    attr.evolve(shape, points=shape.points.lazy_copy())
                    if isinstance(shape.points, LazyList) and not shape.points.is_parsed
                    else shape
                )
                for shape in frame_data.labeled_shapes
            ],
        )

    def get_subset(self, name) -> IDataset:
        class Subset(StreamingSubsetBase):
            def __iter__(_):
                for frame_data in self._frame_data_by_subset[name]:
                    # do not keep parsed lazy list data after this iteration
                    frame_data = self.copy_frame_data_with_replaced_lazy_lists(frame_data)
                    yield self._process_one_frame_data(frame_data)

            def __len__(_):
                return len(self._frame_data_by_subset[name])

            def categories(_):
                return self.categories()

        return Subset(
            subset=name,
            media_type=dm.Image if self._dimension == DimensionType.DIM_2D else dm.PointCloud,
        )

    def categories(self):
        return self._categories


def GetCVATDataExtractor(
    instance_data: Union[ProjectData, CommonData],
    include_images: bool = False,
    format_type: str = None,
    dimension: DimensionType = DimensionType.DIM_2D,
    **kwargs
):
    kwargs.update({
        'include_images': include_images,
        'format_type': format_type,
        'dimension': dimension,
    })
    if isinstance(instance_data, ProjectData):
        return CVATProjectDataExtractor(instance_data, **kwargs)
    else:
        return CvatTaskOrJobDataExtractor(instance_data, **kwargs)


class CvatImportError(Exception):
    pass


@attrs
class CvatDatasetNotFoundError(CvatImportError):
    message: str = ""
    reason: str = ""
    format_name: str = ""
    _docs_base_url = f"{settings.CVAT_DOCS_URL}/manual/advanced/formats/"

    def __str__(self) -> str:
        formatted_format_name = self._format_name_for_docs()
        docs_message = self._docs_message(formatted_format_name)
        display_message = self._clean_display_message()
        return f"{docs_message}. {display_message}"

    def _format_name_for_docs(self) -> str:
        return self.format_name.replace("_", "-")

    def _docs_message(self, formatted_format_name: str) -> str:
        return f"Check [format docs]({self._docs_base_url}format-{formatted_format_name})"

    def _clean_display_message(self) -> str:
        message = re.sub(r'^.*?:', "", self.message)
        if "dataset must contain a file matching pattern" in message:
            message = message.replace("dataset must contain a file matching pattern", "")
            message = message.replace("\n", "")
            message = "Dataset must contain a file:" + message
        return re.sub(r' +', " ", message)

def mangle_image_name(name: str, subset: str, names: defaultdict[tuple[str, str], int]) -> str:
    name, ext = name.rsplit(osp.extsep, maxsplit=1)

    if not names[(subset, name)]:
        names[(subset, name)] += 1
        return osp.extsep.join([name, ext])
    else:
        image_name = f"{name}_{names[(subset, name)]}"
        if not names[(subset, image_name)]:
            names[(subset, name)] += 1
            return osp.extsep.join([image_name, ext])
        else:
            i = 1
            while i < sys.maxsize:
                new_image_name = f"{image_name}_{i}"
                if not names[(subset, new_image_name)]:
                    names[(subset, name)] += 1
                    return osp.extsep.join([new_image_name, ext])
                i += 1
    raise Exception('Cannot mangle image name')

def get_defaulted_subset(subset: str, subsets: list[str]) -> str:
    if subset:
        return subset
    else:
        if dm.DEFAULT_SUBSET_NAME not in subsets:
            return dm.DEFAULT_SUBSET_NAME
        else:
            i = 1
            while i < sys.maxsize:
                if f'{dm.DEFAULT_SUBSET_NAME}_{i}' not in subsets:
                    return f'{dm.DEFAULT_SUBSET_NAME}_{i}'
                i += 1
            raise Exception('Cannot find default name for subset')


class CvatToDmAnnotationConverter:
    def __init__(self,
        cvat_frame_anno: CommonData.Frame,
        label_attrs,
        map_label,
        format_name=None,
        dimension: DimensionType = DimensionType.DIM_2D
    ) -> None:
        self.cvat_frame_anno = cvat_frame_anno
        self.label_attrs = label_attrs
        self.map_label = map_label
        self.format_name = format_name
        self.dimension = dimension
        self.item_anno = []
        self.num_of_tracks = None

    def _convert_attrs(self, label: CommonData.Label, cvat_attrs: CommonData.Attribute):
        cvat_attrs = {a.name: a.value for a in cvat_attrs}

        dm_attr = dict()
        for _, a_desc in self.label_attrs[label]:
            a_name = a_desc['name']

            a_value = cvat_attrs.get(a_name, a_desc['default_value'])
            try:
                if a_desc['input_type'] == AttributeType.NUMBER:
                    a_value = float(a_value)
                elif a_desc['input_type'] == AttributeType.CHECKBOX:
                    a_value = (a_value.lower() == 'true')
                dm_attr[a_name] = a_value
            except Exception as e:
                raise Exception(
                    "Failed to convert attribute '%s'='%s': %s" %
                    (a_name, a_value, e))

        return dm_attr

    def _convert_tag(self, tag: CommonData.Tag) -> Iterable[dm.Annotation]:
        anno_group = tag.group or 0
        anno_label = self.map_label(tag.label)
        anno_attr = self._convert_attrs(tag.label, tag.attributes)
        return [dm.Label(label=anno_label, attributes=anno_attr, group=anno_group)]

    def _convert_tags(self, tags) -> Iterable[dm.Annotation]:
        return reduce(add, map(self._convert_tag, tags), [])

    def _convert_shape(self,
        shape: CommonData.LabeledShape, *, index: int
    ) -> Iterable[dm.Annotation]:
        dm_group = shape.group or 0
        dm_label = self.map_label(shape.label)

        dm_attr = self._convert_attrs(shape.label, shape.attributes)
        dm_attr['occluded'] = shape.occluded

        if shape.type == ShapeType.RECTANGLE:
            dm_attr['rotation'] = shape.rotation

        if hasattr(shape, 'track_id'):
            dm_attr['track_id'] = shape.track_id
            dm_attr['keyframe'] = shape.keyframe

        dm_points = shape.points

        anno = None

        if shape.type == ShapeType.POINTS:
            anno = dm.Points(dm_points,
                label=dm_label, attributes=dm_attr, group=dm_group,
                z_order=shape.z_order)
        elif shape.type == ShapeType.ELLIPSE:
            # TODO: for now Datumaro does not support ellipses
            # so, we convert an ellipse to RLE mask here
            # instead of applying transformation in directly in formats
            anno = EllipsesToMasks.convert_ellipse(SimpleNamespace(**{
                "points": shape.points,
                "label": dm_label,
                "z_order": shape.z_order,
                "rotation": shape.rotation,
                "group": dm_group,
                "attributes": dm_attr,
            }), self.cvat_frame_anno.height, self.cvat_frame_anno.width)
        elif shape.type == ShapeType.MASK:
            anno = MaskConverter.cvat_rle_to_dm_rle(SimpleNamespace(**{
                "points": shape.points,
                "label": dm_label,
                "z_order": shape.z_order,
                "rotation": shape.rotation,
                "group": dm_group,
                "attributes": dm_attr,
            }), self.cvat_frame_anno.height, self.cvat_frame_anno.width)
        elif shape.type == ShapeType.POLYLINE:
            anno = dm.PolyLine(dm_points,
                label=dm_label, attributes=dm_attr, group=dm_group,
                z_order=shape.z_order)
        elif shape.type == ShapeType.POLYGON:
            anno = dm.Polygon(dm_points,
                label=dm_label, attributes=dm_attr, group=dm_group,
                z_order=shape.z_order)
        elif shape.type == ShapeType.RECTANGLE:
            x0, y0, x1, y1 = dm_points
            anno = dm.Bbox(x0, y0, x1 - x0, y1 - y0,
                label=dm_label, attributes=dm_attr, group=dm_group,
                z_order=shape.z_order)
        elif shape.type == ShapeType.CUBOID:
            if self.dimension == DimensionType.DIM_3D:
                anno_id = getattr(shape, 'track_id', None)
                if anno_id is None:
                    anno_id = self.num_of_tracks + index
                position, rotation, scale = dm_points[0:3], dm_points[3:6], dm_points[6:9]
                anno = dm.Cuboid3d(
                    id=anno_id, position=position, rotation=rotation, scale=scale,
                    label=dm_label, attributes=dm_attr, group=dm_group
                )
        elif shape.type == ShapeType.SKELETON:
            elements = []
            for element in shape.elements:
                element_attr = self._convert_attrs(
                    shape.label + element.label, element.attributes)

                if hasattr(element, 'track_id'):
                    element_attr['track_id'] = element.track_id
                    element_attr['keyframe'] = element.keyframe

                element_vis = dm.Points.Visibility.visible
                if element.outside:
                    element_vis = dm.Points.Visibility.absent
                elif element.occluded:
                    element_vis = dm.Points.Visibility.hidden

                elements.append(dm.Points(element.points, [element_vis],
                    label=self.map_label(element.label, shape.label),
                    attributes=element_attr))

            dm_attr["keyframe"] = any([element.attributes.get("keyframe") for element in elements])
            anno = dm.Skeleton(elements, label=dm_label,
                attributes=dm_attr, group=dm_group, z_order=shape.z_order)
        else:
            raise Exception("Unknown shape type '%s'" % shape.type)

        results = []

        if anno:
            results.append(anno)

        return results

    def _convert_shapes(self, shapes: list[CommonData.LabeledShape]) -> Iterable[dm.Annotation]:
        dm_anno = []

        self.num_of_tracks = reduce(
            lambda a, x: a + (1 if getattr(x, 'track_id', None) is not None else 0),
            shapes,
            0
        )

        for index, shape in enumerate(shapes):
            dm_anno.extend(self._convert_shape(shape, index=index))

        return dm_anno

    def convert(self) -> list[dm.Annotation]:
        dm_anno = []
        dm_anno.extend(self._convert_tags(self.cvat_frame_anno.tags))
        dm_anno.extend(self._convert_shapes(self.cvat_frame_anno.labeled_shapes))
        return dm_anno


def convert_cvat_anno_to_dm(
    cvat_frame_anno,
    label_attrs,
    map_label,
    format_name=None,
    dimension=DimensionType.DIM_2D
) -> list[dm.Annotation]:
    converter = CvatToDmAnnotationConverter(
        cvat_frame_anno=cvat_frame_anno,
        label_attrs=label_attrs,
        map_label=map_label,
        format_name=format_name,
        dimension=dimension
    )
    return converter.convert()


def match_dm_item(
    item: dm.DatasetItem,
    instance_data: Union[ProjectData, CommonData],
    root_hint: Optional[str] = None
) -> int:
    is_video = instance_data.meta[instance_data.META_FIELD]['mode'] == 'interpolation'

    frame_number = None
    if frame_number is None and isinstance(item.media, dm.Image):
        frame_number = instance_data.match_frame(item.id + item.media.ext, root_hint)
    if frame_number is None:
        frame_number = instance_data.match_frame(item.id, root_hint, path_has_ext=False)
    if frame_number is None:
        frame_number = datumaro.util.cast(item.attributes.get('frame', item.id), int)
    if frame_number is None and is_video:
        frame_number = datumaro.util.cast(osp.basename(item.id)[len('frame_'):], int)

    if not frame_number in instance_data.frame_info:
        raise CvatImportError("Could not match item id: "
            "'%s' with any task frame" % item.id)
    return frame_number

def find_dataset_root(
    dm_dataset: dm.IDataset, instance_data: Union[ProjectData, CommonData]
) -> Optional[str]:
    longest_path_item = max(dm_dataset, key=lambda item: len(Path(item.id).parts), default=None)
    if longest_path_item is None:
        return None
    longest_path = longest_path_item.id

    matched_frame_number = instance_data.match_frame_fuzzy(longest_path, path_has_ext=False)
    if matched_frame_number is None:
        return None

    longest_match = osp.dirname(instance_data.frame_info[matched_frame_number]['path'])
    prefix = longest_match[:-len(osp.dirname(longest_path)) or None]
    if prefix.endswith('/'):
        prefix = prefix[:-1]

    return prefix

def import_dm_annotations(dm_dataset: dm.Dataset, instance_data: Union[ProjectData, CommonData]):
    if len(dm_dataset) == 0:
        return

    if isinstance(instance_data, ProjectData):
        for sub_dataset, task_data in instance_data.split_dataset(dm_dataset):
            # FIXME: temporary workaround for cvat format
            # will be removed after migration importer to datumaro
            sub_dataset._format = dm_dataset.format
            import_dm_annotations(sub_dataset, task_data)
        return

    shapes = {
        dm.AnnotationType.bbox: ShapeType.RECTANGLE,
        dm.AnnotationType.polygon: ShapeType.POLYGON,
        dm.AnnotationType.polyline: ShapeType.POLYLINE,
        dm.AnnotationType.points: ShapeType.POINTS,
        dm.AnnotationType.cuboid_3d: ShapeType.CUBOID,
        dm.AnnotationType.skeleton: ShapeType.SKELETON,
        dm.AnnotationType.mask: ShapeType.MASK
    }

    sources = {'auto', 'semi-auto', 'manual', 'file', 'consensus'}

    track_formats = [
        'cvat',
        'datumaro',
        'sly_pointcloud',
        'coco',
        'coco_instances',
        'coco_person_keypoints',
        'voc',
        'yolo_ultralytics_detection',
        'yolo_ultralytics_segmentation',
        'yolo_ultralytics_oriented_boxes',
        'yolo_ultralytics_pose',
    ]

    label_cat = dm_dataset.categories()[dm.AnnotationType.label]

    root_hint = find_dataset_root(dm_dataset, instance_data)

    tracks = {}

    for item in dm_dataset:
        frame_number = instance_data.abs_frame_id(
            match_dm_item(item, instance_data, root_hint=root_hint))

        if (isinstance(instance_data.db_instance, Job)
            and instance_data.db_instance.type == JobType.GROUND_TRUTH
            and frame_number not in instance_data.db_instance.segment.frame_set
        ):
            # Given there is a dataset with annotated frames,
            # it would be very hard to create annotations with frame skips for users,
            # so we just skip such annotations. We still need to match the frames.
            continue

        # do not store one-item groups
        group_map = {0: 0}
        group_size = {0: 0}
        for ann in item.annotations:
            if ann.type in shapes:
                group = group_map.get(ann.group)
                if group is None:
                    group = len(group_map)
                    group_map[ann.group] = group
                    group_size[ann.group] = 1
                else:
                    group_size[ann.group] += 1
        group_map = {g: s for g, s in group_size.items()
            if 1 < s and group_map[g]}
        group_map = {g: i for i, g in enumerate([0] + sorted(group_map))}

        for idx, ann in enumerate(item.annotations):
            try:
                if hasattr(ann, 'label') and ann.label is None:
                    raise CvatImportError("annotation has no label")

                attributes = [
                    instance_data.Attribute(name=n, value=str(v))
                    for n, v in ann.attributes.items()
                ]

                points = []
                if ann.type in shapes:
                    points = []
                    if ann.type == dm.AnnotationType.cuboid_3d:
                        points = (*ann.position, *ann.rotation, *ann.scale, 0, 0, 0, 0, 0, 0, 0)
                    elif ann.type == dm.AnnotationType.mask:
                        points = tuple(MaskConverter.dm_mask_to_cvat_rle(ann))
                    elif ann.type != dm.AnnotationType.skeleton:
                        points = tuple(ann.points)

                    rotation = ann.attributes.pop('rotation', 0.0)
                    # Use safe casting to bool instead of plain reading
                    # because in some formats return type can be different
                    # from bool / None
                    # https://github.com/openvinotoolkit/datumaro/issues/719
                    occluded = datumaro.util.cast(
                        ann.attributes.pop('occluded', None), to_bool
                    ) is True
                    keyframe = datumaro.util.cast(
                        ann.attributes.get('keyframe', None), to_bool
                    ) is True
                    outside = datumaro.util.cast(
                        ann.attributes.pop('outside', None), to_bool
                    ) is True

                    track_id = ann.attributes.pop('track_id', None)
                    source = ann.attributes.pop('source').lower() \
                        if ann.attributes.get('source', '').lower() in sources else 'manual'

                    shape_type = shapes[ann.type]
                    if track_id is None or 'keyframe' not in ann.attributes or dm_dataset.format not in track_formats:
                        elements = []
                        if ann.type == dm.AnnotationType.skeleton:
                            for element in ann.elements:
                                element_attributes = [
                                    instance_data.Attribute(name=n, value=str(v))
                                    for n, v in element.attributes.items()
                                ]
                                element_occluded = element.visibility[0] == dm.Points.Visibility.hidden
                                element_outside = element.visibility[0] == dm.Points.Visibility.absent
                                element_source = element.attributes.pop('source').lower() \
                                    if element.attributes.get('source', '').lower() in sources else 'manual'
                                elements.append(instance_data.LabeledShape(
                                    type=shapes[element.type],
                                    frame=frame_number,
                                    points=element.points,
                                    label=label_cat.items[element.label].name,
                                    occluded=element_occluded,
                                    z_order=ann.z_order,
                                    group=group_map.get(ann.group, 0),
                                    source=element_source,
                                    attributes=element_attributes,
                                    elements=[],
                                    outside=element_outside,
                                ))
                        instance_data.add_shape(instance_data.LabeledShape(
                            type=shape_type,
                            frame=frame_number,
                            points=points,
                            label=label_cat.items[ann.label].name,
                            occluded=occluded,
                            z_order=ann.z_order if ann.type != dm.AnnotationType.cuboid_3d else 0,
                            group=group_map.get(ann.group, 0),
                            source=source,
                            rotation=rotation,
                            attributes=attributes,
                            elements=elements,
                        ))
                        continue

                    if dm_dataset.format in track_formats:
                        if track_id not in tracks:
                            tracks[track_id] = {
                                'label': label_cat.items[ann.label].name,
                                'group': group_map.get(ann.group, 0),
                                'source': source,
                                'shapes': [],
                                'elements':{},
                            }

                        track = instance_data.TrackedShape(
                            type=shapes[ann.type],
                            frame=frame_number,
                            occluded=occluded,
                            outside=outside,
                            keyframe=keyframe,
                            points=points,
                            z_order=ann.z_order if ann.type != dm.AnnotationType.cuboid_3d else 0,
                            source=source,
                            rotation=rotation,
                            attributes=attributes,
                        )

                        tracks[track_id]['shapes'].append(track)

                        if ann.type == dm.AnnotationType.skeleton:
                            for element in ann.elements:
                                element_occluded = element.visibility[0] == dm.Points.Visibility.hidden
                                element_outside = element.visibility[0] == dm.Points.Visibility.absent

                                if element.label not in tracks[track_id]['elements']:
                                    tracks[track_id]['elements'][element.label] = instance_data.Track(
                                        label=label_cat.items[element.label].name,
                                        group=0,
                                        source=source,
                                        shapes=[],
                                    )

                                element_attributes = [
                                    instance_data.Attribute(name=n, value=str(v))
                                    for n, v in element.attributes.items()
                                ]
                                element_source = element.attributes.pop('source').lower() \
                                    if element.attributes.get('source', '').lower() in sources else 'manual'

                                tracks[track_id]['elements'][element.label].shapes.append(instance_data.TrackedShape(
                                    type=shapes[element.type],
                                    frame=frame_number,
                                    occluded=element_occluded,
                                    outside=element_outside,
                                    keyframe=keyframe,
                                    points=element.points,
                                    z_order=element.z_order,
                                    source=element_source,
                                    attributes=element_attributes,
                                ))

                elif ann.type == dm.AnnotationType.label:
                    instance_data.add_tag(instance_data.Tag(
                        frame=frame_number,
                        label=label_cat.items[ann.label].name,
                        group=group_map.get(ann.group, 0),
                        source='manual',
                        attributes=attributes,
                    ))
            except Exception as e:
                raise CvatImportError("Image {}: can't import annotation "
                    "#{} ({}): {}".format(item.id, idx, ann.type.name, e)) from e

    def _validate_track_shapes(shapes):
        shapes = sorted(shapes, key=lambda t: t.frame)
        new_shapes = []
        prev_shape = None
        # infer the keyframe shapes and keep only them
        for shape in shapes:
            prev_is_visible = prev_shape and not prev_shape.outside
            cur_is_visible = shape and not shape.outside

            has_gap = False
            if prev_is_visible:
                has_gap = prev_shape.frame + instance_data.frame_step < shape.frame

            if has_gap:
                prev_shape = prev_shape._replace(outside=True, keyframe=True,
                    frame=prev_shape.frame + instance_data.frame_step)
                new_shapes.append(prev_shape)

            if prev_is_visible != cur_is_visible or cur_is_visible and (has_gap or shape.keyframe):
                shape = shape._replace(keyframe=True)
                new_shapes.append(shape)

            prev_shape = shape

        if prev_shape and not prev_shape.outside and (
            prev_shape.frame + instance_data.frame_step <= stop_frame
            # has a gap before the current instance segment end
        ):
            prev_shape = prev_shape._replace(outside=True, keyframe=True,
                frame=prev_shape.frame + instance_data.frame_step)
            new_shapes.append(prev_shape)

        return new_shapes

    stop_frame = int(instance_data.meta[instance_data.META_FIELD]['stop_frame'])
    for track_id, track in tracks.items():
        track['shapes'] = _validate_track_shapes(track['shapes'])

        if ann.type == dm.AnnotationType.skeleton:
            new_elements = {}
            for element_id, element in track['elements'].items():
                new_element_shapes = _validate_track_shapes(element.shapes)
                new_elements[element_id] = element._replace(shapes=new_element_shapes)
            track['elements'] = new_elements

        if track['shapes'] or track['elements']:
            track['elements'] = list(track['elements'].values())
            instance_data.add_track(instance_data.Track(**track))

def import_labels_to_project(project_annotation, dataset: dm.Dataset):
    labels = []
    label_colors = []
    for label in dataset.categories()[dm.AnnotationType.label].items:
        db_label = Label(
            name=label.name,
            color=get_label_color(label.name, label_colors),
            type="any"
        )
        labels.append(db_label)
        label_colors.append(db_label.color)
    project_annotation.add_labels(labels)

def load_dataset_data(project_annotation, dataset: dm.Dataset, project_data):
    if not project_annotation.db_project.label_set.count():
        import_labels_to_project(project_annotation, dataset)
    else:
        for label in dataset.categories()[dm.AnnotationType.label].items:
            if not project_annotation.db_project.label_set.filter(name=label.name).exists():
                if label.name == "background":
                    dataset.transform("remap_labels", mapping={"background": ""}, default="keep")
                    continue
                raise CvatImportError(f'Target project does not have label with name "{label.name}"')
    for subset_id, subset in enumerate(dataset.subsets().values()):
        job = rq.get_current_job()
        job_meta = ImportRQMeta.for_job(job)
        job_meta.status = 'Task from dataset is being created...'
        job_meta.progress = (subset_id + (job_meta.task_progress or 0.)) / len(dataset.subsets().keys())
        job_meta.save()

        task_fields = {
            'project': project_annotation.db_project,
            'name': subset.name,
            'owner': project_annotation.db_project.owner,
            'subset': subset.name,
            'organization': project_annotation.db_project.organization,
        }

        subset_dataset = subset.as_dataset()

        dataset_files = {
            'media': [],
            'data_root': dataset.data_path + osp.sep,
        }

        root_paths = set()
        for dataset_item in subset_dataset:
            if isinstance(dataset_item.media, dm.Image) and dataset_item.media.has_data:
                dataset_files['media'].append(dataset_item.media.path)
                data_root = dataset_item.media.path.rsplit(dataset_item.id, 1)
                if len(data_root) == 2:
                    root_paths.add(data_root[0])
            elif isinstance(dataset_item.media, dm.PointCloud):
                dataset_files['media'].append(dataset_item.media)
                data_root = dataset_item.media.path.rsplit(dataset_item.id, 1)
                if len(data_root) == 2:
                    root_paths.add(data_root[0])

                if isinstance(dataset_item.media.extra_images, list):
                    dataset_files['media'] += \
                        list(map(lambda ri: ri.path, dataset_item.media.extra_images))

        if len(root_paths):
            dataset_files['data_root'] = osp.commonpath(root_paths) + osp.sep

        project_annotation.add_task(task_fields, dataset_files, project_data)

class NoMediaInAnnotationFileError(CvatImportError):
    def __str__(self) -> str:
        return (
            "Can't import media data from the annotation file. "
            "Please upload full dataset as a zip archive."
        )

def detect_dataset(dataset_dir: str, format_name: str, importer: dm.Importer) -> None:
    not_found_error_instance = CvatDatasetNotFoundError()

    def _handle_rejection(format_name: str, reason: RejectionReason, human_message: str) -> None:
        not_found_error_instance.format_name = format_name
        not_found_error_instance.reason = reason
        not_found_error_instance.message = human_message

    detection_env = dm.Environment()
    detection_env.importers.items.clear()
    detection_env.importers.register(format_name, importer)
    detected = detection_env.detect_dataset(
        dataset_dir, depth=4, rejection_callback=_handle_rejection
    )

    if not detected and not_found_error_instance.reason != RejectionReason.detection_unsupported:
        raise not_found_error_instance


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\cron.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import os
import shutil
from abc import ABCMeta, abstractmethod
from datetime import timedelta
from pathlib import Path
from typing import ClassVar, Type

from django.conf import settings
from django.utils import timezone

from cvat.apps.dataset_manager.util import (
    CacheFileOrDirPathParseError,
    ExportCacheManager,
    TmpDirManager,
    get_export_cache_lock,
)
from cvat.apps.dataset_manager.views import (
    EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
    EXPORT_CACHE_LOCK_TTL,
    get_export_cache_ttl,
    log_exception,
)
from cvat.apps.engine.log import ServerLogManager

logger = ServerLogManager(__name__).glob


def clear_export_cache(file_path: Path) -> bool:
    with get_export_cache_lock(
        file_path,
        block=True,
        acquire_timeout=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
        ttl=EXPORT_CACHE_LOCK_TTL,
    ):
        parsed_filename = ExportCacheManager.parse_filename(file_path.name)
        cache_ttl = get_export_cache_ttl(parsed_filename.instance_type)

        if timezone.now().timestamp() <= file_path.stat().st_mtime + cache_ttl.total_seconds():
            logger.debug(f"Export cache file {file_path.name!r} was recently accessed")
            return False

        os.remove(file_path)
        logger.debug(f"Export cache file {file_path.name!r} was successfully removed")
        return True


class BaseCleaner(metaclass=ABCMeta):
    task_description: ClassVar[str]

    def __init__(self) -> None:
        self._number_of_removed_objects = 0

    @property
    def number_of_removed_objects(self) -> int:
        return self._number_of_removed_objects

    @abstractmethod
    def do_cleanup(self):
        pass


class TmpDirectoryCleaner(BaseCleaner):
    task_description: ClassVar[str] = "common temporary directory cleanup"

    def do_cleanup(self) -> None:
        # we do not use locks here when handling objects from tmp directory
        # because undesired race conditions are not possible here:
        # 1. A temporary file/directory can be removed while checking access time.
        #    In that case an exception is expected and is handled by the cron process.
        # 2. A temporary file/directory can be removed by the cron job only when it is outdated.
        # 3. Each temporary file/directory has a unique name, so the race condition when one process is creating an object
        #    and another is removing it - impossible.
        for child in os.scandir(TmpDirManager.TMP_ROOT):
            try:
                if (
                    child.stat().st_atime
                    + timedelta(days=TmpDirManager.TMP_FILE_OR_DIR_RETENTION_DAYS).total_seconds()
                    < timezone.now().timestamp()
                ):
                    if child.is_dir():
                        shutil.rmtree(child.path)
                    else:
                        os.remove(child.path)
                    logger.debug(f"The {child.name} was successfully removed")
                    self._number_of_removed_objects += 1
            except FileNotFoundError:
                # file or directory has been removed by another process
                continue
            except Exception:
                log_exception(logger)


class ExportCacheDirectoryCleaner(BaseCleaner):
    task_description: ClassVar[str] = "export cache directory cleanup"

    def do_cleanup(self) -> None:
        export_cache_dir_path = settings.EXPORT_CACHE_ROOT
        assert os.path.exists(export_cache_dir_path)

        for child in os.scandir(export_cache_dir_path):
            # export cache directory is expected to contain only files
            if not child.is_file():
                logger.warning(f"The {child.name} is not a file, skipping...")
                continue

            try:
                if clear_export_cache(child):
                    self._number_of_removed_objects += 1
            except CacheFileOrDirPathParseError:
                logger.warning(f"Cannot parse {child.name}, skipping...")
                continue

            except Exception:
                log_exception(logger)


def cleanup(CleanerClass: Type[ExportCacheDirectoryCleaner | TmpDirectoryCleaner]) -> None:
    assert issubclass(CleanerClass, BaseCleaner)
    started_at = timezone.now()

    cleaner = CleanerClass()
    cleaner.do_cleanup()

    finished_at = timezone.now()
    logger.info(
        f"The {cleaner.task_description!r} process has been successfully "
        f"completed after {int((finished_at - started_at).total_seconds())} seconds. "
        f"{cleaner.number_of_removed_objects} elements have been removed"
    )


def cleanup_export_cache_directory() -> None:
    cleanup(ExportCacheDirectoryCleaner)


def cleanup_tmp_directory() -> None:
    cleanup(TmpDirectoryCleaner)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\project.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from collections.abc import Mapping
from contextlib import nullcontext
from typing import Any, Callable

import rq
from datumaro.components.errors import DatasetError, DatasetImportError, DatasetNotFoundError
from django.conf import settings
from django.db import transaction

from cvat.apps.dataset_manager.task import TaskAnnotation
from cvat.apps.dataset_manager.util import TmpDirManager
from cvat.apps.engine import models
from cvat.apps.engine.log import DatasetLogManager
from cvat.apps.engine.model_utils import bulk_create
from cvat.apps.engine.rq import ImportRQMeta
from cvat.apps.engine.serializers import DataSerializer, TaskWriteSerializer
from cvat.apps.engine.task import _create_thread as create_task

from .annotation import AnnotationIR
from .bindings import CvatDatasetNotFoundError, CvatImportError, ProjectData, load_dataset_data
from .formats.registry import make_exporter, make_importer

dlogger = DatasetLogManager()

def export_project(
    project_id: int,
    dst_file: str,
    *,
    format_name: str,
    server_url: str | None = None,
    save_images: bool = False,
    temp_dir: str | None = None,
):
    # For big tasks dump function may run for a long time and
    # we dont need to acquire lock after the task has been initialized from DB.
    # But there is the bug with corrupted dump file in case 2 or
    # more dump request received at the same time:
    # https://github.com/cvat-ai/cvat/issues/217
    with transaction.atomic():
        project = ProjectAnnotationAndData(project_id)
        project.init_from_db()

    exporter = make_exporter(format_name)
    with open(dst_file, 'wb') as f:
        project.export(f, exporter, host=server_url, save_images=save_images, temp_dir=temp_dir)

class ProjectAnnotationAndData:
    def __init__(self, pk: int):
        self.db_project = models.Project.objects.get(id=pk)
        self.db_tasks = models.Task.objects.filter(project__id=pk).exclude(data=None).order_by('id')

        self.task_annotations: dict[int, TaskAnnotation] = dict()
        self.annotation_irs: dict[int, AnnotationIR] = dict()

        self.tasks_to_add: list[models.Task] = []

    def reset(self):
        for annotation_ir in self.annotation_irs.values():
            annotation_ir.reset()

    def put(self, tasks_data: Mapping[int,Any]):
        for task_id, data in tasks_data.items():
            self.task_annotations[task_id].put(data)

    def create(self, tasks_data: Mapping[int,Any]):
        for task_id, data in tasks_data.items():
            self.task_annotations[task_id].create(data)

    def update(self, tasks_data: Mapping[int,Any]):
        for task_id, data in tasks_data.items():
            self.task_annotations[task_id].update(data)

    def delete(self, tasks_data: Mapping[int,Any]=None):
        if tasks_data is not None:
            for task_id, data in tasks_data.items():
                self.task_annotations[task_id].put(data)
        else:
            for task_annotation in self.task_annotations.values():
                task_annotation.delete()

    def add_task(self, task_fields: dict, files: dict, project_data: ProjectData = None):
        def split_name(file):
            _, name = file.split(files['data_root'])
            return name


        data_serializer = DataSerializer(data={
            "server_files": files['media'],
            #TODO: following fields should be replaced with proper input values from request in future
            "use_cache": False,
            "use_zip_chunks": True,
            "image_quality": 70,
        })
        data_serializer.is_valid(raise_exception=True)
        db_data = data_serializer.save()
        db_task = TaskWriteSerializer.create(None, {
            **task_fields,
            'data_id': db_data.id,
            'project_id': self.db_project.id
        })
        data = {k:v for k, v in data_serializer.data.items()}
        data['use_zip_chunks'] = data_serializer.validated_data['use_zip_chunks']
        data['use_cache'] = data_serializer.validated_data['use_cache']
        data['copy_data'] = data_serializer.validated_data['copy_data']
        data['server_files_path'] = files['data_root']
        data['stop_frame'] = None
        data['server_files'] = list(map(split_name, data['server_files']))

        create_task(db_task, data, is_dataset_import=True)
        self.db_tasks = models.Task.objects.filter(project__id=self.db_project.id).exclude(data=None).order_by('id')
        self._init_task_from_db(db_task.id)
        if project_data is not None:
            project_data.new_tasks.add(db_task.id)
            project_data.init()

    def add_labels(self, labels: list[models.Label], attributes: list[tuple[str, models.AttributeSpec]] = None):
        for label in labels:
            label.project = self.db_project
            # We need label_id here, so we can't use bulk_create here
            label.save()

        for label_name, attribute in attributes or []:
            label, = filter(lambda l: l.name == label_name, labels)
            attribute.label = label
        if attributes:
            bulk_create(models.AttributeSpec, [a[1] for a in attributes])

    def _init_task_from_db(self, task_id: int) -> None:
        annotation = TaskAnnotation(pk=task_id)
        annotation.init_from_db()
        self.task_annotations[task_id] = annotation
        self.annotation_irs[task_id] = annotation.ir_data

    def init_from_db(self):
        self.reset()

        for task in self.db_tasks:
            self._init_task_from_db(task.id)

    def export(
        self,
        dst_file: io.BufferedWriter,
        exporter: Callable[..., None],
        *,
        host: str = '',
        temp_dir: str | None = None,
        **options
    ):
        project_data = ProjectData(
            annotation_irs=self.annotation_irs,
            db_project=self.db_project,
            host=host
        )

        with (
            TmpDirManager.get_tmp_directory_for_export(
                instance_type=self.db_project.__class__.__name__,
            ) if not temp_dir else nullcontext(temp_dir)
        ) as temp_dir:
            exporter(dst_file, temp_dir, project_data, **options)

    def load_dataset_data(self, *args, **kwargs):
        load_dataset_data(self, *args, **kwargs)

    def import_dataset(self, dataset_file, importer, **options):
        project_data = ProjectData(
            annotation_irs=self.annotation_irs,
            db_project=self.db_project,
            task_annotations=self.task_annotations,
            project_annotation=self,
        )
        project_data.soft_attribute_import = True

        with TmpDirManager.get_tmp_directory() as temp_dir:
            try:
                importer(dataset_file, temp_dir, project_data, load_data_callback=self.load_dataset_data, **options)
            except (DatasetNotFoundError, CvatDatasetNotFoundError) as not_found:
                if settings.CVAT_LOG_IMPORT_ERRORS:
                    dlogger.log_import_error(
                        entity="project",
                        entity_id=self.db_project.id,
                        format_name=importer.DISPLAY_NAME,
                        base_error=str(not_found),
                        dir_path=temp_dir,
                    )

                raise not_found

        self.create({tid: ir.serialize() for tid, ir in self.annotation_irs.items() if tid in project_data.new_tasks})

    @property
    def data(self) -> dict:
        raise NotImplementedError()

@transaction.atomic
def import_dataset_as_project(src_file, project_id, format_name, conv_mask_to_poly):
    rq_job = rq.get_current_job()
    rq_job_meta = ImportRQMeta.for_job(rq_job)
    rq_job_meta.status = 'Dataset import has been started...'
    rq_job_meta.progress = 0.
    rq_job_meta.save()

    project = ProjectAnnotationAndData(project_id)

    importer = make_importer(format_name)
    with open(src_file, 'rb') as f:
        try:
            project.import_dataset(f, importer, conv_mask_to_poly=conv_mask_to_poly)
        except (DatasetError, DatasetImportError, DatasetNotFoundError) as ex:
            raise CvatImportError(str(ex))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\serializers.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework import serializers


class DatasetFormatSerializer(serializers.Serializer):
    name = serializers.CharField(max_length=64, source='DISPLAY_NAME')
    ext = serializers.CharField(max_length=64, source='EXT')
    version = serializers.CharField(max_length=64, source='VERSION')
    enabled = serializers.BooleanField(source='ENABLED')
    dimension = serializers.CharField(max_length=2, source='DIMENSION')

class DatasetFormatsSerializer(serializers.Serializer):
    importers = DatasetFormatSerializer(many=True)
    exporters = DatasetFormatSerializer(many=True)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\task.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import itertools
from collections import OrderedDict
from contextlib import nullcontext
from copy import deepcopy
from enum import Enum
from typing import Callable, Optional, Union

from datumaro.components.errors import DatasetError, DatasetImportError, DatasetNotFoundError
from django.conf import settings
from django.db import transaction
from django.db.models.query import Prefetch, QuerySet
from rest_framework.exceptions import ValidationError

from cvat.apps.dataset_manager.annotation import AnnotationIR, AnnotationManager
from cvat.apps.dataset_manager.bindings import (
    CvatDatasetNotFoundError,
    CvatImportError,
    JobData,
    TaskData,
)
from cvat.apps.dataset_manager.formats.registry import make_exporter, make_importer
from cvat.apps.dataset_manager.util import TmpDirManager, faster_deepcopy
from cvat.apps.engine import models, serializers
from cvat.apps.engine.log import DatasetLogManager
from cvat.apps.engine.model_utils import add_prefetch_fields, bulk_create, get_cached
from cvat.apps.engine.plugins import plugin_decorator
from cvat.apps.engine.utils import take_by
from cvat.apps.events.handlers import handle_annotations_change
from cvat.apps.profiler import silk_profile

dlogger = DatasetLogManager()

class dotdict(OrderedDict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = OrderedDict.get
    __setattr__ = OrderedDict.__setitem__
    __delattr__ = OrderedDict.__delitem__
    __eq__ = lambda self, other: self.id == other.id
    __hash__ = lambda self: self.id

class PatchAction(str, Enum):
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"

    @classmethod
    def values(cls):
        return [item.value for item in cls]

    def __str__(self):
        return self.value

def merge_table_rows(rows, keys_for_merge, field_id):
    # It is necessary to keep a stable order of original rows
    # (e.g. for tracked boxes). Otherwise prev_box.frame can be bigger
    # than next_box.frame.
    merged_rows = OrderedDict()

    # Group all rows by field_id. In grouped rows replace fields in
    # accordance with keys_for_merge structure.
    for row in rows:
        row_id = row[field_id]
        if not row_id in merged_rows:
            merged_rows[row_id] = dotdict(row)
            for key in keys_for_merge:
                merged_rows[row_id][key] = []

        for key in keys_for_merge:
            item = dotdict({v.split('__', 1)[-1]:row[v] for v in keys_for_merge[key]})
            if item.id is not None:
                merged_rows[row_id][key].append(item)

    # Remove redundant keys from final objects
    redundant_keys = [item for values in keys_for_merge.values() for item in values]
    for i in merged_rows:
        for j in redundant_keys:
            del merged_rows[i][j]

    return list(merged_rows.values())


class JobAnnotation:
    @classmethod
    def add_prefetch_info(cls, queryset: QuerySet[models.Job], prefetch_images: bool = True) -> QuerySet[models.Job]:
        assert issubclass(queryset.model, models.Job)

        label_qs = add_prefetch_fields(models.Label.objects.all(), [
            'skeleton',
            'parent',
            'attributespec_set',
        ])
        label_qs = JobData.add_prefetch_info(label_qs)

        task_data_queryset = models.Data.objects.all()
        if prefetch_images:
            task_data_queryset = task_data_queryset.select_related('video').prefetch_related(
                Prefetch('images', queryset=models.Image.objects.order_by('frame'))
            )

        return queryset.select_related(
            'segment',
            'segment__task',
        ).prefetch_related(
            'segment__task__project',
            'segment__task__owner',
            'segment__task__assignee',

            Prefetch('segment__task__data', queryset=task_data_queryset),

            Prefetch('segment__task__label_set', queryset=label_qs),
            Prefetch('segment__task__project__label_set', queryset=label_qs),
        )

    def __init__(
        self,
        pk,
        *,
        lock_job_in_db: bool = False,
        queryset: QuerySet | None = None,
        prefetch_images: bool = False,
        db_job: models.Job | None = None
    ):
        assert db_job is None or lock_job_in_db is False
        assert (db_job is None and queryset is None) or prefetch_images is False
        assert db_job is None or queryset is None
        if db_job is None:
            if queryset is None:
                queryset = self.add_prefetch_info(models.Job.objects, prefetch_images=prefetch_images)

            if lock_job_in_db:
                queryset = queryset.select_for_update()

            self.db_job: models.Job = get_cached(queryset, pk=int(pk))
        else:
            self.db_job: models.Job = db_job

        db_segment = self.db_job.segment
        self.start_frame = db_segment.start_frame
        self.stop_frame = db_segment.stop_frame
        self.ir_data = AnnotationIR(db_segment.task.dimension)

        self.db_labels = {db_label.id:db_label
            for db_label in (db_segment.task.project.label_set.all()
            if db_segment.task.project_id else db_segment.task.label_set.all())}

        self.db_attributes = {}
        for db_label in self.db_labels.values():
            self.db_attributes[db_label.id] = {
                "mutable": OrderedDict(),
                "immutable": OrderedDict(),
                "all": OrderedDict(),
            }
            for db_attr in db_label.attributespec_set.all():
                default_value = dotdict([
                    ('spec_id', db_attr.id),
                    ('value', db_attr.default_value),
                ])
                if db_attr.mutable:
                    self.db_attributes[db_label.id]["mutable"][db_attr.id] = default_value
                else:
                    self.db_attributes[db_label.id]["immutable"][db_attr.id] = default_value

                self.db_attributes[db_label.id]["all"][db_attr.id] = default_value

    def reset(self):
        self.ir_data.reset()

    def _validate_attribute_for_existence(self, db_attr_val, label_id, attr_type):
        if db_attr_val.spec_id not in self.db_attributes[label_id][attr_type]:
            raise ValidationError("spec_id `{}` is invalid".format(db_attr_val.spec_id))

    def _validate_label_for_existence(self, label_id):
        if label_id not in self.db_labels:
            raise ValidationError("label_id `{}` is invalid".format(label_id))

    def _add_missing_shape(self, track, first_shape):
        if first_shape["type"] == "skeleton":
            # in case with skeleton track we always expect to see one shape in track
            first_shape["frame"] = track["frame"]
        else:
            missing_shape = deepcopy(first_shape)
            missing_shape["frame"] = track["frame"]
            missing_shape["outside"] = True
            missing_shape.pop("id", None)
            track["shapes"].append(missing_shape)

    def _correct_frame_of_tracked_shapes(self, track):
        shapes = sorted(track["shapes"], key=lambda a: a["frame"])
        first_shape = shapes[0] if shapes else None

        if first_shape and track["frame"] < first_shape["frame"]:
            self._add_missing_shape(track, first_shape)
        elif first_shape and first_shape["frame"] < track["frame"]:
            track["frame"] = first_shape["frame"]

    def _sync_frames(self, tracks, parent_track):
        if not tracks:
            return

        min_frame = tracks[0]["frame"]

        for track in tracks:
            if parent_track and parent_track.frame < track["frame"]:
                track["frame"] = parent_track.frame

            # track and its first shape must have the same frame
            self._correct_frame_of_tracked_shapes(track)

            if track["frame"] < min_frame:
                min_frame = track["frame"]

        if not parent_track:
            return

        if min_frame < parent_track.frame:
            # parent track cannot have a frame greater than the frame of the child track
            parent_tracked_shape = parent_track.shapes.first()
            parent_track.frame = min_frame
            parent_tracked_shape.frame = min_frame

            parent_tracked_shape.save()
            parent_track.save()

            for track in tracks:
                if parent_track.frame < track["frame"]:
                    track["frame"] = parent_track.frame

                    self._correct_frame_of_tracked_shapes(track)

    def _save_tracks_to_db(self, tracks):

        def create_tracks(tracks, parent_track=None):
            db_tracks = []
            db_track_attr_vals = []
            db_shapes = []
            db_shape_attr_vals = []

            self._sync_frames(tracks, parent_track)

            for track in tracks:
                track_attributes = track.pop("attributes", [])
                shapes = track.pop("shapes")
                elements = track.pop("elements", [])
                db_track = models.LabeledTrack(job=self.db_job, parent=parent_track, **track)

                self._validate_label_for_existence(db_track.label_id)

                for attr in track_attributes:
                    db_attr_val = models.LabeledTrackAttributeVal(**attr, track_id=len(db_tracks))

                    self._validate_attribute_for_existence(db_attr_val, db_track.label_id, "immutable")

                    db_track_attr_vals.append(db_attr_val)

                for shape_idx, shape in enumerate(shapes):
                    shape_attributes = shape.pop("attributes", [])
                    db_shape = models.TrackedShape(**shape, track_id=len(db_tracks))

                    for attr in shape_attributes:
                        db_attr_val = models.TrackedShapeAttributeVal(**attr, shape_id=len(db_shapes))

                        self._validate_attribute_for_existence(db_attr_val, db_track.label_id, "mutable")

                        db_shape_attr_vals.append(db_attr_val)

                    db_shapes.append(db_shape)
                    shape["attributes"] = shape_attributes

                db_tracks.append(db_track)

                track["attributes"] = track_attributes
                track["shapes"] = shapes
                if elements or parent_track is None:
                    track["elements"] = elements

            db_tracks = bulk_create(models.LabeledTrack, db_tracks)

            for db_attr_val in db_track_attr_vals:
                db_attr_val.track_id = db_tracks[db_attr_val.track_id].id

            bulk_create(models.LabeledTrackAttributeVal, db_track_attr_vals)

            for db_shape in db_shapes:
                db_shape.track_id = db_tracks[db_shape.track_id].id

            db_shapes = bulk_create(models.TrackedShape, db_shapes)

            for db_attr_val in db_shape_attr_vals:
                db_attr_val.shape_id = db_shapes[db_attr_val.shape_id].id

            bulk_create(models.TrackedShapeAttributeVal, db_shape_attr_vals,)

            shape_idx = 0
            for track, db_track in zip(tracks, db_tracks):
                track["id"] = db_track.id
                for shape in track["shapes"]:
                    shape["id"] = db_shapes[shape_idx].id
                    shape_idx += 1
                create_tracks(track.get("elements", []), db_track)

        create_tracks(tracks)

        self.ir_data.tracks = tracks

    def _save_shapes_to_db(self, shapes):
        def create_shapes(shapes, parent_shape=None):
            db_shapes = []
            db_attr_vals = []

            for shape in shapes:
                attributes = shape.pop("attributes", [])
                shape_elements = shape.pop("elements", [])
                # FIXME: need to clamp points (be sure that all of them inside the image)
                # Should we check here or implement a validator?
                db_shape = models.LabeledShape(job=self.db_job, parent=parent_shape, **shape)

                self._validate_label_for_existence(db_shape.label_id)

                for attr in attributes:
                    db_attr_val = models.LabeledShapeAttributeVal(**attr, shape_id=len(db_shapes))

                    self._validate_attribute_for_existence(db_attr_val, db_shape.label_id, "all")

                    db_attr_vals.append(db_attr_val)

                db_shapes.append(db_shape)
                shape["attributes"] = attributes
                if shape_elements or parent_shape is None:
                    shape["elements"] = shape_elements

            db_shapes = bulk_create(models.LabeledShape, db_shapes)

            for db_attr_val in db_attr_vals:
                db_attr_val.shape_id = db_shapes[db_attr_val.shape_id].id

            bulk_create(models.LabeledShapeAttributeVal, db_attr_vals)

            for shape, db_shape in zip(shapes, db_shapes):
                shape["id"] = db_shape.id
                create_shapes(shape.get("elements", []), db_shape)

        create_shapes(shapes)

        self.ir_data.shapes = shapes

    def _save_tags_to_db(self, tags):
        db_tags = []
        db_attr_vals = []

        for tag in tags:
            attributes = tag.pop("attributes", [])
            db_tag = models.LabeledImage(job=self.db_job, **tag)

            self._validate_label_for_existence(db_tag.label_id)

            for attr in attributes:
                db_attr_val = models.LabeledImageAttributeVal(**attr)

                self._validate_attribute_for_existence(db_attr_val, db_tag.label_id, "all")

                db_attr_val.tag_id = len(db_tags)
                db_attr_vals.append(db_attr_val)

            db_tags.append(db_tag)
            tag["attributes"] = attributes

        db_tags = bulk_create(models.LabeledImage, db_tags)

        for db_attr_val in db_attr_vals:
            db_attr_val.image_id = db_tags[db_attr_val.tag_id].id

        bulk_create(models.LabeledImageAttributeVal, db_attr_vals)

        for tag, db_tag in zip(tags, db_tags):
            tag["id"] = db_tag.id

        self.ir_data.tags = tags

    def _set_updated_date(self):
        db_task = self.db_job.segment.task
        with transaction.atomic():
            self.db_job.touch()
            db_task.touch()
            if db_project := db_task.project:
                db_project.touch()

    @staticmethod
    def _data_is_empty(data):
        return not (data["tags"] or data["shapes"] or data["tracks"])

    def _create(self, data):
        self.reset()
        self._save_tags_to_db(data["tags"])
        self._save_shapes_to_db(data["shapes"])
        self._save_tracks_to_db(data["tracks"])

    def create(self, data):
        data = self._validate_input_annotations(data)

        self._create(data)
        handle_annotations_change(self.db_job, self.data, "create")

        if not self._data_is_empty(self.data):
            self._set_updated_date()

    def put(self, data):
        data = self._validate_input_annotations(data)

        deleted_data = self._delete()
        handle_annotations_change(self.db_job, deleted_data, "delete")

        deleted_data_is_empty = self._data_is_empty(deleted_data)

        self._create(data)
        handle_annotations_change(self.db_job, self.data, "create")

        if not deleted_data_is_empty or not self._data_is_empty(self.data):
            self._set_updated_date()

    def update(self, data):
        data = self._validate_input_annotations(data)

        # in case with "update" must be called prior any annotations in database changes
        # as this annotations are used to count removed/added shapes
        handle_annotations_change(self.db_job, data.data, "update")
        self._delete(data)
        self._create(data)

        if not self._data_is_empty(self.data):
            self._set_updated_date()

    def _validate_input_annotations(self, data: Union[AnnotationIR, dict]) -> AnnotationIR:
        if not isinstance(data, AnnotationIR):
            data = AnnotationIR(self.db_job.segment.task.dimension, data)

        db_data = self.db_job.segment.task.data

        if data.tracks and db_data.validation_mode == models.ValidationMode.GT_POOL:
            # Only tags and shapes can be used in tasks with GT pool
            raise ValidationError("Tracks are not supported when task validation mode is {}".format(
                models.ValidationMode.GT_POOL
            ))

        return data

    def _delete_job_labeledimages(self, ids__UNSAFE: list[int]) -> None:
        # ids__UNSAFE is a list, received from the user
        # we MUST filter it by job_id additionally before applying to any queries
        ids = self.db_job.labeledimage_set.filter(pk__in=ids__UNSAFE).values_list('id', flat=True)
        models.LabeledImageAttributeVal.objects.filter(image_id__in=ids).delete()
        self.db_job.labeledimage_set.filter(pk__in=ids).delete()

    def _delete_job_labeledshapes(self, ids__UNSAFE: list[int], *, is_subcall: bool = False) -> None:
        # ids__UNSAFE is a list, received from the user
        # we MUST filter it by job_id additionally before applying to any queries
        if is_subcall:
            ids = ids__UNSAFE
        else:
            ids = self.db_job.labeledshape_set.filter(pk__in=ids__UNSAFE).values_list('id', flat=True)
            child_ids = self.db_job.labeledshape_set.filter(parent_id__in=ids).values_list('id', flat=True)
            if len(child_ids):
                self._delete_job_labeledshapes(child_ids, is_subcall=True)

        models.LabeledShapeAttributeVal.objects.filter(shape_id__in=ids).delete()
        self.db_job.labeledshape_set.filter(pk__in=ids).delete()

    def _delete_job_labeledtracks(self, ids__UNSAFE: list[int], *, is_subcall: bool = False) -> None:
        # ids__UNSAFE is a list, received from the user
        # we MUST filter it by job_id additionally before applying to any queries
        if is_subcall:
            ids = ids__UNSAFE
        else:
            ids = self.db_job.labeledtrack_set.filter(pk__in=ids__UNSAFE).values_list('id', flat=True)
            child_ids = self.db_job.labeledtrack_set.filter(parent_id__in=ids).values_list('id', flat=True)
            if len(child_ids):
                self._delete_job_labeledtracks(child_ids, is_subcall=True)

        models.TrackedShapeAttributeVal.objects.filter(shape__track_id__in=ids).delete()
        models.LabeledTrackAttributeVal.objects.filter(track_id__in=ids).delete()
        self.db_job.labeledtrack_set.filter(pk__in=ids).delete()

    def _delete(self, data=None):
        deleted_data = {}
        if data is None:
            self.init_from_db()
            deleted_data = self.data
            models.clear_annotations_in_jobs([self.db_job.id])
        else:
            labeledimage_ids = [image["id"] for image in data["tags"]]
            labeledshape_ids = [shape["id"] for shape in data["shapes"]]
            labeledtrack_ids = [track["id"] for track in data["tracks"]]

            for labeledimage_ids_chunk in take_by(labeledimage_ids, chunk_size=1000):
                self._delete_job_labeledimages(labeledimage_ids_chunk)

            for labeledshape_ids_chunk in take_by(labeledshape_ids, chunk_size=1000):
                self._delete_job_labeledshapes(labeledshape_ids_chunk)

            for labeledtrack_ids_chunk in take_by(labeledtrack_ids, chunk_size=1000):
                self._delete_job_labeledtracks(labeledtrack_ids_chunk)

            deleted_data = {
                "version": self.ir_data.version,
                "tags": data["tags"],
                "shapes": data["shapes"],
                "tracks": data["tracks"],
            }

        self.reset()
        return deleted_data

    def delete(self, data=None):
        deleted_data = self._delete(data)
        if not self._data_is_empty(deleted_data):
            self._set_updated_date()

        handle_annotations_change(self.db_job, deleted_data, "delete")
        return deleted_data

    @staticmethod
    def _extend_attributes(attributeval_set, default_attribute_values):
        shape_attribute_specs_set = set(attr.spec_id for attr in attributeval_set)
        for db_attr in default_attribute_values:
            if db_attr.spec_id not in shape_attribute_specs_set:
                attributeval_set.append(dotdict([
                    ('spec_id', db_attr.spec_id),
                    ('value', db_attr.value),
                ]))

    def _init_tags_from_db(self):
        # NOTE: do not use .prefetch_related() with .values() since it's useless:
        # https://github.com/cvat-ai/cvat/pull/7748#issuecomment-2063695007
        db_tags = self.db_job.labeledimage_set.values(
            'id',
            'frame',
            'label_id',
            'group',
            'source',
            'attribute__spec_id',
            'attribute__value',
            'attribute__id',
        ).order_by('frame').iterator(chunk_size=2000)

        db_tags = merge_table_rows(
            rows=db_tags,
            keys_for_merge={
                "attributes": [
                    'attribute__spec_id',
                    'attribute__value',
                    'attribute__id',
                ],
            },
            field_id='id',
        )

        for db_tag in db_tags:
            self._extend_attributes(db_tag.attributes,
                self.db_attributes[db_tag.label_id]["all"].values())

        serializer = serializers.LabeledImageSerializerFromDB(db_tags, many=True)
        self.ir_data.tags = serializer.data

    def _init_shapes_from_db(self):
        # NOTE: do not use .prefetch_related() with .values() since it's useless:
        # https://github.com/cvat-ai/cvat/pull/7748#issuecomment-2063695007
        db_shapes = self.db_job.labeledshape_set.values(
            'id',
            'label_id',
            'type',
            'frame',
            'group',
            'source',
            'occluded',
            'outside',
            'z_order',
            'rotation',
            'points',
            'parent',
            'attribute__spec_id',
            'attribute__value',
            'attribute__id',
        ).order_by('frame').iterator(chunk_size=2000)

        db_shapes = merge_table_rows(
            rows=db_shapes,
            keys_for_merge={
                'attributes': [
                    'attribute__spec_id',
                    'attribute__value',
                    'attribute__id',
                ],
            },
            field_id='id',
        )

        shapes = {}
        elements = {}
        for db_shape in db_shapes:
            self._extend_attributes(db_shape.attributes,
                self.db_attributes[db_shape.label_id]["all"].values())
            if db_shape['type'] == str(models.ShapeType.SKELETON):
                # skeletons themselves should not have points as they consist of other elements
                # here we ensure that it was initialized correctly
                db_shape['points'] = []

            if db_shape.parent is None:
                db_shape.elements = []
                shapes[db_shape.id] = db_shape
            else:
                if db_shape.parent not in elements:
                    elements[db_shape.parent] = []
                elements[db_shape.parent].append(db_shape)

        for shape_id, shape_elements in elements.items():
            shapes[shape_id].elements = shape_elements

        serializer = serializers.LabeledShapeSerializerFromDB(list(shapes.values()), many=True)
        self.ir_data.shapes = serializer.data

    def _init_tracks_from_db(self):
        # NOTE: do not use .prefetch_related() with .values() since it's useless:
        # https://github.com/cvat-ai/cvat/pull/7748#issuecomment-2063695007
        db_tracks = self.db_job.labeledtrack_set.values(
            "id",
            "frame",
            "label_id",
            "group",
            "source",
            "parent",
            "attribute__spec_id",
            "attribute__value",
            "attribute__id",
            "shape__type",
            "shape__occluded",
            "shape__z_order",
            "shape__rotation",
            "shape__points",
            "shape__id",
            "shape__frame",
            "shape__outside",
            "shape__attribute__spec_id",
            "shape__attribute__value",
            "shape__attribute__id",
        ).order_by('id', 'shape__frame').iterator(chunk_size=2000)

        db_tracks = merge_table_rows(
            rows=db_tracks,
            keys_for_merge={
                "attributes": [
                    "attribute__spec_id",
                    "attribute__value",
                    "attribute__id",
                ],
                "shapes":[
                    "shape__type",
                    "shape__occluded",
                    "shape__z_order",
                    "shape__points",
                    "shape__rotation",
                    "shape__id",
                    "shape__frame",
                    "shape__outside",
                    "shape__attribute__spec_id",
                    "shape__attribute__value",
                    "shape__attribute__id",
                ],
            },
            field_id="id",
        )

        tracks = {}
        elements = {}
        for db_track in db_tracks:
            db_track["shapes"] = merge_table_rows(db_track["shapes"], {
                'attributes': [
                    'attribute__value',
                    'attribute__spec_id',
                    'attribute__id',
                ]
            }, 'id')

            # A result table can consist many equal rows for track/shape attributes
            # We need filter unique attributes manually
            db_track["attributes"] = list(set(db_track["attributes"]))
            self._extend_attributes(db_track.attributes,
                self.db_attributes[db_track.label_id]["immutable"].values())

            default_attribute_values = self.db_attributes[db_track.label_id]["mutable"].values()
            for db_shape in db_track["shapes"]:
                db_shape["attributes"] = list(set(db_shape["attributes"]))
                # in case of trackedshapes need to interpolate attribute values and extend it
                # by previous shape attribute values (not default values)
                self._extend_attributes(db_shape["attributes"], default_attribute_values)
                if db_shape['type'] == str(models.ShapeType.SKELETON):
                    # skeletons themselves should not have points as they consist of other elements
                    # here we ensure that it was initialized correctly
                    db_shape['points'] = []
                default_attribute_values = db_shape["attributes"]

            if db_track.parent is None:
                db_track.elements = []
                tracks[db_track.id] = db_track
            else:
                if db_track.parent not in elements:
                    elements[db_track.parent] = []
                elements[db_track.parent].append(db_track)

        for track_id, track_elements in elements.items():
            tracks[track_id].elements = track_elements

        serializer = serializers.LabeledTrackSerializerFromDB(list(tracks.values()), many=True)
        self.ir_data.tracks = serializer.data

    def _init_version_from_db(self):
        self.ir_data.version = 0 # FIXME: should be removed in the future

    def init_from_db(self):
        self._init_tags_from_db()
        self._init_shapes_from_db()
        self._init_tracks_from_db()
        self._init_version_from_db()

    @property
    def data(self):
        return self.ir_data.data

    def export(
        self,
        dst_file: io.BufferedWriter,
        exporter: Callable[..., None],
        *,
        host: str = '',
        temp_dir: str | None = None,
        **options
    ):
        job_data = JobData(
            annotation_ir=self.ir_data,
            db_job=self.db_job,
            host=host,
        )

        with (
            TmpDirManager.get_tmp_directory_for_export(
                instance_type=self.db_job.__class__.__name__,
            ) if not temp_dir else nullcontext(temp_dir)
        ) as temp_dir:
            exporter(dst_file, temp_dir, job_data, **options)

    def import_annotations(self, src_file, importer, **options):
        job_data = JobData(
            annotation_ir=AnnotationIR(self.db_job.segment.task.dimension),
            db_job=self.db_job,
            create_callback=self.create,
        )
        self.delete()

        with TmpDirManager.get_tmp_directory() as temp_dir:
            try:
                importer(src_file, temp_dir, job_data, **options)
            except (DatasetNotFoundError, CvatDatasetNotFoundError) as not_found:
                if settings.CVAT_LOG_IMPORT_ERRORS:
                    dlogger.log_import_error(
                        entity="job",
                        entity_id=self.db_job.id,
                        format_name=importer.DISPLAY_NAME,
                        base_error=str(not_found),
                        dir_path=temp_dir,
                    )

                raise not_found

        self.create(job_data.data.slice(self.start_frame, self.stop_frame).serialize())


class TaskAnnotation:
    def __init__(self, pk, *, write_only: bool = False):
        self.db_task = models.Task.objects.prefetch_related(
            Prefetch('data__images', queryset=models.Image.objects.order_by('frame'))
        ).get(id=pk)
        self._write_only = write_only

        # TODO: maybe include consensus jobs except for task export
        requested_job_types = [models.JobType.ANNOTATION]
        if self.db_task.data.validation_mode == models.ValidationMode.GT_POOL:
            requested_job_types.append(models.JobType.GROUND_TRUTH)

        self.db_jobs = (
            JobAnnotation.add_prefetch_info(models.Job.objects, prefetch_images=False)
            .filter(segment__task_id=pk, type__in=requested_job_types)
        )

        if not write_only:
            self.ir_data = AnnotationIR(self.db_task.dimension)

    def reset(self):
        self.ir_data.reset()

    def _patch_data(self, data: Union[AnnotationIR, dict], action: Optional[PatchAction]):
        if not isinstance(data, AnnotationIR):
            data = AnnotationIR(self.db_task.dimension, data)

        if self.db_task.data.validation_mode == models.ValidationMode.GT_POOL:
            self._preprocess_input_annotations_for_gt_pool_task(data, action=action)

        splitted_data = {}
        jobs = {}
        for db_job in self.db_jobs:
            jid = db_job.id
            start = db_job.segment.start_frame
            stop = db_job.segment.stop_frame
            jobs[jid] = { "start": start, "stop": stop }
            splitted_data[jid] = (data.slice(start, stop), db_job)

        for jid, (job_data, db_job) in splitted_data.items():
            data = AnnotationIR(self.db_task.dimension)
            if action is None:
                data.data = put_job_data(jid, job_data, db_job=db_job)
            else:
                data.data = patch_job_data(jid, job_data, action, db_job=db_job)

            if not self._write_only:
                if data.version > self.ir_data.version:
                    self.ir_data.version = data.version

                self._merge_data(data, jobs[jid]["start"])

    def _merge_data(self, data: AnnotationIR, start_frame: int):
        annotation_manager = AnnotationManager(self.ir_data, dimension=self.db_task.dimension)
        annotation_manager.merge(data, start_frame, overlap=self.db_task.overlap)

    def put(self, data):
        self._patch_data(data, None)

    def create(self, data):
        self._patch_data(data, PatchAction.CREATE)

    def _preprocess_input_annotations_for_gt_pool_task(
        self, data: Union[AnnotationIR, dict], *, action: Optional[PatchAction]
    ) -> AnnotationIR:
        if not isinstance(data, AnnotationIR):
            data = AnnotationIR(self.db_task.dimension, data)

        if data.tracks:
            # Only tags and shapes are supported in tasks with GT pool
            raise ValidationError("Tracks are not supported when task validation mode is {}".format(
                models.ValidationMode.GT_POOL
            ))

        gt_job = self.db_task.gt_job
        if gt_job is None:
            raise AssertionError(f"Can't find GT job in the task {self.db_task.id}")

        db_data = self.db_task.data
        frame_step = db_data.get_frame_step()

        def _to_rel_frame(abs_frame: int) -> int:
            return (abs_frame - db_data.start_frame) // frame_step

        # Copy GT pool annotations into other jobs, with replacement of any existing annotations
        gt_abs_frame_set = sorted(gt_job.segment.frame_set)
        task_gt_honeypots: dict[int, int] = {} # real_id -> [placeholder_id, ...]
        task_gt_frames: set[int] = set()
        for abs_frame, abs_real_frame in (
            self.db_task.data.images
            .filter(is_placeholder=True, real_frame__in=gt_abs_frame_set)
            .values_list('frame', 'real_frame')
            .iterator(chunk_size=1000)
        ):
            frame = _to_rel_frame(abs_frame)
            task_gt_frames.add(frame)
            task_gt_honeypots.setdefault(_to_rel_frame(abs_real_frame), []).append(frame)

        gt_pool_frames = tuple(map(_to_rel_frame, gt_abs_frame_set))
        if sorted(gt_pool_frames) != list(range(min(gt_pool_frames), max(gt_pool_frames) + 1)):
            raise AssertionError("Expected a continuous GT pool frame set") # to be used in slice()

        gt_annotations = data.slice(min(gt_pool_frames), max(gt_pool_frames))

        if action and not (
            gt_annotations.tags or gt_annotations.shapes or gt_annotations.tracks
        ):
            return

        if not (
            action is None or # put
            action == PatchAction.CREATE
        ):
            # allow validation frame editing only with full task updates
            raise ValidationError(
                "Annotations on validation frames can only be edited via task import or the GT job"
            )

        task_annotation_manager = AnnotationManager(data, dimension=self.db_task.dimension)
        task_annotation_manager.clear_frames(task_gt_frames)

        for ann_type, gt_annotation in itertools.chain(
            zip(itertools.repeat('tag'), gt_annotations.tags),
            zip(itertools.repeat('shape'), gt_annotations.shapes),
        ):
            for honeypot_frame_id in task_gt_honeypots.get(
                gt_annotation["frame"], [] # some GT frames may be unused
            ):
                copied_annotation = faster_deepcopy(gt_annotation)
                copied_annotation["frame"] = honeypot_frame_id

                for ann in itertools.chain(
                    [copied_annotation], copied_annotation.get('elements', [])
                ):
                    ann.pop("id", None)

                if ann_type == 'tag':
                    data.add_tag(copied_annotation)
                elif ann_type == 'shape':
                    data.add_shape(copied_annotation)
                else:
                    assert False

        return data

    def update(self, data):
        self._patch_data(data, PatchAction.UPDATE)

    def delete(self, data=None):
        if data:
            self._patch_data(data, PatchAction.DELETE)
        else:
            for db_job in self.db_jobs:
                delete_job_data(db_job.id, db_job=db_job)

    def init_from_db(self):
        self.reset()

        for db_job in self.db_jobs.select_for_update():
            if db_job.type == models.JobType.GROUND_TRUTH and not (
                self.db_task.data.validation_mode == models.ValidationMode.GT_POOL
            ):
                continue

            gt_annotation = JobAnnotation(db_job.id, db_job=db_job)
            gt_annotation.init_from_db()
            if gt_annotation.ir_data.version > self.ir_data.version:
                self.ir_data.version = gt_annotation.ir_data.version

            self._merge_data(gt_annotation.ir_data, start_frame=db_job.segment.start_frame)

    def export(
        self,
        dst_file: io.BufferedWriter,
        exporter: Callable[..., None],
        *,
        host: str = '',
        temp_dir: str | None = None,
        **options
    ):
        task_data = TaskData(
            annotation_ir=self.ir_data,
            db_task=self.db_task,
            host=host,
        )

        with (
            TmpDirManager.get_tmp_directory_for_export(
                instance_type=self.db_task.__class__.__name__,
            ) if not temp_dir else nullcontext(temp_dir)
        ) as temp_dir:
            exporter(dst_file, temp_dir, task_data, **options)

    def import_annotations(self, src_file, importer, **options):
        task_data = TaskData(
            annotation_ir=AnnotationIR(self.db_task.dimension),
            db_task=self.db_task,
            create_callback=self.create,
        )
        self.delete()

        with TmpDirManager.get_tmp_directory() as temp_dir:
            try:
                importer(src_file, temp_dir, task_data, **options)
            except (DatasetNotFoundError, CvatDatasetNotFoundError) as not_found:
                if settings.CVAT_LOG_IMPORT_ERRORS:
                    dlogger.log_import_error(
                        entity="task",
                        entity_id=self.db_task.id,
                        format_name=importer.DISPLAY_NAME,
                        base_error=str(not_found),
                        dir_path=temp_dir,
                    )

                raise not_found

        self.create(task_data.data.serialize())

    @property
    def data(self):
        return self.ir_data.data


@silk_profile(name="GET job data")
@transaction.atomic
def get_job_data(pk):
    annotation = JobAnnotation(pk)
    annotation.init_from_db()

    return annotation.data


@silk_profile(name="POST job data")
@transaction.atomic
def put_job_data(pk, data: AnnotationIR | dict, *, db_job: models.Job | None = None):
    annotation = JobAnnotation(pk, db_job=db_job)
    annotation.put(data)

    return annotation.data


@silk_profile(name="UPDATE job data")
@plugin_decorator
@transaction.atomic
def patch_job_data(pk, data: AnnotationIR | dict, action: PatchAction, *, db_job: models.Job | None = None):
    annotation = JobAnnotation(pk, db_job=db_job)
    if action == PatchAction.CREATE:
        annotation.create(data)
    elif action == PatchAction.UPDATE:
        annotation.update(data)
    elif action == PatchAction.DELETE:
        return annotation.delete(data)

    return annotation.data


@silk_profile(name="DELETE job data")
@transaction.atomic
def delete_job_data(pk, *, db_job: models.Job | None = None):
    annotation = JobAnnotation(pk, db_job=db_job)
    annotation.delete()


def export_job(
    job_id: int,
    dst_file: str,
    *,
    format_name: str,
    server_url: str | None = None,
    save_images=False,
    temp_dir: str | None = None,
):
    # For big tasks dump function may run for a long time and
    # we dont need to acquire lock after the task has been initialized from DB.
    # But there is the bug with corrupted dump file in case 2 or
    # more dump request received at the same time:
    # https://github.com/cvat-ai/cvat/issues/217
    with transaction.atomic():
        job = JobAnnotation(job_id, prefetch_images=True, lock_job_in_db=True)
        job.init_from_db()

    exporter = make_exporter(format_name)
    with open(dst_file, 'wb') as f:
        job.export(f, exporter, host=server_url, save_images=save_images, temp_dir=temp_dir)


@silk_profile(name="GET task data")
@transaction.atomic
def get_task_data(pk):
    annotation = TaskAnnotation(pk)
    annotation.init_from_db()

    return annotation.data


@silk_profile(name="POST task data")
@transaction.atomic
def put_task_data(pk, data):
    annotation = TaskAnnotation(pk)
    annotation.put(data)

    return annotation.data


@silk_profile(name="UPDATE task data")
@transaction.atomic
def patch_task_data(pk, data, action):
    annotation = TaskAnnotation(pk)
    if action == PatchAction.CREATE:
        annotation.create(data)
    elif action == PatchAction.UPDATE:
        annotation.update(data)
    elif action == PatchAction.DELETE:
        annotation.delete(data)

    return annotation.data


@silk_profile(name="DELETE task data")
@transaction.atomic
def delete_task_data(pk):
    annotation = TaskAnnotation(pk)
    annotation.delete()


def export_task(
    task_id: int,
    dst_file: str,
    *,
    format_name: str,
    server_url: str | None = None,
    save_images: bool = False,
    temp_dir: str | None = None,
    ):
    # For big tasks dump function may run for a long time and
    # we dont need to acquire lock after the task has been initialized from DB.
    # But there is the bug with corrupted dump file in case 2 or
    # more dump request received at the same time:
    # https://github.com/cvat-ai/cvat/issues/217
    with transaction.atomic():
        task = TaskAnnotation(task_id)
        task.init_from_db()

    exporter = make_exporter(format_name)
    with open(dst_file, 'wb') as f:
        task.export(f, exporter, host=server_url, save_images=save_images, temp_dir=temp_dir)


@transaction.atomic
def import_task_annotations(src_file, task_id, format_name, conv_mask_to_poly):
    task = TaskAnnotation(task_id, write_only=True)

    importer = make_importer(format_name)
    with open(src_file, 'rb') as f:
        try:
            task.import_annotations(f, importer, conv_mask_to_poly=conv_mask_to_poly)
        except (DatasetError, DatasetImportError, DatasetNotFoundError) as ex:
            raise CvatImportError(str(ex))


@transaction.atomic
def import_job_annotations(src_file, job_id, format_name, conv_mask_to_poly):
    job = JobAnnotation(job_id, prefetch_images=True)

    importer = make_importer(format_name)
    with open(src_file, 'rb') as f:
        try:
            job.import_annotations(f, importer, conv_mask_to_poly=conv_mask_to_poly)
        except (DatasetError, DatasetImportError, DatasetNotFoundError) as ex:
            raise CvatImportError(str(ex))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tracks_counter.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from collections import defaultdict
from typing import Callable, Dict, TypedDict

from cvat.apps.engine.models import Job, LabeledTrack, ShapeType
from cvat.apps.engine.utils import defaultdict_to_regular

from .util import linear_sort_shapes


class _CountableShape(TypedDict):
    frame: int
    outside: bool


class _CountableTrack(TypedDict):
    type: ShapeType
    id: int
    label_id: int
    shapes: list[_CountableShape]


class TracksCounter:
    """
    The class implements counting of VISIBLE shapes in tracks.
    """

    def __init__(self):
        self._tracks_per_job: Dict[int, Dict[int, _CountableTrack]] = {}
        self._stop_frames_per_job: Dict[int, int] = {}

    def _init_stop_frames(self):
        if self._tracks_per_job:
            self._stop_frames_per_job = dict(
                Job.objects.filter(id__in=self._tracks_per_job.keys()).values_list(
                    "id", "segment__stop_frame"
                )
            )
        else:
            self._stop_frames_per_job = {}

    def count_track_shapes(self, job_id: int, track_id: int):
        track = self._tracks_per_job.get(job_id, {}).get(track_id)
        manual = 0
        interpolated = 0

        if track is not None and track["shapes"]:
            prev_frame = track["shapes"][0]["frame"]
            prev_is_outside = track["shapes"][0]["outside"]

            if not prev_is_outside:
                manual += 1

            for shape in track["shapes"][1:]:
                frame = shape["frame"]
                is_outside = shape["outside"]
                assert (
                    frame > prev_frame
                ), f"{frame} > {prev_frame}. Track id: {track['id']}"  # Catch invalid tracks

                if not prev_is_outside:
                    # -1 means that current keyframe is not interpolated frame
                    interpolated += frame - prev_frame - 1
                if not is_outside:
                    manual += 1

                prev_is_outside = is_outside
                prev_frame = frame

            if not prev_is_outside and prev_frame < self._stop_frames_per_job[job_id]:
                interpolated += self._stop_frames_per_job[job_id] - prev_frame

        return {
            "manual": manual,
            "interpolated": interpolated,
        }

    def load_tracks_from_db(
        self,
        parent_labeledtrack_qs_filter: Callable,
        child_labeledtrack_qs_filter: Callable,
    ):
        unmerged_parent_tracks = (
            parent_labeledtrack_qs_filter(LabeledTrack.objects)
            .filter(parent=None)
            .values_list(
                "job_id",
                "label_id",
                "shape__type",
                "id",
                "shape__frame",
                "shape__outside",
            )
            .order_by("job_id", "label_id", "shape__type", "id", "shape__frame")
        )

        unmerged_child_tracks = (
            child_labeledtrack_qs_filter(LabeledTrack.objects)
            .exclude(parent=None)
            .values_list(
                "parent_id",
                "shape__frame",
                "shape__outside",
            )
            .order_by("parent_id", "shape__frame")
        )

        i = 0
        tracks_per_job = defaultdict(lambda: {})
        tracks_per_id = {}
        rows_count = len(unmerged_parent_tracks)
        while i < rows_count:
            row = unmerged_parent_tracks[i]
            [job_id, label_id, track_type, track_id] = row[0:4]

            if track_type == str(ShapeType.SKELETON):
                tracks_per_id[track_id] = {
                    "type": track_type,
                    "id": track_id,
                    "label_id": label_id,
                    "shapes": [],  # will be filled further from its elements
                }
                tracks_per_job[job_id][track_id] = tracks_per_id[track_id]
                while i < rows_count and unmerged_parent_tracks[i][3] == track_id:
                    # go to next track
                    i += 1
            else:
                tracks_per_id[track_id] = {
                    "type": track_type,
                    "id": track_id,
                    "label_id": label_id,
                    "shapes": [],
                }
                tracks_per_job[job_id][track_id] = tracks_per_id[track_id]

                while i < rows_count and unmerged_parent_tracks[i][3] == track_id:
                    tracks_per_id[track_id]["shapes"].append(
                        {
                            "frame": unmerged_parent_tracks[i][4],
                            "outside": unmerged_parent_tracks[i][5],
                        }
                    )
                    i += 1

        element_shapes_per_parent = defaultdict(
            lambda: defaultdict(
                lambda: {
                    "frame": None,
                    "outside": True,
                }
            )
        )

        for row in unmerged_child_tracks:
            [parent_id, frame, outside] = row
            element_shapes = element_shapes_per_parent[parent_id]
            element_shapes[frame]["frame"] = frame
            if not outside:
                element_shapes[frame]["outside"] = False

        for parent_id, element_shapes in element_shapes_per_parent.items():
            tracks_per_id[parent_id]["shapes"] = linear_sort_shapes(element_shapes.values())

        self._tracks_per_job = defaultdict_to_regular(tracks_per_job)
        self._init_stop_frames()

    def load_tracks_from_job(self, job_id: int, job_tracks: list):
        transformed_tracks = {}
        for track in job_tracks:
            if not track.get("shapes", []) and not track.get("elements", []):
                # track misses any shapes and elements
                continue

            track_type = (
                str(ShapeType.SKELETON) if track.get("elements", []) else track["shapes"][0]["type"]
            )
            if track_type == str(ShapeType.SKELETON):
                # skeleton has keyframe if any of its elements have keyframe
                # keyframe is not outside if any of its elements is not outside
                track_shapes = defaultdict(lambda: {"frame": None, "outside": True})
                skeleton_elements = track.get("elements", [])
                for element_track in skeleton_elements:
                    for element_shape in element_track.get("shapes", []):
                        element_frame = element_shape["frame"]
                        track_shapes[element_frame]["frame"] = element_frame
                        if not element_shape["outside"]:
                            track_shapes[element_frame]["outside"] = False

                transformed_tracks[track["id"]] = {
                    "id": track["id"],
                    "type": track_type,
                    "label_id": track["label_id"],
                    "shapes": linear_sort_shapes(defaultdict_to_regular(track_shapes).values()),
                }
            else:
                transformed_tracks[track["id"]] = {
                    "id": track["id"],
                    "type": track_type,
                    "label_id": track["label_id"],
                    "shapes": linear_sort_shapes(
                        {
                            "frame": shape["frame"],
                            "outside": shape["outside"],
                        }
                        for shape in track["shapes"]
                    ),
                }

            self._tracks_per_job = {job_id: transformed_tracks}
            self._init_stop_frames()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\util.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import inspect
import os
import os.path as osp
import re
import tempfile
import zipfile
from collections.abc import Generator, Iterable
from contextlib import contextmanager
from copy import deepcopy
from datetime import timedelta
from enum import Enum
from threading import Lock
from typing import Any

import attrs
import django_rq
from datumaro.util import to_snake_case
from datumaro.util.os_util import make_file_name
from django.conf import settings
from pottery import Redlock


def current_function_name(depth=1):
    return inspect.getouterframes(inspect.currentframe())[depth].function


def make_zip_archive(src_path, dst_path):
    with zipfile.ZipFile(dst_path, 'w') as archive:
        for (dirpath, _, filenames) in os.walk(src_path):
            for name in filenames:
                path = osp.join(dirpath, name)
                archive.write(path, osp.relpath(path, src_path))


def faster_deepcopy(v):
    "A slightly optimized version of the default deepcopy, can be used as a drop-in replacement."
    # Default deepcopy is very slow, here we do shallow copy for primitive types and containers

    t = type(v)
    if t is dict:
        return {k: faster_deepcopy(vv) for k, vv in v.items()}
    elif t in (list, tuple, set):
        return t(faster_deepcopy(vv) for vv in v)
    elif isinstance(v, (int, float, str, bool)) or v is None:
        return v
    else:
        return deepcopy(v)


class LockNotAvailableError(Exception):
    pass

class CacheFileOrDirPathParseError(Exception):
    pass


def make_export_cache_lock_key(filename: os.PathLike[str]) -> str:
    return f"export_lock:{os.fspath(filename)}"


@contextmanager
def get_export_cache_lock(
    export_path: os.PathLike[str],
    *,
    ttl: int | timedelta,
    block: bool = True,
    acquire_timeout: int | timedelta,
) -> Generator[Lock, Any, Any]:
    assert acquire_timeout is not None, "Endless waiting for the lock should be avoided"

    if isinstance(acquire_timeout, timedelta):
        acquire_timeout = acquire_timeout.total_seconds()

    if acquire_timeout < 0:
        raise ValueError("acquire_timeout must be a non-negative number")


    if isinstance(ttl, timedelta):
        ttl = ttl.total_seconds()
    if not ttl or ttl < 0:
        raise ValueError("ttl must be a non-negative number")

    # https://redis.io/docs/latest/develop/use/patterns/distributed-locks/
    # The lock is exclusive, so it may potentially reduce performance in some cases,
    # where parallel access is potentially possible and valid,
    # e.g. dataset downloading could use a shared lock instead.
    lock = Redlock(
        key=make_export_cache_lock_key(export_path),
        masters={django_rq.get_connection(settings.CVAT_QUEUES.EXPORT_DATA.value)},
        auto_release_time=ttl,
    )
    acquired = lock.acquire(blocking=block, timeout=acquire_timeout)
    try:
        if acquired:
            yield lock
        else:
            raise LockNotAvailableError

    finally:
        if acquired:
            lock.release()

class OperationType(str, Enum):
    EXPORT = "export"


class ExportFileType(str, Enum):
    ANNOTATIONS = "annotations"
    BACKUP = "backup"
    DATASET = "dataset"

    @classmethod
    def values(cls) -> list[str]:
        return list(map(lambda x: x.value, cls))

class InstanceType(str, Enum):
    PROJECT = "project"
    TASK = "task"
    JOB = "job"

    @classmethod
    def values(cls) -> list[str]:
        return list(map(lambda x: x.value, cls))

@attrs.frozen
class _ParsedExportFilename:
    file_type: ExportFileType
    file_ext: str
    instance_type: InstanceType = attrs.field(converter=InstanceType)
    instance_id: int
    instance_timestamp: float = attrs.field(converter=float)


@attrs.frozen
class ParsedDatasetFilename(_ParsedExportFilename):
    format_repr: str


@attrs.frozen
class ParsedBackupFilename(_ParsedExportFilename):
    pass


class TmpDirManager:
    SPLITTER = "-"
    TMP_ROOT = settings.TMP_FILES_ROOT
    TMP_FILE_OR_DIR_RETENTION_DAYS = settings.TMP_FILE_OR_DIR_RETENTION_DAYS

    @classmethod
    @contextmanager
    def get_tmp_directory(
        cls,
        *,
        prefix: str | None = None,
        suffix: str | None = None,
        ignore_cleanup_errors: bool | None = None,
    ) -> Generator[str, Any, Any]:
        """
        The method allows to create a temporary directory and
        ensures that the parent directory uses the CVAT tmp directory
        """
        params = {}
        for k, v in {
            "prefix": prefix,
            "suffix": suffix,
            "ignore_cleanup_errors": ignore_cleanup_errors,
        }.items():
            if v is not None:
                params[k] = v

        with tempfile.TemporaryDirectory(**params, dir=cls.TMP_ROOT) as tmp_dir:
            yield tmp_dir

    @classmethod
    @contextmanager
    def get_tmp_directory_for_export(
        cls,
        *,
        instance_type: str,
    ) -> Generator[str, Any, Any]:
        instance_type = InstanceType(instance_type.lower())
        with cls.get_tmp_directory(
            prefix=cls.SPLITTER.join([OperationType.EXPORT, instance_type]) + cls.SPLITTER
        ) as tmp_dir:
            yield tmp_dir


class ExportCacheManager:
    SPLITTER = "-"
    INSTANCE_PREFIX = "instance"
    FILE_NAME_TEMPLATE = SPLITTER.join([
        "{instance_type}", "{instance_id}", "{file_type}", INSTANCE_PREFIX +
        # store the instance timestamp in the file name to reliably get this information
        # ctime / mtime do not return file creation time on linux
        # mtime is used for file usage checks
        "{instance_timestamp}{optional_suffix}.{file_ext}"
    ])

    @classmethod
    def make_dataset_file_path(
        cls,
        *,
        instance_type: str,
        instance_id: int,
        instance_timestamp: float,
        save_images: bool,
        format_name: str,
    ) -> str:
        from .formats.registry import EXPORT_FORMATS

        file_ext = (EXPORT_FORMATS[format_name].EXT).lower()

        instance_type = InstanceType(instance_type.lower())
        file_type = ExportFileType.DATASET if save_images else ExportFileType.ANNOTATIONS

        normalized_format_name = make_file_name(to_snake_case(format_name))
        filename = cls.FILE_NAME_TEMPLATE.format_map(
            {
                "instance_type": instance_type,
                "instance_id": instance_id,
                "file_type": file_type,
                "instance_timestamp": instance_timestamp,
                "optional_suffix": cls.SPLITTER + normalized_format_name,
                "file_ext": file_ext,
            }
        )

        return osp.join(settings.EXPORT_CACHE_ROOT, filename)

    @classmethod
    def make_backup_file_path(
        cls,
        *,
        instance_type: str,
        instance_id: int,
        instance_timestamp: float,
    ) -> str:
        instance_type = InstanceType(instance_type.lower())
        filename = cls.FILE_NAME_TEMPLATE.format_map(
            {
                "instance_type": instance_type,
                "instance_id": instance_id,
                "file_type": ExportFileType.BACKUP,
                "instance_timestamp": instance_timestamp,
                "optional_suffix": "",
                "file_ext": "zip",
            }
        )
        return osp.join(settings.EXPORT_CACHE_ROOT, filename)

    @classmethod
    def parse_filename(
        cls, filename: str,
    ) -> ParsedDatasetFilename | ParsedBackupFilename:
        basename, file_ext = osp.splitext(filename)
        file_ext = file_ext.strip(".").lower()
        basename_match = re.fullmatch(
            (
                rf"^(?P<instance_type>{'|'.join(InstanceType.values())})"
                rf"{cls.SPLITTER}(?P<instance_id>\d+)"
                rf"{cls.SPLITTER}(?P<file_type>{'|'.join(ExportFileType.values())})"
                rf"{cls.SPLITTER}(?P<unparsed>.+)$"
            ),
            basename,
        )

        if not basename_match:
            raise CacheFileOrDirPathParseError(f"Couldn't parse file name: {basename!r}")

        fragments = basename_match.groupdict()
        fragments["instance_id"] = int(fragments["instance_id"])

        unparsed = fragments.pop("unparsed")[len(cls.INSTANCE_PREFIX):]
        specific_params = {}

        if fragments["file_type"] in (ExportFileType.DATASET, ExportFileType.ANNOTATIONS):
            try:
                instance_timestamp, format_repr = unparsed.split(cls.SPLITTER, maxsplit=1)
            except ValueError:
                raise CacheFileOrDirPathParseError(f"Couldn't parse file name: {basename!r}")

            specific_params["format_repr"] = format_repr
            ParsedFileNameClass = ParsedDatasetFilename
        else:
            instance_timestamp = unparsed
            ParsedFileNameClass = ParsedBackupFilename

        try:
            parsed_file_name = ParsedFileNameClass(
                file_ext=file_ext,
                instance_timestamp=instance_timestamp,
                **fragments,
                **specific_params,
            )
        except ValueError as ex:
            raise CacheFileOrDirPathParseError(f"Couldn't parse file name: {basename!r}") from ex

        return parsed_file_name


def extend_export_file_lifetime(file_path: str):
    # Update the last modification time to extend the export's lifetime,
    # as the last access time is not available on every filesystem.
    # As a result, file deletion by the cleaning job will be postponed.
    os.utime(file_path, None)


def linear_sort_shapes(shapes: Iterable) -> list:
    # as frame range is always has certain range
    # it allows us use efficient linear sorting algorithm
    min_frame = None
    max_frame = None
    d = {}
    for shape in shapes:
        frame = shape["frame"]
        d[frame] = shape
        min_frame = frame if min_frame is None else min(frame, min_frame)
        max_frame = frame if max_frame is None else max(frame, max_frame)

    sorted_shapes = []
    if max_frame is not None:
        for i in range(min_frame, max_frame + 1):
            if i in d:
                sorted_shapes.append(d[i])
    return sorted_shapes


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\views.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import logging
import os
import os.path as osp
import shutil
from datetime import timedelta
from os.path import exists as osp_exists

import django_rq
import rq
from django.conf import settings
from django.utils import timezone
from rq_scheduler import Scheduler

import cvat.apps.dataset_manager.project as project
import cvat.apps.dataset_manager.task as task
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import Job, Project, Task
from cvat.apps.engine.rq import ExportRQMeta
from cvat.apps.engine.utils import get_rq_lock_by_user

from .formats.registry import EXPORT_FORMATS, IMPORT_FORMATS
from .util import (
    ExportCacheManager,
    LockNotAvailableError,
    TmpDirManager,
    current_function_name,
    extend_export_file_lifetime,
    get_export_cache_lock,
)

slogger = ServerLogManager(__name__)

_MODULE_NAME = __package__ + '.' + osp.splitext(osp.basename(__file__))[0]

def log_exception(logger: logging.Logger | None = None, exc_info: bool = True):
    if logger is None:
        logger = slogger.glob
    logger.exception("[%s @ %s]: exception occurred" % \
            (_MODULE_NAME, current_function_name(2)),
        exc_info=exc_info)

DEFAULT_CACHE_TTL = timedelta(seconds=settings.EXPORT_CACHE_TTL)
PROJECT_CACHE_TTL = DEFAULT_CACHE_TTL
TASK_CACHE_TTL = DEFAULT_CACHE_TTL
JOB_CACHE_TTL = DEFAULT_CACHE_TTL
TTL_CONSTS = {
    'project': PROJECT_CACHE_TTL,
    'task': TASK_CACHE_TTL,
    'job': JOB_CACHE_TTL,
}

EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT = timedelta(seconds=settings.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT)
EXPORT_CACHE_LOCK_TTL = timedelta(seconds=settings.EXPORT_CACHE_LOCK_TTL)
EXPORT_LOCKED_RETRY_INTERVAL = timedelta(seconds=settings.EXPORT_LOCKED_RETRY_INTERVAL)


def get_export_cache_ttl(db_instance: str | Project | Task | Job) -> timedelta:
    if isinstance(db_instance, (Project, Task, Job)):
        db_instance = db_instance.__class__.__name__

    return TTL_CONSTS[db_instance.lower()]

def _patch_scheduled_job_status(job: rq.job.Job):
    # NOTE: rq scheduler < 0.14 does not set the appropriate
    # job status (SCHEDULED). This has been fixed in the 0.14 version.
    # https://github.com/rq/rq-scheduler/blob/f7d5787c5f94b5517e209c612ef648f4bfc44f9e/rq_scheduler/scheduler.py#L148
    # FUTURE-TODO: delete manual status setting after upgrading to 0.14
    if job.get_status(refresh=False) != rq.job.JobStatus.SCHEDULED:
        job.set_status(rq.job.JobStatus.SCHEDULED)

def retry_current_rq_job(time_delta: timedelta) -> rq.job.Job:
    # TODO: implement using retries once we move from rq_scheduler to builtin RQ scheduler
    # for better reliability and error reporting

    # This implementation can potentially lead to 2 jobs with the same name running in parallel,
    # if the retry is enqueued immediately.
    assert time_delta.total_seconds() > 0

    current_rq_job = rq.get_current_job()

    def _patched_retry(*_1, **_2):
        scheduler: Scheduler = django_rq.get_scheduler(
            settings.CVAT_QUEUES.EXPORT_DATA.value
        )

        rq_job_meta = ExportRQMeta.for_job(current_rq_job)
        user_id = rq_job_meta.user.id or -1

        with get_rq_lock_by_user(settings.CVAT_QUEUES.EXPORT_DATA.value, user_id):
            scheduled_rq_job: rq.job.Job = scheduler.enqueue_in(
                time_delta,
                current_rq_job.func,
                *current_rq_job.args,
                **current_rq_job.kwargs,
                job_id=current_rq_job.id,
                meta=rq_job_meta.get_meta_on_retry(),
                job_ttl=current_rq_job.ttl,
                job_result_ttl=current_rq_job.result_ttl,
                job_description=current_rq_job.description,
                on_success=current_rq_job.success_callback,
                on_failure=current_rq_job.failure_callback,
            )
            _patch_scheduled_job_status(scheduled_rq_job)

    current_rq_job.retries_left = 1
    setattr(current_rq_job, 'retry', _patched_retry)
    return current_rq_job

def export(
    *,
    dst_format: str,
    project_id: int | None = None,
    task_id: int | None = None,
    job_id: int | None = None,
    server_url: str | None = None,
    save_images: bool = False,
):
    try:
        if task_id is not None:
            logger = slogger.task[task_id]
            export_fn = task.export_task
            db_instance = Task.objects.get(pk=task_id)
        elif project_id is not None:
            logger = slogger.project[project_id]
            export_fn = project.export_project
            db_instance = Project.objects.get(pk=project_id)
        else:
            logger = slogger.job[job_id]
            export_fn = task.export_job
            db_instance = Job.objects.get(pk=job_id)

        cache_ttl = get_export_cache_ttl(db_instance)
        instance_type = db_instance.__class__.__name__

        # As we're not locking the db object here, it can be updated by the time of actual export.
        # The file will be saved with the older timestamp.
        # When it's time to download the file, it will be handled - the export will be restarted.
        # The situation is considered rare, so no locking is used.
        instance_update_time = timezone.localtime(db_instance.updated_date)
        if isinstance(db_instance, Project):
            tasks_update = list(map(
                lambda db_task: timezone.localtime(db_task.updated_date),
                db_instance.tasks.all()
            ))
            instance_update_time = max(tasks_update + [instance_update_time])

        output_path = ExportCacheManager.make_dataset_file_path(
            instance_id=db_instance.id,
            instance_type=instance_type,
            instance_timestamp=instance_update_time.timestamp(),
            save_images=save_images,
            format_name=dst_format
        )

        # acquire a lock 2 times instead of using one long lock:
        # 1. to check whether the file exists or not
        # 2. to create a file when it doesn't exist
        with get_export_cache_lock(
            output_path,
            ttl=EXPORT_CACHE_LOCK_TTL,
            acquire_timeout=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
        ):
            if osp_exists(output_path):
                extend_export_file_lifetime(output_path)
                return output_path

        with TmpDirManager.get_tmp_directory_for_export(instance_type=instance_type) as temp_dir:
            temp_file = osp.join(temp_dir, 'result')
            # create a subdirectory to store export-related files,
            # which will be fully included in the resulting archive
            temp_subdir = osp.join(temp_dir, 'subdir')
            os.makedirs(temp_subdir, exist_ok=True)

            export_fn(db_instance.id, temp_file, format_name=dst_format,
                server_url=server_url, save_images=save_images, temp_dir=temp_subdir)

            with get_export_cache_lock(
                output_path,
                ttl=EXPORT_CACHE_LOCK_TTL,
                acquire_timeout=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
            ):
                shutil.move(temp_file, output_path)

        logger.info(
            f"The {db_instance.__class__.__name__.lower()} '{db_instance.id}' is exported "
            f"as {dst_format!r} at {output_path!r} and available for downloading for the next "
            f"{cache_ttl.total_seconds()} seconds. "
        )

        return output_path
    except LockNotAvailableError:
        # Need to retry later if the lock was not available
        retry_current_rq_job(EXPORT_LOCKED_RETRY_INTERVAL)
        logger.info(
            "Failed to acquire export cache lock. Retrying in {}".format(
                EXPORT_LOCKED_RETRY_INTERVAL
            )
        )
        raise
    except Exception:
        log_exception(logger)
        raise

def export_job_annotations(job_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, job_id=job_id, server_url=server_url, save_images=False)

def export_job_as_dataset(job_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, job_id=job_id, server_url=server_url, save_images=True)

def export_task_as_dataset(task_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, task_id=task_id, server_url=server_url, save_images=True)

def export_task_annotations(task_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, task_id=task_id, server_url=server_url, save_images=False)

def export_project_as_dataset(project_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, project_id=project_id, server_url=server_url, save_images=True)

def export_project_annotations(project_id: int, dst_format: str, *, server_url: str | None = None):
    return export(dst_format=dst_format, project_id=project_id, server_url=server_url, save_images=False)

def get_export_formats():
    return list(EXPORT_FORMATS.values())

def get_import_formats():
    return list(IMPORT_FORMATS.values())

def get_all_formats():
    return {
        'importers': get_import_formats(),
        'exporters': get_export_formats(),
    }


def get_export_callback(db_instance: Project | Task | Job, save_images: bool):
    if isinstance(db_instance, Project):
        return export_project_as_dataset if save_images else export_project_annotations
    elif isinstance(db_instance, Task):
        return export_task_as_dataset if save_images else export_task_annotations

    assert isinstance(db_instance, Job)
    return export_job_as_dataset if save_images else export_job_annotations


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\__init__.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\camvid.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.dataset import Dataset
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import GetCVATDataExtractor, import_dm_annotations
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons
from .utils import make_colormap


@exporter(name="CamVid", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("boxes_to_masks")
        dataset.transform("merge_instance_segments")
        label_map = make_colormap(instance_data)

        dataset.export(
            temp_dir,
            "camvid",
            save_media=save_images,
            apply_colormap=True,
            label_map={label: label_map[label][0] for label in label_map},
        )

    make_zip_archive(temp_dir, dst_file)


@importer(name="CamVid", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    # We do not run detect_dataset before import because the Camvid format
    # has problem with the dataset detection in case of empty annotation file(s)
    # Details in: https://github.com/cvat-ai/datumaro/issues/43
    dataset = Dataset.import_from(temp_dir, "camvid", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\cityscapes.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp

from datumaro.components.dataset import Dataset
from datumaro.plugins.data_formats.cityscapes import write_label_map
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons
from .utils import make_colormap


@exporter(name="Cityscapes", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("boxes_to_masks")
        dataset.transform("merge_instance_segments")

        dataset.export(
            temp_dir,
            "cityscapes",
            save_media=save_images,
            apply_colormap=True,
            label_map={label: info[0] for label, info in make_colormap(instance_data).items()},
        )

    make_zip_archive(temp_dir, dst_file)


@importer(name="Cityscapes", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    labelmap_file = osp.join(temp_dir, "label_colors.txt")
    if not osp.isfile(labelmap_file):
        colormap = {label: info[0] for label, info in make_colormap(instance_data).items()}
        write_label_map(labelmap_file, colormap)

    detect_dataset(temp_dir, format_name="cityscapes", importer=dm_env.importers.get("cityscapes"))
    dataset = Dataset.import_from(temp_dir, "cityscapes", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\coco.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.annotation import AnnotationType
from datumaro.components.dataset import StreamDataset
from datumaro.components.transformer import ItemTransform
from datumaro.plugins.data_formats.coco.importer import CocoImporter

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    NoMediaInAnnotationFileError,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="COCO", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = StreamDataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "coco_instances", save_media=save_images, merge_images=False)

    make_zip_archive(temp_dir, dst_file)


@importer(name="COCO", ext="JSON, ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)
        # We use coco importer because it gives better error message
        detect_dataset(temp_dir, format_name="coco", importer=CocoImporter)
        dataset = StreamDataset.import_from(temp_dir, "coco_instances", env=dm_env)
        if load_data_callback is not None:
            load_data_callback(dataset, instance_data)
        import_dm_annotations(dataset, instance_data)
    else:
        if load_data_callback:
            raise NoMediaInAnnotationFileError()

        dataset = StreamDataset.import_from(src_file.name, "coco_instances", env=dm_env)
        import_dm_annotations(dataset, instance_data)


@exporter(name="COCO Keypoints", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = StreamDataset.from_extractors(extractor, env=dm_env)
        dataset.export(
            temp_dir, "coco_person_keypoints", save_media=save_images, merge_images=False
        )

    make_zip_archive(temp_dir, dst_file)


class RemoveBboxAnnotations(ItemTransform):
    # Boxes would have invalid (skeleton) labels, so remove them
    # TODO: find a way to import boxes
    KEEPS_SUBSETS_INTACT = True

    def transform_item(self, item):
        def convert_annotations():
            return [ann for ann in item.annotations if ann.type != AnnotationType.bbox]

        return item.wrap(annotations=convert_annotations)


@importer(name="COCO Keypoints", ext="JSON, ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)
        # We use coco importer because it gives better error message
        detect_dataset(temp_dir, format_name="coco", importer=CocoImporter)
        dataset = StreamDataset.import_from(temp_dir, "coco_person_keypoints", env=dm_env)
        dataset = dataset.transform(RemoveBboxAnnotations)
        if load_data_callback is not None:
            load_data_callback(dataset, instance_data)
        import_dm_annotations(dataset, instance_data)
    else:
        if load_data_callback:
            raise NoMediaInAnnotationFileError()

        dataset = StreamDataset.import_from(src_file.name, "coco_person_keypoints", env=dm_env)
        dataset = dataset.transform(RemoveBboxAnnotations)
        import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\cvat.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import os.path as osp
import zipfile
from collections import OrderedDict
from glob import glob
from io import BufferedWriter
from typing import Callable, Union

from datumaro.components.annotation import (
    AnnotationType,
    Bbox,
    Label,
    LabelCategories,
    Points,
    Polygon,
    PolyLine,
    Skeleton,
)
from datumaro.components.dataset import Dataset, DatasetItem
from datumaro.components.dataset_base import DEFAULT_SUBSET_NAME, DatasetBase
from datumaro.components.importer import Importer
from datumaro.components.media import Image
from datumaro.plugins.data_formats.cvat.base import CvatImporter as _CvatImporter
from defusedxml import ElementTree

from cvat.apps.dataset_manager.bindings import (
    CVATProjectDataExtractor,
    CvatTaskOrJobDataExtractor,
    JobData,
    NoMediaInAnnotationFileError,
    ProjectData,
    TaskData,
    detect_dataset,
    get_defaulted_subset,
    import_dm_annotations,
    match_dm_item,
)
from cvat.apps.dataset_manager.util import make_zip_archive
from cvat.apps.engine.frame_provider import FrameOutputType, FrameQuality, make_frame_provider

from .registry import dm_env, exporter, importer


class CvatPath:
    IMAGES_DIR = "images"

    MEDIA_EXTS = (".jpg", ".jpeg", ".png")

    BUILTIN_ATTRS = {"occluded", "outside", "keyframe", "track_id"}


class CvatExtractor(DatasetBase):
    _SUPPORTED_SHAPES = ("box", "polygon", "polyline", "points", "skeleton")

    def __init__(self, path, subsets=None):
        assert osp.isfile(path), path
        rootpath = osp.dirname(path)
        images_dir = ""
        if osp.isdir(osp.join(rootpath, CvatPath.IMAGES_DIR)):
            images_dir = osp.join(rootpath, CvatPath.IMAGES_DIR)
        self._images_dir = images_dir
        self._path = path

        if not subsets:
            subsets = self._get_subsets_from_anno(path)
        self._subsets = subsets
        super().__init__(subsets=self._subsets)

        image_items = self._parse_images(images_dir, self._subsets)
        items, categories = self._parse(path)
        self._items = list(self._load_items(items, image_items).values())
        self._categories = categories

    def categories(self):
        return self._categories

    def __iter__(self):
        yield from self._items

    def __len__(self):
        return len(self._items)

    def get(self, _id, subset=DEFAULT_SUBSET_NAME):
        assert subset in self._subsets, "{} not in {}".format(subset, ", ".join(self._subsets))
        return super().get(_id, subset)

    @staticmethod
    def _get_subsets_from_anno(path):
        context = ElementTree.iterparse(path, events=("start", "end"))
        context = iter(context)

        for ev, el in context:
            if ev == "start":
                if el.tag == "subsets":
                    if el.text is not None:
                        subsets = el.text.split("\n")
                        return subsets
            if ev == "end":
                if el.tag == "meta":
                    return [DEFAULT_SUBSET_NAME]
                el.clear()
        return [DEFAULT_SUBSET_NAME]

    @staticmethod
    def _parse_images(image_dir, subsets):
        items = OrderedDict()

        def parse_image_dir(image_dir, subset):
            for file in sorted(glob(image_dir), key=osp.basename):
                name, ext = osp.splitext(osp.basename(file))
                if ext.lower() in CvatPath.MEDIA_EXTS:
                    items[(subset, name)] = DatasetItem(
                        id=name,
                        annotations=[],
                        media=Image.from_file(path=file),
                        subset=subset or DEFAULT_SUBSET_NAME,
                    )

        if subsets == [DEFAULT_SUBSET_NAME] and not osp.isdir(
            osp.join(image_dir, DEFAULT_SUBSET_NAME)
        ):
            parse_image_dir(osp.join(image_dir, "*.*"), None)
        else:
            for subset in subsets:
                parse_image_dir(osp.join(image_dir, subset, "*.*"), subset)
        return items

    @classmethod
    def _parse(cls, path):
        context = ElementTree.iterparse(path, events=("start", "end"))
        context = iter(context)

        categories, tasks_info, attribute_types = cls._parse_meta(context)

        items = OrderedDict()

        track = None
        track_shapes = None
        track_elements = None
        shape = None
        shape_element = None
        tag = None
        attributes = None
        element_attributes = None
        image = None
        subset = None
        for ev, el in context:
            if ev == "start":
                if el.tag == "track":
                    frame_size = (
                        tasks_info[int(el.attrib.get("task_id"))]["frame_size"]
                        if el.attrib.get("task_id")
                        else tuple(tasks_info.values())[0]["frame_size"]
                    )
                    track = {
                        "id": el.attrib["id"],
                        "label": el.attrib.get("label"),
                        "group": int(el.attrib.get("group_id", 0)),
                        "height": frame_size[0],
                        "width": frame_size[1],
                    }
                    subset = el.attrib.get("subset")
                    track_shapes = {}
                elif el.tag == "image":
                    image = {
                        "name": el.attrib.get("name"),
                        "frame": el.attrib["id"],
                        "width": el.attrib.get("width"),
                        "height": el.attrib.get("height"),
                    }
                    subset = el.attrib.get("subset")
                elif el.tag in cls._SUPPORTED_SHAPES and (track or image):
                    if shape and shape["type"] == "skeleton":
                        element_attributes = {}
                        shape_element = {
                            "type": "rectangle" if el.tag == "box" else el.tag,
                            "attributes": element_attributes,
                        }
                        if track:
                            shape_element.update(track)
                        else:
                            shape_element.update(image)
                    else:
                        attributes = {}
                        shape = {
                            "type": "rectangle" if el.tag == "box" else el.tag,
                            "attributes": attributes,
                        }
                        shape["elements"] = []
                        if track:
                            shape.update(track)
                            shape["track_id"] = int(track["id"])
                            shape["frame"] = el.attrib["frame"]
                            track_elements = []
                        if image:
                            shape.update(image)
                elif el.tag == "tag" and image:
                    attributes = {}
                    tag = {
                        "frame": image["frame"],
                        "attributes": attributes,
                        "group": int(el.attrib.get("group_id", 0)),
                        "label": el.attrib["label"],
                    }
                    subset = el.attrib.get("subset")
            elif ev == "end":
                if (
                    el.tag == "attribute"
                    and element_attributes is not None
                    and shape_element is not None
                ):
                    attr_value = el.text or ""
                    attr_type = attribute_types.get(el.attrib["name"])
                    if el.text in ["true", "false"]:
                        attr_value = attr_value == "true"
                    elif attr_type is not None and attr_type != "text":
                        try:
                            attr_value = float(attr_value)
                        except ValueError:
                            pass
                    element_attributes[el.attrib["name"]] = attr_value

                if el.tag == "attribute" and attributes is not None and shape_element is None:
                    attr_value = el.text or ""
                    attr_type = attribute_types.get(el.attrib["name"])
                    if el.text in ["true", "false"]:
                        attr_value = attr_value == "true"
                    elif attr_type is not None and attr_type != "text":
                        try:
                            attr_value = float(attr_value)
                        except ValueError:
                            pass
                    attributes[el.attrib["name"]] = attr_value

                elif (
                    el.tag in cls._SUPPORTED_SHAPES
                    and shape["type"] == "skeleton"
                    and el.tag != "skeleton"
                ):
                    shape_element["label"] = el.attrib.get("label")
                    shape_element["group"] = int(el.attrib.get("group_id", 0))

                    shape_element["type"] = el.tag
                    shape_element["z_order"] = int(el.attrib.get("z_order", 0))

                    if el.tag == "box":
                        shape_element["points"] = list(
                            map(
                                float,
                                [
                                    el.attrib["xtl"],
                                    el.attrib["ytl"],
                                    el.attrib["xbr"],
                                    el.attrib["ybr"],
                                ],
                            )
                        )
                    else:
                        shape_element["points"] = []
                        for pair in el.attrib["points"].split(";"):
                            shape_element["points"].extend(map(float, pair.split(",")))

                    if el.tag == "points" and el.attrib.get("occluded") == "1":
                        shape_element["visibility"] = [Points.Visibility.hidden] * (
                            len(shape_element["points"]) // 2
                        )
                    else:
                        shape_element["occluded"] = el.attrib.get("occluded") == "1"

                    if el.tag == "points" and el.attrib.get("outside") == "1":
                        shape_element["visibility"] = [Points.Visibility.absent] * (
                            len(shape_element["points"]) // 2
                        )
                    else:
                        shape_element["outside"] = el.attrib.get("outside") == "1"

                    if track:
                        shape_element["keyframe"] = el.attrib.get("keyframe") == "1"
                        if shape_element["keyframe"]:
                            track_elements.append(shape_element)
                    else:
                        shape["elements"].append(shape_element)
                    shape_element = None

                elif el.tag in cls._SUPPORTED_SHAPES:
                    if track is not None:
                        shape["frame"] = el.attrib["frame"]
                        shape["outside"] = el.attrib.get("outside") == "1"
                        shape["keyframe"] = el.attrib.get("keyframe") == "1"
                    if image is not None:
                        shape["label"] = el.attrib.get("label")
                        shape["group"] = int(el.attrib.get("group_id", 0))

                    shape["type"] = el.tag
                    shape["occluded"] = el.attrib.get("occluded") == "1"
                    shape["z_order"] = int(el.attrib.get("z_order", 0))
                    shape["rotation"] = float(el.attrib.get("rotation", 0))

                    if el.tag == "box":
                        shape["points"] = list(
                            map(
                                float,
                                [
                                    el.attrib["xtl"],
                                    el.attrib["ytl"],
                                    el.attrib["xbr"],
                                    el.attrib["ybr"],
                                ],
                            )
                        )
                    elif el.tag == "skeleton":
                        shape["points"] = []
                    else:
                        shape["points"] = []
                        for pair in el.attrib["points"].split(";"):
                            shape["points"].extend(map(float, pair.split(",")))

                    if track:
                        if shape["type"] == "skeleton" and track_elements:
                            shape["keyframe"] = True
                            track_shapes[shape["frame"]] = shape
                            track_shapes[shape["frame"]]["elements"] = track_elements
                            track_elements = None
                        elif shape["type"] != "skeleton":
                            track_shapes[shape["frame"]] = shape
                    else:
                        frame_desc = items.get((subset, shape["frame"]), {"annotations": []})
                        frame_desc["annotations"].append(cls._parse_shape_ann(shape, categories))
                        items[(subset, shape["frame"])] = frame_desc

                    shape = None

                elif el.tag == "tag":
                    frame_desc = items.get((subset, tag["frame"]), {"annotations": []})
                    frame_desc["annotations"].append(cls._parse_tag_ann(tag, categories))
                    items[(subset, tag["frame"])] = frame_desc
                    tag = None
                elif el.tag == "track":
                    for track_shape in track_shapes.values():
                        frame_desc = items.get((subset, track_shape["frame"]), {"annotations": []})
                        frame_desc["annotations"].append(
                            cls._parse_shape_ann(track_shape, categories)
                        )
                        items[(subset, track_shape["frame"])] = frame_desc
                    track = None
                elif el.tag == "image":
                    frame_desc = items.get((subset, image["frame"]), {"annotations": []})
                    frame_desc.update(
                        {
                            "name": image.get("name"),
                            "height": image.get("height"),
                            "width": image.get("width"),
                            "subset": subset,
                        }
                    )
                    items[(subset, image["frame"])] = frame_desc
                    image = None
                el.clear()

        return items, categories

    @staticmethod
    def _parse_meta(context):
        ev, el = next(context)
        if not (ev == "start" and el.tag == "annotations"):
            raise Exception("Unexpected token ")

        categories = {}

        tasks_info = {}
        frame_size = [None, None]
        task_id = None
        mode = None
        labels = OrderedDict()
        label = None

        # Recursive descent parser
        el = None
        states = ["annotations"]

        def accepted(expected_state, tag, next_state=None):
            state = states[-1]
            if state == expected_state and el is not None and el.tag == tag:
                if not next_state:
                    next_state = tag
                states.append(next_state)
                return True
            return False

        def consumed(expected_state, tag):
            state = states[-1]
            if state == expected_state and el is not None and el.tag == tag:
                states.pop()
                return True
            return False

        for ev, el in context:
            if ev == "start":
                if accepted("annotations", "meta"):
                    pass
                elif accepted("meta", "task"):
                    pass
                elif accepted("meta", "project"):
                    pass
                elif accepted("project", "tasks"):
                    pass
                elif accepted("tasks", "task"):
                    pass
                elif accepted("task", "id", next_state="task_id"):
                    pass
                elif accepted("task", "segment"):
                    pass
                elif accepted("task", "mode"):
                    pass
                elif accepted("task", "original_size"):
                    pass
                elif accepted("original_size", "height", next_state="frame_height"):
                    pass
                elif accepted("original_size", "width", next_state="frame_width"):
                    pass
                elif accepted("task", "labels"):
                    pass
                elif accepted("project", "labels"):
                    pass
                elif accepted("labels", "label"):
                    label = {"name": None, "attributes": []}
                elif accepted("label", "name", next_state="label_name"):
                    pass
                elif accepted("label", "attributes"):
                    pass
                elif accepted("attributes", "attribute"):
                    pass
                elif accepted("attribute", "name", next_state="attr_name"):
                    pass
                elif accepted("attribute", "input_type", next_state="attr_type"):
                    pass
                elif (
                    accepted("annotations", "image")
                    or accepted("annotations", "track")
                    or accepted("annotations", "tag")
                ):
                    break
                else:
                    pass
            elif ev == "end":
                if consumed("meta", "meta"):
                    break
                elif consumed("project", "project"):
                    pass
                elif consumed("tasks", "tasks"):
                    pass
                elif consumed("task", "task"):
                    tasks_info[task_id] = {
                        "frame_size": frame_size,
                        "mode": mode,
                    }
                    frame_size = [None, None]
                    mode = None
                elif consumed("task_id", "id"):
                    task_id = int(el.text)
                elif consumed("segment", "segment"):
                    pass
                elif consumed("mode", "mode"):
                    mode = el.text
                elif consumed("original_size", "original_size"):
                    pass
                elif consumed("frame_height", "height"):
                    frame_size[0] = int(el.text)
                elif consumed("frame_width", "width"):
                    frame_size[1] = int(el.text)
                elif consumed("label_name", "name"):
                    label["name"] = el.text
                elif consumed("attr_name", "name"):
                    label["attributes"].append({"name": el.text})
                elif consumed("attr_type", "input_type"):
                    label["attributes"][-1]["input_type"] = el.text
                elif consumed("attribute", "attribute"):
                    pass
                elif consumed("attributes", "attributes"):
                    pass
                elif consumed("label", "label"):
                    labels[label["name"]] = label["attributes"]
                    label = None
                elif consumed("labels", "labels"):
                    pass
                else:
                    pass

        assert len(states) == 1 and states[0] == "annotations", (
            "Expected 'meta' section in the annotation file, path: %s" % states
        )

        common_attrs = ["occluded"]
        if "interpolation" in map(lambda t: t["mode"], tasks_info.values()):
            common_attrs.append("keyframe")
            common_attrs.append("outside")
            common_attrs.append("track_id")

        label_cat = LabelCategories(attributes=common_attrs)
        attribute_types = {}
        for label, attrs in labels.items():
            attr_names = {v["name"] for v in attrs}
            label_cat.add(label, attributes=attr_names)
            for attr in attrs:
                attribute_types[attr["name"]] = attr["input_type"]

        categories[AnnotationType.label] = label_cat
        return categories, tasks_info, attribute_types

    @classmethod
    def _parse_shape_ann(cls, ann, categories):
        ann_id = ann.get("id", 0)
        ann_type = ann["type"]

        attributes = ann.get("attributes") or {}
        if "occluded" in categories[AnnotationType.label].attributes:
            attributes["occluded"] = ann.get("occluded", False)
        if "outside" in ann:
            attributes["outside"] = ann["outside"]
        if "keyframe" in ann:
            attributes["keyframe"] = ann["keyframe"]
        if "track_id" in ann:
            attributes["track_id"] = ann["track_id"]
        if "rotation" in ann:
            attributes["rotation"] = ann["rotation"]

        group = ann.get("group")

        label = ann.get("label")
        label_id = categories[AnnotationType.label].find(label)[0]

        z_order = ann.get("z_order", 0)
        points = ann.get("points", [])

        if ann_type == "polyline":
            return PolyLine(
                points,
                label=label_id,
                z_order=z_order,
                id=ann_id,
                attributes=attributes,
                group=group,
            )

        elif ann_type == "polygon":
            return Polygon(
                points,
                label=label_id,
                z_order=z_order,
                id=ann_id,
                attributes=attributes,
                group=group,
            )

        elif ann_type == "points":
            visibility = ann.get("visibility", None)
            return Points(
                points,
                visibility,
                label=label_id,
                z_order=z_order,
                id=ann_id,
                attributes=attributes,
                group=group,
            )

        elif ann_type == "box":
            x, y = points[0], points[1]
            w, h = points[2] - x, points[3] - y
            return Bbox(
                x,
                y,
                w,
                h,
                label=label_id,
                z_order=z_order,
                id=ann_id,
                attributes=attributes,
                group=group,
            )

        elif ann_type == "skeleton":
            elements = []
            for element in ann.get("elements", []):
                elements.append(cls._parse_shape_ann(element, categories))

            return Skeleton(
                elements,
                label=label_id,
                z_order=z_order,
                id=ann_id,
                attributes=attributes,
                group=group,
            )

        else:
            raise NotImplementedError("Unknown annotation type '%s'" % ann_type)

    @classmethod
    def _parse_tag_ann(cls, ann, categories):
        label = ann.get("label")
        label_id = categories[AnnotationType.label].find(label)[0]
        group = ann.get("group")
        attributes = ann.get("attributes")
        return Label(label_id, attributes=attributes, group=group)

    def _load_items(self, parsed, image_items):
        for (subset, frame_id), item_desc in parsed.items():
            name = item_desc.get("name", "frame_%06d.PNG" % int(frame_id))
            image = (
                osp.join(self._images_dir, subset, name)
                if subset
                else osp.join(self._images_dir, name)
            )
            image_size = (item_desc.get("height"), item_desc.get("width"))
            if all(image_size):
                image = Image.from_file(path=image, size=tuple(map(int, image_size)))
            di = image_items.get(
                (subset, osp.splitext(name)[0]), DatasetItem(id=name, annotations=[])
            )
            di.subset = subset or DEFAULT_SUBSET_NAME
            di.annotations = item_desc.get("annotations")
            di.attributes = {"frame": int(frame_id)}
            di.media = image if isinstance(image, Image) else di.media
            image_items[(subset, osp.splitext(name)[0])] = di
        return image_items


dm_env.extractors.register("cvat", CvatExtractor)


class CvatImporter(Importer):
    @classmethod
    def find_sources(cls, path):
        return cls._find_sources_recursive(path, ".xml", "cvat")


dm_env.importers.register("cvat", CvatImporter)


def pairwise(iterable):
    a = iter(iterable)
    return zip(a, a)


def create_xml_dumper(file_object):
    from xml.sax.saxutils import XMLGenerator

    class XmlAnnotationWriter:
        def __init__(self, file):
            self.version = "1.1"
            self.file = file
            self.xmlgen = XMLGenerator(self.file, "utf-8")
            self._level = 0

        def _indent(self, newline=True):
            if newline:
                self.xmlgen.ignorableWhitespace("\n")
            self.xmlgen.ignorableWhitespace("  " * self._level)

        def _add_version(self):
            self._indent()
            self.xmlgen.startElement("version", {})
            self.xmlgen.characters(self.version)
            self.xmlgen.endElement("version")

        def open_document(self):
            self.xmlgen.startDocument()

        def open_root(self):
            self.xmlgen.startElement("annotations", {})
            self._level += 1
            self._add_version()

        def _add_meta(self, meta):
            self._level += 1
            for k, v in meta.items():
                if isinstance(v, OrderedDict):
                    self._indent()
                    self.xmlgen.startElement(k, {})
                    self._add_meta(v)
                    self._indent()
                    self.xmlgen.endElement(k)
                elif isinstance(v, list):
                    self._indent()
                    self.xmlgen.startElement(k, {})
                    for tup in v:
                        self._add_meta(OrderedDict([tup]))
                    self._indent()
                    self.xmlgen.endElement(k)
                else:
                    self._indent()
                    self.xmlgen.startElement(k, {})
                    self.xmlgen.characters(v)
                    self.xmlgen.endElement(k)
            self._level -= 1

        def add_meta(self, meta):
            self._indent()
            self.xmlgen.startElement("meta", {})
            self._add_meta(meta)
            self._indent()
            self.xmlgen.endElement("meta")

        def open_track(self, track):
            self._indent()
            self.xmlgen.startElement("track", track)
            self._level += 1

        def open_image(self, image):
            self._indent()
            self.xmlgen.startElement("image", image)
            self._level += 1

        def open_box(self, box):
            self._indent()
            self.xmlgen.startElement("box", box)
            self._level += 1

        def open_ellipse(self, ellipse):
            self._indent()
            self.xmlgen.startElement("ellipse", ellipse)
            self._level += 1

        def open_polygon(self, polygon):
            self._indent()
            self.xmlgen.startElement("polygon", polygon)
            self._level += 1

        def open_polyline(self, polyline):
            self._indent()
            self.xmlgen.startElement("polyline", polyline)
            self._level += 1

        def open_points(self, points):
            self._indent()
            self.xmlgen.startElement("points", points)
            self._level += 1

        def open_mask(self, points):
            self._indent()
            self.xmlgen.startElement("mask", points)
            self._level += 1

        def open_cuboid(self, cuboid):
            self._indent()
            self.xmlgen.startElement("cuboid", cuboid)
            self._level += 1

        def open_tag(self, tag):
            self._indent()
            self.xmlgen.startElement("tag", tag)
            self._level += 1

        def open_skeleton(self, skeleton):
            self._indent()
            self.xmlgen.startElement("skeleton", skeleton)
            self._level += 1

        def add_attribute(self, attribute):
            self._indent()
            self.xmlgen.startElement("attribute", {"name": attribute["name"]})
            self.xmlgen.characters(attribute["value"])
            self.xmlgen.endElement("attribute")

        def close_box(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("box")

        def close_ellipse(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("ellipse")

        def close_polygon(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("polygon")

        def close_polyline(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("polyline")

        def close_points(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("points")

        def close_mask(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("mask")

        def close_cuboid(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("cuboid")

        def close_tag(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("tag")

        def close_skeleton(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("skeleton")

        def close_image(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("image")

        def close_track(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("track")

        def close_root(self):
            self._level -= 1
            self._indent()
            self.xmlgen.endElement("annotations")
            self._indent()

        def close_document(self):
            self.xmlgen.endDocument()

    return XmlAnnotationWriter(file_object)


def dump_as_cvat_annotation(dumper, annotations):
    dumper.open_root()
    dumper.add_meta(annotations.meta)

    for frame_annotation in annotations.group_by_frame(include_empty=True):
        frame_id = frame_annotation.frame
        image_attrs = OrderedDict([("id", str(frame_id)), ("name", frame_annotation.name)])
        if isinstance(annotations, ProjectData):
            image_attrs.update(
                OrderedDict(
                    [
                        ("subset", frame_annotation.subset),
                        ("task_id", str(frame_annotation.task_id)),
                    ]
                )
            )
        image_attrs.update(
            OrderedDict(
                [("width", str(frame_annotation.width)), ("height", str(frame_annotation.height))]
            )
        )

        # do not keep parsed lazy list data after this iteration
        if isinstance(annotations, ProjectData):
            frame_annotation = CVATProjectDataExtractor.copy_frame_data_with_replaced_lazy_lists(
                frame_annotation
            )
        else:
            frame_annotation = CvatTaskOrJobDataExtractor.copy_frame_data_with_replaced_lazy_lists(
                frame_annotation
            )

        dumper.open_image(image_attrs)

        def dump_labeled_shapes(shapes, is_skeleton=False):
            for shape in shapes:
                dump_data = OrderedDict([("label", shape.label), ("source", shape.source)])
                if is_skeleton:
                    dump_data.update(OrderedDict([("outside", str(int(shape.outside)))]))

                if shape.type != "skeleton":
                    dump_data.update(OrderedDict([("occluded", str(int(shape.occluded)))]))

                if shape.type == "rectangle":
                    dump_data.update(
                        OrderedDict(
                            [
                                ("xtl", "{:.2f}".format(shape.points[0])),
                                ("ytl", "{:.2f}".format(shape.points[1])),
                                ("xbr", "{:.2f}".format(shape.points[2])),
                                ("ybr", "{:.2f}".format(shape.points[3])),
                            ]
                        )
                    )

                    if shape.rotation:
                        dump_data.update(
                            OrderedDict([("rotation", "{:.2f}".format(shape.rotation))])
                        )
                elif shape.type == "ellipse":
                    dump_data.update(
                        OrderedDict(
                            [
                                ("cx", "{:.2f}".format(shape.points[0])),
                                ("cy", "{:.2f}".format(shape.points[1])),
                                ("rx", "{:.2f}".format(shape.points[2] - shape.points[0])),
                                ("ry", "{:.2f}".format(shape.points[1] - shape.points[3])),
                            ]
                        )
                    )

                    if shape.rotation:
                        dump_data.update(
                            OrderedDict([("rotation", "{:.2f}".format(shape.rotation))])
                        )
                elif shape.type == "cuboid":
                    dump_data.update(
                        OrderedDict(
                            [
                                ("xtl1", "{:.2f}".format(shape.points[0])),
                                ("ytl1", "{:.2f}".format(shape.points[1])),
                                ("xbl1", "{:.2f}".format(shape.points[2])),
                                ("ybl1", "{:.2f}".format(shape.points[3])),
                                ("xtr1", "{:.2f}".format(shape.points[4])),
                                ("ytr1", "{:.2f}".format(shape.points[5])),
                                ("xbr1", "{:.2f}".format(shape.points[6])),
                                ("ybr1", "{:.2f}".format(shape.points[7])),
                                ("xtl2", "{:.2f}".format(shape.points[8])),
                                ("ytl2", "{:.2f}".format(shape.points[9])),
                                ("xbl2", "{:.2f}".format(shape.points[10])),
                                ("ybl2", "{:.2f}".format(shape.points[11])),
                                ("xtr2", "{:.2f}".format(shape.points[12])),
                                ("ytr2", "{:.2f}".format(shape.points[13])),
                                ("xbr2", "{:.2f}".format(shape.points[14])),
                                ("ybr2", "{:.2f}".format(shape.points[15])),
                            ]
                        )
                    )
                elif shape.type == "mask":
                    dump_data.update(
                        OrderedDict(
                            [
                                ("rle", f"{list(int (v) for v in shape.points[:-4])}"[1:-1]),
                                ("left", f"{int(shape.points[-4])}"),
                                ("top", f"{int(shape.points[-3])}"),
                                ("width", f"{int(shape.points[-2] - shape.points[-4]) + 1}"),
                                ("height", f"{int(shape.points[-1] - shape.points[-3]) + 1}"),
                            ]
                        )
                    )
                elif shape.type != "skeleton":
                    dump_data.update(
                        OrderedDict(
                            [
                                (
                                    "points",
                                    ";".join(
                                        (
                                            ",".join(("{:.2f}".format(x), "{:.2f}".format(y)))
                                            for x, y in pairwise(shape.points)
                                        )
                                    ),
                                ),
                            ]
                        )
                    )

                if not is_skeleton:
                    dump_data["z_order"] = str(shape.z_order)
                if shape.group:
                    dump_data["group_id"] = str(shape.group)

                if shape.type == "rectangle":
                    dumper.open_box(dump_data)
                elif shape.type == "ellipse":
                    dumper.open_ellipse(dump_data)
                elif shape.type == "polygon":
                    dumper.open_polygon(dump_data)
                elif shape.type == "polyline":
                    dumper.open_polyline(dump_data)
                elif shape.type == "points":
                    dumper.open_points(dump_data)
                elif shape.type == "mask":
                    dumper.open_mask(dump_data)
                elif shape.type == "cuboid":
                    dumper.open_cuboid(dump_data)
                elif shape.type == "skeleton":
                    dumper.open_skeleton(dump_data)
                    dump_labeled_shapes(shape.elements, is_skeleton=True)
                else:
                    raise NotImplementedError("unknown shape type")

                for attr in shape.attributes:
                    dumper.add_attribute(OrderedDict([("name", attr.name), ("value", attr.value)]))

                if shape.type == "rectangle":
                    dumper.close_box()
                elif shape.type == "ellipse":
                    dumper.close_ellipse()
                elif shape.type == "polygon":
                    dumper.close_polygon()
                elif shape.type == "polyline":
                    dumper.close_polyline()
                elif shape.type == "points":
                    dumper.close_points()
                elif shape.type == "cuboid":
                    dumper.close_cuboid()
                elif shape.type == "mask":
                    dumper.close_mask()
                elif shape.type == "skeleton":
                    dumper.close_skeleton()
                else:
                    raise NotImplementedError("unknown shape type")

        dump_labeled_shapes(frame_annotation.labeled_shapes)

        for tag in frame_annotation.tags:
            tag_data = OrderedDict([("label", tag.label), ("source", tag.source)])
            if tag.group:
                tag_data["group_id"] = str(tag.group)
            dumper.open_tag(tag_data)

            for attr in tag.attributes:
                dumper.add_attribute(OrderedDict([("name", attr.name), ("value", attr.value)]))

            dumper.close_tag()

        dumper.close_image()
    dumper.close_root()


def dump_as_cvat_interpolation(dumper, annotations):
    dumper.open_root()
    dumper.add_meta(annotations.meta)

    def dump_shape(shape, element_shapes=None, label=None):
        dump_data = OrderedDict()
        if label is None:
            dump_data.update(OrderedDict([("frame", str(shape.frame))]))
        else:
            dump_data.update(OrderedDict([("label", label)]))
        dump_data.update(OrderedDict([("keyframe", str(int(shape.keyframe)))]))

        if shape.type != "skeleton":
            dump_data.update(
                OrderedDict(
                    [("outside", str(int(shape.outside))), ("occluded", str(int(shape.occluded)))]
                )
            )

        if shape.type == "rectangle":
            dump_data.update(
                OrderedDict(
                    [
                        ("xtl", "{:.2f}".format(shape.points[0])),
                        ("ytl", "{:.2f}".format(shape.points[1])),
                        ("xbr", "{:.2f}".format(shape.points[2])),
                        ("ybr", "{:.2f}".format(shape.points[3])),
                    ]
                )
            )

            if shape.rotation:
                dump_data.update(OrderedDict([("rotation", "{:.2f}".format(shape.rotation))]))
        elif shape.type == "ellipse":
            dump_data.update(
                OrderedDict(
                    [
                        ("cx", "{:.2f}".format(shape.points[0])),
                        ("cy", "{:.2f}".format(shape.points[1])),
                        ("rx", "{:.2f}".format(shape.points[2] - shape.points[0])),
                        ("ry", "{:.2f}".format(shape.points[1] - shape.points[3])),
                    ]
                )
            )

            if shape.rotation:
                dump_data.update(OrderedDict([("rotation", "{:.2f}".format(shape.rotation))]))
        elif shape.type == "mask":
            dump_data.update(
                OrderedDict(
                    [
                        ("rle", f"{list(int (v) for v in shape.points[:-4])}"[1:-1]),
                        ("left", f"{int(shape.points[-4])}"),
                        ("top", f"{int(shape.points[-3])}"),
                        ("width", f"{int(shape.points[-2] - shape.points[-4]) + 1}"),
                        ("height", f"{int(shape.points[-1] - shape.points[-3]) + 1}"),
                    ]
                )
            )
        elif shape.type == "cuboid":
            dump_data.update(
                OrderedDict(
                    [
                        ("xtl1", "{:.2f}".format(shape.points[0])),
                        ("ytl1", "{:.2f}".format(shape.points[1])),
                        ("xbl1", "{:.2f}".format(shape.points[2])),
                        ("ybl1", "{:.2f}".format(shape.points[3])),
                        ("xtr1", "{:.2f}".format(shape.points[4])),
                        ("ytr1", "{:.2f}".format(shape.points[5])),
                        ("xbr1", "{:.2f}".format(shape.points[6])),
                        ("ybr1", "{:.2f}".format(shape.points[7])),
                        ("xtl2", "{:.2f}".format(shape.points[8])),
                        ("ytl2", "{:.2f}".format(shape.points[9])),
                        ("xbl2", "{:.2f}".format(shape.points[10])),
                        ("ybl2", "{:.2f}".format(shape.points[11])),
                        ("xtr2", "{:.2f}".format(shape.points[12])),
                        ("ytr2", "{:.2f}".format(shape.points[13])),
                        ("xbr2", "{:.2f}".format(shape.points[14])),
                        ("ybr2", "{:.2f}".format(shape.points[15])),
                    ]
                )
            )
        elif shape.type != "skeleton":
            dump_data.update(
                OrderedDict(
                    [
                        (
                            "points",
                            ";".join(
                                ["{:.2f},{:.2f}".format(x, y) for x, y in pairwise(shape.points)]
                            ),
                        )
                    ]
                )
            )

        if label is None:
            dump_data["z_order"] = str(shape.z_order)

        if shape.type == "rectangle":
            dumper.open_box(dump_data)
        elif shape.type == "ellipse":
            dumper.open_ellipse(dump_data)
        elif shape.type == "polygon":
            dumper.open_polygon(dump_data)
        elif shape.type == "polyline":
            dumper.open_polyline(dump_data)
        elif shape.type == "points":
            dumper.open_points(dump_data)
        elif shape.type == "mask":
            dumper.open_mask(dump_data)
        elif shape.type == "cuboid":
            dumper.open_cuboid(dump_data)
        elif shape.type == "skeleton":
            if element_shapes and element_shapes.get(shape.frame):
                dumper.open_skeleton(dump_data)
                for element_shape, label in element_shapes.get(shape.frame, []):
                    dump_shape(element_shape, label=label)
        else:
            raise NotImplementedError("unknown shape type")

        if (
            shape.type == "skeleton"
            and element_shapes
            and element_shapes.get(shape.frame)
            or shape.type != "skeleton"
        ):
            for attr in shape.attributes:
                dumper.add_attribute(OrderedDict([("name", attr.name), ("value", attr.value)]))

        if shape.type == "rectangle":
            dumper.close_box()
        elif shape.type == "ellipse":
            dumper.close_ellipse()
        elif shape.type == "polygon":
            dumper.close_polygon()
        elif shape.type == "polyline":
            dumper.close_polyline()
        elif shape.type == "points":
            dumper.close_points()
        elif shape.type == "mask":
            dumper.close_mask()
        elif shape.type == "cuboid":
            dumper.close_cuboid()
        elif shape.type == "skeleton":
            if element_shapes and element_shapes.get(shape.frame):
                dumper.close_skeleton()
        else:
            raise NotImplementedError("unknown shape type")

    def dump_track(idx, track):
        track_id = idx
        dump_data = OrderedDict(
            [("id", str(track_id)), ("label", track.label), ("source", track.source)]
        )

        if hasattr(track, "task_id"):
            (task,) = filter(lambda task: task.id == track.task_id, annotations.tasks)
            dump_data.update(
                OrderedDict(
                    [
                        ("task_id", str(track.task_id)),
                        ("subset", get_defaulted_subset(task.subset, annotations.subsets)),
                    ]
                )
            )

        if track.group:
            dump_data["group_id"] = str(track.group)
        dumper.open_track(dump_data)

        element_shapes = {}
        for element_track in track.elements:
            for element_shape in element_track.shapes:
                if element_shape.frame not in element_shapes:
                    element_shapes[element_shape.frame] = []
                element_shapes[element_shape.frame].append((element_shape, element_track.label))

        for shape in track.shapes:
            dump_shape(shape, element_shapes)

        dumper.close_track()

    counter = 0
    for track in annotations.tracks:
        dump_track(counter, track)
        counter += 1

    for shape in annotations.shapes:
        frame_step = (
            annotations.frame_step
            if not isinstance(annotations, ProjectData)
            else annotations.frame_step[shape.task_id]
        )
        if not isinstance(annotations, ProjectData):
            stop_frame = int(annotations.meta[annotations.META_FIELD]["stop_frame"])
        else:
            task_meta = list(
                filter(
                    lambda task: int(task[1]["id"]) == shape.task_id,
                    annotations.meta[annotations.META_FIELD]["tasks"],
                )
            )[0][1]
            stop_frame = int(task_meta["stop_frame"])
        track = {
            "label": shape.label,
            "group": shape.group,
            "source": shape.source,
            "shapes": [
                annotations.TrackedShape(
                    type=shape.type,
                    points=shape.points,
                    rotation=shape.rotation,
                    occluded=shape.occluded,
                    outside=False,
                    keyframe=True,
                    z_order=shape.z_order,
                    frame=shape.frame,
                    attributes=shape.attributes,
                )
            ]
            + (  # add a finishing frame if it does not hop over the last frame
                [
                    annotations.TrackedShape(
                        type=shape.type,
                        points=shape.points,
                        rotation=shape.rotation,
                        occluded=shape.occluded,
                        outside=True,
                        keyframe=True,
                        z_order=shape.z_order,
                        frame=shape.frame + frame_step,
                        attributes=shape.attributes,
                    )
                ]
                if shape.frame + frame_step < stop_frame
                else []
            ),
            "elements": [
                annotations.Track(
                    label=element.label,
                    group=element.group,
                    source=element.source,
                    shapes=[
                        annotations.TrackedShape(
                            type=element.type,
                            points=element.points,
                            rotation=element.rotation,
                            occluded=element.occluded,
                            outside=element.outside,
                            keyframe=True,
                            z_order=element.z_order,
                            frame=element.frame,
                            attributes=element.attributes,
                        )
                    ]
                    + (  # add a finishing frame if it does not hop over the last frame
                        [
                            annotations.TrackedShape(
                                type=element.type,
                                points=element.points,
                                rotation=element.rotation,
                                occluded=element.occluded,
                                outside=True,
                                keyframe=True,
                                z_order=element.z_order,
                                frame=element.frame + frame_step,
                                attributes=element.attributes,
                            )
                        ]
                        if element.frame + frame_step < stop_frame
                        else []
                    ),
                    elements=[],
                )
                for element in shape.elements
            ],
        }
        if isinstance(annotations, ProjectData):
            track["task_id"] = shape.task_id
            for element in track["elements"]:
                element.task_id = shape.task_id
        dump_track(counter, annotations.Track(**track))
        counter += 1

    dumper.close_root()


def load_anno(file_object, annotations):
    supported_shapes = (
        "box",
        "ellipse",
        "polygon",
        "polyline",
        "points",
        "cuboid",
        "skeleton",
        "mask",
    )
    context = ElementTree.iterparse(file_object, events=("start", "end"))
    context = iter(context)
    next(context)

    track = None
    shape = None
    shape_element = None
    tag = None
    image_is_opened = False
    attributes = None
    elem_attributes = None
    track_elements = None
    for ev, el in context:
        if ev == "start":
            if el.tag == "track":
                track = annotations.Track(
                    label=el.attrib["label"],
                    group=int(el.attrib.get("group_id", 0)),
                    source="file",
                    shapes=[],
                    elements=[],
                )
            elif el.tag == "image":
                image_is_opened = True
                frame_id = annotations.abs_frame_id(
                    match_dm_item(
                        DatasetItem(
                            id=osp.splitext(el.attrib["name"])[0],
                            attributes={"frame": el.attrib["id"]},
                            media=Image.from_file(path=el.attrib["name"]),
                        ),
                        instance_data=annotations,
                    )
                )
            elif el.tag in supported_shapes and (track is not None or image_is_opened):
                if shape and shape["type"] == "skeleton":
                    elem_attributes = []
                    shape_element = {
                        "attributes": elem_attributes,
                        "points": [],
                        "type": "rectangle" if el.tag == "box" else el.tag,
                    }
                    if track is not None and el.attrib["label"] not in track_elements:
                        track_elements[el.attrib["label"]] = annotations.Track(
                            label=el.attrib["label"],
                            group=0,
                            source="file",
                            shapes=[],
                            elements=[],
                        )
                else:
                    attributes = []
                    shape = {
                        "attributes": attributes,
                        "points": [],
                        "type": "rectangle" if el.tag == "box" else el.tag,
                    }
                    if track is None:
                        shape["elements"] = []
                    elif shape["type"] == "skeleton":
                        shape["frame"] = el.attrib["frame"]
                        if track_elements is None:
                            track_elements = {}
            elif el.tag == "tag" and image_is_opened:
                attributes = []
                tag = {
                    "frame": frame_id,
                    "label": el.attrib["label"],
                    "group": int(el.attrib.get("group_id", 0)),
                    "attributes": attributes,
                    "source": "file",
                }
        elif ev == "end":
            if el.tag == "attribute" and elem_attributes is not None and shape_element is not None:
                elem_attributes.append(
                    annotations.Attribute(name=el.attrib["name"], value=el.text or "")
                )
            if el.tag == "attribute" and attributes is not None and shape_element is None:
                attributes.append(
                    annotations.Attribute(name=el.attrib["name"], value=el.text or "")
                )
            if el.tag in supported_shapes and shape["type"] == "skeleton" and el.tag != "skeleton":
                shape_element["label"] = el.attrib["label"]

                shape_element["occluded"] = el.attrib["occluded"] == "1"
                shape_element["outside"] = el.attrib["outside"] == "1"
                shape_element["elements"] = []

                if el.tag == "box":
                    shape_element["points"].append(float(el.attrib["xtl"]))
                    shape_element["points"].append(float(el.attrib["ytl"]))
                    shape_element["points"].append(float(el.attrib["xbr"]))
                    shape_element["points"].append(float(el.attrib["ybr"]))
                elif el.tag == "ellipse":
                    shape_element["points"].append(float(el.attrib["cx"]))
                    shape_element["points"].append(float(el.attrib["cy"]))
                    shape_element["points"].append(
                        float("{:.2f}".format(float(el.attrib["cx"]) + float(el.attrib["rx"])))
                    )
                    shape_element["points"].append(
                        float("{:.2f}".format(float(el.attrib["cy"]) - float(el.attrib["ry"])))
                    )
                elif el.tag == "cuboid":
                    shape_element["points"].append(float(el.attrib["xtl1"]))
                    shape_element["points"].append(float(el.attrib["ytl1"]))
                    shape_element["points"].append(float(el.attrib["xbl1"]))
                    shape_element["points"].append(float(el.attrib["ybl1"]))
                    shape_element["points"].append(float(el.attrib["xtr1"]))
                    shape_element["points"].append(float(el.attrib["ytr1"]))
                    shape_element["points"].append(float(el.attrib["xbr1"]))
                    shape_element["points"].append(float(el.attrib["ybr1"]))

                    shape_element["points"].append(float(el.attrib["xtl2"]))
                    shape_element["points"].append(float(el.attrib["ytl2"]))
                    shape_element["points"].append(float(el.attrib["xbl2"]))
                    shape_element["points"].append(float(el.attrib["ybl2"]))
                    shape_element["points"].append(float(el.attrib["xtr2"]))
                    shape_element["points"].append(float(el.attrib["ytr2"]))
                    shape_element["points"].append(float(el.attrib["xbr2"]))
                    shape_element["points"].append(float(el.attrib["ybr2"]))
                else:
                    for pair in el.attrib["points"].split(";"):
                        shape_element["points"].extend(map(float, pair.split(",")))

                if track is None:
                    shape_element["frame"] = frame_id
                    shape_element["source"] = "file"
                    shape["elements"].append(annotations.LabeledShape(**shape_element))
                else:
                    shape_element["frame"] = shape["frame"]
                    shape_element["keyframe"] = el.attrib["keyframe"] == "1"
                    if shape_element["keyframe"]:
                        track_elements[el.attrib["label"]].shapes.append(
                            annotations.TrackedShape(**shape_element)
                        )
                shape_element = None

            elif el.tag in supported_shapes:
                if track is not None:
                    shape["frame"] = el.attrib["frame"]
                    shape["outside"] = el.attrib.get("outside", "0") == "1"
                    shape["keyframe"] = el.attrib["keyframe"] == "1"
                else:
                    shape["frame"] = frame_id
                    shape["label"] = el.attrib["label"]
                    shape["group"] = int(el.attrib.get("group_id", 0))
                    shape["source"] = "file"
                    shape["outside"] = False

                shape["occluded"] = el.attrib.get("occluded", "0") == "1"
                shape["z_order"] = int(el.attrib.get("z_order", 0))
                shape["rotation"] = float(el.attrib.get("rotation", 0))

                if el.tag == "box":
                    shape["points"].append(float(el.attrib["xtl"]))
                    shape["points"].append(float(el.attrib["ytl"]))
                    shape["points"].append(float(el.attrib["xbr"]))
                    shape["points"].append(float(el.attrib["ybr"]))
                elif el.tag == "ellipse":
                    shape["points"].append(float(el.attrib["cx"]))
                    shape["points"].append(float(el.attrib["cy"]))
                    shape["points"].append(
                        float("{:.2f}".format(float(el.attrib["cx"]) + float(el.attrib["rx"])))
                    )
                    shape["points"].append(
                        float("{:.2f}".format(float(el.attrib["cy"]) - float(el.attrib["ry"])))
                    )
                elif el.tag == "mask":
                    shape["points"] = list(map(int, el.attrib["rle"].split(",")))
                    shape["points"].append(int(el.attrib["left"]))
                    shape["points"].append(int(el.attrib["top"]))
                    shape["points"].append(int(el.attrib["left"]) + int(el.attrib["width"]) - 1)
                    shape["points"].append(int(el.attrib["top"]) + int(el.attrib["height"]) - 1)
                elif el.tag == "cuboid":
                    shape["points"].append(float(el.attrib["xtl1"]))
                    shape["points"].append(float(el.attrib["ytl1"]))
                    shape["points"].append(float(el.attrib["xbl1"]))
                    shape["points"].append(float(el.attrib["ybl1"]))
                    shape["points"].append(float(el.attrib["xtr1"]))
                    shape["points"].append(float(el.attrib["ytr1"]))
                    shape["points"].append(float(el.attrib["xbr1"]))
                    shape["points"].append(float(el.attrib["ybr1"]))

                    shape["points"].append(float(el.attrib["xtl2"]))
                    shape["points"].append(float(el.attrib["ytl2"]))
                    shape["points"].append(float(el.attrib["xbl2"]))
                    shape["points"].append(float(el.attrib["ybl2"]))
                    shape["points"].append(float(el.attrib["xtr2"]))
                    shape["points"].append(float(el.attrib["ytr2"]))
                    shape["points"].append(float(el.attrib["xbr2"]))
                    shape["points"].append(float(el.attrib["ybr2"]))
                elif el.tag == "skeleton":
                    pass
                else:
                    for pair in el.attrib["points"].split(";"):
                        shape["points"].extend(map(float, pair.split(",")))

                if track is not None:
                    if shape["keyframe"]:
                        track.shapes.append(annotations.TrackedShape(**shape))
                else:
                    annotations.add_shape(annotations.LabeledShape(**shape))
                shape = None

            elif el.tag == "track":
                if track.shapes[0].type == "mask":
                    # convert mask tracks to shapes
                    # because mask track are not supported
                    annotations.add_shape(
                        annotations.LabeledShape(
                            **{
                                "attributes": track.shapes[0].attributes,
                                "points": track.shapes[0].points,
                                "type": track.shapes[0].type,
                                "occluded": track.shapes[0].occluded,
                                "frame": track.shapes[0].frame,
                                "source": track.shapes[0].source,
                                "rotation": track.shapes[0].rotation,
                                "z_order": track.shapes[0].z_order,
                                "group": track.shapes[0].group,
                                "label": track.label,
                            }
                        )
                    )
                else:
                    if track_elements is not None:
                        for element in track_elements.values():
                            track.elements.append(element)
                        track_elements = None
                    annotations.add_track(track)
                track = None
            elif el.tag == "image":
                image_is_opened = False
            elif el.tag == "tag":
                annotations.add_tag(annotations.Tag(**tag))
                tag = None
            el.clear()


def dump_task_or_job_anno(dst_file, instance_data, callback):
    dumper = create_xml_dumper(dst_file)
    dumper.open_document()
    callback(dumper, instance_data)
    dumper.close_document()


def dump_project_anno(dst_file: BufferedWriter, project_data: ProjectData, callback: Callable):
    dumper = create_xml_dumper(dst_file)
    dumper.open_document()
    callback(dumper, project_data)
    dumper.close_document()


def dump_media_files(
    instance_data: Union[TaskData, JobData], img_dir: str, project_data: ProjectData = None
):
    frame_provider = make_frame_provider(instance_data.db_instance)

    ext = ""
    if instance_data.meta[instance_data.META_FIELD]["mode"] == "interpolation":
        ext = frame_provider.VIDEO_FRAME_EXT

    frames = frame_provider.iterate_frames(
        start_frame=instance_data.start,
        stop_frame=instance_data.stop,
        quality=FrameQuality.ORIGINAL,
        out_type=FrameOutputType.BUFFER,
    )
    included_frames = instance_data.get_included_frames()

    for frame_id, frame in zip(instance_data.rel_range, frames):
        # exclude deleted frames and honeypots
        if frame_id not in included_frames:
            continue
        frame_name = (
            instance_data.frame_info[frame_id]["path"]
            if project_data is None
            else project_data.frame_info[(instance_data.db_instance.id, frame_id)]["path"]
        )
        img_path = osp.join(img_dir, frame_name + ext)
        os.makedirs(osp.dirname(img_path), exist_ok=True)
        with open(img_path, "wb") as f:
            f.write(frame.data.getvalue())


def _export_task_or_job(dst_file, temp_dir, instance_data, anno_callback, save_images=False):
    with open(osp.join(temp_dir, "annotations.xml"), "wb") as f:
        dump_task_or_job_anno(f, instance_data, anno_callback)

    if save_images:
        dump_media_files(instance_data, osp.join(temp_dir, "images"))

    make_zip_archive(temp_dir, dst_file)


def _export_project(
    dst_file: str,
    temp_dir: str,
    project_data: ProjectData,
    anno_callback: Callable,
    save_images: bool = False,
):
    with open(osp.join(temp_dir, "annotations.xml"), "wb") as f:
        dump_project_anno(f, project_data, anno_callback)

    if save_images:
        for task_data in project_data.all_task_data:
            subset = get_defaulted_subset(task_data.db_instance.subset, project_data.subsets)
            subset_dir = osp.join(temp_dir, "images", subset)
            os.makedirs(subset_dir, exist_ok=True)
            dump_media_files(task_data, subset_dir, project_data)

    make_zip_archive(temp_dir, dst_file)


@exporter(name="CVAT for video", ext="ZIP", version="1.1")
def _export_video(dst_file, temp_dir, instance_data, save_images=False):
    if isinstance(instance_data, ProjectData):
        _export_project(
            dst_file,
            temp_dir,
            instance_data,
            anno_callback=dump_as_cvat_interpolation,
            save_images=save_images,
        )
    else:
        _export_task_or_job(
            dst_file,
            temp_dir,
            instance_data,
            anno_callback=dump_as_cvat_interpolation,
            save_images=save_images,
        )


@exporter(name="CVAT for images", ext="ZIP", version="1.1")
def _export_images(dst_file, temp_dir, instance_data, save_images=False):
    if isinstance(instance_data, ProjectData):
        _export_project(
            dst_file,
            temp_dir,
            instance_data,
            anno_callback=dump_as_cvat_annotation,
            save_images=save_images,
        )
    else:
        _export_task_or_job(
            dst_file,
            temp_dir,
            instance_data,
            anno_callback=dump_as_cvat_annotation,
            save_images=save_images,
        )


@importer(name="CVAT", ext="XML, ZIP", version="1.1")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    is_zip = zipfile.is_zipfile(src_file)
    src_file.seek(0)
    if is_zip:
        zipfile.ZipFile(src_file).extractall(temp_dir)

        detect_dataset(temp_dir, format_name="cvat", importer=_CvatImporter)
        if isinstance(instance_data, ProjectData):
            dataset = Dataset.import_from(temp_dir, "cvat", env=dm_env)
            if load_data_callback is not None:
                load_data_callback(dataset, instance_data)
            import_dm_annotations(dataset, instance_data)
        else:
            anno_paths = glob(osp.join(temp_dir, "**", "*.xml"), recursive=True)
            for p in anno_paths:
                load_anno(p, instance_data)
    else:
        if load_data_callback:
            raise NoMediaInAnnotationFileError()

        load_anno(src_file, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\datumaro.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.dataset import Dataset

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    NoMediaInAnnotationFileError,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive
from cvat.apps.engine.models import DimensionType

from .registry import dm_env, exporter, importer


@exporter(name="Datumaro", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data=instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "datumaro", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="Datumaro", ext="JSON, ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)

        detect_dataset(temp_dir, format_name="datumaro", importer=dm_env.importers.get("datumaro"))
        dataset = Dataset.import_from(temp_dir, "datumaro", env=dm_env)
    else:
        if load_data_callback:
            raise NoMediaInAnnotationFileError()

        dataset = Dataset.import_from(src_file.name, "datumaro", env=dm_env)

    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


@exporter(name="Datumaro 3D", ext="ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(
        instance_data=instance_data, include_images=save_images, dimension=DimensionType.DIM_3D
    ) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "datumaro", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="Datumaro 3D", ext="JSON, ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)

        detect_dataset(temp_dir, format_name="datumaro", importer=dm_env.importers.get("datumaro"))
        dataset = Dataset.import_from(temp_dir, "datumaro", env=dm_env)
    else:
        if load_data_callback:
            raise NoMediaInAnnotationFileError()

        dataset = Dataset.import_from(src_file.name, "datumaro", env=dm_env)

    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\icdar.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.annotation import AnnotationType, Caption, Label, LabelCategories
from datumaro.components.dataset import Dataset
from datumaro.components.transformer import ItemTransform

from cvat.apps.dataset_manager.bindings import GetCVATDataExtractor, import_dm_annotations
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons


class AddLabelToAnns(ItemTransform):
    def __init__(self, extractor, label):
        super().__init__(extractor)

        assert isinstance(label, str)
        self._categories = {}
        label_cat = self._extractor.categories().get(AnnotationType.label)
        if not label_cat:
            label_cat = LabelCategories()
        self._label = label_cat.add(label)
        self._categories[AnnotationType.label] = label_cat

    def categories(self):
        return self._categories

    def transform_item(self, item):
        annotations = item.annotations
        for ann in annotations:
            if ann.type in [AnnotationType.polygon, AnnotationType.bbox, AnnotationType.mask]:
                ann.label = self._label
        return item.wrap(annotations=annotations)


class CaptionToLabel(ItemTransform):
    def __init__(self, extractor, label):
        super().__init__(extractor)

        assert isinstance(label, str)
        self._categories = {}
        label_cat = self._extractor.categories().get(AnnotationType.label)
        if not label_cat:
            label_cat = LabelCategories()
        self._label = label_cat.add(label)
        self._categories[AnnotationType.label] = label_cat

    def categories(self):
        return self._categories

    def transform_item(self, item):
        annotations = item.annotations
        captions = [ann for ann in annotations if ann.type == AnnotationType.caption]
        for ann in captions:
            annotations.append(Label(self._label, attributes={"text": ann.caption}))
            annotations.remove(ann)
        return item.wrap(annotations=annotations)


class LabelToCaption(ItemTransform):
    def transform_item(self, item):
        annotations = item.annotations
        anns = [p for p in annotations if "text" in p.attributes]
        for ann in anns:
            annotations.append(Caption(ann.attributes["text"]))
            annotations.remove(ann)
        return item.wrap(annotations=annotations)


@exporter(name="ICDAR Recognition", ext="ZIP", version="1.0")
def _export_recognition(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(LabelToCaption)
        dataset.export(temp_dir, "icdar_word_recognition", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="ICDAR Recognition", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    # We do not run detect_dataset before import because the ICDAR format
    # has problem with the dataset detection in case of empty annotation file(s)
    # Details in: https://github.com/cvat-ai/datumaro/issues/43
    dataset = Dataset.import_from(temp_dir, "icdar_word_recognition", env=dm_env)
    dataset.transform(CaptionToLabel, label="icdar")
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


@exporter(name="ICDAR Localization", ext="ZIP", version="1.0")
def _export_localization(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "icdar_text_localization", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="ICDAR Localization", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    # We do not run detect_dataset before import because the ICDAR format
    # has problem with the dataset detection in case of empty annotation file(s)
    # Details in: https://github.com/cvat-ai/datumaro/issues/43
    dataset = Dataset.import_from(temp_dir, "icdar_text_localization", env=dm_env)
    dataset.transform(AddLabelToAnns, label="icdar")
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


@exporter(name="ICDAR Segmentation", ext="ZIP", version="1.0")
def _export_segmentation(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("boxes_to_masks")
        dataset.transform("merge_instance_segments")
        dataset.export(temp_dir, "icdar_text_segmentation", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="ICDAR Segmentation", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    # We do not run detect_dataset before import because the ICDAR format
    # has problem with the dataset detection in case of empty annotation file(s)
    # Details in: https://github.com/cvat-ai/datumaro/issues/43
    dataset = Dataset.import_from(temp_dir, "icdar_text_segmentation", env=dm_env)
    dataset.transform(AddLabelToAnns, label="icdar")
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\imagenet.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp
import zipfile
from glob import glob

from datumaro.components.dataset import Dataset
from datumaro.plugins.transforms import Rename

from cvat.apps.dataset_manager.bindings import GetCVATDataExtractor, import_dm_annotations
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="ImageNet", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        if save_images:
            dataset.export(temp_dir, "imagenet", save_media=save_images)
        else:
            dataset.export(temp_dir, "imagenet_txt", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="ImageNet", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    # We do not run detect_dataset before import because the Imagenet format
    # has problem with the dataset detection in case of empty annotation file(s)
    # Details in: https://github.com/cvat-ai/datumaro/issues/43
    if glob(osp.join(temp_dir, "*.txt")):
        dataset = Dataset.import_from(temp_dir, "imagenet_txt", env=dm_env)
    else:
        dataset = Dataset.import_from(temp_dir, "imagenet", env=dm_env)
        # Rename dataset items from "label:name" to "label/name" for frame matching to work
        dataset = dataset.transform(Rename, regex="|([^:]+):(.*)|\\1/\\2|")
        if load_data_callback is not None:
            load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\kitti.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp

from datumaro.components.dataset import Dataset
from datumaro.plugins.data_formats.kitti.format import KittiPath, write_label_map
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons
from .utils import make_colormap


@exporter(name="KITTI", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)

        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("merge_instance_segments")
        dataset.export(
            temp_dir,
            format="kitti",
            label_map={k: v[0] for k, v in make_colormap(instance_data).items()},
            apply_colormap=True,
            save_media=save_images,
        )

    make_zip_archive(temp_dir, dst_file)


@importer(name="KITTI", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    color_map = {k: v[0] for k, v in make_colormap(instance_data).items()}
    color_map_path = osp.join(temp_dir, KittiPath.LABELMAP_FILE)
    if not osp.isfile(color_map_path):
        write_label_map(color_map_path, color_map)

    detect_dataset(temp_dir, format_name="kitti", importer=dm_env.importers.get("kitti"))
    dataset = Dataset.import_from(temp_dir, format="kitti", env=dm_env)
    labels_meta = instance_data.meta[instance_data.META_FIELD]["labels"]
    if "background" not in [label["name"] for _, label in labels_meta]:
        dataset.filter('/item/annotation[label != "background"]', filter_annotations=True)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)

    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\labelme.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.dataset import Dataset
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.formats.transformations import MaskToPolygonTransformation
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="LabelMe", ext="ZIP", version="3.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "label_me", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="LabelMe", ext="ZIP", version="3.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="label_me", importer=dm_env.importers.get("label_me"))
    dataset = Dataset.import_from(temp_dir, "label_me", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\lfw.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.dataset import Dataset
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@importer(name="LFW", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="lfw", importer=dm_env.importers.get("lfw"))
    dataset = Dataset.import_from(temp_dir, "lfw")
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


@exporter(name="LFW", ext="ZIP", version="1.0")
def _exporter(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, format="lfw", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\market1501.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.annotation import AnnotationType, Label, LabelCategories
from datumaro.components.dataset import Dataset
from datumaro.components.transformer import ItemTransform

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


class AttrToLabelAttr(ItemTransform):
    def __init__(self, extractor, label):
        super().__init__(extractor)

        assert isinstance(label, str)
        self._categories = {}
        label_cat = self._extractor.categories().get(AnnotationType.label)
        if not label_cat:
            label_cat = LabelCategories()
        self._label = label_cat.add(label)
        self._categories[AnnotationType.label] = label_cat

    def categories(self):
        return self._categories

    def transform_item(self, item):
        annotations = list(item.annotations)
        attributes = item.attributes
        if item.attributes:
            annotations.append(Label(self._label, attributes=item.attributes))
            attributes = {}
        return item.wrap(annotations=annotations, attributes=attributes)


class LabelAttrToAttr(ItemTransform):
    def __init__(self, extractor, label):
        super().__init__(extractor)

        assert isinstance(label, str)
        label_cat = self._extractor.categories().get(AnnotationType.label)
        self._label = label_cat.find(label)[0]

    def transform_item(self, item):
        annotations = list(item.annotations)
        attributes = dict(item.attributes)
        if self._label is not None:
            labels = [
                ann
                for ann in annotations
                if ann.type == AnnotationType.label and ann.label == self._label
            ]
            if len(labels) == 1:
                attributes.update(labels[0].attributes)
                annotations.remove(labels[0])
        return item.wrap(annotations=annotations, attributes=attributes)


@exporter(name="Market-1501", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)

        dataset.transform(LabelAttrToAttr, label="market-1501")
        dataset.export(temp_dir, "market1501", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="Market-1501", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="market1501", importer=dm_env.importers.get("market1501"))
    dataset = Dataset.import_from(temp_dir, "market1501", env=dm_env)
    dataset.transform(AttrToLabelAttr, label="market-1501")
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\mask.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.dataset import Dataset
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons
from .utils import make_colormap


@exporter(name="Segmentation mask", ext="ZIP", version="1.1")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("boxes_to_masks")
        dataset.transform("merge_instance_segments")

        dataset.export(
            temp_dir,
            "voc_segmentation",
            save_media=save_images,
            apply_colormap=True,
            label_map=make_colormap(instance_data),
        )

    make_zip_archive(temp_dir, dst_file)


@importer(name="Segmentation mask", ext="ZIP", version="1.1")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="voc", importer=dm_env.importers.get("voc"))
    dataset = Dataset.import_from(temp_dir, "voc", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\mot.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import datumaro as dm
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import GetCVATDataExtractor, detect_dataset
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


def _import_to_task(dataset, instance_data):
    tracks = {}
    label_cat = dataset.categories()[dm.AnnotationType.label]

    for item in dataset:
        # NOTE: MOT frames start from 1
        # job has an offset, for task offset is 0
        frame_number = int(item.id) - 1 + instance_data.start
        frame_number = instance_data.abs_frame_id(frame_number)

        for ann in item.annotations:
            if ann.type != dm.AnnotationType.bbox:
                continue

            occluded = ann.attributes.pop("occluded", False) is True
            track_id = ann.attributes.pop("track_id", None)
            attributes = [
                instance_data.Attribute(name=n, value=str(v)) for n, v in ann.attributes.items()
            ]
            if track_id is None:
                # Extension. Import regular boxes:
                instance_data.add_shape(
                    instance_data.LabeledShape(
                        type="rectangle",
                        label=label_cat.items[ann.label].name,
                        points=ann.points,
                        occluded=occluded,
                        z_order=ann.z_order,
                        group=0,
                        frame=frame_number,
                        attributes=attributes,
                        source="manual",
                    )
                )
                continue

            shape = instance_data.TrackedShape(
                type="rectangle",
                points=ann.points,
                occluded=occluded,
                outside=False,
                keyframe=True,
                z_order=ann.z_order,
                frame=frame_number,
                attributes=attributes,
                source="manual",
            )

            # build trajectories as lists of shapes in track dict
            if track_id not in tracks:
                tracks[track_id] = instance_data.Track(
                    label_cat.items[ann.label].name, 0, "manual", []
                )
            tracks[track_id].shapes.append(shape)

    for track in tracks.values():
        # MOT annotations do not require frames to be ordered
        track.shapes.sort(key=lambda t: t.frame)

        # insert outside=True in skips between the frames track is visible
        prev_shape_idx = 0
        prev_shape = track.shapes[0]
        for shape in track.shapes[1:]:
            has_skip = instance_data.frame_step < shape.frame - prev_shape.frame
            if has_skip and not prev_shape.outside:
                prev_shape = prev_shape._replace(
                    outside=True, frame=prev_shape.frame + instance_data.frame_step
                )
                prev_shape_idx += 1
                track.shapes.insert(prev_shape_idx, prev_shape)
            prev_shape = shape
            prev_shape_idx += 1

        # Append a shape with outside=True to finish the track
        last_shape = track.shapes[-1]
        if last_shape.frame + instance_data.frame_step <= int(
            instance_data.meta[instance_data.META_FIELD]["stop_frame"]
        ):
            track.shapes.append(
                last_shape._replace(outside=True, frame=last_shape.frame + instance_data.frame_step)
            )
        instance_data.add_track(track)


@exporter(name="MOT", ext="ZIP", version="1.1")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = dm.Dataset.from_extractors(extractor, env=dm_env)

        dataset.export(temp_dir, "mot_seq_gt", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="MOT", ext="ZIP", version="1.1")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="mot_seq", importer=dm_env.importers.get("mot_seq"))
    dataset = dm.Dataset.import_from(temp_dir, "mot_seq", env=dm_env)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)

    # Dirty way to determine instance type to avoid circular dependency
    if hasattr(instance_data, "_db_project"):
        for sub_dataset, task_data in instance_data.split_dataset(dataset):
            _import_to_task(sub_dataset, task_data)
    else:
        _import_to_task(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\mots.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.annotation import AnnotationType
from datumaro.components.dataset import Dataset
from datumaro.components.transformer import ItemTransform
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    find_dataset_root,
    match_dm_item,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons


class KeepTracks(ItemTransform):
    def transform_item(self, item):
        return item.wrap(annotations=[a for a in item.annotations if "track_id" in a.attributes])


def _import_to_task(dataset, instance_data):
    tracks = {}
    label_cat = dataset.categories()[AnnotationType.label]

    root_hint = find_dataset_root(dataset, instance_data)

    shift = 0
    for item in dataset:
        frame_number = instance_data.abs_frame_id(
            match_dm_item(item, instance_data, root_hint=root_hint)
        )

        track_ids = set()

        for ann in item.annotations:
            if ann.type != AnnotationType.polygon:
                continue

            track_id = ann.attributes["track_id"]
            group_id = track_id

            if track_id in track_ids:
                # use negative id for tracks with the same id on the same frame
                shift -= 1
                track_id = shift
            else:
                track_ids.add(track_id)

            shape = instance_data.TrackedShape(
                type="polygon",
                points=ann.points,
                occluded=ann.attributes.get("occluded") is True,
                outside=False,
                keyframe=True,
                z_order=ann.z_order,
                frame=frame_number,
                attributes=[],
                source="manual",
                group=group_id,
            )

            # build trajectories as lists of shapes in track dict
            if track_id not in tracks:
                tracks[track_id] = instance_data.Track(
                    label_cat.items[ann.label].name, 0, "manual", []
                )
            tracks[track_id].shapes.append(shape)

    for track in tracks.values():
        track.shapes.sort(key=lambda t: t.frame)

        # insert outside=True in skips between the frames track is visible
        prev_shape_idx = 0
        prev_shape = track.shapes[0]
        for shape in track.shapes[1:]:
            has_skip = instance_data.frame_step < shape.frame - prev_shape.frame
            if has_skip and not prev_shape.outside:
                prev_shape = prev_shape._replace(
                    outside=True, frame=prev_shape.frame + instance_data.frame_step
                )
                prev_shape_idx += 1
                track.shapes.insert(prev_shape_idx, prev_shape)
            prev_shape = shape
            prev_shape_idx += 1

        # Append a shape with outside=True to finish the track
        last_shape = track.shapes[-1]
        if last_shape.frame + instance_data.frame_step <= int(
            instance_data.meta[instance_data.META_FIELD]["stop_frame"]
        ):
            track.shapes.append(
                last_shape._replace(outside=True, frame=last_shape.frame + instance_data.frame_step)
            )
        instance_data.add_track(track)


@exporter(name="MOTS PNG", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(KeepTracks)  # can only export tracks
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("boxes_to_masks")
        dataset.transform("merge_instance_segments")

        dataset.export(temp_dir, "mots_png", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="MOTS PNG", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="mots", importer=dm_env.importers.get("mots"))
    dataset = Dataset.import_from(temp_dir, "mots", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)

    # Dirty way to determine instance type to avoid circular dependency
    if hasattr(instance_data, "_db_project"):
        for sub_dataset, task_data in instance_data.split_dataset(dataset):
            _import_to_task(sub_dataset, task_data)
    else:
        _import_to_task(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\openimages.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import glob
import os.path as osp

from datumaro.components.dataset import Dataset, DatasetItem
from datumaro.plugins.data_formats.open_images import OpenImagesPath
from datumaro.util.image import DEFAULT_IMAGE_META_FILE_NAME
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    find_dataset_root,
    import_dm_annotations,
    match_dm_item,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import MaskToPolygonTransformation, RotatedBoxesToPolygons


def find_item_ids(path):
    image_desc_patterns = (
        OpenImagesPath.FULL_IMAGE_DESCRIPTION_FILE_NAME,
        *OpenImagesPath.SUBSET_IMAGE_DESCRIPTION_FILE_PATTERNS,
    )

    image_desc_patterns = (
        osp.join(path, OpenImagesPath.ANNOTATIONS_DIR, pattern) for pattern in image_desc_patterns
    )

    for pattern in image_desc_patterns:
        for path in glob.glob(pattern):
            with open(path, "r") as desc:
                next(desc)
                for row in desc:
                    yield row.split(",")[0]


@exporter(name="Open Images V6", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, task_data, save_images=False):
    with GetCVATDataExtractor(task_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RotatedBoxesToPolygons)
        dataset.transform("polygons_to_masks")
        dataset.transform("merge_instance_segments")

        dataset.export(temp_dir, "open_images", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="Open Images V6", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    image_meta_path = osp.join(
        temp_dir, OpenImagesPath.ANNOTATIONS_DIR, DEFAULT_IMAGE_META_FILE_NAME
    )
    image_meta = None

    if not osp.isfile(image_meta_path):
        image_meta = {}
        item_ids = list(find_item_ids(temp_dir))

        root_hint = find_dataset_root(
            [DatasetItem(id=item_id) for item_id in item_ids], instance_data
        )

        for item_id in item_ids:
            frame_info = None
            try:
                frame_id = match_dm_item(DatasetItem(id=item_id), instance_data, root_hint)
                frame_info = instance_data.frame_info[frame_id]
            except Exception:  # nosec
                pass
            if frame_info is not None:
                image_meta[item_id] = (frame_info["height"], frame_info["width"])

    detect_dataset(
        temp_dir, format_name="open_images", importer=dm_env.importers.get("open_images")
    )
    dataset = Dataset.import_from(temp_dir, "open_images", image_meta=image_meta, env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\pascal_voc.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import os.path as osp
import shutil
from glob import glob

from datumaro.components.dataset import Dataset
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.formats.transformations import MaskToPolygonTransformation
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="PASCAL VOC", ext="ZIP", version="1.1")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)

        dataset.export(temp_dir, "voc", save_media=save_images, label_map="source")

    make_zip_archive(temp_dir, dst_file)


@importer(name="PASCAL VOC", ext="ZIP", version="1.1")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    Archive(src_file.name).extractall(temp_dir)

    # put label map from the task if not present
    labelmap_file = osp.join(temp_dir, "labelmap.txt")
    if not osp.isfile(labelmap_file):
        labels_meta = instance_data.meta[instance_data.META_FIELD]["labels"]
        labels = (label["name"] + ":::" for _, label in labels_meta)
        with open(labelmap_file, "w") as f:
            f.write("\n".join(labels))

    # support flat archive layout
    anno_dir = osp.join(temp_dir, "Annotations")
    if not osp.isdir(anno_dir):
        anno_files = glob(osp.join(temp_dir, "**", "*.xml"), recursive=True)
        subsets_dir = osp.join(temp_dir, "ImageSets", "Main")
        os.makedirs(subsets_dir, exist_ok=True)
        with open(osp.join(subsets_dir, "train.txt"), "w") as subset_file:
            for f in anno_files:
                subset_file.write(osp.splitext(osp.basename(f))[0] + "\n")

        os.makedirs(anno_dir, exist_ok=True)
        for f in anno_files:
            shutil.move(f, anno_dir)

    detect_dataset(temp_dir, format_name="voc", importer=dm_env.importers.get("voc"))
    dataset = Dataset.import_from(temp_dir, "voc", env=dm_env)
    dataset = MaskToPolygonTransformation.convert_dataset(dataset, **kwargs)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\pointcloud.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.dataset import Dataset

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive
from cvat.apps.engine.models import DimensionType

from .registry import dm_env, exporter, importer


@exporter(name="Sly Point Cloud Format", ext="ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _export_images(dst_file, temp_dir, task_data, save_images=False):
    with GetCVATDataExtractor(
        task_data,
        include_images=save_images,
        format_type="sly_pointcloud",
        dimension=DimensionType.DIM_3D,
    ) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(
            temp_dir, "sly_pointcloud", save_media=save_images, allow_undeclared_attrs=True
        )

    make_zip_archive(temp_dir, dst_file)


@importer(name="Sly Point Cloud Format", ext="ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)

        detect_dataset(
            temp_dir, format_name="sly_pointcloud", importer=dm_env.importers.get("sly_pointcloud")
        )
        dataset = Dataset.import_from(temp_dir, "sly_pointcloud", env=dm_env)
    else:
        dataset = Dataset.import_from(src_file.name, "sly_pointcloud", env=dm_env)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\registry.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from datumaro.components.project import Environment
from cvat.apps.engine.models import DimensionType


dm_env = Environment()


class _Format:
    NAME = ""
    EXT = ""
    VERSION = ""
    DISPLAY_NAME = "{NAME} {VERSION}"
    ENABLED = True


class Exporter(_Format):
    def __call__(self, dst_file, temp_dir, instance_data, **options):
        raise NotImplementedError()


class Importer(_Format):
    def __call__(self, src_file, temp_dir, instance_data, load_data_callback=None, **options):
        raise NotImplementedError()


def _wrap_format(
    f_or_cls, klass, name, version, ext, display_name, enabled, dimension=DimensionType.DIM_2D
):
    import inspect

    assert inspect.isclass(f_or_cls) or inspect.isfunction(f_or_cls)
    if inspect.isclass(f_or_cls):
        assert hasattr(f_or_cls, "__call__")
        target = f_or_cls
    elif inspect.isfunction(f_or_cls):

        class wrapper(klass):
            # pylint: disable=arguments-differ
            def __call__(self, *args, **kwargs):
                f_or_cls(*args, **kwargs)

        wrapper.__name__ = f_or_cls.__name__
        wrapper.__module__ = f_or_cls.__module__
        target = wrapper

    target.NAME = name or klass.NAME or f_or_cls.__name__
    target.VERSION = version or klass.VERSION
    target.EXT = ext or klass.EXT
    target.DISPLAY_NAME = (display_name or klass.DISPLAY_NAME).format(
        NAME=name, VERSION=version, EXT=ext
    )
    assert all([target.NAME, target.VERSION, target.EXT, target.DISPLAY_NAME])
    target.DIMENSION = dimension
    target.ENABLED = enabled

    return target


EXPORT_FORMATS = {}


def format_for(export_format, mode):
    format_name = export_format
    if export_format not in EXPORT_FORMATS:
        if mode == "annotation":
            format_name = "CVAT for images 1.1"
        else:
            format_name = "CVAT for video 1.1"
    return format_name


def exporter(name, version, ext, display_name=None, enabled=True, dimension=DimensionType.DIM_2D):
    assert name not in EXPORT_FORMATS, "Export format '%s' already registered" % name

    def wrap_with_params(f_or_cls):
        t = _wrap_format(
            f_or_cls,
            Exporter,
            name=name,
            ext=ext,
            version=version,
            display_name=display_name,
            enabled=enabled,
            dimension=dimension,
        )
        key = t.DISPLAY_NAME
        assert key not in EXPORT_FORMATS, "Export format '%s' already registered" % name
        EXPORT_FORMATS[key] = t
        return t

    return wrap_with_params


IMPORT_FORMATS = {}


def importer(name, version, ext, display_name=None, enabled=True, dimension=DimensionType.DIM_2D):
    def wrap_with_params(f_or_cls):
        t = _wrap_format(
            f_or_cls,
            Importer,
            name=name,
            ext=ext,
            version=version,
            display_name=display_name,
            enabled=enabled,
            dimension=dimension,
        )
        key = t.DISPLAY_NAME
        assert key not in IMPORT_FORMATS, "Import format '%s' already registered" % name
        IMPORT_FORMATS[key] = t
        return t

    return wrap_with_params


def make_importer(name):
    return IMPORT_FORMATS[name]()


def make_exporter(name):
    return EXPORT_FORMATS[name]()


# pylint: disable=unused-import
import cvat.apps.dataset_manager.formats.coco
import cvat.apps.dataset_manager.formats.cvat
import cvat.apps.dataset_manager.formats.datumaro
import cvat.apps.dataset_manager.formats.labelme
import cvat.apps.dataset_manager.formats.mask
import cvat.apps.dataset_manager.formats.mot
import cvat.apps.dataset_manager.formats.mots
import cvat.apps.dataset_manager.formats.pascal_voc
import cvat.apps.dataset_manager.formats.yolo
import cvat.apps.dataset_manager.formats.imagenet
import cvat.apps.dataset_manager.formats.camvid
import cvat.apps.dataset_manager.formats.widerface
import cvat.apps.dataset_manager.formats.vggface2
import cvat.apps.dataset_manager.formats.market1501
import cvat.apps.dataset_manager.formats.icdar
import cvat.apps.dataset_manager.formats.velodynepoint
import cvat.apps.dataset_manager.formats.pointcloud
import cvat.apps.dataset_manager.formats.kitti
import cvat.apps.dataset_manager.formats.lfw
import cvat.apps.dataset_manager.formats.cityscapes
import cvat.apps.dataset_manager.formats.openimages


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\transformations.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
from itertools import chain

import cv2
import datumaro as dm
import numpy as np
from pycocotools import mask as mask_utils


class RotatedBoxesToPolygons(dm.ItemTransform):
    def _rotate_point(self, p, angle, cx, cy):
        [x, y] = p
        rx = cx + math.cos(angle) * (x - cx) - math.sin(angle) * (y - cy)
        ry = cy + math.sin(angle) * (x - cx) + math.cos(angle) * (y - cy)
        return rx, ry

    def transform_item(self, item):
        annotations = item.annotations[:]
        anns = [
            p for p in annotations if p.type == dm.AnnotationType.bbox and p.attributes["rotation"]
        ]
        for ann in anns:
            rotation = math.radians(ann.attributes["rotation"])
            x0, y0, x1, y1 = ann.points
            [cx, cy] = [(x0 + (x1 - x0) / 2), (y0 + (y1 - y0) / 2)]
            anno_points = list(
                chain.from_iterable(
                    map(
                        lambda p: self._rotate_point(p, rotation, cx, cy),
                        [(x0, y0), (x1, y0), (x1, y1), (x0, y1)],
                    )
                )
            )

            annotations.remove(ann)
            annotations.append(
                dm.Polygon(
                    anno_points,
                    label=ann.label,
                    attributes=ann.attributes,
                    group=ann.group,
                    z_order=ann.z_order,
                )
            )

        return item.wrap(annotations=annotations)


class MaskConverter:
    @staticmethod
    def cvat_rle_to_dm_rle(shape, img_h: int, img_w: int) -> dm.RleMask:
        "Converts a CVAT RLE to a Datumaro / COCO mask"

        # use COCO representation of CVAT RLE to avoid python loops
        left, top, right, bottom = [math.trunc(v) for v in shape.points[-4:]]
        h = bottom - top + 1
        w = right - left + 1
        cvat_as_coco_rle_uncompressed = {
            "counts": shape.points[:-4],
            "size": [w, h],
        }
        cvat_as_coco_rle_compressed = mask_utils.frPyObjects(
            [cvat_as_coco_rle_uncompressed], h=h, w=w
        )[0]

        # expand the mask to the full image size
        tight_mask = mask_utils.decode(cvat_as_coco_rle_compressed).transpose()
        full_mask = np.zeros((img_h, img_w), dtype=np.uint8)
        full_mask[top : bottom + 1, left : right + 1] = tight_mask

        # obtain RLE
        coco_rle = mask_utils.encode(np.asfortranarray(full_mask))
        return dm.RleMask(
            rle=coco_rle,
            label=shape.label,
            z_order=shape.z_order,
            attributes=shape.attributes,
            group=shape.group,
        )

    @classmethod
    def dm_mask_to_cvat_rle(cls, dm_mask: dm.Mask) -> list[int]:
        "Converts a Datumaro mask to a CVAT RLE"

        # get tight mask
        x, y, w, h = dm_mask.get_bbox()
        top = int(y)
        left = int(x)
        bottom = int(max(y, y + h - 1))
        right = int(max(x, x + w - 1))
        tight_binary_mask = dm_mask.image[top : bottom + 1, left : right + 1]

        # obtain RLE
        cvat_rle = cls.rle(tight_binary_mask.reshape(-1))
        cvat_rle += [left, top, right, bottom]
        return cvat_rle

    @classmethod
    def rle(cls, arr: np.ndarray) -> list[int]:
        "Computes RLE for a flat array"
        # adapted from https://stackoverflow.com/a/32681075

        n = len(arr)
        if n == 0:
            return []

        pairwise_unequal = arr[1:] != arr[:-1]
        rle = np.diff(np.nonzero(pairwise_unequal)[0], prepend=-1, append=n - 1)

        # CVAT RLE starts from 0
        cvat_rle = rle.tolist()
        if arr[0] != 0:
            cvat_rle.insert(0, 0)

        return cvat_rle


class EllipsesToMasks:
    @staticmethod
    def _convert(ellipse, img_h, img_w):
        cx, cy, rightX, topY = ellipse.points
        rx = rightX - cx
        ry = cy - topY
        center = (round(cx), round(cy))
        axis = (round(rx), round(ry))
        angle = ellipse.rotation
        mat = np.zeros((img_h, img_w), dtype=np.uint8)

        # TODO: has bad performance for big masks, try to find a better solution
        cv2.ellipse(mat, center, axis, angle, 0, 360, 255, thickness=-1)

        rle = mask_utils.encode(np.asfortranarray(mat))
        return rle

    @staticmethod
    def convert_ellipse(ellipse, img_h, img_w):
        def _lazy_convert():
            return EllipsesToMasks._convert(ellipse, img_h, img_w)

        return dm.RleMask(
            rle=_lazy_convert,
            label=ellipse.label,
            z_order=ellipse.z_order,
            attributes=ellipse.attributes,
            group=ellipse.group,
        )


class MaskToPolygonTransformation:
    """
    Manages common logic for mask to polygons conversion in dataset import.
    This usecase is supposed for backward compatibility for the transition period.
    """

    @classmethod
    def declare_arg_names(cls):
        return ["conv_mask_to_poly"]

    @classmethod
    def convert_dataset(cls, dataset, **kwargs):
        if kwargs.get("conv_mask_to_poly", True):
            dataset.transform("masks_to_polygons")
        return dataset


class SetKeyframeForEveryTrackShape(dm.ItemTransform):
    KEEPS_SUBSETS_INTACT = True

    def transform_item(self, item):
        def convert_annotations():
            annotations = []
            for ann in item.annotations:
                if "track_id" in ann.attributes:
                    ann = ann.wrap(attributes=dict(ann.attributes, keyframe=True))
                annotations.append(ann)
            return annotations

        return item.wrap(annotations=convert_annotations)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\utils.py =====
# Copyright (C) 2019-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import itertools
import operator
import os.path as osp
from hashlib import blake2s

from datumaro.util.os_util import make_file_name


def get_color_from_index(index):
    def get_bit(number, index):
        return (number >> index) & 1

    color = [0, 0, 0]

    for j in range(7, -1, -1):
        for c in range(3):
            color[c] |= get_bit(index, c) << j
        index >>= 3

    return tuple(color)


DEFAULT_COLORMAP_PATH = osp.join(osp.dirname(__file__), "predefined_colors.txt")


def parse_default_colors(file_path=None):
    if file_path is None:
        file_path = DEFAULT_COLORMAP_PATH

    colors = {}
    with open(file_path) as f:
        for line in f:
            line = line.strip()
            if not line or line[0] == "#":
                continue
            _, label, color = line.split(":")
            colors[label] = tuple(map(int, color.split(",")))
    return colors


def normalize_label(label):
    label = make_file_name(label)  # basically, convert to ASCII lowercase
    label = label.replace("-", "_")
    return label


def rgb2hex(color):
    return "#{0:02x}{1:02x}{2:02x}".format(*color)


def hex2rgb(color):
    return tuple(int(color.lstrip("#")[i : i + 2], 16) for i in (0, 2, 4))


def make_colormap(instance_data):
    labels = [label for _, label in instance_data.meta[instance_data.META_FIELD]["labels"]]
    label_names = [label["name"] for label in labels]

    if "background" not in label_names:
        labels.insert(0, {"name": "background", "color": "#000000"})

    return {label["name"]: [hex2rgb(label["color"]), [], []] for label in labels}


def generate_color(color, used_colors):
    def tint_shade_color():
        for added_color in (255, 0):
            for factor in range(1, 10):
                yield tuple(map(lambda c: int(c + (added_color - c) * factor / 10), color))

    def get_unused_color():
        def get_avg_color(index):
            sorted_colors = sorted(used_colors, key=operator.itemgetter(index))
            max_dist_pair = max(
                zip(sorted_colors, sorted_colors[1:]),
                key=lambda c_pair: c_pair[1][index] - c_pair[0][index],
            )
            return (max_dist_pair[0][index] + max_dist_pair[1][index]) // 2

        return tuple(get_avg_color(i) for i in range(3))

    # try to tint and shade color firstly
    for new_color in tint_shade_color():
        if new_color not in used_colors:
            return new_color

    return get_unused_color()


def get_label_color(label_name, label_colors):
    predefined = parse_default_colors()
    label_colors = tuple(hex2rgb(c) for c in label_colors if c)
    used_colors = set(itertools.chain(predefined.values(), label_colors))
    normalized_name = normalize_label(label_name)

    color = predefined.get(normalized_name, None)
    if color is None:
        name_hash = int.from_bytes(
            blake2s(normalized_name.encode(), digest_size=3).digest(), byteorder="big"
        )
        color = get_color_from_index(name_hash)

    if color in label_colors:
        color = generate_color(color, used_colors)

    return rgb2hex(color)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\velodynepoint.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.dataset import Dataset
from datumaro.components.transformer import ItemTransform

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive
from cvat.apps.engine.models import DimensionType

from .registry import dm_env, exporter, importer


class RemoveTrackingInformation(ItemTransform):
    def transform_item(self, item):
        annotations = list(item.annotations)
        for anno in annotations:
            if hasattr(anno, "attributes") and "track_id" in anno.attributes:
                del anno.attributes["track_id"]
        return item.wrap(annotations=annotations)


@exporter(name="Kitti Raw Format", ext="ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _export_images(dst_file, temp_dir, task_data, save_images=False):
    with GetCVATDataExtractor(
        task_data,
        include_images=save_images,
        format_type="kitti_raw",
        dimension=DimensionType.DIM_3D,
    ) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.transform(RemoveTrackingInformation)
        dataset.export(temp_dir, "kitti_raw", save_media=save_images, reindex=True)

    make_zip_archive(temp_dir, dst_file)


@importer(name="Kitti Raw Format", ext="ZIP", version="1.0", dimension=DimensionType.DIM_3D)
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    if zipfile.is_zipfile(src_file):
        zipfile.ZipFile(src_file).extractall(temp_dir)
        detect_dataset(
            temp_dir, format_name="kitti_raw", importer=dm_env.importers.get("kitti_raw")
        )
        dataset = Dataset.import_from(temp_dir, "kitti_raw", env=dm_env)
    else:
        dataset = Dataset.import_from(src_file.name, "kitti_raw", env=dm_env)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\vggface2.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.dataset import Dataset

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    TaskData,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="VGGFace2", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "vgg_face2", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="VGGFace2", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="vgg_face2", importer=dm_env.importers.get("vgg_face2"))
    dataset = Dataset.import_from(temp_dir, "vgg_face2", env=dm_env)
    if isinstance(instance_data, TaskData):
        dataset.transform("rename", regex=r"|([^/]+/)?(.+)|\2|")
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\widerface.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import zipfile

from datumaro.components.dataset import Dataset

from cvat.apps.dataset_manager.bindings import (
    GetCVATDataExtractor,
    detect_dataset,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer


@exporter(name="WiderFace", ext="ZIP", version="1.0")
def _export(dst_file, temp_dir, instance_data, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = Dataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, "wider_face", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@importer(name="WiderFace", ext="ZIP", version="1.0")
def _import(src_file, temp_dir, instance_data, load_data_callback=None, **kwargs):
    zipfile.ZipFile(src_file).extractall(temp_dir)

    detect_dataset(temp_dir, format_name="wider_face", importer=dm_env.importers.get("wider_face"))
    dataset = Dataset.import_from(temp_dir, "wider_face", env=dm_env)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\yolo.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT
import os.path as osp
from glob import glob
from typing import Callable, Optional

from datumaro.components.annotation import AnnotationType
from datumaro.components.dataset import StreamDataset
from datumaro.components.dataset_base import DatasetItem
from pyunpack import Archive

from cvat.apps.dataset_manager.bindings import (
    CommonData,
    CVATDataExtractorMixin,
    GetCVATDataExtractor,
    ProjectData,
    detect_dataset,
    find_dataset_root,
    import_dm_annotations,
    match_dm_item,
)
from cvat.apps.dataset_manager.util import make_zip_archive

from .registry import dm_env, exporter, importer
from .transformations import SetKeyframeForEveryTrackShape


def _export_common(
    dst_file: str,
    temp_dir: str,
    instance_data: ProjectData | CommonData,
    format_name: str,
    *,
    save_images: bool = False,
    **kwargs,
):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = StreamDataset.from_extractors(extractor, env=dm_env)
        dataset.export(temp_dir, format_name, save_media=save_images, **kwargs)

    make_zip_archive(temp_dir, dst_file)


@exporter(name="YOLO", ext="ZIP", version="1.1")
def _export_yolo(*args, **kwargs):
    _export_common(*args, format_name="yolo", **kwargs)


def _import_common(
    src_file,
    temp_dir: str,
    instance_data: ProjectData | CommonData,
    format_name: str,
    *,
    load_data_callback: Optional[Callable] = None,
    import_kwargs: dict | None = None,
    **kwargs,
):
    Archive(src_file.name).extractall(temp_dir)

    image_info = {}
    extractor = dm_env.extractors.get(format_name)
    frames = [
        extractor.name_from_path(osp.relpath(p, temp_dir))
        for p in glob(osp.join(temp_dir, "**", "*.txt"), recursive=True)
    ]
    root_hint = find_dataset_root([DatasetItem(id=frame) for frame in frames], instance_data)
    for frame in frames:
        frame_info = None
        try:
            frame_id = match_dm_item(DatasetItem(id=frame), instance_data, root_hint=root_hint)
            frame_info = instance_data.frame_info[frame_id]
        except Exception:  # nosec
            pass
        if frame_info is not None:
            image_info[frame] = (frame_info["height"], frame_info["width"])

    detect_dataset(temp_dir, format_name=format_name, importer=dm_env.importers.get(format_name))
    dataset = StreamDataset.import_from(
        temp_dir, format_name, env=dm_env, image_info=image_info, **(import_kwargs or {})
    )
    dataset = dataset.transform(SetKeyframeForEveryTrackShape)
    if load_data_callback is not None:
        load_data_callback(dataset, instance_data)
    import_dm_annotations(dataset, instance_data)


@importer(name="YOLO", ext="ZIP", version="1.1")
def _import_yolo(*args, **kwargs):
    _import_common(*args, format_name="yolo", **kwargs)


@exporter(name="Ultralytics YOLO Detection", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_detection(*args, **kwargs):
    _export_common(*args, format_name="yolo_ultralytics_detection", **kwargs)


@exporter(name="Ultralytics YOLO Detection Track", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_detection_track(*args, **kwargs):
    _export_common(*args, format_name="yolo_ultralytics_detection", write_track_id=True, **kwargs)


@exporter(name="Ultralytics YOLO Oriented Bounding Boxes", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_oriented_boxes(*args, **kwargs):
    _export_common(*args, format_name="yolo_ultralytics_oriented_boxes", **kwargs)


@exporter(name="Ultralytics YOLO Segmentation", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_segmentation(dst_file, temp_dir, instance_data, *, save_images=False):
    with GetCVATDataExtractor(instance_data, include_images=save_images) as extractor:
        dataset = StreamDataset.from_extractors(extractor, env=dm_env)
        dataset = dataset.transform("masks_to_polygons")
        dataset.export(temp_dir, "yolo_ultralytics_segmentation", save_media=save_images)

    make_zip_archive(temp_dir, dst_file)


@exporter(name="Ultralytics YOLO Pose", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_pose(*args, **kwargs):
    _export_common(*args, format_name="yolo_ultralytics_pose", **kwargs)


@exporter(name="Ultralytics YOLO Classification", ext="ZIP", version="1.0")
def _export_yolo_ultralytics_classification(*args, **kwargs):
    _export_common(*args, format_name="yolo_ultralytics_classification", **kwargs)


@importer(name="Ultralytics YOLO Detection", ext="ZIP", version="1.0")
def _import_yolo_ultralytics_detection(*args, **kwargs):
    _import_common(*args, format_name="yolo_ultralytics_detection", **kwargs)


@importer(name="Ultralytics YOLO Segmentation", ext="ZIP", version="1.0")
def _import_yolo_ultralytics_segmentation(*args, **kwargs):
    _import_common(*args, format_name="yolo_ultralytics_segmentation", **kwargs)


@importer(name="Ultralytics YOLO Oriented Bounding Boxes", ext="ZIP", version="1.0")
def _import_yolo_ultralytics_oriented_boxes(*args, **kwargs):
    _import_common(*args, format_name="yolo_ultralytics_oriented_boxes", **kwargs)


@importer(name="Ultralytics YOLO Pose", ext="ZIP", version="1.0")
def _import_yolo_ultralytics_pose(src_file, temp_dir, instance_data, **kwargs):
    instance_meta = instance_data.meta[instance_data.META_FIELD]
    categories = CVATDataExtractorMixin.load_categories(instance_meta["labels"])
    point_categories = categories.get(AnnotationType.points)
    label_categories = categories.get(AnnotationType.label)
    true_skeleton_point_labels = {
        label_categories[label_id].name: category.labels
        for label_id, category in point_categories.items.items()
    }

    _import_common(
        src_file,
        temp_dir,
        instance_data,
        format_name="yolo_ultralytics_pose",
        import_kwargs=dict(skeleton_sub_labels=true_skeleton_point_labels),
        **kwargs,
    )


@importer(name="Ultralytics YOLO Classification", ext="ZIP", version="1.0")
def _import_yolo_ultralytics_classification(*args, **kwargs):
    _import_common(*args, format_name="yolo_ultralytics_classification", **kwargs)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\formats\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\management\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\management\commands\cleanuplegacyexportcache.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import shutil
from contextlib import suppress
from pathlib import Path

from django.core.management.base import BaseCommand
from django.utils import timezone

from cvat.apps.engine.models import Job, Project, Task


class Command(BaseCommand):
    help = "Cleanup outdated export cache"

    def handle(self, *args, **options):
        def update_progress():
            progress = (i + 1) / objects_count
            done = int(progress_bar_len * progress)
            progress_bar = "#" * done + "-" * (progress_bar_len - done)
            self.stdout.write(f"\rProgress: |{progress_bar}| {progress:.0%}", ending="")

        now = timezone.now()
        progress_bar_len = shutil.get_terminal_size().columns // 2

        for Model in (Project, Task, Job):
            self.stdout.write(f"\nDeleting the export cache for {Model.__name__.lower()}s...")
            queryset = Model.objects.filter(created_date__lt=now)
            objects_count = queryset.count()
            if objects_count < 1:
                continue

            msg = (
                f"{objects_count} folders are going to be checked"
                if objects_count > 1
                else "1 folder is going to be checked"
            )
            self.stdout.write(msg)

            for i, obj in enumerate(queryset.iterator()):
                update_progress()
                export_cache_dir = Path(obj.get_dirname()) / "export_cache"
                with suppress(FileNotFoundError):
                    shutil.rmtree(export_cache_dir)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\management\commands\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\test_annotation.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from unittest import TestCase

from cvat.apps.dataset_manager.annotation import AnnotationIR, TrackManager
from cvat.apps.engine.models import DimensionType


class TrackManagerTest(TestCase):
    def _check_interpolation(self, track):
        interpolated = TrackManager.get_interpolated_shapes(track, 0, 7, "2d")

        self.assertEqual(
            [
                {"frame": 0, "keyframe": True, "outside": False},
                {"frame": 1, "keyframe": False, "outside": False},
                {"frame": 2, "keyframe": True, "outside": True},
                # frame = 3 should be skipped as it is outside and interpolated
                {"frame": 4, "keyframe": True, "outside": False},
                {"frame": 5, "keyframe": False, "outside": False},
                {"frame": 6, "keyframe": False, "outside": False},
            ],
            [
                {k: v for k, v in shape.items() if k in ["frame", "keyframe", "outside"]}
                for shape in interpolated
            ],
        )

    def test_point_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "source": "manual",
            "attributes": [],
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0],
                    "type": "points",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "type": "points",
                    "occluded": False,
                    "outside": True,
                },
                {
                    "frame": 4,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "type": "points",
                    "occluded": False,
                    "outside": False,
                },
            ],
        }

        self._check_interpolation(track)

    def test_polygon_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0, 5.0, 2.0],
                    "type": "polygon",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0, 7.0, 6.0, 4.0, 5.0],
                    "type": "polygon",
                    "occluded": False,
                    "outside": True,
                },
                {
                    "frame": 4,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0, 7.0, 6.0, 4.0, 5.0],
                    "type": "polygon",
                    "occluded": False,
                    "outside": False,
                },
            ],
        }

        self._check_interpolation(track)

    def test_bbox_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": True,
                },
                {
                    "frame": 4,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                },
            ],
        }

        self._check_interpolation(track)

    def test_line_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "polyline",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "polyline",
                    "occluded": False,
                    "outside": True,
                },
                {
                    "frame": 4,
                    "attributes": [],
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "polyline",
                    "occluded": False,
                    "outside": False,
                },
            ],
        }

        self._check_interpolation(track)

    def test_outside_bbox_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": True,
                    "attributes": [],
                },
                {
                    "frame": 4,
                    "points": [5.0, 6.0, 7.0, 8.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": True,
                    "attributes": [],
                },
            ],
        }

        expected_shapes = [
            {
                "frame": 0,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 1,
                "points": [2.0, 3.0, 4.0, 5.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 2,
                "points": [3.0, 4.0, 5.0, 6.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": True,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 4,
                "points": [5.0, 6.0, 7.0, 8.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": True,
                "attributes": [],
                "keyframe": True,
            },
        ]

        interpolated_shapes = TrackManager.get_interpolated_shapes(track, 0, 5, "2d")
        self.assertEqual(expected_shapes, interpolated_shapes)

    def test_outside_polygon_interpolation(self):
        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
                    "type": "polygon",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "points": [3.0, 4.0, 5.0, 6.0, 7.0, 8.0],
                    "type": "polygon",
                    "occluded": False,
                    "outside": True,
                    "attributes": [],
                },
            ],
        }

        expected_shapes = [
            {
                "frame": 0,
                "points": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
                "type": "polygon",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 1,
                "points": [2.0, 3.0, 4.0, 5.0, 6.0, 7.0],
                "type": "polygon",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 2,
                "points": [3.0, 4.0, 5.0, 6.0, 7.0, 8.0],
                "type": "polygon",
                "occluded": False,
                "outside": True,
                "attributes": [],
                "keyframe": True,
            },
        ]

        interpolated_shapes = TrackManager.get_interpolated_shapes(track, 0, 3, "2d")
        self.assertEqual(expected_shapes, interpolated_shapes)

    def test_duplicated_shape_interpolation(self):
        # there should not be any new tracks with duplicated shapes,
        # but it is possible that the database still contains some
        expected_shapes = [
            {
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "points": [100, 100, 200, 200],
                "frame": 0,
                "attributes": [],
                "rotation": 0,
            },
            {
                "type": "rectangle",
                "occluded": False,
                "outside": True,
                "points": [100, 100, 200, 200],
                "frame": 1,
                "attributes": [],
                "rotation": 0,
            },
        ]
        track = {
            "id": 666,
            "frame": 0,
            "group": None,
            "source": "manual",
            "attributes": [],
            "elements": [],
            "label": "cat",
            "shapes": expected_shapes + [expected_shapes[-1]],
        }

        interpolated_shapes = TrackManager.get_interpolated_shapes(track, 0, 2, "2d")
        self.assertEqual(expected_shapes, interpolated_shapes)

    def test_deleted_frames_with_keyframes_are_ignored(self):
        deleted_frames = [2]
        end_frame = 5

        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,  # deleted in the task
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 4,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
            ],
        }

        expected_shapes = [
            {
                "frame": 0,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 1,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 3,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 4,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
        ]

        interpolated_shapes = TrackManager.get_interpolated_shapes(
            track, 0, end_frame, DimensionType.DIM_2D, deleted_frames=deleted_frames
        )
        self.assertEqual(expected_shapes, interpolated_shapes)

    def test_keyframes_on_excluded_frames_are_not_ignored(self):
        end_frame = 5

        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,
                    "points": [3.0, 4.0, 5.0, 6.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 4,
                    "points": [7.0, 8.0, 9.0, 10.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
            ],
        }

        all_expected_shapes = [
            {
                "frame": 0,
                "points": [1.0, 2.0, 3.0, 4.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 1,
                "points": [2.0, 3.0, 4.0, 5.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 2,
                "points": [3.0, 4.0, 5.0, 6.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
            {
                "frame": 3,
                "points": [5.0, 6.0, 7.0, 8.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 4,
                "points": [7.0, 8.0, 9.0, 10.0],
                "rotation": 0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": True,
            },
        ]

        for included_frames in [None, [1, 3]]:
            interpolated_shapes = TrackManager.get_interpolated_shapes(
                track, 0, end_frame, DimensionType.DIM_2D, included_frames=included_frames
            )
            expected_shapes = [
                shape
                for shape in all_expected_shapes
                if included_frames is None or shape["frame"] in included_frames
            ]
            self.assertEqual(expected_shapes, interpolated_shapes)

    def test_keyframes_on_deleted_frames_with_specific_requested_frames_are_ignored(self):
        deleted_frames = [2]  # the task has deleted frames
        included_frames = [1, 3]  # and current track view requires only specific frames
        end_frame = 5

        track = {
            "frame": 0,
            "label_id": 0,
            "group": None,
            "attributes": [],
            "source": "manual",
            "shapes": [
                {
                    "frame": 0,
                    "points": [1.0, 2.0, 3.0, 4.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 2,  # deleted
                    "points": [0, 0, 1, 1],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": True,
                    "outside": False,
                    "attributes": [],
                },
                {
                    "frame": 4,
                    "points": [9.0, 10.0, 11.0, 12.0],
                    "rotation": 0,
                    "type": "rectangle",
                    "occluded": False,
                    "outside": False,
                    "attributes": [],
                },
            ],
        }

        expected_shapes = [
            {
                "frame": 1,
                "points": [3.0, 4.0, 5.0, 6.0],
                "rotation": 0.0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
            {
                "frame": 3,
                "points": [7.0, 8.0, 9.0, 10.0],
                "rotation": 0.0,
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "attributes": [],
                "keyframe": False,
            },
        ]

        interpolated_shapes = TrackManager.get_interpolated_shapes(
            track,
            0,
            end_frame,
            DimensionType.DIM_2D,
            included_frames=included_frames,
            deleted_frames=deleted_frames,
        )
        self.assertEqual(expected_shapes, interpolated_shapes)


class AnnotationIRTest(TestCase):
    def test_slice_track_does_not_duplicate_outside_frame_on_the_end(self):
        track_shapes = [
            {
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "points": [100, 100, 200, 200],
                "frame": 0,
                "attributes": [],
                "rotation": 0,
            },
            {
                "type": "rectangle",
                "occluded": False,
                "outside": True,
                "points": [100, 100, 200, 200],
                "frame": 1,
                "attributes": [],
                "rotation": 0,
            },
            {
                "type": "rectangle",
                "occluded": False,
                "outside": False,
                "points": [111, 111, 222, 222],
                "frame": 10,
                "attributes": [],
                "rotation": 0,
            },
        ]
        data = {
            "tags": [],
            "shapes": [],
            "tracks": [
                {
                    "id": 666,
                    "frame": 0,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "elements": [],
                    "label": "cat",
                    "shapes": track_shapes,
                }
            ],
        }
        annotation = AnnotationIR(dimension=DimensionType.DIM_2D, data=data)
        sliced_annotation = annotation.slice(0, 1)
        self.assertEqual(sliced_annotation.data["tracks"][0]["shapes"], track_shapes[0:2])


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\test_formats.py =====

# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp
import tempfile
import zipfile
from io import BytesIO

import datumaro
import numpy as np
from datumaro.components.annotation import Mask
from datumaro.components.dataset import Dataset, DatasetItem
from django.contrib.auth.models import Group, User
from rest_framework import status

import cvat.apps.dataset_manager as dm
from cvat.apps.dataset_manager.annotation import AnnotationIR
from cvat.apps.dataset_manager.bindings import (
    CvatTaskOrJobDataExtractor,
    TaskData,
    find_dataset_root,
)
from cvat.apps.dataset_manager.task import TaskAnnotation
from cvat.apps.dataset_manager.tests.utils import (
    ensure_extractors_efficiency,
    ensure_streaming_importers,
)
from cvat.apps.dataset_manager.util import make_zip_archive
from cvat.apps.engine.models import Task
from cvat.apps.engine.tests.utils import (
    ApiTestBase,
    ForceLogin,
    generate_image_file,
    get_paginated_collection,
)


class _DbTestBase(ApiTestBase):
    def setUp(self):
        super().setUp()

    @classmethod
    def setUpTestData(cls):
        cls.create_db_users()

    @classmethod
    def create_db_users(cls):
        group, _ = Group.objects.get_or_create(name="adm")

        admin = User.objects.create_superuser(
            username="test", password="test", email="")
        admin.groups.add(group)

        cls.user = admin

    def _put_api_v2_task_id_annotations(self, tid, data):
        with ForceLogin(self.user, self.client):
            response = self.client.put("/api/tasks/%s/annotations" % tid,
                data=data, format="json")

        return response

    def _put_api_v2_job_id_annotations(self, jid, data):
        with ForceLogin(self.user, self.client):
            response = self.client.put("/api/jobs/%s/annotations" % jid,
                data=data, format="json")

        return response

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid,
                data=image_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/%s" % tid)

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                ))
                response.data["labels"] = labels_response

            task = response.data

        return task


@ensure_extractors_efficiency
class TaskExportTest(_DbTestBase):
    def _generate_custom_annotations(self, annotations, task):
        self._put_api_v2_task_id_annotations(task["id"], annotations)
        return annotations

    def _generate_annotations(self, task):
        annotations = {
            "version": 0,
            "tags": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "attributes": []
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][0]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][0]["id"],
                    "group": 1,
                    "source": "manual",
                    "attributes": [],
                    "points": [100, 300.222, 400, 500, 1, 3],
                    "type": "points",
                    "occluded": False
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][0]["id"],
                    "group": 1,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 400, 500, 1, 3],
                    "type": "polyline",
                    "occluded": False
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True
                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False
                        }
                    ]
                },
            ]
        }
        return self._generate_custom_annotations(annotations, task)

    def _generate_task_images(self, count): # pylint: disable=no-self-use
        images = {
            "client_files[%d]" % i: generate_image_file("image_%d.jpg" % i)
            for i in range(count)
        }
        images["image_quality"] = 75
        return images

    def _generate_task(self, images, **overrides):
        task = {
            "name": "my task #1",
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {
                    "name": "car",
                    "attributes": [
                        {
                            "name": "model",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "mazda",
                            "values": ["bmw", "mazda", "renault"]
                        },
                        {
                            "name": "parked",
                            "mutable": True,
                            "input_type": "checkbox",
                            "default_value": "false",
                            "values": [],
                        },
                    ]
                },
                {"name": "person"},
            ]
        }
        task.update(overrides)
        return self._create_task(task, images)

    @staticmethod
    def _test_export(check, task, format_name, **export_args):
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = osp.join(temp_dir, format_name)
            dm.task.export_task(task["id"], file_path,
                format_name=format_name, **export_args)

            check(file_path)

    def test_export_formats_query(self):
        formats = dm.views.get_export_formats()

        self.assertEqual({f.DISPLAY_NAME for f in formats},
        {
            'COCO 1.0',
            'COCO Keypoints 1.0',
            'CVAT for images 1.1',
            'CVAT for video 1.1',
            'Datumaro 1.0',
            'Datumaro 3D 1.0',
            'LabelMe 3.0',
            'MOT 1.1',
            'MOTS PNG 1.0',
            'PASCAL VOC 1.1',
            'Segmentation mask 1.1',
            'YOLO 1.1',
            'ImageNet 1.0',
            'CamVid 1.0',
            'WiderFace 1.0',
            'VGGFace2 1.0',
            'Market-1501 1.0',
            'ICDAR Recognition 1.0',
            'ICDAR Localization 1.0',
            'ICDAR Segmentation 1.0',
            'Kitti Raw Format 1.0',
            'Sly Point Cloud Format 1.0',
            'KITTI 1.0',
            'LFW 1.0',
            'Cityscapes 1.0',
            'Open Images V6 1.0',
            'Ultralytics YOLO Classification 1.0',
            'Ultralytics YOLO Oriented Bounding Boxes 1.0',
            'Ultralytics YOLO Detection 1.0',
            'Ultralytics YOLO Detection Track 1.0',
            'Ultralytics YOLO Pose 1.0',
            'Ultralytics YOLO Segmentation 1.0',
        })

    def test_import_formats_query(self):
        formats = dm.views.get_import_formats()

        self.assertEqual({f.DISPLAY_NAME for f in formats},
        {
            'COCO 1.0',
            'COCO Keypoints 1.0',
            'CVAT 1.1',
            'LabelMe 3.0',
            'MOT 1.1',
            'MOTS PNG 1.0',
            'PASCAL VOC 1.1',
            'Segmentation mask 1.1',
            'YOLO 1.1',
            'ImageNet 1.0',
            'CamVid 1.0',
            'WiderFace 1.0',
            'VGGFace2 1.0',
            'Market-1501 1.0',
            'ICDAR Recognition 1.0',
            'ICDAR Localization 1.0',
            'ICDAR Segmentation 1.0',
            'Kitti Raw Format 1.0',
            'Sly Point Cloud Format 1.0',
            'KITTI 1.0',
            'LFW 1.0',
            'Cityscapes 1.0',
            'Open Images V6 1.0',
            'Datumaro 1.0',
            'Datumaro 3D 1.0',
            'Ultralytics YOLO Classification 1.0',
            'Ultralytics YOLO Oriented Bounding Boxes 1.0',
            'Ultralytics YOLO Detection 1.0',
            'Ultralytics YOLO Pose 1.0',
            'Ultralytics YOLO Segmentation 1.0',
        })

    def test_exports(self):
        def check(file_path):
            with open(file_path, 'rb') as f:
                self.assertTrue(len(f.read()) != 0)

        for f in dm.views.get_export_formats():
            if not f.ENABLED:
                self.skipTest("Format is disabled")

            format_name = f.DISPLAY_NAME
            if format_name == "VGGFace2 1.0":
                self.skipTest("Format is disabled")

            for save_images in { True, False }:
                images = self._generate_task_images(3)
                task = self._generate_task(images)
                self._generate_annotations(task)
                with self.subTest(format=format_name, save_images=save_images):
                    self._test_export(check, task,
                        format_name, save_images=save_images)

    def test_empty_images_are_exported(self):
        dm_env = dm.formats.registry.dm_env

        for format_name, importer_name in [
            ('COCO 1.0', 'coco'),
            ('COCO Keypoints 1.0', 'coco_person_keypoints'),
            ('CVAT for images 1.1', 'cvat'),
            # ('CVAT for video 1.1', 'cvat'), # does not support
            ('Datumaro 1.0', 'datumaro'),
            ('LabelMe 3.0', 'label_me'),
            # ('MOT 1.1', 'mot_seq'), # does not support
            # ('MOTS PNG 1.0', 'mots_png'), # does not support
            ('PASCAL VOC 1.1', 'voc'),
            ('Segmentation mask 1.1', 'voc'),
            ('YOLO 1.1', 'yolo'),
            ('ImageNet 1.0', 'imagenet_txt'),
            ('CamVid 1.0', 'camvid'),
            ('WiderFace 1.0', 'wider_face'),
            ('VGGFace2 1.0', 'vgg_face2'),
            ('Market-1501 1.0', 'market1501'),
            ('ICDAR Recognition 1.0', 'icdar_word_recognition'),
            ('ICDAR Localization 1.0', 'icdar_text_localization'),
            ('ICDAR Segmentation 1.0', 'icdar_text_segmentation'),
            # ('KITTI 1.0', 'kitti') format does not support empty annotations
            ('LFW 1.0', 'lfw'),
            # ('Cityscapes 1.0', 'cityscapes'), does not support, empty annotations
            ('Ultralytics YOLO Classification 1.0', 'yolo_ultralytics_classification'),
            ('Ultralytics YOLO Oriented Bounding Boxes 1.0', 'yolo_ultralytics_oriented_boxes'),
            ('Ultralytics YOLO Detection 1.0', 'yolo_ultralytics_detection'),
            ('Ultralytics YOLO Pose 1.0', 'yolo_ultralytics_pose'),
            ('Ultralytics YOLO Segmentation 1.0', 'yolo_ultralytics_segmentation'),
        ]:
            with self.subTest(format=format_name):
                if not dm.formats.registry.EXPORT_FORMATS[format_name].ENABLED:
                    self.skipTest("Format is disabled")

                images = self._generate_task_images(3)
                task = self._generate_task(images)

                def check(file_path):
                    def load_dataset(src):
                        return datumaro.components.dataset. \
                            Dataset.import_from(src, importer_name, env=dm_env)

                    if zipfile.is_zipfile(file_path):
                        with tempfile.TemporaryDirectory() as tmp_dir:
                            zipfile.ZipFile(file_path).extractall(tmp_dir)
                            dataset = load_dataset(tmp_dir)
                            self.assertEqual(len(dataset), task["size"])
                    else:
                        dataset = load_dataset(file_path)
                        self.assertEqual(len(dataset), task["size"])

                self._test_export(check, task, format_name, save_images=False)

    def test_can_skip_outside(self):
        images = self._generate_task_images(3)
        task = self._generate_task(images)
        self._generate_annotations(task)
        task_ann = TaskAnnotation(task["id"])
        task_ann.init_from_db()
        task_data = TaskData(task_ann.ir_data, Task.objects.get(pk=task["id"]))

        extractor = CvatTaskOrJobDataExtractor(task_data)
        dm_dataset = datumaro.components.project.Dataset.from_extractors(extractor)
        self.assertEqual(4, len(dm_dataset.get("image_1").annotations))

    def test_no_outside_shapes_in_per_frame_export(self):
        images = self._generate_task_images(3)
        task = self._generate_task(images)
        self._generate_annotations(task)
        task_ann = TaskAnnotation(task["id"])
        task_ann.init_from_db()
        task_data = TaskData(task_ann.ir_data, Task.objects.get(pk=task["id"]))

        outside_count = 0
        for f in task_data.group_by_frame(include_empty=True):
            for ann in f.labeled_shapes:
                if getattr(ann, 'outside', None):
                    outside_count += 1
        self.assertEqual(0, outside_count)

    def test_cant_make_rel_frame_id_from_unknown(self):
        images = self._generate_task_images(3)
        images['frame_filter'] = 'step=2'
        task = self._generate_task(images)
        task_data = TaskData(AnnotationIR('2d'), Task.objects.get(pk=task['id']),)

        with self.assertRaisesRegex(ValueError, r'Unknown'):
            task_data.rel_frame_id(1) # the task has only 0 and 2 frames

    def test_can_make_rel_frame_id_from_known(self):
        images = self._generate_task_images(6)
        images['frame_filter'] = 'step=2'
        images['start_frame'] = 1
        task = self._generate_task(images)
        task_data = TaskData(AnnotationIR('2d'), Task.objects.get(pk=task['id']))

        self.assertEqual(2, task_data.rel_frame_id(5))

    def test_cant_make_abs_frame_id_from_unknown(self):
        images = self._generate_task_images(3)
        images['frame_filter'] = 'step=2'
        task = self._generate_task(images)
        task_data = TaskData(AnnotationIR('2d'), Task.objects.get(pk=task['id']))

        with self.assertRaisesRegex(ValueError, r'Unknown'):
            task_data.abs_frame_id(2) # the task has only 0 and 1 indices

    def test_can_make_abs_frame_id_from_known(self):
        images = self._generate_task_images(6)
        images['frame_filter'] = 'step=2'
        images['start_frame'] = 1
        task = self._generate_task(images)
        task_data = TaskData(AnnotationIR('2d'), Task.objects.get(pk=task['id']))

        self.assertEqual(5, task_data.abs_frame_id(2))

    def _get_task_jobs(self, tid):
        with ForceLogin(self.user, self.client):
            return get_paginated_collection(lambda page: self.client.get(
                '/api/jobs?task_id=%s&page=%s' % (tid, page), format="json"
            ))

    def test_frames_outside_are_not_generated(self):
        # https://github.com/openvinotoolkit/cvat/issues/2827
        images = self._generate_task_images(10)
        images['start_frame'] = 0
        task = self._generate_task(images, overlap=3, segment_size=6)
        jobs = sorted(self._get_task_jobs(task["id"]), key=lambda v: v["id"])
        annotations = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": [
                {
                    "frame": 6,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 6,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [],
                        },
                    ]
                },
            ]
        }
        self._put_api_v2_job_id_annotations(jobs[2]["id"], annotations)

        task_ann = TaskAnnotation(task["id"])
        task_ann.init_from_db()
        task_data = TaskData(task_ann.ir_data, Task.objects.get(pk=task['id']))

        i = -1
        for i, frame in enumerate(task_data.group_by_frame()):
            self.assertTrue(frame.frame in range(6, 10))
        self.assertEqual(i + 1, 4)

    def _delete_job_frames(self, job_id: int, deleted_frames: list[int]):
        with ForceLogin(self.user, self.client):
            response = self.client.patch(
                f"/api/jobs/{job_id}/data/meta?org=",
                data=dict(deleted_frames=deleted_frames),
                format="json"
            )
            assert response.status_code == status.HTTP_200_OK, response.status_code

    def test_track_keyframes_on_deleted_frames_do_not_affect_later_frames(self):
        images = self._generate_task_images(4)
        task = self._generate_task(images)
        job = self._get_task_jobs(task["id"])[0]

        annotations = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1, 2, 3, 4],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [],
                        },
                        {
                            "frame": 1,
                            "points": [5, 6, 7, 8],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": True,
                            "attributes": [],
                        },
                        {
                            "frame": 2,
                            "points": [9, 10, 11, 12],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [],
                        },
                    ]
                },
            ]
        }
        self._put_api_v2_job_id_annotations(job["id"], annotations)
        self._delete_job_frames(job["id"], [2])

        task_ann = TaskAnnotation(task["id"])
        task_ann.init_from_db()
        task_data = TaskData(task_ann.ir_data, Task.objects.get(pk=task["id"]))
        extractor = CvatTaskOrJobDataExtractor(task_data)
        dm_dataset = Dataset.from_extractors(extractor)

        assert len(dm_dataset.get("image_3").annotations) == 0


class FrameMatchingTest(_DbTestBase):
    def _generate_task_images(self, paths): # pylint: disable=no-self-use
        f = BytesIO()
        with zipfile.ZipFile(f, 'w') as archive:
            for path in paths:
                archive.writestr(path, generate_image_file(path).getvalue())
        f.name = 'images.zip'
        f.seek(0)

        return {
            'client_files[0]': f,
            'image_quality': 75,
        }

    def _generate_task(self, images):
        task = {
            "name": "my task #1",
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {
                    "name": "car",
                    "attributes": [
                        {
                            "name": "model",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "mazda",
                            "values": ["bmw", "mazda", "renault"]
                        },
                        {
                            "name": "parked",
                            "mutable": True,
                            "input_type": "checkbox",
                            "default_value": "false",
                            "values": [],
                        },
                    ]
                },
                {"name": "person"},
            ]
        }
        return self._create_task(task, images)

    def test_frame_matching(self):
        task_paths = [
            'a.jpg',
            'a/a.jpg',
            'a/b.jpg',
            'b/a.jpg',
            'b/c.jpg',
            'a/b/c.jpg',
            'a/b/d.jpg',
        ]

        images = self._generate_task_images(task_paths)
        task = self._generate_task(images)
        task_data = TaskData(AnnotationIR('2d'), Task.objects.get(pk=task["id"]))

        for input_path, expected, root in [
            ('z.jpg', None, ''), # unknown item
            ('z/a.jpg', None, ''), # unknown item

            ('d.jpg', 'a/b/d.jpg', 'a/b'), # match with root hint
            ('b/d.jpg', 'a/b/d.jpg', 'a'), # match with root hint
        ] + list(zip(task_paths, task_paths, [None] * len(task_paths))): # exact matches
            with self.subTest(input=input_path):
                actual = task_data.match_frame(input_path, root)
                if actual is not None:
                    actual = task_data.frame_info[actual]['path']
                self.assertEqual(expected, actual)

    def test_dataset_root(self):
        for task_paths, dataset_paths, expected in [
            ([ 'a.jpg', 'b/c/a.jpg' ], [ 'a.jpg', 'b/c/a.jpg' ], ''),
            ([ 'b/a.jpg', 'b/c/a.jpg' ], [ 'a.jpg', 'c/a.jpg' ], 'b'), # 'images from share' case
            ([ 'b/c/a.jpg' ], [ 'a.jpg' ], 'b/c'), # 'images from share' case
            ([ 'a.jpg' ], [ 'z.jpg' ], None),
        ]:
            with self.subTest(expected=expected):
                images = self._generate_task_images(task_paths)
                task = self._generate_task(images)
                task_data = TaskData(AnnotationIR('2d'),
                    Task.objects.get(pk=task["id"]))
                dataset = [DatasetItem(id=osp.splitext(p)[0]) for p in dataset_paths]

                root = find_dataset_root(dataset, task_data)
                self.assertEqual(expected, root)


@ensure_streaming_importers
class TaskAnnotationsImportTest(_DbTestBase):
    def _generate_custom_annotations(self, annotations, task):
        self._put_api_v2_task_id_annotations(task["id"], annotations)
        return annotations

    def _generate_task_images(self, count, name="image", **image_params):
        images = {
            "client_files[%d]" % i: generate_image_file("%s_%d.jpg" % (name, i),
                **image_params)
            for i in range(count)
        }
        images["image_quality"] = 75
        return images

    def _generate_task_images_by_names(self, names, **image_params):
        images = {
            f"client_files[{i}]": generate_image_file(f"{name}.jpg", **image_params)
            for i, name in enumerate(names)
        }
        images["image_quality"] = 75
        return images

    def _generate_task(self, images, annotation_format, **overrides):
        labels = []
        if annotation_format in ["ICDAR Recognition 1.0",
                "ICDAR Localization 1.0"]:
            labels = [{
                "name": "icdar",
                "attributes": [{
                    "name": "text",
                    "mutable": False,
                    "input_type": "text",
                    "values": ["word1", "word2"]
                }]
            }]
        elif annotation_format == "ICDAR Segmentation 1.0":
            labels = [{
                "name": "icdar",
                "attributes": [
                    {
                        "name": "text",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["word_1", "word_2", "word_3"]
                    },
                    {
                        "name": "index",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["0", "1", "2"]
                    },
                    {
                        "name": "color",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["100 110 240", "10 15 20", "120 128 64"]
                    },
                    {
                        "name": "center",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["1 2", "2 4", "10 45"]
                    },
                ]
            }]
        elif annotation_format == "Market-1501 1.0":
            labels = [{
                "name": "market-1501",
                "attributes": [
                    {
                        "name": "query",
                        "mutable": False,
                        "input_type": "select",
                        "values": ["True", "False"]
                    },
                    {
                        "name": "camera_id",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["0", "1", "2", "3"]
                    },
                    {
                        "name": "person_id",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["1", "2", "3"]
                    },
                ]
            }]
        else:
            labels = [
                {
                    "name": "car",
                    "attributes": [
                        {
                            "name": "model",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "mazda",
                            "values": ["bmw", "mazda", "renault"]
                        },
                        {
                            "name": "parked",
                            "mutable": True,
                            "input_type": "checkbox",
                            "default_value": "false",
                            "values": [],
                        }
                    ]
                },
                {
                    "name": "background",
                    "attributes": [],
                },
                {"name": "person"}
            ]

        task = {
            "name": "my task #1",
            "overlap": 0,
            "segment_size": 100,
            "labels": labels
        }
        task.update(overrides)
        return self._create_task(task, images)

    def _generate_annotations(self, task, annotation_format):
        shapes = []
        tracks = []
        tags = []

        if annotation_format in ["ICDAR Recognition 1.0",
                "ICDAR Localization 1.0"]:
            shapes = [{
                "frame": 0,
                "label_id": task["labels"][0]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [
                    {
                        "spec_id": task["labels"][0]["attributes"][0]["id"],
                        "value": task["labels"][0]["attributes"][0]["values"][0]
                    },
                ],
                "points": [1.0, 2.1, 10.6, 53.22],
                "type": "rectangle",
                "occluded": False,
            }]
        elif annotation_format == "Market-1501 1.0":
            tags = [{
                "frame": 1,
                "label_id": task["labels"][0]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [
                    {
                        "spec_id": task["labels"][0]["attributes"][0]["id"],
                        "value": task["labels"][0]["attributes"][0]["values"][1]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][1]["id"],
                        "value": task["labels"][0]["attributes"][1]["values"][2]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][2]["id"],
                        "value": task["labels"][0]["attributes"][2]["values"][0]
                    }
                ],
            }]
        elif annotation_format == "ICDAR Segmentation 1.0":
            shapes = [{
                "frame": 0,
                "label_id": task["labels"][0]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [
                    {
                        "spec_id": task["labels"][0]["attributes"][0]["id"],
                        "value": task["labels"][0]["attributes"][0]["values"][0]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][1]["id"],
                        "value": task["labels"][0]["attributes"][1]["values"][0]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][2]["id"],
                        "value": task["labels"][0]["attributes"][2]["values"][1]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][3]["id"],
                        "value": task["labels"][0]["attributes"][3]["values"][2]
                    }
                ],
                "points": [1.0, 2.1, 10.6, 53.22],
                "type": "rectangle",
                "occluded": False,
            }]
        else:
            rectangle_shape_wo_attrs = {
                "frame": 1,
                "label_id": task["labels"][1]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [],
                "points": [2.0, 2.1, 40, 10.7],
                "type": "rectangle",
                "occluded": False,
            }

            rectangle_shape_with_attrs = {
                "frame": 0,
                "label_id": task["labels"][0]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [
                    {
                        "spec_id": task["labels"][0]["attributes"][0]["id"],
                        "value": task["labels"][0]["attributes"][0]["values"][0]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][1]["id"],
                        "value": task["labels"][0]["attributes"][1]["default_value"]
                    }
                ],
                "points": [1.0, 2.1, 10.6, 13.22],
                "type": "rectangle",
                "occluded": False,
            }

            track_wo_attrs = {
                "frame": 0,
                "label_id": task["labels"][1]["id"],
                "group": 0,
                "source": "manual",
                "attributes": [],
                "shapes": [
                    {
                        "frame": 0,
                        "attributes": [],
                        "points": [1.0, 2.1, 10.6, 53.22, 30, 20.222],
                        "type": "polygon",
                        "occluded": False,
                        "outside": False
                    }
                ]
            }

            tag_wo_attrs = {
                "frame": 0,
                "label_id": task["labels"][0]["id"],
                "group": None,
                "attributes": []
            }

            tag_with_attrs = {
                "frame": 1,
                "label_id": task["labels"][0]["id"],
                "group": 3,
                "source": "manual",
                "attributes": [
                    {
                        "spec_id": task["labels"][0]["attributes"][0]["id"],
                        "value": task["labels"][0]["attributes"][0]["values"][1]
                    },
                    {
                        "spec_id": task["labels"][0]["attributes"][1]["id"],
                        "value": task["labels"][0]["attributes"][1]["default_value"]
                    }
                ],
            }

            if annotation_format == "VGGFace2 1.0":
                shapes = [rectangle_shape_wo_attrs]
            elif annotation_format == "CVAT 1.1":
                shapes = [rectangle_shape_wo_attrs,
                    rectangle_shape_with_attrs]
                tags = [tag_with_attrs, tag_wo_attrs]
            elif annotation_format == "MOTS PNG 1.0":
                tracks = [track_wo_attrs]
            else:
                shapes = [rectangle_shape_wo_attrs, \
                    rectangle_shape_with_attrs]
                tags = [tag_wo_attrs]
                tracks = [track_wo_attrs]

        annotations = {
            "version": 0,
            "tags": tags,
            "shapes": shapes,
            "tracks": tracks
        }

        return self._generate_custom_annotations(annotations, task)

    def _test_can_import_annotations(self, task, import_format):
        with tempfile.TemporaryDirectory() as temp_dir:
            file_path = osp.join(temp_dir, import_format)

            export_format = import_format
            if import_format == "CVAT 1.1":
                export_format = "CVAT for images 1.1"

            dm.task.export_task(task["id"], file_path, format_name=export_format)
            expected_ann = TaskAnnotation(task["id"])
            expected_ann.init_from_db()

            dm.task.import_task_annotations(file_path, task["id"], import_format, True)
            actual_ann = TaskAnnotation(task["id"])
            actual_ann.init_from_db()

            self.assertEqual(len(expected_ann.data), len(actual_ann.data))

    def test_can_import_annotations_for_image_with_dots_in_filename(self):
        for f in dm.views.get_import_formats():
            format_name = f.DISPLAY_NAME

            if format_name == "Market-1501 1.0":
                images = self._generate_task_images_by_names(["img0.0.0_0", "1.0_c3s1_000000_00", "img0.0.0_1"])
            else:
                images = self._generate_task_images(3, "img0.0.0")
            task = self._generate_task(images, format_name)
            self._generate_annotations(task, format_name)

            with self.subTest(format=format_name):
                if not f.ENABLED:
                    self.skipTest("Format is disabled")

                self._test_can_import_annotations(task, format_name)

    def test_can_import_mots_annotations_with_splited_masks(self):
        #https://github.com/openvinotoolkit/cvat/issues/3360

        format_name = 'MOTS PNG 1.0'
        source_dataset = Dataset.from_iterable([
            DatasetItem(id='image_0',
                annotations=[
                    Mask(np.array([[1, 1, 1, 0, 1, 1, 1]] * 5),
                    label=0, attributes={'track_id': 0})
                ]
            )
        ], categories=['label_0'])

        with tempfile.TemporaryDirectory() as temp_dir:
            dataset_dir = osp.join(temp_dir, 'dataset')
            source_dataset.export(dataset_dir, 'mots_png')
            dataset_path = osp.join(temp_dir, 'annotations.zip')
            make_zip_archive(dataset_dir, dataset_path)

            images = self._generate_task_images(1, size=(5, 7))
            task = {
                'name': 'test',
                "overlap": 0,
                "segment_size": 100,
                "labels": [{'name': 'label_0'}]
            }
            task.update()
            task = self._create_task(task, images)

            dm.task.import_task_annotations(dataset_path, task['id'], format_name, True)
            self._test_can_import_annotations(task, format_name)



# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\test_rest_api_formats.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import copy
import itertools
import json
import multiprocessing
import os
import os.path as osp
import random
import xml.etree.ElementTree as ET
import zipfile
from contextlib import ExitStack, contextmanager
from datetime import timedelta
from functools import partial
from io import BytesIO
from pathlib import Path
from tempfile import TemporaryDirectory
from time import sleep
from typing import Any, Callable, ClassVar, Optional, overload
from unittest.mock import DEFAULT as MOCK_DEFAULT
from unittest.mock import MagicMock, patch

import av
import numpy as np
from attr import define, field
from datumaro.components.comparator import EqualityComparator
from datumaro.components.dataset import Dataset
from django.contrib.auth.models import Group, User
from PIL import Image
from rest_framework import status

import cvat.apps.dataset_manager as dm
from cvat.apps.dataset_manager.bindings import CvatTaskOrJobDataExtractor, TaskData
from cvat.apps.dataset_manager.cron import clear_export_cache
from cvat.apps.dataset_manager.task import TaskAnnotation
from cvat.apps.dataset_manager.tests.utils import (
    TestDir,
    ensure_extractors_efficiency,
    ensure_streaming_importers,
)
from cvat.apps.dataset_manager.util import get_export_cache_lock
from cvat.apps.dataset_manager.views import export
from cvat.apps.engine.models import Task
from cvat.apps.engine.tests.utils import ExportApiTestBase, ForceLogin, get_paginated_collection

projects_path = osp.join(osp.dirname(__file__), 'assets', 'projects.json')
with open(projects_path) as file:
    projects = json.load(file)

tasks_path = osp.join(osp.dirname(__file__), 'assets', 'tasks.json')
with open(tasks_path) as file:
    tasks = json.load(file)

annotation_path = osp.join(osp.dirname(__file__), 'assets', 'annotations.json')
with open(annotation_path) as file:
    annotations = json.load(file)

DEFAULT_ATTRIBUTES_FORMATS = [
    "VGGFace2 1.0",
    "WiderFace 1.0",
    "Ultralytics YOLO Classification 1.0",
    "YOLO 1.1",
    "Ultralytics YOLO Detection 1.0",
    "Ultralytics YOLO Detection Track 1.0",
    "Ultralytics YOLO Segmentation 1.0",
    "Ultralytics YOLO Oriented Bounding Boxes 1.0",
    "Ultralytics YOLO Pose 1.0",
    "PASCAL VOC 1.1",
    "Segmentation mask 1.1",
    "ImageNet 1.0",
    "Cityscapes 1.0",
    "MOTS PNG 1.0",
]


def generate_image_file(filename, size=(100, 50)):
    f = BytesIO()
    image = Image.new('RGB', size=size)
    image.save(f, 'jpeg')
    f.name = filename
    f.seek(0)
    return f


def generate_video_file(filename, width=1280, height=720, duration=1, fps=25, codec_name='mpeg4'):
    f = BytesIO()
    total_frames = duration * fps
    file_ext = os.path.splitext(filename)[1][1:]
    container = av.open(f, mode='w', format=file_ext)

    stream = container.add_stream(codec_name=codec_name, rate=fps)
    stream.width = width
    stream.height = height
    stream.pix_fmt = 'yuv420p'

    for frame_i in range(total_frames):
        img = np.empty((stream.width, stream.height, 3))
        img[:, :, 0] = 0.5 + 0.5 * np.sin(2 * np.pi * (0 / 3 + frame_i / total_frames))
        img[:, :, 1] = 0.5 + 0.5 * np.sin(2 * np.pi * (1 / 3 + frame_i / total_frames))
        img[:, :, 2] = 0.5 + 0.5 * np.sin(2 * np.pi * (2 / 3 + frame_i / total_frames))

        img = np.round(255 * img).astype(np.uint8)
        img = np.clip(img, 0, 255)

        frame = av.VideoFrame.from_ndarray(img, format='rgb24')
        for packet in stream.encode(frame):
            container.mux(packet)

    # Flush stream
    for packet in stream.encode():
        container.mux(packet)

    # Close the file
    container.close()
    f.name = filename
    f.seek(0)

    return [(width, height)] * total_frames, f


def compare_datasets(expected: Dataset, actual: Dataset):
    # we need this function to allow for a bit of variation in the rotation attribute
    comparator = EqualityComparator(ignored_attrs=["rotation"])

    output = comparator.compare_datasets(expected, actual)
    unmatched = output["mismatches"]
    expected_extra = output["a_extra_items"]
    actual_extra = output["b_extra_items"]
    errors = output["errors"]
    assert not unmatched, f"Datasets have unmatched items: {unmatched}"
    assert not actual_extra, f"Actual has following extra items: {actual_extra}"
    assert not expected_extra, f"Expected has following extra items: {expected_extra}"
    assert not errors, f"There were following errors while comparing datasets: {errors}"

    for item_a, item_b in zip(expected, actual):
        for ann_a, ann_b in zip(item_a.annotations, item_b.annotations):
            assert (
                abs(ann_a.attributes.get("rotation", 0) - ann_b.attributes.get("rotation", 0))
                < 0.01
            )


class _DbTestBase(ExportApiTestBase):
    @classmethod
    def setUpTestData(cls):
        cls.create_db_users()

    @classmethod
    def create_db_users(cls):
        (group_admin, _) = Group.objects.get_or_create(name="admin")
        (group_user, _) = Group.objects.get_or_create(name="user")

        user_admin = User.objects.create_superuser(username="admin", email="",
            password="admin")
        user_admin.groups.add(group_admin)
        user_dummy = User.objects.create_user(username="user", password="user")
        user_dummy.groups.add(group_user)

        cls.admin = user_admin
        cls.user = user_dummy

    def _put_api_v2_task_id_annotations(self, tid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.put("/api/tasks/%s/annotations" % tid,
                data=data, format="json")

        return response

    def _put_api_v2_job_id_annotations(self, jid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.put("/api/jobs/%s/annotations" % jid,
                data=data, format="json")

        return response

    @staticmethod
    def _generate_task_images(count, name_offsets = 0): # pylint: disable=no-self-use
        images = {"client_files[%d]" % i: generate_image_file("image_%d.jpg" % (i + name_offsets)) for i in range(count)}
        images["image_quality"] = 75
        return images

    @staticmethod
    def _generate_task_videos(count):  # pylint: disable=no-self-use
        videos = {"client_files[%d]" % i: generate_video_file("video_%d.mp4" % i) for i in range(count)}
        videos["image_quality"] = 75
        return videos

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid,
                data=image_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/%s" % tid)

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                ))
                response.data["labels"] = labels_response

            task = response.data

        return task

    def _create_project(self, data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/projects', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            project = response.data

        return project

    def _get_jobs(self, task_id):
        with ForceLogin(self.admin, self.client):
            values = get_paginated_collection(lambda page:
                self.client.get("/api/jobs?task_id={}&page={}".format(task_id, page))
            )
        return values

    def _get_tasks(self, project_id):
        with ForceLogin(self.admin, self.client):
            values = get_paginated_collection(lambda page:
                self.client.get("/api/tasks", data={"project_id": project_id, "page": page})
            )
        return values

    def _get_data_from_task(self, task_id, include_images):
        task_ann = TaskAnnotation(task_id)
        task_ann.init_from_db()
        task_data = TaskData(task_ann.ir_data, Task.objects.get(pk=task_id))
        extractor = CvatTaskOrJobDataExtractor(task_data, include_images=include_images)
        return Dataset.from_extractors(extractor)

    def _delete_request(self, path, user):
        with ForceLogin(user, self.client):
            response = self.client.delete(path)
        return response

    @staticmethod
    def _make_attribute_value(key_get_values, attribute):
        assert key_get_values in ["default", "random"]
        if key_get_values == "random":
            if attribute["input_type"] == "number":
                start = int(attribute["values"][0])
                stop = int(attribute["values"][1]) + 1
                step = int(attribute["values"][2])
                return str(random.randrange(start, stop, step))  # nosec B311 NOSONAR
            return random.choice(attribute["values"])  # nosec B311 NOSONAR
        assert key_get_values == "default"
        return attribute["default_value"]

    @staticmethod
    def _make_annotations_for_task(task, name_ann, key_get_values):
        def fill_one_attribute_in_element(is_item_tracks, element, attribute):
            spec_id = attribute["id"]
            value = _DbTestBase._make_attribute_value(key_get_values, attribute)

            if is_item_tracks and attribute["mutable"]:
                for index_shape, _ in enumerate(element["shapes"]):
                    element["shapes"][index_shape]["attributes"].append({
                        "spec_id": spec_id,
                        "value": value,
                    })
            else:
                element["attributes"].append({
                    "spec_id": spec_id,
                    "value": value,
                })

        def fill_all_attributes_in_element(is_item_tracks, element, label):
            element["label_id"] = label["id"]

            for attribute in label["attributes"]:
                fill_one_attribute_in_element(is_item_tracks, element, attribute)

            sub_elements = element.get("elements", [])
            sub_labels = label.get("sublabels", [])
            for sub_element, sub_label in zip(sub_elements, sub_labels):
                fill_all_attributes_in_element(is_item_tracks, sub_element, sub_label)

        tmp_annotations = copy.deepcopy(annotations[name_ann])

        for item in ["tags", "shapes", "tracks"]:
            for _element in tmp_annotations.get(item, []):
                fill_all_attributes_in_element(item == "tracks", _element, task["labels"][0])

        return tmp_annotations

    def _create_annotations(self, task, name_ann, key_get_values):
        tmp_annotations = self._make_annotations_for_task(task, name_ann, key_get_values)
        response = self._put_api_v2_task_id_annotations(task["id"], tmp_annotations)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def _create_annotations_in_job(self, task, job_id,  name_ann, key_get_values):
        tmp_annotations = self._make_annotations_for_task(task, name_ann, key_get_values)
        response = self._put_api_v2_job_id_annotations(job_id, tmp_annotations)
        self.assertEqual(response.status_code, status.HTTP_200_OK, msg=response.json())

    def _upload_file(self, url, data, user):
        response = self._put_request(url, user, data={"annotation_file": data}, format="multipart")
        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)
        response = self._put_request(url, user)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def _check_downloaded_file(self, file_name):
        if not osp.exists(file_name):
            raise FileNotFoundError(f"File '{file_name}' was not downloaded")

    def _generate_url_remove_tasks_annotations(self, task_id):
        return f"/api/tasks/{task_id}/annotations"

    def _generate_url_upload_tasks_annotations(self, task_id, upload_format_name):
        return f"/api/tasks/{task_id}/annotations?format={upload_format_name}"

    def _generate_url_upload_job_annotations(self, job_id, upload_format_name):
        return f"/api/jobs/{job_id}/annotations?format={upload_format_name}"

    def _generate_url_upload_project_dataset(self, project_id, format_name):
        return f"/api/projects/{project_id}/dataset?format={format_name}"

    def _remove_annotations(self, url, user):
        response = self._delete_request(url, user)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        return response

    def _delete_project(self, project_id, user):
        response = self._delete_request(f'/api/projects/{project_id}', user)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        return response

    @staticmethod
    def _save_file_from_response(response, file_name):
        if response.status_code == status.HTTP_200_OK:
            content = b"".join(response.streaming_content)
            with open(file_name, "wb") as f:
                f.write(content)


@ensure_extractors_efficiency
@ensure_streaming_importers
class TaskDumpUploadTest(_DbTestBase):
    def test_api_v2_dump_and_upload_annotations_with_objects_type_is_shape(self):
        test_name = self._testMethodName
        dump_formats = dm.views.get_export_formats()
        upload_formats = dm.views.get_import_formats()
        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED,'file_exists': True, 'annotation_loaded': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True, 'annotation_loaded': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False, 'annotation_loaded': False},
        }

        with TestDir() as test_dir:
            # Dump annotations with objects type is shape
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DISPLAY_NAME in [
                    'Kitti Raw Format 1.0', 'Sly Point Cloud Format 1.0',
                    'Datumaro 3D 1.0'
                ]:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(format=dump_format_name):
                    images = self._generate_task_images(3)
                    # create task with annotations
                    if dump_format_name in [
                        "Cityscapes 1.0", "COCO Keypoints 1.0",
                        "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                        "ICDAR Segmentation 1.0", "Market-1501 1.0", "MOT 1.1",
                        "Ultralytics YOLO Pose 1.0",
                    ]:
                        task = self._create_task(tasks[dump_format_name], images)
                    else:
                        task = self._create_task(tasks["main"], images)
                    task_id = task["id"]
                    if dump_format_name in DEFAULT_ATTRIBUTES_FORMATS + [
                        "Datumaro 1.0",
                    ]:
                        self._create_annotations(task, dump_format_name, "default")
                    else:
                        self._create_annotations(task, dump_format_name, "random")

                    # dump annotations
                    export_params = {
                        "format": dump_format_name,
                    }

                    for user, edata in list(expected.items()):
                        self._clear_temp_data() # clean up from previous tests and iterations

                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        user_name = edata['name']
                        file_zip_name = osp.join(test_dir, f'{test_name}_{user_name}_{dump_format_name}.zip')

                        response = self._export_task_annotations(user, task_id, query_params=export_params, expected_4xx_status_code=expected_4xx_status_code)
                        self._save_file_from_response(response, file_zip_name)
                        self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])

            # Upload annotations with objects type is shape
            for upload_format in upload_formats:
                upload_format_name = upload_format.DISPLAY_NAME
                if upload_format_name == "CVAT 1.1":
                    file_zip_name = osp.join(test_dir, f'{test_name}_admin_CVAT for images 1.1.zip')
                else:
                    file_zip_name = osp.join(test_dir, f'{test_name}_admin_{upload_format_name}.zip')
                if not upload_format.ENABLED or not osp.exists(file_zip_name):
                    continue
                with self.subTest(format=upload_format_name):
                    if upload_format_name in [
                        "MOTS PNG 1.0",  # issue #2925 and changed points values
                    ]:
                        self.skipTest("Format is fail")
                    if osp.exists(file_zip_name):
                        for user, edata in list(expected.items()):
                            # remove all annotations from task (create new task without annotation)
                            images = self._generate_task_images(3)
                            if upload_format_name in [
                                "Cityscapes 1.0", "COCO Keypoints 1.0",
                                "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                                "ICDAR Segmentation 1.0", "Market-1501 1.0", "MOT 1.1",
                                "Ultralytics YOLO Pose 1.0",
                            ]:
                                task = self._create_task(tasks[upload_format_name], images)
                            else:
                                task = self._create_task(tasks["main"], images)
                            task_id = task["id"]
                            url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)

                            with open(file_zip_name, 'rb') as binary_file:
                                response = self._put_request(
                                    url,
                                    user,
                                    data={"annotation_file": binary_file},
                                    format="multipart",
                                )
                                self.assertEqual(response.status_code, edata['accept code'])
                                response = self._put_request(url, user)
                                self.assertEqual(response.status_code, edata['create code'])

    def test_api_v2_dump_annotations_with_objects_type_is_track(self):
        test_name = self._testMethodName

        dump_formats = dm.views.get_export_formats()
        upload_formats = dm.views.get_import_formats()
        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True, 'annotation_loaded': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True, 'annotation_loaded': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False, 'annotation_loaded': False},
        }

        with TestDir() as test_dir:
            # Dump annotations with objects type is track
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DISPLAY_NAME in [
                    'Kitti Raw Format 1.0','Sly Point Cloud Format 1.0',
                    'Datumaro 3D 1.0'
                ]:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(format=dump_format_name):
                    # create task with annotations
                    video = self._generate_task_videos(1)
                    if dump_format_name in [
                        "Cityscapes 1.0", "COCO Keypoints 1.0",
                        "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                        "ICDAR Segmentation 1.0", "Market-1501 1.0", "MOT 1.1",
                        "Ultralytics YOLO Pose 1.0",
                    ]:
                        task = self._create_task(tasks[dump_format_name], video)
                    else:
                        task = self._create_task(tasks["main"], video)
                    task_id = task["id"]

                    if dump_format_name in DEFAULT_ATTRIBUTES_FORMATS:
                        self._create_annotations(task, dump_format_name, "default")
                    else:
                        self._create_annotations(task, dump_format_name, "random")

                    # dump annotations
                    export_params = {
                        "format": dump_format_name,
                    }

                    for user, edata in list(expected.items()):
                        self._clear_temp_data() # clean up from previous tests and iterations

                        user_name = edata['name']
                        file_zip_name = osp.join(test_dir, f'{test_name}_{user_name}_{dump_format_name}.zip')
                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        response = self._export_task_annotations(user, task_id, query_params=export_params, expected_4xx_status_code=expected_4xx_status_code)
                        self._save_file_from_response(response, file_zip_name)
                        self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])
            # Upload annotations with objects type is track
            for upload_format in upload_formats:
                upload_format_name = upload_format.DISPLAY_NAME
                if upload_format_name == "CVAT 1.1":
                    file_zip_name = osp.join(test_dir, f'{test_name}_admin_CVAT for video 1.1.zip')
                else:
                    file_zip_name = osp.join(test_dir, f'{test_name}_admin_{upload_format_name}.zip')
                if not upload_format.ENABLED or not osp.exists(file_zip_name):
                    continue
                with self.subTest(format=upload_format_name):
                    if upload_format_name in [
                        "MOTS PNG 1.0",  # issue #2925 and changed points values
                    ]:
                        self.skipTest("Format is fail")
                    if osp.exists(file_zip_name):
                        for user, edata in list(expected.items()):
                            # remove all annotations from task (create new task without annotation)
                            video = self._generate_task_videos(1)
                            if upload_format_name in [
                                "Cityscapes 1.0", "COCO Keypoints 1.0",
                                "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                                "ICDAR Segmentation 1.0", "Market-1501 1.0", "MOT 1.1",
                                "Ultralytics YOLO Pose 1.0",
                            ]:
                                task = self._create_task(tasks[upload_format_name], video)
                            else:
                                task = self._create_task(tasks["main"], video)
                            task_id = task["id"]
                            url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)

                            with open(file_zip_name, 'rb') as binary_file:
                                response = self._put_request(
                                    url,
                                    user,
                                    data={"annotation_file": binary_file},
                                    format="multipart",
                                )
                                self.assertEqual(response.status_code, edata['accept code'])
                                response = self._put_request(url, user)
                                self.assertEqual(response.status_code, edata['create code'])

    def test_api_v2_dump_tag_annotations(self):
        dump_format_name = "CVAT for images 1.1"
        test_cases = ['all', 'first']
        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False},
        }
        export_params = {
            "format": dump_format_name,
        }

        for test_case in test_cases:
            images = self._generate_task_images(10)
            task = self._create_task(tasks["change overlap and segment size"], images)
            task_id = task["id"]
            jobs = self._get_jobs(task_id)

            if test_case == "all":
                for job in jobs:
                    self._create_annotations_in_job(task, job["id"], "CVAT for images 1.1 tag", "default")
            else:
                self._create_annotations_in_job(task, jobs[0]["id"], "CVAT for images 1.1 tag", "default")

            for user, edata in list(expected.items()):
                with self.subTest(format=f"{edata['name']}"):
                    with TestDir() as test_dir:
                        self._clear_temp_data() # clean up from previous tests and iterations

                        user_name = edata['name']
                        file_zip_name = osp.join(test_dir, f'{user_name}.zip')
                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        response = self._export_task_annotations(user, task_id, query_params=export_params, expected_4xx_status_code=expected_4xx_status_code)
                        self._save_file_from_response(response, file_zip_name)
                        self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])

    def test_api_v2_dump_and_upload_annotations_with_objects_are_different_images(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for images 1.1"
        upload_types = ["task", "job"]

        images = self._generate_task_images(2)
        task = self._create_task(tasks["main"], images)
        task_id = task["id"]
        export_params = {
            "format": dump_format_name,
        }

        for upload_type in upload_types:
            with self.subTest(format=type):
                with TestDir() as test_dir:
                    if upload_type == "task":
                        self._create_annotations(task, "CVAT for images 1.1 different types", "random")
                    else:
                        jobs = self._get_jobs(task_id)
                        job_id = jobs[0]["id"]
                        self._create_annotations_in_job(task, job_id, "CVAT for images 1.1 different types", "random")

                    file_zip_name = osp.join(test_dir, f'{test_name}_{upload_type}.zip')
                    self._export_task_annotations(self.admin, task_id, query_params=export_params, file_path=file_zip_name)
                    self.assertEqual(osp.exists(file_zip_name), True)

                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)
                    if upload_type == "task":
                        url_upload = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
                    else:
                        jobs = self._get_jobs(task_id)
                        url_upload = self._generate_url_upload_job_annotations(jobs[0]["id"], "CVAT 1.1")

                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url_upload, binary_file, self.admin)

                        response = self._get_request(f"/api/tasks/{task_id}/annotations", self.admin)
                        self.assertEqual(len(response.data["shapes"]), 2)
                        self.assertEqual(len(response.data["tracks"]), 0)

    def test_api_v2_dump_and_upload_annotations_with_objects_are_different_video(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for video 1.1"
        upload_types = ["task", "job"]

        video = self._generate_task_videos(1)
        task = self._create_task(tasks["main"], video)
        task_id = task["id"]

        export_params = {
            "format": dump_format_name,
        }

        for upload_type in upload_types:
            with self.subTest(format=type):
                with TestDir() as test_dir:
                    if upload_type == "task":
                        self._create_annotations(task, "CVAT for images 1.1 different types", "random")
                    else:
                        jobs = self._get_jobs(task_id)
                        job_id = jobs[0]["id"]
                        self._create_annotations_in_job(task, job_id, "CVAT for images 1.1 different types", "random")

                    file_zip_name = osp.join(test_dir, f'{test_name}_{upload_type}.zip')
                    self._export_task_annotations(self.admin, task_id, query_params=export_params, file_path=file_zip_name)
                    self.assertEqual(osp.exists(file_zip_name), True)
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)
                    if upload_type == "task":
                        url_upload = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
                    else:
                        jobs = self._get_jobs(task_id)
                        url_upload = self._generate_url_upload_job_annotations(jobs[0]["id"], "CVAT 1.1")

                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url_upload, binary_file, self.admin)
                        self.assertEqual(osp.exists(file_zip_name), True)

                        response = self._get_request(f"/api/tasks/{task_id}/annotations", self.admin)
                        self.assertEqual(len(response.data["shapes"]), 0)
                        self.assertEqual(len(response.data["tracks"]), 2)

    def test_api_v2_dump_and_upload_with_objects_type_is_track_and_outside_property(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for video 1.1"
        video = self._generate_task_videos(1)
        task = self._create_task(tasks["main"], video)
        self._create_annotations(task, "CVAT for video 1.1 slice track", "random")
        task_id = task["id"]

        with TestDir() as test_dir:
            file_zip_name = osp.join(test_dir, f'{test_name}.zip')
            data = {
                "format": dump_format_name,
            }
            self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
            self.assertEqual(osp.exists(file_zip_name), True)

            with open(file_zip_name, 'rb') as binary_file:
                url = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
                self._upload_file(url, binary_file, self.admin)

    def test_api_v2_dump_and_upload_with_objects_type_is_track_and_keyframe_property(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for video 1.1"

        video = self._generate_task_videos(1)
        task = self._create_task(tasks["main"], video)
        self._create_annotations(task, "CVAT for video 1.1 slice track keyframe", "random")
        task_id = task["id"]

        with TestDir() as test_dir:
            file_zip_name = osp.join(test_dir, f'{test_name}.zip')

            data = {
                "format": dump_format_name,
            }
            self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
            self.assertEqual(osp.exists(file_zip_name), True)

            with open(file_zip_name, 'rb') as binary_file:
                url = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
                self._upload_file(url, binary_file, self.admin)

    def test_api_v2_dump_upload_annotations_from_several_jobs(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for images 1.1"

        images = self._generate_task_images(10)
        task = self._create_task(tasks["change overlap and segment size"], images)
        task_id = task["id"]
        jobs = self._get_jobs(task_id)
        for job in jobs:
            self._create_annotations_in_job(task, job["id"], "CVAT for images 1.1 merge", "random")

        with TestDir() as test_dir:
            file_zip_name = osp.join(test_dir, f'{test_name}.zip')
            data = {
                "format": dump_format_name,
            }
            self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
            self.assertEqual(osp.exists(file_zip_name), True)

            # remove annotations
            url = self._generate_url_remove_tasks_annotations(task_id)
            self._remove_annotations(url, self.admin)
            url = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
            with open(file_zip_name, 'rb') as binary_file:
                self._upload_file(url, binary_file, self.admin)

    def test_api_v2_dump_annotations_from_several_jobs(self):
        test_name = self._testMethodName
        dump_formats = ["CVAT for images 1.1", "CVAT for video 1.1"]
        test_cases = ['all', 'first']

        for dump_format_name in dump_formats:

            images = self._generate_task_images(10)
            task = self._create_task(tasks["change overlap and segment size"], images)
            task_id = task["id"]

            for test_case in test_cases:
                with TestDir() as test_dir:
                    jobs = self._get_jobs(task_id)
                    if test_case == "all":
                        for job in jobs:
                            self._create_annotations_in_job(task, job["id"], dump_format_name, "default")
                    else:
                        self._create_annotations_in_job(task, jobs[0]["id"], dump_format_name, "default")

                    file_zip_name = osp.join(test_dir, f'{test_name}.zip')
                    data = {
                        "format": dump_format_name,
                    }
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self.assertEqual(osp.exists(file_zip_name), True)

                    # remove annotations
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)
                    url = self._generate_url_upload_tasks_annotations(task_id, "CVAT 1.1")
                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

    def test_api_v2_export_dataset(self):
        test_name = self._testMethodName
        dump_formats = dm.views.get_export_formats()

        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False},
        }

        with TestDir() as test_dir:
            # Dump annotations with objects type is shape
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DISPLAY_NAME != "CVAT for images 1.1":
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(format=dump_format_name):
                    images = self._generate_task_images(3)
                    # create task with annotations
                    if dump_format_name in [
                        "Cityscapes 1.0", "COCO Keypoints 1.0",
                        "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                        "ICDAR Segmentation 1.0", "Market-1501 1.0", "MOT 1.1",
                        "Ultralytics YOLO Pose 1.0",
                    ]:
                        task = self._create_task(tasks[dump_format_name], images)
                    else:
                        task = self._create_task(tasks["main"], images)
                    task_id = task["id"]
                    # dump annotations
                    for user, edata in list(expected.items()):
                        self._clear_temp_data() # clean up from previous tests and iterations

                        user_name = edata['name']
                        file_zip_name = osp.join(test_dir, f'{test_name}_{user_name}_{dump_format_name}.zip')
                        data = {
                            "format": dump_format_name,
                        }
                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        self._export_task_dataset(user, task_id, query_params=data, file_path=file_zip_name, expected_4xx_status_code=expected_4xx_status_code)
                        self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])

    def test_api_v2_dump_empty_frames(self):
        dump_formats = dm.views.get_export_formats()
        upload_formats = dm.views.get_import_formats()

        with TestDir() as test_dir:
            for dump_format in dump_formats:
                if not dump_format.ENABLED:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(format=dump_format_name):
                    images = self._generate_task_images(3)
                    task = self._create_task(tasks["no attributes"], images)
                    task_id = task["id"]
                    self._create_annotations(task, "empty annotation", "default")

                    file_zip_name = osp.join(test_dir, f'empty_{dump_format_name}.zip')
                    data = {
                        "format": dump_format_name,
                    }
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self.assertEqual(osp.exists(file_zip_name), True)

            for upload_format in upload_formats:
                upload_format_name = upload_format.DISPLAY_NAME
                if upload_format_name == "CVAT 1.1":
                    file_zip_name = osp.join(test_dir, 'empty_CVAT for images 1.1.zip')
                else:
                    file_zip_name = osp.join(test_dir, f'empty_{upload_format_name}.zip')
                if not osp.exists(file_zip_name) or not upload_format.ENABLED:
                    continue
                with self.subTest(format=upload_format_name):
                    if upload_format_name in [
                        "MOTS PNG 1.0",  # issue #2925 and changed points values
                        "KITTI 1.0", # format does not support empty annotation
                        "Cityscapes 1.0" # formats doesn't support empty annotations
                    ]:
                        self.skipTest("Format is fail")
                    images = self._generate_task_images(3)
                    task = self._create_task(tasks["no attributes"], images)
                    task_id = task["id"]

                    url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)

                    with open(file_zip_name, 'rb') as binary_file:
                        response = self._put_request(
                            url,
                            self.admin,
                            data={"annotation_file": binary_file},
                            format="multipart",
                        )
                        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)
                        response = self._put_request(url, self.admin)
                        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
                        self.assertIsNone(response.data)

    def test_api_v2_rewriting_annotations(self):
        test_name = self._testMethodName
        dump_formats = dm.views.get_export_formats()
        with TestDir() as test_dir:
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DIMENSION == dm.bindings.DimensionType.DIM_3D:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME

                with self.subTest(format=dump_format_name):
                    if dump_format_name in [
                        "MOTS PNG 1.0",  # issue #2925 and changed points values
                        "Cityscapes 1.0" # expanding annotations due to background mask
                    ]:
                        self.skipTest("Format is fail")

                    images = self._generate_task_images(3)
                    if dump_format_name in [
                        "Market-1501 1.0",
                        "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                        "ICDAR Segmentation 1.0", "COCO Keypoints 1.0", "Ultralytics YOLO Pose 1.0",
                    ]:
                        task = self._create_task(tasks[dump_format_name], images)
                    else:
                        task = self._create_task(tasks["main"], images)
                    task_id = task["id"]

                    if dump_format_name in DEFAULT_ATTRIBUTES_FORMATS + [
                        "MOT 1.1", "Datumaro 1.0", "Open Images V6 1.0", "KITTI 1.0",
                    ]:
                        self._create_annotations(task, dump_format_name, "default")
                    else:
                        self._create_annotations(task, dump_format_name, "random")

                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()
                    task_ann_prev_data = task_ann.data

                    file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')
                    data = {
                        "format": dump_format_name,
                    }
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self.assertEqual(osp.exists(file_zip_name), True)

                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)

                    self._create_annotations(task, "CVAT for images 1.1 many jobs", "default")

                    if dump_format_name == "CVAT for images 1.1" or dump_format_name == "CVAT for video 1.1":
                        dump_format_name = "CVAT 1.1"
                    elif dump_format_name == "Ultralytics YOLO Detection Track 1.0":
                        dump_format_name = "Ultralytics YOLO Detection 1.0"
                    url = self._generate_url_upload_tasks_annotations(task_id, dump_format_name)

                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()
                    task_ann_data = task_ann.data
                    self.assertEqual(len(task_ann_data["shapes"]), len(task_ann_prev_data["shapes"]))

    def test_api_v2_tasks_annotations_dump_and_upload_many_jobs_with_datumaro(self):
        test_name = self._testMethodName
        upload_format_name = "CVAT 1.1"
        include_images_params = (False, True)
        dump_format_names = ("CVAT for images 1.1", "CVAT for video 1.1")

        for dump_format_name, include_images in itertools.product(dump_format_names, include_images_params):
            with self.subTest(f"{dump_format_name}_include_images_{include_images}"):
                # create task with annotations
                images = self._generate_task_images(13)
                task = self._create_task(tasks["many jobs"], images)
                self._create_annotations(task, f'{dump_format_name} many jobs', "default")

                task_id = task["id"]
                data_from_task_before_upload = self._get_data_from_task(task_id, include_images)

                # dump annotations
                with TestDir() as test_dir:
                    file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')

                    data = {
                        "format": dump_format_name,
                    }
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self._check_downloaded_file(file_zip_name)

                    # remove annotations
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)

                    # upload annotations
                    url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)
                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    # equals annotations
                    data_from_task_after_upload = self._get_data_from_task(task_id, include_images)
                    compare_datasets(data_from_task_before_upload, data_from_task_after_upload)

    def test_api_v2_tasks_annotations_dump_and_upload_with_datumaro(self):
        test_name = self._testMethodName
        # get formats
        dump_formats = dm.views.get_export_formats()
        include_images_params = (False, True)
        for dump_format, include_images in itertools.product(dump_formats, include_images_params):
            if dump_format.ENABLED:
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(dump_format_name):
                    if dump_format_name in [
                        "MOT 1.1",
                        "CamVid 1.0", # issue #2840 and changed points values
                        "MOTS PNG 1.0", # changed points values
                        "Segmentation mask 1.1", # changed points values
                        "ICDAR Segmentation 1.0", # changed points values
                        "Open Images V6 1.0", # changed points values
                        'Kitti Raw Format 1.0',
                        'Sly Point Cloud Format 1.0',
                        'KITTI 1.0', # changed points values
                        'Cityscapes 1.0', # changed points value
                        'Datumaro 3D 1.0'
                    ]:
                        self.skipTest("Format is fail")

                    # create task
                    images = self._generate_task_images(3)
                    if dump_format_name in [
                        "Market-1501 1.0", "Cityscapes 1.0",
                        "ICDAR Localization 1.0", "ICDAR Recognition 1.0",
                        "ICDAR Segmentation 1.0", "COCO Keypoints 1.0",
                        "Ultralytics YOLO Pose 1.0",
                    ]:
                        task = self._create_task(tasks[dump_format_name], images)
                    else:
                        task = self._create_task(tasks["main"], images)

                    # create annotations
                    if dump_format_name in DEFAULT_ATTRIBUTES_FORMATS + [
                        "MOT 1.1", "LFW 1.0",
                        "Open Images V6 1.0", "Datumaro 1.0", "KITTI 1.0",
                    ]:
                        self._create_annotations(task, dump_format_name, "default")
                    else:
                        self._create_annotations(task, dump_format_name, "random")

                    task_id = task["id"]
                    data_from_task_before_upload = self._get_data_from_task(task_id, include_images)

                    # dump annotations
                    with TestDir() as test_dir:
                        file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')
                        data = {
                            "format": dump_format_name,
                        }
                        self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                        self._check_downloaded_file(file_zip_name)

                        # remove annotations
                        url = self._generate_url_remove_tasks_annotations(task_id)
                        self._remove_annotations(url, self.admin)

                        # upload annotations
                        if dump_format_name in ["CVAT for images 1.1", "CVAT for video 1.1"]:
                            upload_format_name = "CVAT 1.1"
                        elif dump_format_name in ['Ultralytics YOLO Detection Track 1.0']:
                            upload_format_name = 'Ultralytics YOLO Detection 1.0'
                        else:
                            upload_format_name = dump_format_name
                        url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)
                        with open(file_zip_name, 'rb') as binary_file:
                            self._upload_file(url, binary_file, self.admin)

                            # equals annotations
                        data_from_task_after_upload = self._get_data_from_task(task_id, include_images)
                        compare_datasets(data_from_task_before_upload, data_from_task_after_upload)

    def test_api_v2_check_duplicated_polygon_points(self):
        test_name = self._testMethodName
        images = self._generate_task_images(10)
        task = self._create_task(tasks["main"], images)
        task_id = task["id"]
        data = {
            "format": "CVAT for video 1.1",
        }
        annotation_name = "CVAT for video 1.1 polygon"
        self._create_annotations(task, annotation_name, "default")
        annotation_points = annotations[annotation_name]["tracks"][0]["shapes"][0]['points']

        with TestDir() as test_dir:
            file_zip_name = osp.join(test_dir, f'{test_name}.zip')
            self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
            self._check_downloaded_file(file_zip_name)

            folder_name = osp.join(test_dir, f'{test_name}')
            with zipfile.ZipFile(file_zip_name, 'r') as zip_ref:
                zip_ref.extractall(folder_name)

            tree = ET.parse(osp.join(folder_name, 'annotations.xml'))
            root = tree.getroot()
            for polygon in root.findall("./track[@id='0']/polygon"):
                polygon_points = polygon.attrib["points"].replace(",", ";")
                polygon_points = [float(p) for p in polygon_points.split(";")]
                self.assertEqual(polygon_points, annotation_points)

    def test_api_v2_check_widerface_with_all_attributes(self):
        test_name = self._testMethodName
        dump_format_name = "WiderFace 1.0"
        upload_format_name = "WiderFace 1.0"

        for include_images in (False, True):
            with self.subTest():
                # create task with annotations
                images = self._generate_task_images(3)
                task = self._create_task(tasks["widerface with all attributes"], images)
                self._create_annotations(task, f'{dump_format_name}', "random")

                task_id = task["id"]
                data_from_task_before_upload = self._get_data_from_task(task_id, include_images)

                # dump annotations
                data = {
                    "format": dump_format_name,
                }
                with TestDir() as test_dir:
                    file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self._check_downloaded_file(file_zip_name)

                    # remove annotations
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)

                    # upload annotations
                    url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)
                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    # equals annotations
                    data_from_task_after_upload = self._get_data_from_task(task_id, include_images)
                    compare_datasets(data_from_task_before_upload, data_from_task_after_upload)

    def test_api_v2_check_mot_with_shapes_only(self):
        test_name = self._testMethodName
        format_name = "MOT 1.1"

        for include_images in (False, True):
            with self.subTest():
                # create task with annotations
                images = self._generate_task_images(3)
                task = self._create_task(tasks[format_name], images)
                self._create_annotations(task, f'{format_name} shapes only', "default")

                task_id = task["id"]
                data_from_task_before_upload = self._get_data_from_task(task_id, include_images)

                # dump annotations
                data = {
                    "format": format_name,
                }
                with TestDir() as test_dir:
                    file_zip_name = osp.join(test_dir, f'{test_name}_{format_name}.zip')
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self._check_downloaded_file(file_zip_name)

                    # remove annotations
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)

                    # upload annotations
                    url = self._generate_url_upload_tasks_annotations(task_id, format_name)
                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    # equals annotations
                    data_from_task_after_upload = self._get_data_from_task(task_id, include_images)
                    compare_datasets(data_from_task_before_upload, data_from_task_after_upload)

    def test_api_v2_check_attribute_import_in_tracks(self):
        test_name = self._testMethodName
        dump_format_name = "CVAT for video 1.1"
        upload_format_name = "CVAT 1.1"

        for include_images in (False, True):
            with self.subTest():
                # create task with annotations
                images = self._generate_task_images(13)
                task = self._create_task(tasks["many jobs"], images)
                self._create_annotations(task, f'{dump_format_name} attributes in tracks', "default")

                task_id = task["id"]
                data_from_task_before_upload = self._get_data_from_task(task_id, include_images)

                # dump annotations
                data = {
                    "format": dump_format_name,
                }
                with TestDir() as test_dir:
                    file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')
                    self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                    self._check_downloaded_file(file_zip_name)

                    # remove annotations
                    url = self._generate_url_remove_tasks_annotations(task_id)
                    self._remove_annotations(url, self.admin)

                    # upload annotations
                    url = self._generate_url_upload_tasks_annotations(task_id, upload_format_name)
                    with open(file_zip_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    # equals annotations
                    data_from_task_after_upload = self._get_data_from_task(task_id, include_images)
                    compare_datasets(data_from_task_before_upload, data_from_task_after_upload)

    def test_api_v2_check_skeleton_tracks_with_missing_shapes(self):
        test_name = self._testMethodName
        format_name = "COCO Keypoints 1.0"

        # create task with annotations
        for whole_task in (False, True):
            for name_ann in [
                "many jobs skeleton tracks with missing shapes",
                "many jobs skeleton tracks with missing shapes - skeleton is outside",
                "many jobs skeleton tracks with missing shapes - some points present",
            ]:
                with self.subTest():
                    images = self._generate_task_images(25)
                    task = self._create_task(tasks['many jobs skeleton'], images)
                    task_id = task["id"]

                    if whole_task:
                        self._create_annotations(task, name_ann, "default")
                    else:
                        job_id = next(
                            job["id"]
                            for job in self._get_jobs(task_id)
                            if job["start_frame"] == annotations[name_ann]["tracks"][0]["frame"]
                        )
                        self._create_annotations_in_job(task, job_id, name_ann, "default")

                    # dump annotations
                    data = {"format": format_name}
                    with TestDir() as test_dir:
                        file_zip_name = osp.join(test_dir, f'{test_name}_{format_name}.zip')
                        self._export_task_annotations(self.admin, task_id, query_params=data, file_path=file_zip_name)
                        self._check_downloaded_file(file_zip_name)

                        # remove annotations
                        url = self._generate_url_remove_tasks_annotations(task_id)
                        self._remove_annotations(url, self.admin)

                        # upload annotations
                        url = self._generate_url_upload_tasks_annotations(task_id, format_name)
                        with open(file_zip_name, 'rb') as binary_file:
                            self._upload_file(url, binary_file, self.admin)


class ExportBehaviorTest(_DbTestBase):
    @define
    class SharedBase:
        condition: multiprocessing.Condition = field(factory=multiprocessing.Condition, init=False)

    @define
    class SharedBool(SharedBase):
        value: multiprocessing.Value = field(
            factory=partial(multiprocessing.Value, 'i', 0), init=False
        )

        def set(self, value: bool = True):
            self.value.value = int(value)

        def get(self) -> bool:
            return bool(self.value.value)

    @define
    class SharedString(SharedBase):
        MAX_LEN: ClassVar[int] = 2048

        value: multiprocessing.Value = field(
            factory=partial(multiprocessing.Array, 'c', MAX_LEN), init=False
        )

        def set(self, value: str):
            self.value.get_obj().value = value.encode()[ : self.MAX_LEN - 1]

        def get(self) -> str:
            return self.value.get_obj().value.decode()

    class _LockTimeoutError(Exception):
        pass

    def setUp(self):
        self.export_cache_lock = multiprocessing.Lock()

    @contextmanager
    def patched_get_export_cache_lock(self, export_path, *, ttl: int | timedelta, block: bool = True, acquire_timeout: int | timedelta):
        # fakeredis lock acquired in a subprocess won't be visible to other processes
        # just implement the lock here
        from cvat.apps.dataset_manager.util import LockNotAvailableError

        assert acquire_timeout
        assert ttl

        if isinstance(acquire_timeout, timedelta):
            acquire_timeout = acquire_timeout.total_seconds()

        acquired = self.export_cache_lock.acquire(
            block=block, timeout=acquire_timeout
        )

        if not acquired:
            raise LockNotAvailableError

        try:
            yield
        finally:
            self.export_cache_lock.release()

    @overload
    @classmethod
    def set_condition(cls, var: SharedBool, value: bool = True): ...

    @overload
    @classmethod
    def set_condition(cls, var: SharedBase, value: Any): ...

    _not_set = object()

    @classmethod
    def set_condition(cls, var: SharedBase, value: Any = _not_set):
        if isinstance(var, cls.SharedBool) and value is cls._not_set:
            value = True

        with var.condition:
            var.set(value)
            var.condition.notify()

    @classmethod
    def wait_condition(cls, var: SharedBase, timeout: Optional[int] = 5):
        with var.condition:
            if not var.get() and not var.condition.wait(timeout):
                raise cls._LockTimeoutError

    @staticmethod
    def side_effect(f: Callable, *args, **kwargs) -> Callable:
        """
        Wraps the passed function to be executed with the given parameters
        and return the regular mock output
        """

        def wrapped(*_, **__):
            f(*args, **kwargs)
            return MOCK_DEFAULT

        return wrapped

    @staticmethod
    def chain_side_effects(*calls: Callable) -> Callable:
        """
        Makes a callable that calls all the passed functions sequentially,
        and returns the last call result
        """

        def wrapped(*args, **kwargs):
            result = MOCK_DEFAULT

            for f in calls:
                new_result = f(*args, **kwargs)
                if new_result is not MOCK_DEFAULT:
                    result = new_result

            return result

        return wrapped

    @staticmethod
    @contextmanager
    def process_closing(process: multiprocessing.Process, *, timeout: Optional[int] = 10):
        try:
            yield process
        finally:
            if process.is_alive():
                process.terminate()

            process.join(timeout=timeout)
            process.close()

    def _setup_task_with_annotations(
        self,
        *,
        number_of_images: int = 3,
        format_name: str | None = None,
        name_ann: str | None = None,
    ):
        assert format_name or name_ann
        images = self._generate_task_images(number_of_images)
        task = self._create_task(tasks["main"], images)
        self._create_annotations(task, name_ann or f"{format_name} many jobs", "default")

        return task

    def test_concurrent_export_and_cleanup(self):
        side_effect = self.side_effect
        chain_side_effects = self.chain_side_effects
        set_condition = self.set_condition
        wait_condition = self.wait_condition
        _LockTimeoutError = self._LockTimeoutError
        process_closing = self.process_closing

        format_name = "CVAT for images 1.1"

        export_file_path = self.SharedString()
        export_checked_the_file = self.SharedBool()
        clear_has_been_finished = self.SharedBool()
        clear_removed_the_file = self.SharedBool()
        export_outdated_after = timedelta(seconds=4)

        EXPORT_CACHE_LOCK_TTL = 4
        EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT = EXPORT_CACHE_LOCK_TTL * 2

        def _export(*_, task_id: int):
            import sys
            from os import replace as original_replace
            from os.path import exists as original_exists

            from cvat.apps.dataset_manager.task import export_task as original_export_task
            from cvat.apps.dataset_manager.views import log_exception as original_log_exception

            def patched_log_exception(logger=None, exc_info=True):
                cur_exc_info = sys.exc_info() if exc_info is True else exc_info
                if (
                    cur_exc_info
                    and cur_exc_info[1]
                    and isinstance(cur_exc_info[1], _LockTimeoutError)
                ):
                    return  # don't spam in logs with expected errors

                original_log_exception(logger, exc_info)

            with (
                patch("cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_TTL", new=EXPORT_CACHE_LOCK_TTL),
                patch("cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT",
                      new=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT),
                patch(
                    "cvat.apps.dataset_manager.views.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                # We need to mock the function directly imported into the module
                # to ensure that the `export_checked_the_file` condition is set
                # only after checking whether a file exists inside an acquired lock
                patch("cvat.apps.dataset_manager.views.osp_exists") as mock_osp_exists,
                patch(
                    "cvat.apps.dataset_manager.views.shutil.move", side_effect=original_replace
                ) as mock_os_replace,
                patch("cvat.apps.dataset_manager.views.log_exception", new=patched_log_exception),
                patch("cvat.apps.dataset_manager.views.task.export_task") as mock_export_fn,
            ):
                mock_osp_exists.side_effect = chain_side_effects(
                    original_exists,
                    side_effect(set_condition, export_checked_the_file),
                )
                mock_export_fn.side_effect = chain_side_effects(
                    original_export_task,
                    side_effect(wait_condition, clear_has_been_finished),
                )
                result_file = export(dst_format=format_name, task_id=task_id)
                set_condition(export_file_path, result_file)
                mock_os_replace.assert_not_called()

        def _clear(*_, file_path: str):
            from os import remove as original_remove

            with (
                patch("cvat.apps.dataset_manager.cron.EXPORT_CACHE_LOCK_TTL", new=EXPORT_CACHE_LOCK_TTL),
                patch("cvat.apps.dataset_manager.cron.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT", new=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT),
                patch(
                    "cvat.apps.dataset_manager.cron.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                patch(
                    "cvat.apps.dataset_manager.views.os.remove"
                ) as mock_os_remove,
                patch(
                    "cvat.apps.dataset_manager.views.TTL_CONSTS",
                    new={"task": export_outdated_after},
                ),
            ):
                mock_os_remove.side_effect = chain_side_effects(
                    original_remove,
                    side_effect(set_condition, clear_removed_the_file),
                )

                clear_export_cache(file_path=Path(file_path))
                set_condition(clear_has_been_finished)

                mock_os_remove.assert_not_called()

        # The problem checked is TOCTOU / race condition for file existence check and
        # further file update / removal. There are several possible variants of the problem.
        # An example:
        # 1. export checks the file exists -> file is not outdated -> need to touch file's updated_date
        # 2. clear checks the file exists, and matches the creation timestamp
        # 3. export updates the files's modification date and does not run actual export
        # 4. remove removes the actual export file
        # Thus, we have no exported file after the successful export.

        # note: it is not possible to achieve the situation
        # when clear process deletes newly "re-created by export process"
        # file instead of the checked one since file names contain a timestamp.

        # Other variants can be variations on the intermediate calls, such as getmtime:
        # - export: exists()
        # - clear: remove()
        # - export: getmtime() -> an exception

        # - clear_1: exists()
        # - clear_2: remove()
        # - clear_1: getmtime() -> an exception
        # etc.

        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        # create a file in the export cache
        first_export_path = export(dst_format=format_name, task_id=task_id)

        initial_file_modfication_time = os.path.getmtime(first_export_path)
        # make sure that a file in the export cache is outdated by timeout
        # and a file would have to be deleted if the export was not running in parallel
        sleep(export_outdated_after.seconds + 1)

        processes_finished_correctly = False
        with ExitStack() as es:
            # Run both operations concurrently
            # Threads could be faster, but they can't be terminated
            export_process = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_export,
                        args=(
                            self.export_cache_lock,
                            export_checked_the_file,
                        ),
                        kwargs=dict(task_id=task_id),
                    )
                )
            )
            clear_process = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_clear,
                        args=(
                            self.export_cache_lock,
                            export_checked_the_file,
                        ),
                        kwargs=dict(
                            file_path=first_export_path
                        ),
                    )
                )
            )

            export_process.start()

            wait_condition(export_checked_the_file)  # ensure the expected execution order
            clear_process.start()

            # A deadlock (interrupted by a timeout error) is the positive outcome in this test,
            # if the problem is fixed.
            # clear() must wait for the export cache lock release (acquired by export()).
            # It must be finished by a timeout, as export() holds it, waiting
            clear_process.join(timeout=15)
            export_process.join(timeout=15)

            self.assertFalse(export_process.is_alive())
            self.assertFalse(clear_process.is_alive())

            # All the expected exceptions should be handled in the process callbacks.
            # This is to avoid passing the test with unexpected errors
            self.assertEqual(export_process.exitcode, 0)
            self.assertEqual(clear_process.exitcode, 0)

            processes_finished_correctly = True

        self.assertTrue(processes_finished_correctly)
        self.assertFalse(clear_removed_the_file.get())

        new_export_path = export_file_path.get()
        self.assertGreater(len(new_export_path), 0)
        self.assertTrue(osp.isfile(new_export_path))
        self.assertTrue(osp.isfile(first_export_path))
        self.assertGreater(os.path.getmtime(first_export_path), initial_file_modfication_time)

        # terminate() may break the locks, don't try to acquire
        # https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.terminate
        self.assertTrue(export_checked_the_file.get())

    def test_concurrent_download_and_cleanup(self):
        side_effect = self.side_effect
        chain_side_effects = self.chain_side_effects
        set_condition = self.set_condition
        wait_condition = self.wait_condition
        process_closing = self.process_closing

        format_name = "CVAT for images 1.1"

        download_checked_the_file = self.SharedBool()
        clear_removed_the_file = self.SharedBool()

        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        download_url: str | None = None

        def _download(*_, task_id: int, export_path: str):
            from os.path import exists as original_exists

            def patched_osp_exists(path: str):
                result = original_exists(path)

                if path == export_path:
                    set_condition(download_checked_the_file)
                    wait_condition(
                        clear_removed_the_file, timeout=20
                    )  # wait more than the process timeout

                return result

            with (
                patch(
                    "cvat.apps.engine.background.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                patch("cvat.apps.engine.background.osp.exists") as mock_osp_exists,
                TemporaryDirectory() as temp_dir,
            ):
                mock_osp_exists.side_effect = patched_osp_exists

                response = self._get_request(download_url, self.admin)
                self.assertEqual(response.status_code, status.HTTP_200_OK)

                self._save_file_from_response(response, osp.join(temp_dir, "export.zip"))

                mock_osp_exists.assert_called()

        def _clear(*_, file_path: str):
            from os import remove as original_remove

            from cvat.apps.dataset_manager.util import LockNotAvailableError

            with (
                patch("cvat.apps.dataset_manager.cron.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT", new=3),
                patch(
                    "cvat.apps.dataset_manager.cron.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                patch("cvat.apps.dataset_manager.cron.os.remove") as mock_os_remove,
                patch(
                    "cvat.apps.dataset_manager.views.TTL_CONSTS", new={"task": timedelta(seconds=0)}
                ),
            ):
                mock_os_remove.side_effect = chain_side_effects(
                    original_remove,
                    side_effect(set_condition, clear_removed_the_file),
                )

                exited_by_timeout = False
                try:
                    clear_export_cache(file_path=Path(file_path))
                except LockNotAvailableError:
                    # should come from waiting for get_export_cache_lock
                    exited_by_timeout = True

                assert exited_by_timeout, "LockNotAvailableError should have been raised"

        # The problem checked is TOCTOU / race condition for file existence check and
        # further file reading / removal. There are several possible variants of the problem.
        # An example:
        # 1. download exports the file
        # 2. download checks the export is still relevant
        # 3. clear checks the file exists
        # 4. clear removes the export file
        # 5. download checks if the file exists -> an exception
        #
        # There can be variations on the intermediate calls, such as:
        # - download: exists()
        # - clear: remove()
        # - download: open() -> an exception
        # etc.

        export_path = None

        def patched_export(*args, **kwargs):
            nonlocal export_path

            result = export(*args, **kwargs)
            export_path = result

            return result

        with patch("cvat.apps.dataset_manager.views.export", new=patched_export):
            response = self._export_task_annotations(
                self.admin, task_id, query_params={"format": format_name},
                download_locally=False
            )
            download_url = response.json().get("result_url")
            assert download_url, "The result_url param was not found in the server response"

        processes_finished_correctly = False
        with ExitStack() as es:
            # Run both operations concurrently
            # Threads could be faster, but they can't be terminated
            download_process = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_download,
                        args=(download_checked_the_file, clear_removed_the_file),
                        kwargs=dict(task_id=task_id, export_path=export_path),
                    )
                )
            )
            clear_process = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_clear,
                        args=(download_checked_the_file, clear_removed_the_file),
                        kwargs=dict(file_path=export_path),
                    )
                )
            )

            download_process.start()

            wait_condition(download_checked_the_file)  # ensure the expected execution order
            clear_process.start()

            # A deadlock (interrupted by a timeout error) is the positive outcome in this test,
            # if the problem is fixed.
            # clear() must wait for the export cache lock release (acquired by download()).
            # It must be finished by a timeout, as download() holds it, waiting
            clear_process.join(timeout=10)

            # download() must wait for the clear() file existence check and fail because of timeout
            download_process.join(timeout=5)

            self.assertTrue(download_process.is_alive())
            self.assertFalse(clear_process.is_alive())

            download_process.terminate()
            download_process.join(timeout=5)

            # All the expected exceptions should be handled in the process callbacks.
            # This is to avoid passing the test with unexpected errors
            self.assertEqual(download_process.exitcode, -15)  # sigterm
            self.assertEqual(clear_process.exitcode, 0)

            processes_finished_correctly = True

        self.assertTrue(processes_finished_correctly)

        # terminate() may break the locks, don't try to acquire
        # https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Process.terminate
        self.assertTrue(download_checked_the_file.get())

        self.assertFalse(clear_removed_the_file.get())

    def test_export_can_create_file(self):
        format_name = "CVAT for images 1.1"
        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        with (
            patch("cvat.apps.dataset_manager.views.TTL_CONSTS", new={"task": timedelta(seconds=0)}),
        ):
            export_path = export(dst_format=format_name, task_id=task_id)

        self.assertTrue(osp.isfile(export_path))

    def test_export_cache_lock_can_raise_on_releasing_expired_lock(self):
        from pottery import ReleaseUnlockedLock

        with self.assertRaises(ReleaseUnlockedLock):
            lock_time = 2
            with get_export_cache_lock("test_export_path", ttl=lock_time, acquire_timeout=5):
                sleep(lock_time + 1)

    def test_export_can_request_retry_on_locking_failure(self):
        format_name = "CVAT for images 1.1"
        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        from cvat.apps.dataset_manager.util import LockNotAvailableError

        with (
            patch(
                "cvat.apps.dataset_manager.views.get_export_cache_lock",
                side_effect=LockNotAvailableError,
            ) as mock_get_export_cache_lock,
            patch("cvat.apps.dataset_manager.views.rq.get_current_job") as mock_rq_get_current_job,
            patch("cvat.apps.dataset_manager.views.django_rq.get_scheduler"),
            self.assertRaises(LockNotAvailableError),
        ):
            mock_rq_job = MagicMock(timeout=5)
            mock_rq_get_current_job.return_value = mock_rq_job

            export(dst_format=format_name, task_id=task_id)

        mock_get_export_cache_lock.assert_called()
        self.assertEqual(mock_rq_job.retries_left, 1)

    def test_export_can_reuse_older_file_if_still_relevant(self):
        format_name = "CVAT for images 1.1"
        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        first_export_path = export(dst_format=format_name, task_id=task_id)

        from os.path import exists as original_exists

        with (
            patch(
                "cvat.apps.dataset_manager.views.osp_exists", side_effect=original_exists
            ) as mock_osp_exists,
            patch("cvat.apps.dataset_manager.views.shutil.move") as mock_os_replace,
        ):
            second_export_path = export(dst_format=format_name, task_id=task_id)

        self.assertEqual(first_export_path, second_export_path)
        mock_osp_exists.assert_called_with(first_export_path)
        mock_os_replace.assert_not_called()

    def test_initiate_concurrent_export_by_different_users(self):
        side_effect = self.side_effect
        chain_side_effects = self.chain_side_effects
        process_closing = self.process_closing
        wait_condition = self.wait_condition
        set_condition = self.set_condition

        export_1_checked_file = self.SharedBool()
        export_1_made_export = self.SharedBool()
        export_1_replaced_file = self.SharedBool()

        export_2_checked_file = self.SharedBool()
        export_2_made_export = self.SharedBool()
        export_2_replaced_file = self.SharedBool()

        format_name = "CVAT for images 1.1"

        LOCK_TTL = 4
        LOCK_ACQUISITION_TIMEOUT = LOCK_TTL * 2

        def _export_1(
            *_,
            task_id: int,
            result_queue: multiprocessing.Queue,
        ):
            from os import replace as original_replace

            from cvat.apps.dataset_manager.task import export_task as original_export_task

            with (
                patch("cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_TTL", new=LOCK_TTL),
                patch(
                    "cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT",
                    new=LOCK_ACQUISITION_TIMEOUT,
                ),
                patch(
                    "cvat.apps.dataset_manager.views.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                patch("cvat.apps.dataset_manager.views.shutil.move") as mock_os_replace,
                patch("cvat.apps.dataset_manager.views.task.export_task") as mock_export_fn,
                patch("cvat.apps.dataset_manager.views.django_rq.get_scheduler"),
            ):
                mock_export_fn.side_effect = chain_side_effects(
                    side_effect(set_condition, export_1_checked_file),
                    original_export_task,
                    side_effect(wait_condition, export_2_checked_file),
                    side_effect(set_condition, export_1_made_export),
                )

                mock_os_replace.side_effect = chain_side_effects(
                    original_replace,
                    side_effect(set_condition, export_1_replaced_file),
                )
                result_file_path = export(dst_format=format_name, task_id=task_id)
                result_queue.put(result_file_path)

                mock_export_fn.assert_called_once()
                mock_os_replace.assert_called_once()

        def _export_2(
            *_,
            task_id: int,
            result_queue: multiprocessing.Queue,
        ):
            from os import replace as original_replace

            from cvat.apps.dataset_manager.task import export_task as original_export_task

            with (
                patch("cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_TTL", new=LOCK_TTL),
                patch(
                    "cvat.apps.dataset_manager.views.EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT",
                    new=LOCK_ACQUISITION_TIMEOUT,
                ),
                patch(
                    "cvat.apps.dataset_manager.views.get_export_cache_lock",
                    new=self.patched_get_export_cache_lock,
                ),
                patch("cvat.apps.dataset_manager.views.shutil.move") as mock_os_replace,
                patch("cvat.apps.dataset_manager.views.task.export_task") as mock_export_fn,
                patch("cvat.apps.dataset_manager.views.django_rq.get_scheduler"),
            ):
                mock_export_fn.side_effect = chain_side_effects(
                    side_effect(set_condition, export_2_checked_file),
                    original_export_task,
                    side_effect(wait_condition, export_1_replaced_file),
                    side_effect(set_condition, export_2_made_export),
                )

                mock_os_replace.side_effect = chain_side_effects(
                    original_replace,
                    side_effect(set_condition, export_2_replaced_file),
                )
                result_file_path = export(dst_format=format_name, task_id=task_id)
                result_queue.put(result_file_path)

                mock_export_fn.assert_called_once()
                mock_os_replace.assert_called_once()

        task = self._setup_task_with_annotations(format_name=format_name)

        with ExitStack() as es:
            result_queue = multiprocessing.Queue()
            number_of_processes = 2
            export_process_1 = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_export_1,
                        kwargs=dict(
                            task_id=task["id"],
                            result_queue=result_queue,
                        ),
                    )
                )
            )
            export_process_2 = es.enter_context(
                process_closing(
                    multiprocessing.Process(
                        target=_export_2,
                        kwargs=dict(
                            task_id=task["id"],
                            result_queue=result_queue,
                        ),
                    )
                )
            )

            export_process_1.start()
            wait_condition(export_1_checked_file)

            export_process_2.start()
            export_process_2.join(timeout=20)
            export_process_1.join(timeout=20)

            self.assertFalse(export_process_1.is_alive())
            self.assertFalse(export_process_2.is_alive())

            self.assertEqual(export_process_1.exitcode, 0)
            self.assertEqual(export_process_2.exitcode, 0)
            paths = {result_queue.get() for _ in range(number_of_processes)}
            result_queue.close()

            self.assertTrue(len(paths) == 1)
            self.assertNotEqual(paths, {None})
            self.assertTrue(osp.isfile(list(paths)[0]))

            for cond in (
                export_1_checked_file, export_1_made_export, export_1_replaced_file,
                export_2_checked_file, export_2_made_export, export_2_replaced_file
            ):
                self.assertTrue(cond.get())

    def test_cleanup_can_remove_file(self):
        format_name = "CVAT for images 1.1"
        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        export_path = export(dst_format=format_name, task_id=task_id)

        with (
            patch("cvat.apps.dataset_manager.views.TTL_CONSTS", new={"task": timedelta(seconds=0)}),
        ):
            export_path = export(dst_format=format_name, task_id=task_id)
            clear_export_cache(file_path=Path(export_path))

        self.assertFalse(osp.isfile(export_path))


    def test_cleanup_can_fail_if_no_file(self):
        from cvat.apps.dataset_manager.util import CacheFileOrDirPathParseError
        with self.assertRaises(CacheFileOrDirPathParseError):
            clear_export_cache(file_path=Path("non existent file path"))

    def test_cleanup_can_defer_removal_if_file_is_used_recently(self):
        from os import remove as original_remove
        format_name = "CVAT for images 1.1"
        task = self._setup_task_with_annotations(format_name=format_name)
        task_id = task["id"]

        export_path = export(dst_format=format_name, task_id=task_id)

        with (
            patch("cvat.apps.dataset_manager.views.TTL_CONSTS", new={"task": timedelta(hours=1)}),
            patch("cvat.apps.dataset_manager.cron.os.remove", side_effect=original_remove) as mock_os_remove,
        ):
            export_path = export(dst_format=format_name, task_id=task_id)
            clear_export_cache(file_path=Path(export_path))
            mock_os_remove.assert_not_called()

        self.assertTrue(osp.isfile(export_path))

    def test_cleanup_cron_job_can_delete_cached_files(self):
        from cvat.apps.dataset_manager.cron import cleanup_export_cache_directory

        def _get_project_task_job_ids():
            project = self._create_project(projects["main"])
            project_id = project["id"]

            images = self._generate_task_images(3)
            task = self._create_task(
                data=tasks["task in project #1"],
                image_data=images,
            )
            task_id = task["id"]
            job_id = self._get_jobs(task_id)[0]["id"]
            return project_id, task_id, job_id

        # remove chunks from the cache
        self._clear_temp_data()
        project_id, task_id, job_id = _get_project_task_job_ids()

        for resource, rid in zip(("project", "task", "job"), (project_id, task_id, job_id)):
            for save_images in (True, False):
                export_path = export(
                    dst_format="CVAT for images 1.1",
                    save_images=save_images,
                    **{resource + "_id": rid},
                )
                self.assertTrue(osp.isfile(export_path))
                self.assertTrue(resource in export_path)

                with (
                    patch(
                        "cvat.apps.dataset_manager.views.TTL_CONSTS",
                        new={resource: timedelta(seconds=0)},
                    ),
                    patch(
                        "cvat.apps.dataset_manager.cron.clear_export_cache",
                        side_effect=clear_export_cache,
                    ) as mock_clear_export_cache,
                ):
                    cleanup_export_cache_directory()
                    mock_clear_export_cache.assert_called_once()

                self.assertFalse(osp.exists(export_path))


@ensure_extractors_efficiency
@ensure_streaming_importers
class ProjectDumpUpload(_DbTestBase):
    def test_api_v2_export_import_dataset(self):
        test_name = self._testMethodName
        dump_formats = dm.views.get_export_formats()
        upload_formats = dm.views.get_import_formats()

        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False},
        }

        with TestDir() as test_dir:
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DIMENSION == dm.bindings.DimensionType.DIM_3D:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                if dump_format_name in [
                    'Cityscapes 1.0', 'LFW 1.0', 'Market-1501 1.0',
                    'MOT 1.1',
                ]:
                    # TO-DO: fix bug for this formats
                    continue
                project = copy.deepcopy(projects['main'])
                if dump_format_name in tasks:
                    project['labels'] = tasks[dump_format_name]['labels']
                project = self._create_project(project)
                tasks['task in project #1']['project_id'] = project['id']
                task = self._create_task(tasks['task in project #1'], self._generate_task_images(3))

                export_params = {
                    "format": dump_format_name,
                }

                if dump_format_name in DEFAULT_ATTRIBUTES_FORMATS + [
                    "Datumaro 1.0", "MOT 1.1",
                ]:
                    self._create_annotations(task, dump_format_name, "default")
                else:
                    self._create_annotations(task, dump_format_name, "random")

                for user, edata in list(expected.items()):
                    self._clear_temp_data() # clean up from previous tests and iterations

                    user_name = edata['name']
                    file_zip_name = osp.join(test_dir, f'{test_name}_{user_name}_{dump_format_name}.zip')
                    expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                    self._export_project_dataset(
                        user, project['id'], query_params=export_params,
                        file_path=file_zip_name, expected_4xx_status_code=expected_4xx_status_code
                    )
                    self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])

            for upload_format in upload_formats:
                if not upload_format.ENABLED or upload_format.DIMENSION == dm.bindings.DimensionType.DIM_3D:
                    continue
                upload_format_name = upload_format.DISPLAY_NAME
                if upload_format_name in [
                    'Cityscapes 1.0', 'LFW 1.0', 'Market-1501 1.0',
                    'MOT 1.1',
                ]:
                    # TO-DO: fix bug for this formats
                    continue
                for user, edata in list(expected.items()):
                    project = copy.deepcopy(projects['main'])
                    if upload_format_name in tasks:
                        project['labels'] = tasks[upload_format_name]['labels']
                    project = self._create_project(project)
                    file_zip_name = osp.join(test_dir, f"{test_name}_{edata['name']}_{upload_format_name}.zip")
                    url = self._generate_url_upload_project_dataset(project['id'], upload_format_name)

                    if osp.exists(file_zip_name):
                        with open(file_zip_name, 'rb') as binary_file:
                            response = self._post_request(
                                url,
                                user,
                                data={"dataset_file": binary_file},
                                format="multipart",
                            )
                            self.assertEqual(response.status_code, edata['accept code'])

    def test_api_v2_export_annotations(self):
        test_name = self._testMethodName
        dump_formats = dm.views.get_export_formats()

        expected = {
            self.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                         'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            self.user: {'name': 'user', 'code': status.HTTP_200_OK, 'create code': status.HTTP_201_CREATED,
                        'accept code': status.HTTP_202_ACCEPTED, 'file_exists': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'create code': status.HTTP_401_UNAUTHORIZED,
                   'accept code': status.HTTP_401_UNAUTHORIZED, 'file_exists': False},
        }

        with TestDir() as test_dir:
            for dump_format in dump_formats:
                if not dump_format.ENABLED or dump_format.DIMENSION == dm.bindings.DimensionType.DIM_3D:
                    continue
                dump_format_name = dump_format.DISPLAY_NAME
                with self.subTest(format=dump_format_name):
                    project = self._create_project(projects['main'])
                    pid = project['id']
                    images = self._generate_task_images(3)
                    tasks['task in project #1']['project_id'] = pid
                    self._create_task(tasks['task in project #1'], images)
                    images = self._generate_task_images(3, 3)
                    tasks['task in project #2']['project_id'] = pid
                    self._create_task(tasks['task in project #2'], images)

                    for user, edata in list(expected.items()):
                        self._clear_temp_data() # clean up from previous tests and iterations

                        user_name = edata['name']
                        file_zip_name = osp.join(test_dir, f'{test_name}_{user_name}_{dump_format_name}.zip')

                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        self._export_project_dataset(
                            user, project['id'], query_params={"format": dump_format_name},
                            file_path=file_zip_name, expected_4xx_status_code=expected_4xx_status_code
                        )
                        self.assertEqual(osp.exists(file_zip_name), edata['file_exists'])

    def test_api_v2_dump_upload_annotations_with_objects_type_is_track(self):
        test_name = self._testMethodName
        upload_format_name = dump_format_name = "COCO Keypoints 1.0"
        user = self.admin

        with TestDir() as test_dir:
            # Dump annotations with objects type is track
            # create task with annotations
            project_dict = copy.deepcopy(projects['main'])
            task_dict = copy.deepcopy(tasks[dump_format_name])
            project_dict["labels"] = task_dict["labels"]
            del task_dict["labels"]
            for label in project_dict["labels"]:
                label["attributes"] = [{
                    "name": "is_crowd",
                    "mutable": False,
                    "input_type": "checkbox",
                    "default_value": "false",
                    "values": ["false", "true"]
                }]
            project = self._create_project(project_dict)
            pid = project['id']
            video = self._generate_task_videos(1)
            task_dict['project_id'] = pid
            task = self._create_task(task_dict, video)
            task_id = task["id"]
            self._create_annotations(task, "skeleton track", "default")
            # dump annotations
            self._clear_rq_jobs()  # clean up from previous tests and iterations

            file_zip_name = osp.join(test_dir, f'{test_name}_{dump_format_name}.zip')
            self._export_project_dataset(
                user, project['id'], query_params={"format": dump_format_name}, file_path=file_zip_name
            )
            self.assertEqual(osp.exists(file_zip_name), True)

            data_from_task_before_upload = self._get_data_from_task(task_id, True)

            # Upload annotations with objects type is track
            project = self._create_project(project_dict)
            url = self._generate_url_upload_project_dataset(project["id"], upload_format_name)

            with open(file_zip_name, 'rb') as binary_file:
                response = self._post_request(
                    url,
                    user,
                    data={"dataset_file": binary_file},
                    format="multipart",
                )
                self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)

            # equals annotations
            new_task = self._get_tasks(project["id"])[0]
            data_from_task_after_upload = self._get_data_from_task(new_task["id"], True)
            compare_datasets(data_from_task_before_upload, data_from_task_after_upload)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\test_streaming_efficiency.py =====
import os
import tempfile
from typing import Callable, Optional
from unittest import TestCase, mock
from unittest.mock import MagicMock, Mock

import numpy as np
from datumaro import AnnotationType, Bbox, LabelCategories
from datumaro.components import media
from datumaro.components.dataset import StreamDataset
from datumaro.components.dataset_base import (
    CategoriesInfo,
    DatasetItem,
    StreamingDatasetBase,
    StreamingSubsetBase,
)
from django.contrib.auth.models import Group, User
from rest_framework import status
from rq.job import Job as RQJob

from cvat.apps.dataset_manager.bindings import (
    CommonData,
    CVATProjectDataExtractor,
    CvatTaskOrJobDataExtractor,
    JobData,
    ProjectData,
    TaskData,
    import_dm_annotations,
)
from cvat.apps.dataset_manager.formats import registry
from cvat.apps.dataset_manager.formats.registry import dm_env, importer
from cvat.apps.dataset_manager.project import import_dataset_as_project
from cvat.apps.dataset_manager.task import import_job_annotations, import_task_annotations
from cvat.apps.dataset_manager.util import TmpDirManager
from cvat.apps.engine.models import LabelType
from cvat.apps.engine.tests.utils import (
    ApiTestBase,
    ForceLogin,
    generate_image_file,
    get_paginated_collection,
)


class TestExtractors(TestCase):
    @staticmethod
    def _make_mock_frame(data_cls, **kwargs):
        mock = Mock(spec=data_cls.Frame)
        for key, value in kwargs.items():
            setattr(mock, key, value)
        mock.labeled_shapes = []
        mock.tags = []
        mock.height = 10
        mock.width = 10
        if data_cls is not ProjectData:
            mock._replace.return_value = mock
        return mock

    @staticmethod
    def _make_counting_data_extractor_cls(extractor_cls):
        class CountingDataExtractor(extractor_cls):
            def __init__(self, *args, **kwargs):
                self.item_anns_processed = 0
                super().__init__(*args, **kwargs)

            def _read_cvat_anno(self, *args, **kwargs):
                self.item_anns_processed += 1
                return super()._read_cvat_anno(*args, **kwargs)

        return CountingDataExtractor

    def _make_mock_instance_data(self, data_cls, item_ids):
        instance_data = MagicMock(spec_set=data_cls)
        instance_data.META_FIELD = "meta"
        instance_data.meta = dict(
            meta=dict(
                subset="foo",
                labels=[
                    ("label", dict(name="name", attributes=[], type=LabelType.ANY)),
                ],
            ),
        )
        subsets = sorted(set(subset for _, subset in item_ids))
        if data_cls is TaskData:
            assert len(subsets) == 1
            instance_data.db_instance = Mock(id=0)
        elif data_cls is JobData:
            assert len(subsets) == 1
            instance_data.db_instance.segment.task = Mock(id=0)
        elif data_cls is ProjectData:
            instance_data.tasks = [Mock(id=index) for index in range(len(subsets))]

        def mock_group_by_frame(*args, **kwargs):
            for index, (item_id, subset) in enumerate(item_ids):
                yield self._make_mock_frame(
                    data_cls=data_cls,
                    id=index,
                    task_id=subsets.index(subset),
                    name=f"{item_id}.png",
                    subset=subset,
                    frame=index,
                )

        instance_data.group_by_frame.side_effect = mock_group_by_frame

        if data_cls is not ProjectData:
            instance_data.__len__.return_value = len(item_ids)
            instance_data.frame_info = {
                index: dict(id=item_id, subset=subset)
                for index, (item_id, subset) in enumerate(item_ids)
            }
            instance_data.get_included_frames.return_value = set(instance_data.frame_info.keys())

        return instance_data

    @mock.patch("attr.evolve", lambda x, **kwargs: x)
    def test_can_stream_efficiently_on_export(self):
        for data_cls in [TaskData, JobData, ProjectData]:
            if data_cls is ProjectData:
                item_ids = {
                    ("image", "foo"),
                    ("another_image", "foo"),
                    ("third_image", "bar"),
                    ("fourth_image", "bar"),
                }
            else:
                item_ids = {("image", "foo"), ("another_image", "foo")}
            with self.subTest(data_cls=data_cls.__name__):
                instance_data = self._make_mock_instance_data(data_cls, item_ids)
                extractor_cls = (
                    CVATProjectDataExtractor
                    if data_cls is ProjectData
                    else CvatTaskOrJobDataExtractor
                )
                extractor_cls = self._make_counting_data_extractor_cls(extractor_cls)

                with extractor_cls(instance_data=instance_data) as extractor:
                    dataset = StreamDataset.from_extractors(extractor, env=dm_env)
                    # iterated over instance frame data on init
                    assert instance_data.group_by_frame.call_count == 1

                    # does not convert annotations to get various dataset properties
                    len(dataset)
                    list(dataset.subsets())
                    assert extractor.item_anns_processed == 0

                    # does not convert annotations to get various subset properties
                    for subset in dataset.subsets():
                        subset_dataset = dataset.get_subset(subset).as_dataset()
                        len(subset_dataset)
                        list(subset_dataset.subsets())
                    assert extractor.item_anns_processed == 0

                    # does not convert annotations to iterate items
                    list(dataset)
                    for subset in dataset.subsets():
                        subset_dataset = dataset.get_subset(subset).as_dataset()
                        list(subset_dataset)
                    assert extractor.item_anns_processed == 0

                    # initiates annotations only when they are accessed
                    list(item.annotations for item in dataset)
                    assert extractor.item_anns_processed == len(item_ids)

                    # does not keep annotations in memory
                    list(item.annotations for item in dataset)
                    assert extractor.item_anns_processed == len(item_ids) * 2

                    # did not iterate over instance frame data anymore
                    assert instance_data.group_by_frame.call_count == 1


class TestImporters(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        cls.create_db_users()

    @classmethod
    def create_db_users(cls):
        group, _ = Group.objects.get_or_create(name="adm")
        admin = User.objects.create_superuser(username="test", password="test", email="")
        admin.groups.add(group)
        cls.user = admin

    def setUp(self):
        super().setUp()

        project = self._create_project()
        self.project_id = project["id"]
        task = self._generate_task()
        self.task_id = task["id"]
        jobs = list(self._get_task_jobs(self.task_id))
        assert len(jobs) == 1
        self.job_id = jobs[0]["id"]

        self.dir_name = tempfile.TemporaryDirectory()
        self.extractor = self._make_extractor(self.dir_name.name)
        self._make_importer()

    def tearDown(self):
        del registry.IMPORT_FORMATS["dummy_format 1.0"]
        self.dir_name.cleanup()
        super().tearDown()

    def _make_extractor(self, dir_name):
        image = media.Image.from_numpy(data=np.ones((4, 2, 3)))
        subsets = ["train", "test", "foo"]

        dataset_items = {}
        for subset in subsets:
            dataset_items[subset] = []
            for index in [1, 2]:
                item_id = f"{subset}_{index}"
                image_path = os.path.join(dir_name, f"{item_id}.png")
                image.save(image_path)
                dataset_items[subset].append((item_id, image_path))

        class DummyStreamingExtractor(StreamingDatasetBase):
            def __init__(self):
                super().__init__(subsets=subsets)
                self.ann_init_count = 0

            def _gen_anns(self):
                assert self.ann_init_count < 6
                self.ann_init_count += 1
                return [Bbox(x=5, y=5, w=2, h=2, label=1)]

            def get_subset(self, name: str) -> StreamingSubsetBase:
                class _SubsetExtractor(StreamingSubsetBase):
                    def __iter__(_):
                        for item_id, image_path in dataset_items[name]:
                            yield DatasetItem(
                                id=item_id,
                                subset=name,
                                media=media.Image.from_file(image_path),
                                annotations=self._gen_anns,
                            )

                    def categories(_):
                        return self.categories()

                return _SubsetExtractor()

            def categories(self) -> CategoriesInfo:
                return {AnnotationType.label: LabelCategories.from_iterable(["a", "b", "c"])}

        return DummyStreamingExtractor()

    def _generate_task(self):
        images = {
            "client_files[0]": generate_image_file("train_1.jpg", size=(4, 2)),
            "client_files[1]": generate_image_file("train_2.jpg", size=(4, 2)),
            "image_quality": 75,
        }
        task = {
            "name": "my task #1",
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "a"},
                {"name": "b"},
                {"name": "c"},
            ],
        }
        return self._create_task(task, images)

    def _create_project(self):
        data = {
            "name": "my project #1",
            "labels": [
                {"name": "a"},
                {"name": "b"},
                {"name": "c"},
            ],
        }
        with ForceLogin(self.user, self.client):
            response = self.client.post("/api/projects", data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            project = response.data

        return project

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post("/api/tasks", data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid, data=image_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/%s" % tid)

            if 200 <= response.status_code < 400:
                labels_response = list(
                    get_paginated_collection(
                        lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                    )
                )
                response.data["labels"] = labels_response

            task = response.data

        return task

    def _get_task_jobs(self, tid):
        with ForceLogin(self.user, self.client):
            return get_paginated_collection(
                lambda page: self.client.get(
                    "/api/jobs?task_id=%s&page=%s" % (tid, page), format="json"
                )
            )

    def _make_importer(self):
        @importer(name="dummy_format", ext="ZIP", version="1.0")
        def _dummy_importer(
            src_file,
            temp_dir: str,
            instance_data: ProjectData | CommonData,
            *,
            load_data_callback: Optional[Callable] = None,
            import_kwargs: dict | None = None,
            **kwargs,
        ):
            extractor = self.extractor

            # import single subset to job and task
            if not isinstance(instance_data, ProjectData):
                extractor = extractor.get_subset("train")

            dataset = StreamDataset.from_extractors(extractor)
            dataset._source_path = temp_dir
            if load_data_callback is not None:
                load_data_callback(dataset, instance_data)
            import_dm_annotations(dataset, instance_data)

    def test_import_job_annotations_efficiency(self):
        with TmpDirManager.get_tmp_directory() as temp_dir:
            fake_file_name = os.path.join(temp_dir, "fake.zip")
            open(fake_file_name, "w").close()
            import_job_annotations(fake_file_name, self.job_id, "dummy_format 1.0", True)
            assert self.extractor.ann_init_count == 2

    def test_import_task_annotations_efficiency(self):
        with TmpDirManager.get_tmp_directory() as temp_dir:
            fake_file_name = os.path.join(temp_dir, "fake.zip")
            open(fake_file_name, "w").close()
            import_task_annotations(fake_file_name, self.task_id, "dummy_format 1.0", True)
            assert self.extractor.ann_init_count == 2

    @mock.patch("rq.get_current_job")
    def test_import_project_annotations_efficiency(self, mock_current_job):
        mock_current_job.return_value = Mock(spec=RQJob, meta=dict())
        mock_current_job.return_value.save_meta = lambda: None

        with TmpDirManager.get_tmp_directory() as temp_dir:
            fake_file_name = os.path.join(temp_dir, "fake.zip")
            open(fake_file_name, "w").close()
            import_dataset_as_project(fake_file_name, self.project_id, "dummy_format 1.0", True)
            assert self.extractor.ann_init_count == 6


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import tempfile
import unittest
from types import TracebackType
from typing import Optional
from unittest.mock import patch

from datumaro.components.dataset import StreamDataset
from datumaro.util.os_util import rmfile, rmtree

from cvat.apps.dataset_manager.bindings import CVATProjectDataExtractor, CvatTaskOrJobDataExtractor
from cvat.apps.dataset_manager.util import current_function_name


class FileRemover:
    def __init__(self, path: str, is_dir: bool = False):
        self.path = path
        self.is_dir = is_dir

    def __enter__(self) -> str:
        return self.path

    def __exit__(
        self,
        exc_type: Optional[type[BaseException]],
        exc_value: Optional[BaseException],
        traceback: Optional[TracebackType],
    ) -> None:
        if self.is_dir:
            try:
                rmtree(self.path)
            except unittest.SkipTest:
                # Suppress skip test errors from git.util.rmtree
                if not exc_type:
                    raise
        else:
            rmfile(self.path)


class TestDir(FileRemover):
    """
    Creates a temporary directory for a test. Uses the name of
    the test function to name the directory.

    Usage:

    .. code-block::

        with TestDir() as test_dir:
            ...
    """

    def __init__(self, path: Optional[str] = None, frame_id: int = 2):
        if not path:
            prefix = f"temp_{current_function_name(frame_id)}-"
        else:
            prefix = None
        self._prefix = prefix

        super().__init__(path, is_dir=True)

    def __enter__(self) -> str:
        """
        Creates a test directory.

        Returns: path to the directory
        """

        path = self.path

        if path is None:
            path = tempfile.mkdtemp(dir=os.getcwd(), prefix=self._prefix)
            self.path = path
        else:
            os.makedirs(path, exist_ok=False)

        return path


def ensure_streaming_importers(cls):
    original_import_from = StreamDataset.import_from

    def mock_import_from(*args, **kwargs):
        dataset = original_import_from(*args, **kwargs)
        extractor = dataset._data._source

        annotation_initiation_counter = 0

        class ExtractorWrapper:
            def __getattr__(self, attr):
                return getattr(extractor, attr)

            def __len__(self):
                return len(extractor)

            def __iter__(self):
                nonlocal annotation_initiation_counter
                for item in extractor:
                    yield item
                    if item.annotations_are_initialized:
                        annotation_initiation_counter += 1
                    # annotations should be initialized once per item and no more
                    assert annotation_initiation_counter <= len(extractor)

        dataset._data._source = ExtractorWrapper()

        return dataset

    return patch.object(StreamDataset, "import_from", mock_import_from)(cls)


def ensure_extractors_efficiency(cls):
    def make_mock_extractor(extractor_cls):
        assert hasattr(extractor_cls, "_read_cvat_anno")

        class MockExtractor(extractor_cls):
            def __init__(self, *args, **kwargs):
                self.ann_init_counter = 0
                super().__init__(*args, **kwargs)

            def _read_cvat_anno(self, *args, **kwargs):
                self.ann_init_counter += 1
                # annotations should be initialized once per item and no more
                assert self.ann_init_counter <= len(self)
                return super()._read_cvat_anno(*args, **kwargs)

        return MockExtractor

    cls = patch(
        "cvat.apps.dataset_manager.bindings.CvatTaskOrJobDataExtractor",
        make_mock_extractor(CvatTaskOrJobDataExtractor),
    )(cls)
    cls = patch(
        "cvat.apps.dataset_manager.bindings.CVATProjectDataExtractor",
        make_mock_extractor(CVATProjectDataExtractor),
    )(cls)
    return cls


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_manager\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\models.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0001_initial.py =====
# Generated by Django 2.1.3 on 2018-12-05 13:24

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ("engine", "0014_job_max_shape_id"),
    ]

    replaces = [("git", "0001_initial")]

    operations = [
        migrations.CreateModel(
            name="GitData",
            fields=[
                (
                    "task",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE,
                        primary_key=True,
                        serialize=False,
                        to="engine.Task",
                    ),
                ),
                ("url", models.URLField(max_length=2000)),
                ("path", models.CharField(max_length=256)),
                ("sync_date", models.DateTimeField(auto_now_add=True)),
                ("status", models.CharField(default="!sync", max_length=20)),
            ],
            options={
                "db_table": "git_gitdata",
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0002_auto_20190123_1305.py =====
# Generated by Django 2.1.3 on 2019-01-23 10:05

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("dataset_repo", "0001_initial"),
    ]

    replaces = [("git", "0002_auto_20190123_1305")]

    operations = [
        migrations.AlterField(
            model_name="gitdata",
            name="status",
            field=models.CharField(default="!sync", max_length=20),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0003_gitdata_lfs.py =====
# Generated by Django 2.1.3 on 2019-02-05 17:08

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("dataset_repo", "0002_auto_20190123_1305"),
    ]

    replaces = [("git", "0003_gitdata_lfs")]

    operations = [
        migrations.AddField(
            model_name="gitdata",
            name="lfs",
            field=models.BooleanField(default=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0004_rename.py =====
from django.db import migrations


def update_contenttypes_table(apps, schema_editor):
    content_type_model = apps.get_model("contenttypes", "ContentType")
    content_type_model.objects.filter(app_label="git").update(app_label="dataset_repo")


class Migration(migrations.Migration):

    dependencies = [
        ("dataset_repo", "0003_gitdata_lfs"),
    ]

    operations = [
        migrations.AlterModelTable("gitdata", "dataset_repo_gitdata"),
        migrations.RunPython(update_contenttypes_table),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0005_auto_20201019_1100.py =====
# Generated by Django 3.1.1 on 2020-10-19 11:00

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("dataset_repo", "0004_rename"),
    ]

    operations = [
        migrations.AlterModelTable(
            name="gitdata",
            table=None,
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0006_gitdata_format.py =====
# Generated by Django 3.1.13 on 2021-10-26 10:10

from django.db import migrations, models


def update_default_format_field(apps, schema_editor):
    GitData = apps.get_model("dataset_repo", "GitData")
    for git_data in GitData.objects.all():
        if not git_data.format:
            git_data.format = (
                "CVAT for images 1.1"
                if git_data.task.mode == "annotation"
                else "CVAT for video 1.1"
            )
            git_data.save()


class Migration(migrations.Migration):

    dependencies = [
        ("dataset_repo", "0005_auto_20201019_1100"),
    ]

    operations = [
        migrations.AddField(
            model_name="gitdata",
            name="format",
            field=models.CharField(blank=True, max_length=256),
        ),
        migrations.RunPython(update_default_format_field),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\0007_delete_gitdata.py =====
# Generated by Django 4.2.1 on 2023-09-25 17:05

from django.db import migrations


class Migration(migrations.Migration):
    dependencies = [
        ("dataset_repo", "0006_gitdata_format"),
    ]

    operations = [
        migrations.DeleteModel(
            name="GitData",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\dataset_repo\migrations\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\admin.py =====

# Copyright (C) 2018-2024 Intel Corporation
#
# SPDX-License-Identifier: MIT

from django.contrib import admin

from .models import (
    AnnotationGuide,
    Asset,
    AttributeSpec,
    CloudStorage,
    Data,
    Job,
    Label,
    Project,
    Segment,
    Storage,
    Task,
)


class JobInline(admin.TabularInline):
    model = Job
    can_delete = False

    autocomplete_fields = ('assignee', )
    readonly_fields = ('type', )

    def has_add_permission(self, request, obj):
        return False

class SegmentInline(admin.TabularInline):
    model = Segment
    show_change_link = True
    readonly_fields = ('start_frame', 'stop_frame')
    can_delete = False

    def has_add_permission(self, request, obj):
        return False

class AttributeSpecInline(admin.TabularInline):
    model = AttributeSpec
    extra = 0
    max_num = None

    readonly_fields = ('mutable', 'input_type', 'default_value', 'values')

    def has_add_permission(self, _request, obj):
        return False

class LabelInline(admin.TabularInline):
    model = Label
    show_change_link = True
    extra = 0
    max_num = None

    readonly_fields = ('task', 'project', 'parent', 'type')

class AssetInline(admin.TabularInline):
    model = Asset
    extra = 0
    max_num = None

    list_display = ('__str__', )
    readonly_fields = ('filename', 'created_date')
    autocomplete_fields = ('owner', )

    def has_add_permission(self, _request, obj):
        return False


class DataAdmin(admin.ModelAdmin):
    model = Data
    fields = ('chunk_size', 'size', 'image_quality', 'start_frame', 'stop_frame', 'frame_filter', 'compressed_chunk_type', 'original_chunk_type')
    readonly_fields = fields
    autocomplete_fields = ('cloud_storage', )

    def has_change_permission(self, request, obj=None):
        return False

    def has_delete_permission(self, request, obj=None):
        return False

    def has_module_permission(self, request):
        return False

    def has_add_permission(self, _request):
        return False

class StorageAdmin(admin.ModelAdmin):
    model = Storage
    list_display = ('__str__', 'location', 'cloud_storage')
    autocomplete_fields = ('cloud_storage', )

    def has_module_permission(self, request):
        return False

    def has_add_permission(self, _request):
        return False

class LabelAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'name', 'type', 'task', 'project')
    search_fields = ('name', 'task__name', 'project__name')
    readonly_fields = ('task', 'project', 'parent', 'type')

    def has_add_permission(self, _request):
        return False

    inlines = [
        AttributeSpecInline
    ]

class SegmentAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'task', 'start_frame', 'stop_frame', 'type')
    search_fields = ('task__name', )
    readonly_fields = ('task', 'start_frame', 'stop_frame', 'type', 'frames')

    def has_delete_permission(self, request, obj=None):
        return False

    def has_module_permission(self, request):
        return False

    inlines = [
        JobInline
    ]

class ProjectAdmin(admin.ModelAdmin):
    date_hierarchy = 'created_date'
    list_display = ('name', 'organization', 'owner', 'created_date', 'updated_date', 'status')
    search_fields = ('name', 'owner__username', 'owner__first_name',
        'owner__last_name', 'owner__email', 'assignee__username', 'assignee__first_name',
        'assignee__last_name', 'organization__slug')

    radio_fields = {
        'status': admin.VERTICAL,
    }

    readonly_fields = ('created_date', 'updated_date')
    autocomplete_fields = ('owner', 'assignee', 'organization')
    raw_id_fields = ('source_storage', 'target_storage')

    inlines = [
        LabelInline
    ]

    def has_add_permission(self, _request):
        return False

class TaskAdmin(admin.ModelAdmin):
    date_hierarchy = 'created_date'
    list_display = ('name', 'dimension', 'mode', 'organization', 'owner', 'assignee', 'created_date', 'updated_date')
    search_fields = ('name', 'mode', 'owner__username', 'owner__first_name',
        'owner__last_name', 'owner__email', 'assignee__username', 'assignee__first_name',
        'assignee__last_name', 'organization__slug')

    radio_fields = {
        'status': admin.VERTICAL,
    }

    readonly_fields = ('created_date', 'updated_date', 'overlap', 'segment_size', 'data', 'dimension')
    autocomplete_fields = ('project', 'owner', 'assignee', 'organization')
    raw_id_fields = ('source_storage', 'target_storage', 'data')

    inlines = [
        SegmentInline,
        LabelInline
    ]

    def has_add_permission(self, request):
        return False

class CloudStorageAdmin(admin.ModelAdmin):
    date_hierarchy = 'created_date'
    list_display = ('__str__', 'resource', 'owner', 'created_date', 'updated_date')
    search_fields = ('provider_type', 'display_name', 'resource', 'owner__username', 'owner__first_name',
        'owner__last_name', 'owner__email', 'organization__slug')

    radio_fields = {
        'credentials_type': admin.VERTICAL,
    }

    readonly_fields = ('created_date', 'updated_date', 'provider_type')
    autocomplete_fields = ('owner', 'organization')

    def has_add_permission(self, request):
        return False

class AnnotationGuideAdmin(admin.ModelAdmin):
    list_display = ('__str__', 'task', 'project', 'is_public')
    search_fields = ('task__name', 'project__name')

    autocomplete_fields = ('task', 'project')

    def has_add_permission(self, request):
        return False

    inlines = [
        AssetInline
    ]

admin.site.register(Task, TaskAdmin)
admin.site.register(Segment, SegmentAdmin)
admin.site.register(Label, LabelAdmin)
admin.site.register(Project, ProjectAdmin)
admin.site.register(Storage, StorageAdmin)
admin.site.register(CloudStorage, CloudStorageAdmin)
admin.site.register(Data, DataAdmin)
admin.site.register(AnnotationGuide, AnnotationGuideAdmin)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\apps.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class EngineConfig(AppConfig):
    name = "cvat.apps.engine"

    def ready(self):
        from django.conf import settings

        from . import default_settings

        for key in dir(default_settings):
            if key.isupper() and not hasattr(settings, key):
                setattr(settings, key, getattr(default_settings, key))

        # Required to define signals in application
        import cvat.apps.engine.signals

        # Required in order to silent "unused-import" in pyflake
        assert cvat.apps.engine.signals

        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\background.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp
from abc import ABC, abstractmethod
from dataclasses import dataclass
from datetime import datetime
from typing import Any, ClassVar, Optional, Union
from urllib.parse import quote

import django_rq
from django.conf import settings
from django.http.response import HttpResponseBadRequest
from django_rq.queues import DjangoRQ, DjangoScheduler
from rest_framework import serializers, status
from rest_framework.response import Response
from rest_framework.reverse import reverse
from rq.job import Job as RQJob
from rq.job import JobStatus as RQJobStatus

import cvat.apps.dataset_manager as dm
from cvat.apps.dataset_manager.formats.registry import EXPORT_FORMATS
from cvat.apps.dataset_manager.util import get_export_cache_lock
from cvat.apps.dataset_manager.views import get_export_cache_ttl, get_export_callback
from cvat.apps.engine import models
from cvat.apps.engine.backup import ProjectExporter, TaskExporter, create_backup
from cvat.apps.engine.cloud_provider import export_resource_to_cloud_storage
from cvat.apps.engine.location import StorageType, get_location_configuration
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import Location, RequestAction, RequestSubresource, RequestTarget, Task
from cvat.apps.engine.permissions import get_cloud_storage_for_import_or_export
from cvat.apps.engine.rq import ExportRQMeta, RQId, define_dependent_job
from cvat.apps.engine.serializers import RqIdSerializer
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import (
    build_annotations_file_name,
    build_backup_file_name,
    get_rq_lock_by_user,
    get_rq_lock_for_job,
    is_dataset_export,
    sendfile,
)
from cvat.apps.events.handlers import handle_dataset_export

slogger = ServerLogManager(__name__)

REQUEST_TIMEOUT = 60
# it's better to return LockNotAvailableError instead of response with 504 status
LOCK_TTL = REQUEST_TIMEOUT - 5
LOCK_ACQUIRE_TIMEOUT = LOCK_TTL - 5


class ResourceExportManager(ABC):
    QUEUE_NAME = settings.CVAT_QUEUES.EXPORT_DATA.value
    SUPPORTED_RESOURCES: ClassVar[set[RequestSubresource]]
    SUPPORTED_SUBRESOURCES: ClassVar[set[RequestSubresource]]

    def __init__(
        self,
        db_instance: Union[models.Project, models.Task, models.Job],
        request: ExtendedRequest,
    ) -> None:
        """
        Args:
            db_instance (Union[models.Project, models.Task, models.Job]): Model instance
            request (ExtendedRequest): Incoming HTTP request
        """
        self.db_instance = db_instance
        self.request = request
        self.resource = db_instance.__class__.__name__.lower()
        if self.resource not in self.SUPPORTED_RESOURCES:
            raise ValueError("Unexpected type of db_instance: {}".format(type(db_instance)))

    ### Initialization logic ###

    @abstractmethod
    def initialize_export_args(self) -> None: ...

    @abstractmethod
    def validate_export_args(self) -> Response | None: ...

    @abstractmethod
    def build_rq_id(self) -> str: ...

    def handle_existing_rq_job(
        self, rq_job: Optional[RQJob], queue: DjangoRQ
    ) -> Optional[Response]:
        if not rq_job:
            return None

        rq_job_status = rq_job.get_status(refresh=False)

        if rq_job_status in {RQJobStatus.STARTED, RQJobStatus.QUEUED}:
            return Response(
                data="Export request is being processed",
                status=status.HTTP_409_CONFLICT,
            )

        if rq_job_status == RQJobStatus.DEFERRED:
            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)

        if rq_job_status == RQJobStatus.SCHEDULED:
            scheduler: DjangoScheduler = django_rq.get_scheduler(queue.name, queue=queue)
            # remove the job id from the set with scheduled keys
            scheduler.cancel(rq_job)
            rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)

        rq_job.delete()
        return None

    @abstractmethod
    def get_download_api_endpoint_view_name(self) -> str: ...

    def make_result_url(self, *, rq_id: str) -> str:
        view_name = self.get_download_api_endpoint_view_name()
        result_url = reverse(view_name, args=[self.db_instance.pk], request=self.request)

        return result_url + f"?rq_id={quote(rq_id)}"

    def get_updated_date_timestamp(self) -> str:
        # use only updated_date for the related resource, don't check children objects
        # because every child update should touch the updated_date of the parent resource
        return datetime.strftime(self.db_instance.updated_date, "%Y_%m_%d_%H_%M_%S")

    @abstractmethod
    def get_result_filename(self) -> str: ...

    @abstractmethod
    def send_events(self) -> None: ...

    @abstractmethod
    def setup_background_job(self, queue: DjangoRQ, rq_id: str) -> None: ...

    def export(self) -> Response:
        self.initialize_export_args()

        if invalid_response := self.validate_export_args():
            return invalid_response

        queue: DjangoRQ = django_rq.get_queue(self.QUEUE_NAME)
        rq_id = self.build_rq_id()

        # ensure that there is no race condition when processing parallel requests
        with get_rq_lock_for_job(queue, rq_id):
            rq_job = queue.fetch_job(rq_id)
            if response := self.handle_existing_rq_job(rq_job, queue):
                return response
            self.setup_background_job(queue, rq_id)

        self.send_events()

        serializer = RqIdSerializer({"rq_id": rq_id})
        return Response(serializer.data, status=status.HTTP_202_ACCEPTED)

    ### Logic related to prepared file downloading ###

    def validate_rq_id(self, rq_id: str) -> None:
        parsed_rq_id = RQId.parse(rq_id)

        if (
            parsed_rq_id.action != RequestAction.EXPORT
            or parsed_rq_id.target != RequestTarget(self.resource)
            or parsed_rq_id.identifier != self.db_instance.pk
            or parsed_rq_id.subresource not in self.SUPPORTED_SUBRESOURCES
        ):
            raise ValueError("The provided request id does not match exported target or resource")

    def download_file(self) -> Response:
        queue: DjangoRQ = django_rq.get_queue(self.QUEUE_NAME)
        rq_id = self.request.query_params.get("rq_id")

        if not rq_id:
            return HttpResponseBadRequest("Missing request id in the query parameters")

        try:
            self.validate_rq_id(rq_id)
        except ValueError:
            return HttpResponseBadRequest("Invalid export request id")

        # ensure that there is no race condition when processing parallel requests
        with get_rq_lock_for_job(queue, rq_id):
            rq_job = queue.fetch_job(rq_id)

            if not rq_job:
                return HttpResponseBadRequest("Unknown export request id")

            # define status once to avoid refreshing it on each check
            # FUTURE-TODO: get_status will raise InvalidJobOperation exception instead of returning None in one of the next releases
            rq_job_status = rq_job.get_status(refresh=False)

            if rq_job_status != RQJobStatus.FINISHED:
                return HttpResponseBadRequest("The export process is not finished")

            rq_job_meta = ExportRQMeta.for_job(rq_job)
            file_path = rq_job.return_value()

            if not file_path:
                return (
                    Response(
                        "A result for exporting job was not found for finished RQ job",
                        status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    )
                    if rq_job_meta.result_url  # user tries to download a final file locally while the export is made to cloud storage
                    else HttpResponseBadRequest(
                        "The export process has no result file to be downloaded locally"
                    )
                )

            with get_export_cache_lock(
                file_path, ttl=LOCK_TTL, acquire_timeout=LOCK_ACQUIRE_TIMEOUT
            ):
                if not osp.exists(file_path):
                    return Response(
                        "The exported file has expired, please retry exporting",
                        status=status.HTTP_404_NOT_FOUND,
                    )

                return sendfile(
                    self.request,
                    file_path,
                    attachment=True,
                    attachment_filename=rq_job_meta.result_filename,
                )


def cancel_and_delete(rq_job: RQJob) -> None:
    # In the case the server is configured with ONE_RUNNING_JOB_IN_QUEUE_PER_USER
    # we have to enqueue dependent jobs after canceling one.
    rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)
    rq_job.delete()


class DatasetExportManager(ResourceExportManager):
    SUPPORTED_RESOURCES = {RequestTarget.PROJECT, RequestTarget.TASK, RequestTarget.JOB}
    SUPPORTED_SUBRESOURCES = {RequestSubresource.DATASET, RequestSubresource.ANNOTATIONS}

    @dataclass
    class ExportArgs:
        format: str
        filename: str
        save_images: bool
        location_config: dict[str, Any]

        @property
        def location(self) -> Location:
            return self.location_config["location"]

    def initialize_export_args(self) -> None:
        save_images = is_dataset_export(self.request)
        self.export_callback = get_export_callback(self.db_instance, save_images=save_images)

        format_name = self.request.query_params.get("format", "")
        filename = self.request.query_params.get("filename", "")

        try:
            location_config = get_location_configuration(
                db_instance=self.db_instance,
                query_params=self.request.query_params,
                field_name=StorageType.TARGET,
            )
        except ValueError as ex:
            raise serializers.ValidationError(str(ex)) from ex

        location = location_config["location"]

        if location not in Location.list():
            raise serializers.ValidationError(
                f"Unexpected location {location} specified for the request"
            )

        self.export_args = self.ExportArgs(
            format=format_name,
            filename=filename,
            save_images=save_images,
            location_config=location_config,
        )

    def validate_export_args(self):
        format_desc = {f.DISPLAY_NAME: f for f in dm.views.get_export_formats()}.get(
            self.export_args.format
        )
        if format_desc is None:
            raise serializers.ValidationError("Unknown format specified for the request")
        elif not format_desc.ENABLED:
            return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)

    def build_rq_id(self):
        return RQId(
            RequestAction.EXPORT,
            RequestTarget(self.resource),
            self.db_instance.pk,
            subresource=(
                RequestSubresource.DATASET
                if self.export_args.save_images
                else RequestSubresource.ANNOTATIONS
            ),
            format=self.export_args.format,
            user_id=self.request.user.id,
        ).render()

    def send_events(self):
        handle_dataset_export(
            self.db_instance,
            format_name=self.export_args.format,
            cloud_storage_id=self.export_args.location_config.get("storage_id"),
            save_images=self.export_args.save_images,
        )

    def setup_background_job(
        self,
        queue: DjangoRQ,
        rq_id: str,
    ) -> None:
        try:
            if self.request.scheme:
                server_address = self.request.scheme + "://"
            server_address += self.request.get_host()
        except Exception:
            server_address = None

        cache_ttl = get_export_cache_ttl(self.db_instance)

        user_id = self.request.user.id

        func = self.export_callback
        func_args = (self.db_instance.id, self.export_args.format)
        result_url = None

        if self.export_args.location == Location.CLOUD_STORAGE:
            try:
                storage_id = self.export_args.location_config["storage_id"]
            except KeyError:
                raise serializers.ValidationError(
                    "Cloud storage location was selected as the destination,"
                    " but cloud storage id was not specified"
                )

            db_storage = get_cloud_storage_for_import_or_export(
                storage_id=storage_id,
                request=self.request,
                is_default=self.export_args.location_config["is_default"],
            )

            func = export_resource_to_cloud_storage
            func_args = (
                db_storage,
                self.export_callback,
            ) + func_args
        else:
            db_storage = None
            result_url = self.make_result_url(rq_id=rq_id)

        with get_rq_lock_by_user(queue, user_id):
            result_filename = self.get_result_filename()
            meta = ExportRQMeta.build_for(
                request=self.request,
                db_obj=self.db_instance,
                result_url=result_url,
                result_filename=result_filename,
            )
            queue.enqueue_call(
                func=func,
                args=func_args,
                kwargs={
                    "server_url": server_address,
                },
                job_id=rq_id,
                meta=meta,
                depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),
                result_ttl=cache_ttl.total_seconds(),
                failure_ttl=cache_ttl.total_seconds(),
            )

    def get_result_filename(self) -> str:
        filename = self.export_args.filename

        if not filename:
            instance_timestamp = self.get_updated_date_timestamp()
            filename = build_annotations_file_name(
                class_name=self.resource,
                identifier=self.db_instance.id,
                timestamp=instance_timestamp,
                format_name=self.export_args.format,
                is_annotation_file=not self.export_args.save_images,
                extension=(EXPORT_FORMATS[self.export_args.format].EXT).lower(),
            )

        return filename

    def get_download_api_endpoint_view_name(self) -> str:
        return f"{self.resource}-download-dataset"


class BackupExportManager(ResourceExportManager):
    SUPPORTED_RESOURCES = {RequestTarget.PROJECT, RequestTarget.TASK}
    SUPPORTED_SUBRESOURCES = {RequestSubresource.BACKUP}

    @dataclass
    class ExportArgs:
        filename: str
        location_config: dict[str, Any]

        @property
        def location(self) -> Location:
            return self.location_config["location"]

    def initialize_export_args(self) -> None:
        self.export_callback = create_backup
        filename = self.request.query_params.get("filename", "")

        location_config = get_location_configuration(
            db_instance=self.db_instance,
            query_params=self.request.query_params,
            field_name=StorageType.TARGET,
        )
        self.export_args = self.ExportArgs(filename, location_config)

    def validate_export_args(self):
        return

    def get_result_filename(self) -> str:
        filename = self.export_args.filename

        if not filename:
            instance_timestamp = self.get_updated_date_timestamp()

            filename = build_backup_file_name(
                class_name=self.resource,
                identifier=self.db_instance.name,
                timestamp=instance_timestamp,
            )

        return filename

    def build_rq_id(self):
        return RQId(
            RequestAction.EXPORT,
            RequestTarget(self.resource),
            self.db_instance.pk,
            subresource=RequestSubresource.BACKUP,
            user_id=self.request.user.id,
        ).render()

    # FUTURE-TODO: move into ResourceExportManager
    def setup_background_job(
        self,
        queue: DjangoRQ,
        rq_id: str,
    ) -> None:
        cache_ttl = get_export_cache_ttl(self.db_instance)
        user_id = self.request.user.id

        if isinstance(self.db_instance, Task):
            logger = slogger.task[self.db_instance.pk]
            Exporter = TaskExporter
        else:
            logger = slogger.project[self.db_instance.pk]
            Exporter = ProjectExporter

        func = self.export_callback
        func_args = (
            self.db_instance.id,
            Exporter,
            logger,
            cache_ttl,
        )
        result_url = None

        if self.export_args.location == Location.CLOUD_STORAGE:
            try:
                storage_id = self.export_args.location_config["storage_id"]
            except KeyError:
                raise serializers.ValidationError(
                    "Cloud storage location was selected as the destination,"
                    " but cloud storage id was not specified"
                )

            db_storage = get_cloud_storage_for_import_or_export(
                storage_id=storage_id,
                request=self.request,
                is_default=self.export_args.location_config["is_default"],
            )

            func = export_resource_to_cloud_storage
            func_args = (
                db_storage,
                self.export_callback,
            ) + func_args
        else:
            result_url = self.make_result_url(rq_id=rq_id)

        with get_rq_lock_by_user(queue, user_id):
            result_filename = self.get_result_filename()
            meta = ExportRQMeta.build_for(
                request=self.request,
                db_obj=self.db_instance,
                result_url=result_url,
                result_filename=result_filename,
            )

            queue.enqueue_call(
                func=func,
                args=func_args,
                job_id=rq_id,
                meta=meta,
                depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),
                result_ttl=cache_ttl.total_seconds(),
                failure_ttl=cache_ttl.total_seconds(),
            )

    def get_download_api_endpoint_view_name(self) -> str:
        return f"{self.resource}-download-backup"

    def send_events(self):
        # FUTURE-TODO: send events to event store
        pass


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\backup.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import mimetypes
import os
import re
import shutil
import tempfile
import uuid
from abc import ABCMeta, abstractmethod
from collections.abc import Collection, Iterable
from copy import deepcopy
from datetime import timedelta
from enum import Enum
from logging import Logger
from tempfile import NamedTemporaryFile
from typing import Any, ClassVar, Optional, Type, Union
from zipfile import ZipFile

import django_rq
import rapidjson
from django.conf import settings
from django.core.exceptions import ObjectDoesNotExist
from django.db import transaction
from django.utils import timezone
from rest_framework import serializers, status
from rest_framework.exceptions import ValidationError
from rest_framework.parsers import JSONParser
from rest_framework.renderers import JSONRenderer
from rest_framework.response import Response

import cvat.apps.dataset_manager as dm
from cvat.apps.dataset_manager.bindings import CvatImportError
from cvat.apps.dataset_manager.util import (
    ExportCacheManager,
    TmpDirManager,
    extend_export_file_lifetime,
    get_export_cache_lock,
)
from cvat.apps.dataset_manager.views import (
    EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
    EXPORT_CACHE_LOCK_TTL,
    EXPORT_LOCKED_RETRY_INTERVAL,
    LockNotAvailableError,
    log_exception,
    retry_current_rq_job,
)
from cvat.apps.engine import models
from cvat.apps.engine.cloud_provider import (
    db_storage_to_storage_instance,
    import_resource_from_cloud_storage,
)
from cvat.apps.engine.location import StorageType, get_location_configuration
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import (
    DataChoice,
    Location,
    RequestAction,
    RequestSubresource,
    RequestTarget,
    StorageChoice,
    StorageMethodChoice,
)
from cvat.apps.engine.permissions import get_cloud_storage_for_import_or_export
from cvat.apps.engine.rq import ImportRQMeta, RQId, define_dependent_job
from cvat.apps.engine.serializers import (
    AnnotationGuideWriteSerializer,
    AssetWriteSerializer,
    AttributeSerializer,
    DataSerializer,
    JobWriteSerializer,
    LabeledDataSerializer,
    LabelSerializer,
    ProjectFileSerializer,
    ProjectReadSerializer,
    RqIdSerializer,
    SegmentSerializer,
    SimpleJobSerializer,
    TaskFileSerializer,
    TaskReadSerializer,
    ValidationParamsSerializer,
)
from cvat.apps.engine.task import JobFileMapping, _create_thread
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import (
    av_scan_paths,
    get_rq_lock_by_user,
    import_resource_with_clean_up_after,
    process_failed_job,
)

slogger = ServerLogManager(__name__)

class Version(Enum):
    V1 = '1.0'

def _get_label_mapping(db_labels):
    label_mapping = {db_label.id: db_label.name for db_label in db_labels}
    for db_label in db_labels:
        label_mapping[db_label.id] = {
            'value': db_label.name,
            'attributes': {},
        }
        for db_attribute in db_label.attributespec_set.all():
            label_mapping[db_label.id]['attributes'][db_attribute.id] = db_attribute.name

    return label_mapping

def _write_annotation_guide(zip_object, annotation_guide, guide_filename, assets_dirname, target_dir=None):
    if annotation_guide is not None:
        md = annotation_guide.markdown
        assets = annotation_guide.assets.all()
        assets_dirname = os.path.join(target_dir or '', assets_dirname)
        guide_filename = os.path.join(target_dir or '', guide_filename)

        for db_asset in assets:
            md = md.replace(f'/api/assets/{str(db_asset.pk)}', os.path.join(assets_dirname, db_asset.filename))
            file = os.path.join(settings.ASSETS_ROOT, str(db_asset.pk), db_asset.filename)
            with open(file, 'rb') as asset_file:
                zip_object.writestr(os.path.join(assets_dirname, db_asset.filename), asset_file.read())
        zip_object.writestr(guide_filename, data=md)

def _read_annotation_guide(zip_object, guide_filename, assets_dirname):
    files = zip_object.namelist()
    if guide_filename in files:
        annotation_guide = io.BytesIO(zip_object.read(guide_filename))
        assets = filter(lambda x: x.startswith(f'{assets_dirname}/'), files)
        assets = list(map(lambda x: (x, zip_object.read(x)), assets))

        if len(assets) > settings.ASSET_MAX_COUNT_PER_GUIDE:
            raise ValidationError(f'Maximum number of assets per guide reached')
        for asset in assets:
            if len(asset[1]) / (1024 * 1024) > settings.ASSET_MAX_SIZE_MB:
                raise ValidationError(f'Maximum size of asset is {settings.ASSET_MAX_SIZE_MB} MB')
            if mimetypes.guess_type(asset[0])[0] not in settings.ASSET_SUPPORTED_TYPES:
                raise ValidationError(f'File is not supported as an asset. Supported are {settings.ASSET_SUPPORTED_TYPES}')

        return annotation_guide.getvalue(), assets

    return None, []

def _import_annotation_guide(guide_data, assets):
    guide_serializer = AnnotationGuideWriteSerializer(data=guide_data)
    markdown = guide_data['markdown']
    if guide_serializer.is_valid(raise_exception=True):
        guide_serializer.save()

    for asset in assets:
        name, data = asset
        basename = os.path.basename(name)
        asset_serializer = AssetWriteSerializer(data={
            'filename': basename,
            'guide_id': guide_serializer.instance.id,
        })
        if asset_serializer.is_valid(raise_exception=True):
            asset_serializer.save()
            markdown = markdown.replace(f'{name}', f'/api/assets/{asset_serializer.instance.pk}')
            path = os.path.join(settings.ASSETS_ROOT, str(asset_serializer.instance.uuid))
            os.makedirs(path)
            with open(os.path.join(path, basename), 'wb') as destination:
                destination.write(data)

    guide_serializer.instance.markdown = markdown
    guide_serializer.instance.save()

class _BackupBase():
    ANNOTATION_GUIDE_FILENAME = 'annotation_guide.md'
    ASSETS_DIRNAME = 'assets'

    def __init__(self, *args, logger=None, **kwargs):
        super().__init__(*args, **kwargs)
        self._logger = logger

    def _prepare_meta(self, allowed_keys, meta):
        keys_to_drop = set(meta.keys()) - allowed_keys
        if keys_to_drop:
            if self._logger:
                self._logger.warning('the following keys are dropped {}'.format(keys_to_drop))
            for key in keys_to_drop:
                del meta[key]

        return meta

    def _prepare_label_meta(self, label):
        allowed_fields = {
            'name',
            'color',
            'attributes',
            'type',
            'svg',
            'sublabels',
        }
        self._prepare_meta(allowed_fields, label)
        for sublabel in label['sublabels']:
            sublabel_id = sublabel['id']
            sublabel_name = sublabel['name']
            label['svg'] = label['svg'].replace(f'data-label-id="{sublabel_id}"', f'data-label-name="{sublabel_name}"')

            self._prepare_meta(allowed_fields, sublabel)
            sublabel['attributes'] = [self._prepare_attribute_meta(a) for a in sublabel['attributes']]
        return label

    def _prepare_attribute_meta(self, attribute):
        allowed_fields = {
            'name',
            'mutable',
            'input_type',
            'default_value',
            'values',
        }
        return self._prepare_meta(allowed_fields, attribute)

class _TaskBackupBase(_BackupBase):
    MANIFEST_FILENAME = 'task.json'
    MEDIA_MANIFEST_FILENAME = 'manifest.jsonl'
    MEDIA_MANIFEST_INDEX_FILENAME = 'index.json'
    ANNOTATIONS_FILENAME = 'annotations.json'
    DATA_DIRNAME = 'data'
    TASK_DIRNAME = 'task'

    def _prepare_task_meta(self, task):
        allowed_fields = {
            'name',
            'bug_tracker',
            'status',
            'subset',
            'labels',
            'consensus_replicas',
        }

        return self._prepare_meta(allowed_fields, task)

    def _prepare_data_meta(self, data):
        allowed_fields = {
            'chunk_size',
            'image_quality',
            'start_frame',
            'stop_frame',
            'frame_filter',
            'chunk_type',
            'storage_method',
            'storage',
            'sorting_method',
            'deleted_frames',
            'custom_segments',
            'job_file_mapping',
            'validation_layout'
        }

        self._prepare_meta(allowed_fields, data)

        if 'validation_layout' in data:
            self._prepare_meta(
                allowed_keys={'mode', 'frames', 'frames_per_job_count'},
                meta=data['validation_layout']
            )

        if 'frame_filter' in data and not data['frame_filter']:
            data.pop('frame_filter')

        return data

    def _prepare_job_meta(self, job):
        allowed_fields = {
            'status',
            'type',
        }
        return self._prepare_meta(allowed_fields, job)

    def _prepare_annotations(self, annotations, label_mapping):
        allowed_fields = {
            'label',
            'label_id',
            'type',
            'occluded',
            'outside',
            'z_order',
            'points',
            'rotation',
            'frame',
            'group',
            'source',
            'attributes',
            'shapes',
            'elements',
        }

        def _update_attribute(attribute, label):
            if 'name' in attribute:
                source, dest = attribute.pop('name'), 'spec_id'
            else:
                source, dest = attribute.pop('spec_id'), 'name'
            attribute[dest] = label_mapping[label]['attributes'][source]

        def _update_label(shape, parent_label=''):
            if 'label_id' in shape:
                source = shape.pop('label_id')
                shape['label'] = label_mapping[source]['value']
            elif 'label' in shape:
                source = parent_label + shape.pop('label')
                shape['label_id'] = label_mapping[source]['value']

            return source

        def _prepare_shapes(shapes, parent_label=''):
            for shape in shapes:
                label = _update_label(shape, parent_label)
                for attr in shape['attributes']:
                    _update_attribute(attr, label)

                _prepare_shapes(shape.get('elements', []), label)

                self._prepare_meta(allowed_fields, shape)

        def _prepare_tracks(tracks, parent_label=''):
            for track in tracks:
                label = _update_label(track, parent_label)
                for shape in track['shapes']:
                    for attr in shape['attributes']:
                        _update_attribute(attr, label)
                    self._prepare_meta(allowed_fields, shape)

                _prepare_tracks(track.get('elements', []), label)

                for attr in track['attributes']:
                    _update_attribute(attr, label)
                self._prepare_meta(allowed_fields, track)

        for tag in annotations['tags']:
            label = _update_label(tag)
            for attr in tag['attributes']:
                _update_attribute(attr, label)
            self._prepare_meta(allowed_fields, tag)

        _prepare_shapes(annotations['shapes'])
        _prepare_tracks(annotations['tracks'])

        return annotations

    def _get_db_jobs(self):
        if not self._db_task:
            return

        db_segments = list(self._db_task.segment_set.all().prefetch_related('job_set'))
        db_segments.sort(key=lambda i: i.job_set.first().id)

        for db_segment in db_segments:
            yield from sorted(db_segment.job_set.all(), key=lambda db_job: db_job.id)

class _ExporterBase(metaclass=ABCMeta):
    ModelClass: ClassVar[models.Project | models.Task]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def _write_files(source_dir, zip_object, files, target_dir):
        for filename in files:
            arcname = os.path.normpath(
                os.path.join(
                    target_dir,
                    os.path.relpath(filename, source_dir),
                )
            )
            zip_object.write(filename=filename, arcname=arcname)

    def _write_directory(self, source_dir, zip_object, target_dir, recursive=True, exclude_files=None):
        for root, dirs, files in os.walk(source_dir, topdown=True):
            if not recursive:
                dirs.clear()

            if files:
                self._write_files(
                    source_dir=source_dir,
                    zip_object=zip_object,
                    files=(os.path.join(root, f) for f in files if not exclude_files or f not in exclude_files),
                    target_dir=target_dir,
                )

    @abstractmethod
    def export_to(self, file: str | ZipFile, target_dir: str | None = None):
        ...

    @classmethod
    def get_object(cls, pk: int) -> models.Project | models.Task:
        # FUTURE-FIXME: need to check permissions one more time when background task is called
        try:
            return cls.ModelClass.objects.get(pk=pk)
        except ObjectDoesNotExist:
            raise ValidationError(f'Such a {cls.ModelClass.__name__.lower()} does not exist')


class TaskExporter(_ExporterBase, _TaskBackupBase):
    ModelClass: ClassVar[models.Task] = models.Task

    def __init__(self, pk, version=Version.V1):
        super().__init__(logger=slogger.task[pk])

        self._db_task: models.Task = (
            models.Task.objects
            .prefetch_related('data__images', 'annotation_guide__assets')
            .select_related('data__video', 'data__validation_layout', 'annotation_guide')
            .get(pk=pk)
        )

        self._db_data: models.Data = self._db_task.data
        self._version = version

        db_labels = (self._db_task.project if self._db_task.project_id else self._db_task).label_set.all().prefetch_related(
            'attributespec_set')
        self._label_mapping = _get_label_mapping(db_labels)

    def _write_annotation_guide(self, zip_object, target_dir=None):
        annotation_guide = self._db_task.annotation_guide if hasattr(self._db_task, 'annotation_guide') else None
        _write_annotation_guide(zip_object, annotation_guide, self.ANNOTATION_GUIDE_FILENAME, self.ASSETS_DIRNAME, target_dir = target_dir)

    def _write_data(self, zip_object, target_dir=None):
        target_data_dir = os.path.join(target_dir, self.DATA_DIRNAME) if target_dir else self.DATA_DIRNAME
        if self._db_data.storage == StorageChoice.LOCAL:
            data_dir = self._db_data.get_upload_dirname()
            self._write_directory(
                source_dir=data_dir,
                zip_object=zip_object,
                target_dir=target_data_dir,
                exclude_files=[self.MEDIA_MANIFEST_INDEX_FILENAME]
            )
        elif self._db_data.storage == StorageChoice.SHARE:
            data_dir = settings.SHARE_ROOT
            if hasattr(self._db_data, 'video'):
                media_files = (os.path.join(data_dir, self._db_data.video.path), )
            else:
                media_files = (os.path.join(data_dir, im.path) for im in self._db_data.images.all())

            self._write_files(
                source_dir=data_dir,
                zip_object=zip_object,
                files=media_files,
                target_dir=target_data_dir,
            )

            self._write_files(
                source_dir=self._db_data.get_upload_dirname(),
                zip_object=zip_object,
                files=[self._db_data.get_manifest_path()],
                target_dir=target_data_dir,
            )
        elif self._db_data.storage == StorageChoice.CLOUD_STORAGE:
            assert self._db_task.dimension != models.DimensionType.DIM_3D, "Cloud storage cannot contain 3d images"
            assert not hasattr(self._db_data, 'video'), "Only images can be stored in cloud storage"
            assert self._db_data.related_files.count() == 0, "No related images can be stored in cloud storage"
            media_files = [im.path for im in self._db_data.images.all()]
            cloud_storage_instance = db_storage_to_storage_instance(self._db_data.cloud_storage)
            with tempfile.TemporaryDirectory() as tmp_dir:
                cloud_storage_instance.bulk_download_to_dir(files=media_files, upload_dir=tmp_dir)
                self._write_files(
                    source_dir=tmp_dir,
                    zip_object=zip_object,
                    files=[
                        os.path.join(tmp_dir, file)
                        for file in media_files
                    ],
                    target_dir=target_data_dir,
                )
            self._write_files(
                source_dir=self._db_data.get_upload_dirname(),
                zip_object=zip_object,
                files=[self._db_data.get_manifest_path()],
                target_dir=target_data_dir,
            )
        else:
            raise NotImplementedError

    def _write_task(self, zip_object, target_dir=None):
        task_dir = self._db_task.get_dirname()
        target_task_dir = os.path.join(target_dir, self.TASK_DIRNAME) if target_dir else self.TASK_DIRNAME
        self._write_directory(
            source_dir=task_dir,
            zip_object=zip_object,
            target_dir=target_task_dir,
            recursive=False,
        )

    def _write_manifest(self, zip_object, target_dir=None):
        def serialize_task():
            task_serializer = TaskReadSerializer(self._db_task)
            for field in ('url', 'owner', 'assignee'):
                task_serializer.fields.pop(field)

            task_labels = LabelSerializer(self._db_task.get_labels(prefetch=True), many=True)

            serialized_task = task_serializer.data
            if serialized_task.pop('consensus_enabled', False):
                serialized_task['consensus_replicas'] = self._db_task.consensus_replicas

            task = self._prepare_task_meta(serialized_task)
            task['labels'] = [self._prepare_label_meta(l) for l in task_labels.data if not l['has_parent']]
            for label in task['labels']:
                label['attributes'] = [self._prepare_attribute_meta(a) for a in label['attributes']]

            return task

        def serialize_segment(db_segment):
            segment_serializer = SegmentSerializer(db_segment)
            segment_serializer.fields.pop('jobs')
            serialized_segment = segment_serializer.data

            segment_type = serialized_segment.pop("type")
            if (
                self._db_task.segment_size == 0 and segment_type == models.SegmentType.RANGE
                or self._db_data.validation_mode == models.ValidationMode.GT_POOL
            ):
                serialized_segment.update(serialize_segment_file_names(db_segment))

            return serialized_segment

        def serialize_jobs():
            db_segments = list(self._db_task.segment_set.all())
            db_segments.sort(key=lambda i: i.job_set.first().id)

            serialized_jobs = []
            for db_segment in db_segments:
                serialized_segment = serialize_segment(db_segment)

                db_jobs = list(db_segment.job_set.all())
                db_jobs.sort(key=lambda v: v.id)
                for db_job in db_jobs:
                    job_serializer = SimpleJobSerializer(db_job)
                    for field in ('url', 'assignee'):
                        job_serializer.fields.pop(field)

                    serialized_job = self._prepare_job_meta(job_serializer.data)
                    serialized_job.update(deepcopy(serialized_segment))
                    serialized_jobs.append(serialized_job)

            return serialized_jobs

        def serialize_segment_file_names(db_segment: models.Segment):
            if self._db_task.mode == 'annotation':
                files: Iterable[models.Image] = self._db_data.images.order_by('frame').all()
                return {'files': [files[f].path for f in sorted(db_segment.frame_set)]}
            else:
                assert False, (
                    "Backups with custom file mapping are not supported"
                    " in the 'interpolation' task mode"
                )

        def serialize_data():
            data_serializer = DataSerializer(self._db_data)
            data = data_serializer.data
            data['chunk_type'] = self._db_data.compressed_chunk_type

            # There are no deleted frames in DataSerializer so we need to pick it
            data['deleted_frames'] = self._db_data.deleted_frames

            if self._db_task.segment_size == 0:
                data['custom_segments'] = True

            if (
                (validation_layout := getattr(self._db_data, 'validation_layout', None)) and
                validation_layout.mode == models.ValidationMode.GT_POOL
            ):
                validation_params_serializer = ValidationParamsSerializer({
                    "mode": validation_layout.mode,
                    "frame_selection_method": models.JobFrameSelectionMethod.MANUAL,
                    "frames_per_job_count": validation_layout.frames_per_job_count,
                })
                validation_params = validation_params_serializer.data
                media_filenames = dict(
                    self._db_data.images
                    .order_by('frame')
                    .filter(
                        frame__gte=min(validation_layout.frames),
                        frame__lte=max(validation_layout.frames),
                    )
                    .values_list('frame', 'path')
                    .all()
                )
                validation_params['frames'] = [
                    media_filenames[frame] for frame in validation_layout.frames
                ]
                data['validation_layout'] = validation_params

            if self._db_data.storage == StorageChoice.CLOUD_STORAGE:
                data["storage"] = StorageChoice.LOCAL

            return self._prepare_data_meta(data)

        task = serialize_task()
        task['version'] = self._version.value
        task['data'] = serialize_data()
        task['jobs'] = serialize_jobs()

        target_manifest_file = os.path.join(target_dir, self.MANIFEST_FILENAME) if target_dir else self.MANIFEST_FILENAME
        zip_object.writestr(target_manifest_file, data=JSONRenderer().render(task))

    def _write_annotations(self, zip_object: ZipFile, target_dir: Optional[str] = None) -> None:
        def serialize_annotations():
            db_jobs = self._get_db_jobs()
            db_job_ids = (j.id for j in db_jobs)
            for db_job_id in db_job_ids:
                annotations = dm.task.get_job_data(db_job_id)
                annotations_serializer = LabeledDataSerializer(data=annotations)
                annotations_serializer.is_valid(raise_exception=True)
                yield self._prepare_annotations(annotations_serializer.data, self._label_mapping)

        annotations = serialize_annotations()
        target_annotations_file = os.path.join(target_dir, self.ANNOTATIONS_FILENAME) if target_dir else self.ANNOTATIONS_FILENAME
        with zip_object.open(target_annotations_file, 'w') as f:
            rapidjson.dump(annotations, f)

    def _export_task(self, zip_obj, target_dir=None):
        self._write_data(zip_obj, target_dir)
        self._write_task(zip_obj, target_dir)
        self._write_manifest(zip_obj, target_dir)
        self._write_annotations(zip_obj, target_dir)
        self._write_annotation_guide(zip_obj, target_dir)

    def export_to(self, file: str | ZipFile, target_dir: str | None = None):
        if self._db_task.data.storage_method == StorageMethodChoice.FILE_SYSTEM and \
                self._db_task.data.storage == StorageChoice.SHARE:
            raise Exception('The task cannot be exported because it does not contain any raw data')

        if isinstance(file, str):
            with ZipFile(file, 'w') as zf:
                self._export_task(zip_obj=zf, target_dir=target_dir)
        elif isinstance(file, ZipFile):
            self._export_task(zip_obj=file, target_dir=target_dir)
        else:
            raise ValueError('Unsupported type of file argument')

class _ImporterBase():
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @staticmethod
    def _read_version(manifest):
        version = manifest.pop('version')
        try:
            return Version(version)
        except ValueError:
            raise ValueError('{} version is not supported'.format(version))

    @staticmethod
    def _prepare_dirs(filepath):
        target_dir = os.path.dirname(filepath)
        if not os.path.exists(target_dir):
            os.makedirs(target_dir)

    def _create_labels(self, labels, db_task=None, db_project=None, parent_label=None):
        label_mapping = {}
        if db_task:
            label_relation = {
                'task': db_task
            }
        else:
            label_relation = {
                'project': db_project
            }


        for label in labels:
            label_name = label['name']
            attributes = label.pop('attributes', [])
            svg = label.pop('svg', '')
            sublabels  = label.pop('sublabels', [])

            db_label = models.Label.objects.create(**label_relation, parent=parent_label, **label)
            label_mapping[(parent_label.name if parent_label else '') + label_name] = {
                'value': db_label.id,
                'attributes': {},
            }

            label_mapping.update(self._create_labels(sublabels, db_task, db_project, db_label))

            if db_label.type == str(models.LabelType.SKELETON):
                for db_sublabel in list(db_label.sublabels.all()):
                    svg = svg.replace(f'data-label-name="{db_sublabel.name}"', f'data-label-id="{db_sublabel.id}"')
                models.Skeleton.objects.create(root=db_label, svg=svg)

            for attribute in attributes:
                attribute_name = attribute['name']
                attribute_serializer = AttributeSerializer(data=attribute)
                attribute_serializer.is_valid(raise_exception=True)
                db_attribute = attribute_serializer.save(label=db_label)
                label_mapping[(parent_label.name if parent_label else '') + label_name]['attributes'][attribute_name] = db_attribute.id

        return label_mapping

class TaskImporter(_ImporterBase, _TaskBackupBase):
    def __init__(self, file, user_id, org_id=None, project_id=None, subdir=None, label_mapping=None):
        super().__init__(logger=slogger.glob)
        self._file = file
        self._subdir = subdir
        "Task subdirectory with the separator included, e.g. task_0/"

        self._user_id = user_id
        self._org_id = org_id
        self._manifest, self._annotations, self._annotation_guide, self._assets = self._read_meta()
        self._version = self._read_version(self._manifest)
        self._labels_mapping = label_mapping
        self._db_task = None
        self._project_id=project_id

    def _read_annotation_guide(self, zip_object):
        annotation_guide_filename = os.path.join(self._subdir or '', self.ANNOTATION_GUIDE_FILENAME)
        assets_dirname = os.path.join(self._subdir or '', self.ASSETS_DIRNAME)
        return _read_annotation_guide(zip_object, annotation_guide_filename, assets_dirname)

    def _read_meta(self):
        def read(zip_object):
            manifest_filename = os.path.join(self._subdir or '', self.MANIFEST_FILENAME)
            annotations_filename = os.path.join(self._subdir or '', self.ANNOTATIONS_FILENAME)
            manifest = JSONParser().parse(io.BytesIO(zip_object.read(manifest_filename)))
            annotations = JSONParser().parse(io.BytesIO(zip_object.read(annotations_filename)))
            annotation_guide, assets = self._read_annotation_guide(zip_object)
            return manifest, annotations, annotation_guide, assets

        if isinstance(self._file, str):
            with ZipFile(self._file, 'r') as input_file:
                return read(input_file)
        elif isinstance(self._file, ZipFile):
            return read(self._file)

        raise ValueError('Unsupported type of file argument')

    def _create_annotations(self, db_job, annotations):
        self._prepare_annotations(annotations, self._labels_mapping)

        serializer = LabeledDataSerializer(data=annotations)
        serializer.is_valid(raise_exception=True)
        dm.task.put_job_data(db_job.id, serializer.data)

    @staticmethod
    def _calculate_segment_size(jobs):
        # The type field will be missing in backups create before the GT jobs were introduced
        jobs = [
            j for j in jobs
            if j.get("type", models.JobType.ANNOTATION) == models.JobType.ANNOTATION
        ]

        segment_size = jobs[0]['stop_frame'] - jobs[0]['start_frame'] + 1
        overlap = 0 if len(jobs) == 1 else jobs[0]['stop_frame'] - jobs[1]['start_frame'] + 1

        return segment_size, overlap

    @staticmethod
    def _parse_segment_frames(*, jobs: dict[str, Any]) -> JobFileMapping:
        segments = []

        for i, segment in enumerate(jobs):
            segment_size = segment['stop_frame'] - segment['start_frame'] + 1
            if segment_frames := segment.get('frames'):
                segment_frames = set(segment_frames)
                segment_range = range(segment['start_frame'], segment['stop_frame'] + 1)
                if not segment_frames.issubset(segment_range):
                    raise ValidationError(
                        "Segment frames must be inside the range [start_frame; stop_frame]"
                    )

                segment_size = len(segment_frames)

            segment_files = segment['files']
            if len(segment_files) != segment_size:
                raise ValidationError(f"segment {i}: segment files do not match segment size")

            segments.append(segment_files)

        return segments

    def _copy_input_files(
        self,
        input_archive: Union[ZipFile, str],
        output_task_path: str,
        *,
        excluded_filenames: Optional[Collection[str]] = None,
    ) -> list[str]:
        if isinstance(input_archive, str):
            with ZipFile(input_archive, 'r') as zf:
                return self._copy_input_files(
                    input_archive=zf,
                    output_task_path=output_task_path,
                    excluded_filenames=excluded_filenames,
                )

        input_task_dirname = self.TASK_DIRNAME
        input_data_dirname = self.DATA_DIRNAME
        output_data_path = self._db_task.data.get_upload_dirname()
        uploaded_files = []
        for file_path in input_archive.namelist():
            if file_path.endswith('/') or self._subdir and not file_path.startswith(self._subdir):
                continue

            file_name = os.path.relpath(file_path, self._subdir)
            if excluded_filenames and file_name in excluded_filenames:
                continue

            if file_name.startswith(input_data_dirname + '/'):
                target_file = os.path.join(
                    output_data_path, os.path.relpath(file_name, input_data_dirname)
                )

                self._prepare_dirs(target_file)
                with open(target_file, "wb") as out, input_archive.open(file_path) as source:
                    shutil.copyfileobj(source, out)

                uploaded_files.append(os.path.relpath(file_name, input_data_dirname))
            elif file_name.startswith(input_task_dirname + '/'):
                target_file = os.path.join(
                    output_task_path, os.path.relpath(file_name, input_task_dirname)
                )

                self._prepare_dirs(target_file)
                with open(target_file, "wb") as out, input_archive.open(file_path) as source:
                    shutil.copyfileobj(source, out)

        return uploaded_files

    def _import_task(self):
        data = self._manifest.pop('data')
        labels = self._manifest.pop('labels')
        jobs = self._manifest.pop('jobs')

        self._prepare_task_meta(self._manifest)
        self._manifest['owner_id'] = self._user_id
        self._manifest['project_id'] = self._project_id

        self._prepare_data_meta(data)

        excluded_input_files = [os.path.join(self.DATA_DIRNAME, self.MEDIA_MANIFEST_INDEX_FILENAME)]

        job_file_mapping = None
        if data.pop('custom_segments', False):
            job_file_mapping = self._parse_segment_frames(jobs=[
                v for v in jobs if v.get('type') == models.JobType.ANNOTATION
            ])

            for d in [self._manifest, data]:
                for k in [
                    'segment_size', 'overlap', 'start_frame', 'stop_frame',
                    'sorting_method', 'frame_filter', 'filename_pattern'
                ]:
                    d.pop(k, None)
        else:
            self._manifest['segment_size'], self._manifest['overlap'] = \
                self._calculate_segment_size(jobs)

        validation_params = data.pop('validation_layout', None)
        if validation_params:
            validation_params['frame_selection_method'] = models.JobFrameSelectionMethod.MANUAL
            validation_params_serializer = ValidationParamsSerializer(data=validation_params)
            validation_params_serializer.is_valid(raise_exception=True)
            validation_params = validation_params_serializer.data

            gt_jobs = [v for v in jobs if v.get('type') == models.JobType.GROUND_TRUTH]
            if not gt_jobs:
                raise ValidationError("Can't find any GT jobs info in the backup files")
            elif len(gt_jobs) != 1:
                raise ValidationError("A task can have only one GT job info in the backup files")

            validation_params['frames'] = validation_params_serializer.initial_data['frames']

            if validation_params['mode'] == models.ValidationMode.GT_POOL:
                gt_job_frames = self._parse_segment_frames(jobs=gt_jobs)[0]
                if set(gt_job_frames) != set(validation_params_serializer.initial_data['frames']):
                    raise ValidationError("GT job frames do not match validation frames")

                # Validation frames can have a different order, we must use the GT job order
                if not job_file_mapping:
                    raise ValidationError("Expected segment info in the backup files")

                job_file_mapping.append(gt_job_frames)

            data['validation_params'] = validation_params

        if job_file_mapping and (
            not validation_params or validation_params['mode'] != models.ValidationMode.GT_POOL
        ):
            # It's currently prohibited to have repeated file names in jobs.
            # DataSerializer checks for this, but we don't need it for tasks with a GT pool
            data['job_file_mapping'] = job_file_mapping

        self._db_task = models.Task.objects.create(**self._manifest, organization_id=self._org_id)

        task_data_path = self._db_task.get_dirname()
        if os.path.isdir(task_data_path):
            shutil.rmtree(task_data_path)
        os.makedirs(task_data_path)

        if not self._labels_mapping:
            self._labels_mapping = self._create_labels(db_task=self._db_task, labels=labels)

        data_serializer = DataSerializer(data=data)
        data_serializer.is_valid(raise_exception=True)
        db_data = data_serializer.save()
        self._db_task.data = db_data
        self._db_task.save()

        uploaded_files = self._copy_input_files(
            self._file, task_data_path, excluded_filenames=excluded_input_files
        )

        data['use_zip_chunks'] = data.pop('chunk_type') == DataChoice.IMAGESET
        data = data_serializer.data
        data['client_files'] = uploaded_files

        if job_file_mapping or (
            validation_params and validation_params['mode'] == models.ValidationMode.GT_POOL
        ):
            data['job_file_mapping'] = job_file_mapping

        if validation_params:
            data['validation_params'] = validation_params

        _create_thread(self._db_task.pk, data.copy(), is_backup_restore=True)
        self._db_task.refresh_from_db()
        db_data.refresh_from_db()

        db_data.deleted_frames = data_serializer.initial_data.get('deleted_frames', [])
        db_data.storage = StorageChoice.LOCAL
        db_data.save(update_fields=['storage', 'deleted_frames'])

        if not validation_params:
            # In backups created before addition of GT pools there was no validation_layout field
            # Recreate Ground Truth jobs
            self._import_gt_jobs(jobs)

        for db_job, job in zip(self._get_db_jobs(), jobs):
            db_job.status = job['status']
            db_job.save()

    def _import_gt_jobs(self, jobs):
        for job in jobs:
            # The type field will be missing in backups created before the GT jobs were introduced
            try:
                raw_job_type = job.get("type", models.JobType.ANNOTATION.value)
                job_type = models.JobType(raw_job_type)
            except ValueError:
                raise ValidationError(f"Unexpected job type {raw_job_type}")

            if job_type == models.JobType.GROUND_TRUTH:
                job_serializer = JobWriteSerializer(data={
                    'task_id': self._db_task.id,
                    'type': job_type.value,
                    'frame_selection_method': models.JobFrameSelectionMethod.MANUAL.value,
                    'frames': job['frames']
                })
                job_serializer.is_valid(raise_exception=True)
                job_serializer.save()
            elif job_type in [models.JobType.ANNOTATION, models.JobType.CONSENSUS_REPLICA]:
                continue
            else:
                assert False

    def _import_annotations(self):
        db_jobs = self._get_db_jobs()
        for db_job, annotations in zip(db_jobs, self._annotations):
            self._create_annotations(db_job, annotations)

    def _import_annotation_guide(self):
        if self._annotation_guide:
            markdown = self._annotation_guide.decode()
            _import_annotation_guide({ 'markdown': markdown, 'task_id': self._db_task.id }, self._assets)

    def import_task(self):
        self._import_task()
        self._import_annotations()
        self._import_annotation_guide()
        return self._db_task

@transaction.atomic
def _import_task(filename, user, org_id):
    av_scan_paths(filename)
    task_importer = TaskImporter(filename, user, org_id)
    db_task = task_importer.import_task()
    return db_task.id

class _ProjectBackupBase(_BackupBase):
    MANIFEST_FILENAME = 'project.json'
    TASKNAME_TEMPLATE = 'task_{}'

    def _prepare_project_meta(self, project):
        allowed_fields = {
            'bug_tracker',
            'labels',
            'name',
            'status',
        }

        return self._prepare_meta(allowed_fields, project)

class ProjectExporter(_ExporterBase, _ProjectBackupBase):
    ModelClass: ClassVar[models.Project] = models.Project

    def __init__(self, pk, version=Version.V1):
        super().__init__(logger=slogger.project[pk])
        self._db_project = self.ModelClass.objects.prefetch_related('tasks', 'annotation_guide__assets').select_related('annotation_guide').get(pk=pk)
        self._version = version

        db_labels = self._db_project.label_set.all().prefetch_related('attributespec_set')
        self._label_mapping = _get_label_mapping(db_labels)

    def _write_annotation_guide(self, zip_object, target_dir=None):
        annotation_guide = self._db_project.annotation_guide if hasattr(self._db_project, 'annotation_guide') else None
        _write_annotation_guide(zip_object, annotation_guide, self.ANNOTATION_GUIDE_FILENAME, self.ASSETS_DIRNAME, target_dir = target_dir)

    def _write_tasks(self, zip_object):
        for idx, db_task in enumerate(self._db_project.tasks.all().order_by('id')):
            if db_task.data is not None:
                TaskExporter(db_task.id, self._version).export_to(zip_object, self.TASKNAME_TEMPLATE.format(idx))

    def _write_manifest(self, zip_object):
        def serialize_project():
            project_serializer = ProjectReadSerializer(self._db_project)
            for field in ('assignee', 'owner', 'url'):
                project_serializer.fields.pop(field)

            project_labels = LabelSerializer(self._db_project.get_labels(prefetch=True), many=True).data

            project = self._prepare_project_meta(project_serializer.data)
            project['labels'] = [self._prepare_label_meta(l) for l in project_labels if not l['has_parent']]
            for label in project['labels']:
                label['attributes'] = [self._prepare_attribute_meta(a) for a in label['attributes']]

            return project

        project = serialize_project()
        project['version'] = self._version.value

        zip_object.writestr(self.MANIFEST_FILENAME, data=JSONRenderer().render(project))

    def export_to(self, file: str, target_dir: str | None = None):
        with ZipFile(file, 'w') as output_file:
            self._write_annotation_guide(output_file)
            self._write_manifest(output_file)
            self._write_tasks(output_file)

class ProjectImporter(_ImporterBase, _ProjectBackupBase):
    TASKNAME_RE = r'task_(\d+)/'

    def __init__(self, filename, user_id, org_id=None):
        super().__init__(logger=slogger.glob)
        self._filename = filename
        self._user_id = user_id
        self._org_id = org_id
        self._manifest, self._annotation_guide, self._assets = self._read_meta()
        self._version = self._read_version(self._manifest)
        self._db_project = None
        self._labels_mapping = {}

    def _read_annotation_guide(self, zip_object):
        return _read_annotation_guide(zip_object, self.ANNOTATION_GUIDE_FILENAME, self.ASSETS_DIRNAME)

    def _read_meta(self):
        with ZipFile(self._filename, 'r') as input_file:
            manifest = JSONParser().parse(io.BytesIO(input_file.read(self.MANIFEST_FILENAME)))
            annotation_guide, assets = self._read_annotation_guide(input_file)

        return manifest, annotation_guide, assets

    def _import_project(self):
        labels = self._manifest.pop('labels')

        self._prepare_project_meta(self._manifest)
        self._manifest["owner_id"] = self._user_id

        self._db_project = models.Project.objects.create(**self._manifest, organization_id=self._org_id)
        project_path = self._db_project.get_dirname()
        if os.path.isdir(project_path):
            shutil.rmtree(project_path)
        os.makedirs(project_path)

        self._labels_mapping = self._create_labels(db_project=self._db_project, labels=labels)

    def _import_tasks(self):
        def get_tasks(zip_object):
            tasks = {}
            for fname in zip_object.namelist():
                m = re.match(self.TASKNAME_RE, fname)
                if m:
                    tasks[int(m.group(1))] = m.group(0)
            return [v for _, v in sorted(tasks.items())]

        with ZipFile(self._filename, 'r') as zf:
            task_dirs = get_tasks(zf)
            for task_dir in task_dirs:
                TaskImporter(
                    file=zf,
                    user_id=self._user_id,
                    org_id=self._org_id,
                    project_id=self._db_project.id,
                    subdir=task_dir,
                    label_mapping=self._labels_mapping).import_task()

    def _import_annotation_guide(self):
        if self._annotation_guide:
            markdown = self._annotation_guide.decode()
            _import_annotation_guide({ 'markdown': markdown, 'project_id': self._db_project.id }, self._assets)

    def import_project(self):
        self._import_project()
        self._import_annotation_guide()
        self._import_tasks()

        return self._db_project

@transaction.atomic
def _import_project(filename, user, org_id):
    av_scan_paths(filename)
    project_importer = ProjectImporter(filename, user, org_id)
    db_project = project_importer.import_project()
    return db_project.id


def create_backup(
    instance_id: int,
    Exporter: Type[ProjectExporter | TaskExporter],
    logger: Logger,
    cache_ttl: timedelta,
):
    db_instance = Exporter.get_object(instance_id)
    instance_type = db_instance.__class__.__name__
    instance_timestamp = timezone.localtime(db_instance.updated_date).timestamp()

    output_path = ExportCacheManager.make_backup_file_path(
        instance_id=db_instance.id,
        instance_type=instance_type,
        instance_timestamp=instance_timestamp
    )

    try:
        with get_export_cache_lock(
            output_path,
            block=True,
            acquire_timeout=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
            ttl=EXPORT_CACHE_LOCK_TTL,
        ):
            # output_path includes timestamp of the last update
            if os.path.exists(output_path):
                extend_export_file_lifetime(output_path)
                return output_path

        with TmpDirManager.get_tmp_directory_for_export(instance_type=instance_type) as tmp_dir:
            temp_file = os.path.join(tmp_dir, 'dump')
            exporter = Exporter(db_instance.id)
            exporter.export_to(temp_file)

            with get_export_cache_lock(
                output_path,
                block=True,
                acquire_timeout=EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT,
                ttl=EXPORT_CACHE_LOCK_TTL,
            ):
                shutil.move(temp_file, output_path)

            logger.info(
                f"The {db_instance.__class__.__name__.lower()} '{db_instance.id}' is backed up at {output_path!r} "
                f"and available for downloading for the next {cache_ttl}."
            )

        return output_path
    except LockNotAvailableError:
        # Need to retry later if the lock was not available
        retry_current_rq_job(EXPORT_LOCKED_RETRY_INTERVAL)
        logger.info(
            "Failed to acquire export cache lock. Retrying in {}".format(
                EXPORT_LOCKED_RETRY_INTERVAL
            )
        )
        raise

    except Exception:
        log_exception(logger)
        raise


def _import(
    importer: TaskImporter | ProjectImporter,
    request: ExtendedRequest,
    queue: django_rq.queues.DjangoRQ,
    rq_id: str,
    Serializer: type[TaskFileSerializer] | type[ProjectFileSerializer],
    file_field_name: str,
    location_conf: dict,
    filename: str | None = None,
):
    rq_job = queue.fetch_job(rq_id)

    if not rq_job:
        org_id = getattr(request.iam_context['organization'], 'id', None)
        location = location_conf.get('location')

        if location == Location.LOCAL:
            if not filename:
                serializer = Serializer(data=request.data)
                serializer.is_valid(raise_exception=True)
                payload_file = serializer.validated_data[file_field_name]
                with NamedTemporaryFile(
                    prefix='cvat_',
                    dir=settings.TMP_FILES_ROOT,
                    delete=False) as tf:
                    filename = tf.name
                    for chunk in payload_file.chunks():
                        tf.write(chunk)
        else:
            file_name = request.query_params.get('filename')
            assert file_name, "The filename wasn't specified"
            try:
                storage_id = location_conf['storage_id']
            except KeyError:
                raise serializers.ValidationError(
                    'Cloud storage location was selected as the source,'
                    ' but cloud storage id was not specified')

            db_storage = get_cloud_storage_for_import_or_export(
                storage_id=storage_id, request=request,
                is_default=location_conf['is_default'])

            key = filename
            with NamedTemporaryFile(prefix='cvat_', dir=settings.TMP_FILES_ROOT, delete=False) as tf:
                filename = tf.name

        func = import_resource_with_clean_up_after
        func_args = (importer, filename, request.user.id, org_id)

        if location == Location.CLOUD_STORAGE:
            func_args = (db_storage, key, func) + func_args
            func = import_resource_from_cloud_storage

        user_id = request.user.id

        with get_rq_lock_by_user(queue, user_id):
            meta = ImportRQMeta.build_for(
                request=request,
                db_obj=None,
                tmp_file=filename,
            )
            rq_job = queue.enqueue_call(
                func=func,
                args=func_args,
                job_id=rq_id,
                meta=meta,
                depends_on=define_dependent_job(queue, user_id),
                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),
                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()
            )
    else:
        rq_job_meta = ImportRQMeta.for_job(rq_job)
        if rq_job_meta.user.id != request.user.id:
            return Response(status=status.HTTP_403_FORBIDDEN)

        if rq_job.is_finished:
            project_id = rq_job.return_value()
            rq_job.delete()
            return Response({'id': project_id}, status=status.HTTP_201_CREATED)
        elif rq_job.is_failed:
            exc_info = process_failed_job(rq_job)
            # RQ adds a prefix with exception class name
            import_error_prefix = '{}.{}'.format(
                CvatImportError.__module__, CvatImportError.__name__)
            if exc_info.startswith(import_error_prefix):
                exc_info = exc_info.replace(import_error_prefix + ': ', '')
                return Response(data=exc_info,
                    status=status.HTTP_400_BAD_REQUEST)
            else:
                return Response(data=exc_info,
                    status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    serializer = RqIdSerializer(data={'rq_id': rq_id})
    serializer.is_valid(raise_exception=True)

    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)

def get_backup_dirname():
    return TmpDirManager.TMP_ROOT

def import_project(request: ExtendedRequest, queue_name: str, filename: str | None = None):
    if 'rq_id' in request.data:
        rq_id = request.data['rq_id']
    else:
        rq_id = RQId(
            RequestAction.IMPORT, RequestTarget.PROJECT, uuid.uuid4(),
            subresource=RequestSubresource.BACKUP,
        ).render()
    Serializer = ProjectFileSerializer
    file_field_name = 'project_file'

    location_conf = get_location_configuration(
        query_params=request.query_params,
        field_name=StorageType.SOURCE,
    )

    queue = django_rq.get_queue(queue_name)

    return _import(
        importer=_import_project,
        request=request,
        queue=queue,
        rq_id=rq_id,
        Serializer=Serializer,
        file_field_name=file_field_name,
        location_conf=location_conf,
        filename=filename
    )

def import_task(request: ExtendedRequest, queue_name: str, filename: str | None = None):
    rq_id = request.data.get('rq_id', RQId(
        RequestAction.IMPORT, RequestTarget.TASK, uuid.uuid4(),
        subresource=RequestSubresource.BACKUP,
    ).render())
    Serializer = TaskFileSerializer
    file_field_name = 'task_file'

    location_conf = get_location_configuration(
        query_params=request.query_params,
        field_name=StorageType.SOURCE
    )

    queue = django_rq.get_queue(queue_name)

    return _import(
        importer=_import_task,
        request=request,
        queue=queue,
        rq_id=rq_id,
        Serializer=Serializer,
        file_field_name=file_field_name,
        location_conf=location_conf,
        filename=filename
    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\cache.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import os
import os.path
import pickle  # nosec
import tempfile
import time
import zipfile
import zlib
from collections.abc import Collection, Generator, Iterator, Sequence
from contextlib import ExitStack, closing
from datetime import datetime, timezone
from itertools import groupby, pairwise
from typing import Any, Callable, Optional, Union, overload

import attrs
import av
import cv2
import django_rq
import PIL.Image
import PIL.ImageOps
import rq
from django.conf import settings
from django.core.cache import caches
from django.db import models as django_models
from django.utils import timezone as django_tz
from redis.exceptions import LockError
from rest_framework.exceptions import NotFound, ValidationError
from rq.job import JobStatus as RQJobStatus

from cvat.apps.engine import models
from cvat.apps.engine.cloud_provider import (
    Credentials,
    db_storage_to_storage_instance,
    get_cloud_storage_instance,
)
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.media_extractors import (
    FrameQuality,
    IChunkWriter,
    ImageReaderWithManifest,
    Mpeg4ChunkWriter,
    Mpeg4CompressedChunkWriter,
    VideoReader,
    VideoReaderWithManifest,
    ZipChunkWriter,
    ZipCompressedChunkWriter,
    load_image,
)
from cvat.apps.engine.rq import RQMetaWithFailureInfo
from cvat.apps.engine.utils import (
    CvatChunkTimestampMismatchError,
    format_list,
    get_rq_lock_for_job,
    md5_hash,
)
from utils.dataset_manifest import ImageManifestManager

slogger = ServerLogManager(__name__)


DataWithMime = tuple[io.BytesIO, str]
_CacheItem = tuple[io.BytesIO, str, int, Union[datetime, None]]


def enqueue_create_chunk_job(
    queue: rq.Queue,
    rq_job_id: str,
    create_callback: Callback,
    *,
    rq_job_result_ttl: int = 60,
    rq_job_failure_ttl: int = 3600 * 24 * 14,  # 2 weeks
) -> rq.job.Job:
    try:
        with get_rq_lock_for_job(queue, rq_job_id):
            rq_job = queue.fetch_job(rq_job_id)

            if not rq_job or (
                # Enqueue the job if the chunk was deleted but the RQ job still exists.
                # This can happen in cases involving jobs with honeypots and
                # if the job wasn't collected by the requesting process for any reason.
                rq_job.get_status(refresh=False)
                in {RQJobStatus.FINISHED, RQJobStatus.FAILED, RQJobStatus.CANCELED}
            ):
                rq_job = queue.enqueue(
                    create_callback,
                    job_id=rq_job_id,
                    result_ttl=rq_job_result_ttl,
                    failure_ttl=rq_job_failure_ttl,
                )
    except LockError:
        raise TimeoutError(f"Cannot acquire lock for {rq_job_id}")

    return rq_job


def wait_for_rq_job(rq_job: rq.job.Job):
    retries = settings.CVAT_CHUNK_CREATE_TIMEOUT // settings.CVAT_CHUNK_CREATE_CHECK_INTERVAL or 1
    while retries > 0:
        job_status = rq_job.get_status()
        if job_status in ("finished",):
            return
        elif job_status in ("failed",):
            rq_job.get_meta()  # refresh from Redis
            job_meta = RQMetaWithFailureInfo.for_job(rq_job)
            exc_type = job_meta.exc_type or Exception
            exc_args = job_meta.exc_args or ("Cannot create chunk",)
            raise exc_type(*exc_args)

        time.sleep(settings.CVAT_CHUNK_CREATE_CHECK_INTERVAL)
        retries -= 1

    raise TimeoutError(f"Chunk processing takes too long {rq_job.id}")


def _is_run_inside_rq() -> bool:
    return rq.get_current_job() is not None


def _convert_args_for_callback(func_args: list[Any]) -> list[Any]:
    result = []
    for func_arg in func_args:
        if _is_run_inside_rq():
            result.append(func_arg)
        else:
            if isinstance(
                func_arg,
                django_models.Model,
            ):
                result.append(func_arg.id)
            elif isinstance(func_arg, list):
                result.append(_convert_args_for_callback(func_arg))
            else:
                result.append(func_arg)

    return result


@attrs.frozen
class Callback:
    _callable: Callable[..., DataWithMime] = attrs.field(
        validator=attrs.validators.is_callable(),
    )
    _args: list[Any] = attrs.field(
        factory=list,
        validator=attrs.validators.instance_of(list),
        converter=_convert_args_for_callback,
    )
    _kwargs: dict[str, Union[bool, int, float, str, None]] = attrs.field(
        factory=dict,
        validator=attrs.validators.deep_mapping(
            key_validator=attrs.validators.instance_of(str),
            value_validator=attrs.validators.instance_of((bool, int, float, str, type(None))),
            mapping_validator=attrs.validators.instance_of(dict),
        ),
    )

    def __call__(self) -> DataWithMime:
        return self._callable(*self._args, **self._kwargs)


class MediaCache:
    _QUEUE_NAME = settings.CVAT_QUEUES.CHUNKS.value
    _QUEUE_JOB_PREFIX_TASK = "chunks:prepare-item-"
    _CACHE_NAME = "media"
    _PREVIEW_TTL = settings.CVAT_PREVIEW_CACHE_TTL

    @staticmethod
    def _cache():
        return caches[MediaCache._CACHE_NAME]

    @staticmethod
    def _get_checksum(value: bytes) -> int:
        return zlib.crc32(value)

    def _get_or_set_cache_item(
        self,
        key: str,
        create_callback: Callback,
        *,
        cache_item_ttl: Optional[int] = None,
    ) -> _CacheItem:
        item = self._get_cache_item(key)
        if item:
            return item

        return self._create_cache_item(
            key,
            create_callback,
            cache_item_ttl=cache_item_ttl,
        )

    @classmethod
    def _get_queue(cls) -> rq.Queue:
        return django_rq.get_queue(cls._QUEUE_NAME)

    @classmethod
    def _make_queue_job_id(cls, key: str) -> str:
        return f"{cls._QUEUE_JOB_PREFIX_TASK}{key}"

    @staticmethod
    def _drop_return_value(func: Callable[..., DataWithMime], *args: Any, **kwargs: Any):
        func(*args, **kwargs)

    @classmethod
    def _create_and_set_cache_item(
        cls,
        key: str,
        create_callback: Callback,
        cache_item_ttl: Optional[int] = None,
    ) -> DataWithMime:
        timestamp = django_tz.now()
        item_data = create_callback()
        item_data_bytes = item_data[0].getvalue()
        item = (item_data[0], item_data[1], cls._get_checksum(item_data_bytes), timestamp)

        # allow empty data to be set in cache to prevent
        # future rq jobs from being enqueued to prepare the item
        cache = cls._cache()
        with get_rq_lock_for_job(
            cls._get_queue(),
            key,
        ):
            cached_item = cache.get(key)
            if cached_item is not None and timestamp <= cached_item[3]:
                item = cached_item
            else:
                cache.set(key, item, timeout=cache_item_ttl or cache.default_timeout)

        return item

    def _create_cache_item(
        self,
        key: str,
        create_callback: Callback,
        *,
        cache_item_ttl: Optional[int] = None,
    ) -> _CacheItem:
        slogger.glob.info(f"Starting to prepare chunk: key {key}")
        if _is_run_inside_rq():
            item = self._create_and_set_cache_item(
                key,
                create_callback,
                cache_item_ttl=cache_item_ttl,
            )
        else:
            rq_job = enqueue_create_chunk_job(
                queue=self._get_queue(),
                rq_job_id=self._make_queue_job_id(key),
                create_callback=Callback(
                    callable=self._drop_return_value,
                    args=[
                        self._create_and_set_cache_item,
                        key,
                        create_callback,
                    ],
                    kwargs={
                        "cache_item_ttl": cache_item_ttl,
                    },
                ),
            )
            wait_for_rq_job(rq_job)
            item = self._get_cache_item(key)

        slogger.glob.info(f"Ending to prepare chunk: key {key}")

        return item

    def _delete_cache_item(self, key: str):
        self._cache().delete(key)
        slogger.glob.info(f"Removed the cache key {key}")

    def _bulk_delete_cache_items(self, keys: Sequence[str]):
        self._cache().delete_many(keys)
        slogger.glob.info(f"Removed the cache keys {format_list(keys)}")

    def _get_cache_item(self, key: str) -> Optional[_CacheItem]:
        try:
            item = self._cache().get(key)
        except pickle.UnpicklingError:
            slogger.glob.error(f"Unable to get item from cache: key {key}", exc_info=True)
            item = None

        if not item:
            return None

        item_data = item[0].getbuffer() if isinstance(item[0], io.BytesIO) else item[0]
        item_checksum = item[2] if len(item) == 4 else None
        if item_checksum != self._get_checksum(item_data):
            slogger.glob.info(f"Cache item {key} checksum mismatch")
            return None

        return item

    def _validate_cache_item_timestamp(
        self, item: _CacheItem, expected_timestamp: datetime
    ) -> _CacheItem:
        if item[3] < expected_timestamp:
            raise CvatChunkTimestampMismatchError(
                f"Cache timestamp mismatch. Item_ts: {item[3]}, expected_ts: {expected_timestamp}"
            )

        return item

    @classmethod
    def _has_key(cls, key: str) -> bool:
        return cls._cache().has_key(key)

    @staticmethod
    def _make_cache_key_prefix(
        obj: Union[models.Task, models.Segment, models.Job, models.CloudStorage],
    ) -> str:
        if isinstance(obj, models.Task):
            return f"task_{obj.id}"
        elif isinstance(obj, models.Segment):
            return f"segment_{obj.id}"
        elif isinstance(obj, models.Job):
            return f"job_{obj.id}"
        elif isinstance(obj, models.CloudStorage):
            return f"cloudstorage_{obj.id}"
        else:
            assert False, f"Unexpected object type {type(obj)}"

    @classmethod
    def _make_chunk_key(
        cls,
        db_obj: Union[models.Task, models.Segment, models.Job],
        chunk_number: int,
        *,
        quality: FrameQuality,
    ) -> str:
        return f"{cls._make_cache_key_prefix(db_obj)}_chunk_{chunk_number}_{quality}"

    def _make_preview_key(self, db_obj: Union[models.Segment, models.CloudStorage]) -> str:
        return f"{self._make_cache_key_prefix(db_obj)}_preview"

    def _make_segment_task_chunk_key(
        self,
        db_obj: models.Segment,
        chunk_number: int,
        *,
        quality: FrameQuality,
    ) -> str:
        return f"{self._make_cache_key_prefix(db_obj)}_task_chunk_{chunk_number}_{quality}"

    def _make_frame_context_images_chunk_key(self, db_data: models.Data, frame_number: int) -> str:
        return f"context_images_{db_data.id}_{frame_number}"

    @overload
    def _to_data_with_mime(self, cache_item: _CacheItem) -> DataWithMime: ...

    @overload
    def _to_data_with_mime(
        self, cache_item: Optional[_CacheItem], *, allow_none: bool = False
    ) -> Optional[DataWithMime]: ...

    def _to_data_with_mime(
        self, cache_item: Optional[_CacheItem], *, allow_none: bool = False
    ) -> Optional[DataWithMime]:
        if not cache_item:
            if allow_none:
                return None

            raise ValueError("A cache item is not allowed to be None")

        return cache_item[:2]

    def get_or_set_segment_chunk(
        self, db_segment: models.Segment, chunk_number: int, *, quality: FrameQuality
    ) -> DataWithMime:

        item = self._get_or_set_cache_item(
            self._make_chunk_key(db_segment, chunk_number, quality=quality),
            Callback(
                callable=self.prepare_segment_chunk,
                args=[db_segment, chunk_number],
                kwargs={"quality": quality},
            ),
        )
        db_segment.refresh_from_db(fields=["chunks_updated_date"])

        return self._to_data_with_mime(
            self._validate_cache_item_timestamp(item, db_segment.chunks_updated_date)
        )

    def get_task_chunk(
        self, db_task: models.Task, chunk_number: int, *, quality: FrameQuality
    ) -> Optional[DataWithMime]:
        return self._to_data_with_mime(
            self._get_cache_item(
                key=self._make_chunk_key(db_task, chunk_number, quality=quality),
            ),
            allow_none=True,
        )

    def get_or_set_task_chunk(
        self,
        db_task: models.Task,
        chunk_number: int,
        set_callback: Callback,
        *,
        quality: FrameQuality,
    ) -> DataWithMime:

        item = self._get_or_set_cache_item(
            self._make_chunk_key(db_task, chunk_number, quality=quality),
            set_callback,
        )
        db_task.refresh_from_db(fields=["segment_set"])

        return self._to_data_with_mime(
            self._validate_cache_item_timestamp(item, db_task.get_chunks_updated_date())
        )

    def get_segment_task_chunk(
        self, db_segment: models.Segment, chunk_number: int, *, quality: FrameQuality
    ) -> Optional[DataWithMime]:
        return self._to_data_with_mime(
            self._get_cache_item(
                key=self._make_segment_task_chunk_key(db_segment, chunk_number, quality=quality),
            ),
            allow_none=True,
        )

    def get_or_set_segment_task_chunk(
        self,
        db_segment: models.Segment,
        chunk_number: int,
        *,
        quality: FrameQuality,
        set_callback: Callback,
    ) -> DataWithMime:

        item = self._get_or_set_cache_item(
            self._make_segment_task_chunk_key(db_segment, chunk_number, quality=quality),
            set_callback,
        )
        db_segment.refresh_from_db(fields=["chunks_updated_date"])

        return self._to_data_with_mime(
            self._validate_cache_item_timestamp(item, db_segment.chunks_updated_date),
        )

    def get_or_set_selective_job_chunk(
        self, db_job: models.Job, chunk_number: int, *, quality: FrameQuality
    ) -> DataWithMime:
        return self._to_data_with_mime(
            self._get_or_set_cache_item(
                self._make_chunk_key(db_job, chunk_number, quality=quality),
                Callback(
                    callable=self.prepare_masked_range_segment_chunk,
                    args=[db_job.segment, chunk_number],
                    kwargs={
                        "quality": quality,
                    },
                ),
            )
        )

    def get_or_set_segment_preview(self, db_segment: models.Segment) -> DataWithMime:
        return self._to_data_with_mime(
            self._get_or_set_cache_item(
                self._make_preview_key(db_segment),
                Callback(
                    callable=self._prepare_segment_preview,
                    args=[db_segment],
                ),
                cache_item_ttl=self._PREVIEW_TTL,
            )
        )

    def remove_segment_chunk(
        self, db_segment: models.Segment, chunk_number: str, *, quality: str
    ) -> None:
        self._delete_cache_item(
            self._make_chunk_key(db_segment, chunk_number=chunk_number, quality=quality)
        )

    def remove_context_images_chunk(self, db_data: models.Data, frame_number: str) -> None:
        self._delete_cache_item(
            self._make_frame_context_images_chunk_key(db_data, frame_number=frame_number)
        )

    def remove_segments_chunks(self, params: Sequence[dict[str, Any]]) -> None:
        """
        Removes several segment chunks from the cache.

        The function expects a sequence of remove_segment_chunk() parameters as dicts.
        """
        # TODO: add a version of this function
        # that removes related cache elements as well (context images, previews, ...)
        # to provide encapsulation

        # TODO: add a generic bulk cleanup function for different objects, including related ones
        # (likely a bulk key aggregator should be used inside to reduce requests count)

        keys_to_remove = []
        for item_params in params:
            db_obj = item_params.pop("db_segment")
            keys_to_remove.append(self._make_chunk_key(db_obj, **item_params))

        self._bulk_delete_cache_items(keys_to_remove)

    def remove_context_images_chunks(self, params: Sequence[dict[str, Any]]) -> None:
        """
        Removes several context image chunks from the cache.

        The function expects a sequence of remove_context_images_chunk() parameters as dicts.
        """

        keys_to_remove = []
        for item_params in params:
            db_obj = item_params.pop("db_data")
            keys_to_remove.append(self._make_frame_context_images_chunk_key(db_obj, **item_params))

        self._bulk_delete_cache_items(keys_to_remove)

    def get_cloud_preview(self, db_storage: models.CloudStorage) -> Optional[DataWithMime]:
        return self._to_data_with_mime(
            self._get_cache_item(self._make_preview_key(db_storage)), allow_none=True
        )

    def get_or_set_cloud_preview(self, db_storage: models.CloudStorage) -> DataWithMime:
        return self._to_data_with_mime(
            self._get_or_set_cache_item(
                self._make_preview_key(db_storage),
                Callback(
                    callable=self._prepare_cloud_preview,
                    args=[db_storage],
                ),
                cache_item_ttl=self._PREVIEW_TTL,
            )
        )

    def get_or_set_frame_context_images_chunk(
        self, db_data: models.Data, frame_number: int
    ) -> DataWithMime:
        return self._to_data_with_mime(
            self._get_or_set_cache_item(
                self._make_frame_context_images_chunk_key(db_data, frame_number),
                Callback(
                    callable=self.prepare_context_images_chunk,
                    args=[db_data, frame_number],
                ),
            )
        )

    @staticmethod
    def _read_raw_images(
        db_task: models.Task,
        frame_ids: Sequence[int],
        *,
        manifest_path: str,
    ):
        db_data = db_task.data

        if os.path.isfile(manifest_path) and db_data.storage == models.StorageChoice.CLOUD_STORAGE:
            reader = ImageReaderWithManifest(manifest_path)
            with ExitStack() as es:
                db_cloud_storage = db_data.cloud_storage
                assert db_cloud_storage, "Cloud storage instance was deleted"
                credentials = Credentials()
                credentials.convert_from_db(
                    {
                        "type": db_cloud_storage.credentials_type,
                        "value": db_cloud_storage.credentials,
                    }
                )
                details = {
                    "resource": db_cloud_storage.resource,
                    "credentials": credentials,
                    "specific_attributes": db_cloud_storage.get_specific_attributes(),
                }
                cloud_storage_instance = get_cloud_storage_instance(
                    cloud_provider=db_cloud_storage.provider_type, **details
                )

                tmp_dir = es.enter_context(tempfile.TemporaryDirectory(prefix="cvat"))
                files_to_download = []
                checksums = []
                media = []
                for item in reader.iterate_frames(frame_ids):
                    file_name = f"{item['name']}{item['extension']}"
                    fs_filename = os.path.join(tmp_dir, file_name)

                    files_to_download.append(file_name)
                    checksums.append(item.get("checksum", None))
                    media.append((fs_filename, fs_filename, None))

                cloud_storage_instance.bulk_download_to_dir(
                    files=files_to_download, upload_dir=tmp_dir
                )

                for checksum, media_item in zip(checksums, media):
                    if checksum and not md5_hash(media_item[1]) == checksum:
                        slogger.cloud_storage[db_cloud_storage.id].warning(
                            "Hash sums of files {} do not match".format(file_name)
                        )
                    yield load_image(media_item)
        else:
            requested_frame_iter = iter(frame_ids)
            next_requested_frame_id = next(requested_frame_iter, None)
            if next_requested_frame_id is None:
                return

            # TODO: find a way to use prefetched results, if provided
            db_images = (
                db_data.images.order_by("frame")
                .filter(frame__gte=frame_ids[0], frame__lte=frame_ids[-1])
                .values_list("frame", "path")
                .all()
            )

            raw_data_dir = db_data.get_raw_data_dirname()
            media = []
            for frame_id, frame_path in db_images:
                if frame_id == next_requested_frame_id:
                    source_path = os.path.join(raw_data_dir, frame_path)
                    media.append((source_path, source_path, None))

                    next_requested_frame_id = next(requested_frame_iter, None)

                if next_requested_frame_id is None:
                    break

            assert next_requested_frame_id is None

            if db_task.dimension == models.DimensionType.DIM_2D:
                media = map(load_image, media)

            yield from media

    @staticmethod
    def _read_raw_frames(
        db_task: Union[models.Task, int], frame_ids: Sequence[int]
    ) -> Generator[tuple[Union[av.VideoFrame, PIL.Image.Image], str, str], None, None]:
        if isinstance(db_task, int):
            db_task = models.Task.objects.get(pk=db_task)

        for prev_frame, cur_frame in pairwise(frame_ids):
            assert (
                prev_frame <= cur_frame
            ), f"Requested frame ids must be sorted, got a ({prev_frame}, {cur_frame}) pair"

        db_data = db_task.data

        manifest_path = db_data.get_manifest_path()

        if hasattr(db_data, "video"):
            source_path = os.path.join(db_data.get_raw_data_dirname(), db_data.video.path)

            reader = VideoReaderWithManifest(
                manifest_path=manifest_path,
                source_path=source_path,
                allow_threading=False,
            )
            if not os.path.isfile(manifest_path):
                try:
                    reader.manifest.link(source_path, force=True)
                    reader.manifest.create()
                except Exception as e:
                    slogger.task[db_task.id].warning(
                        f"Failed to create video manifest: {e}", exc_info=True
                    )
                    reader = None

            if reader:
                for frame in reader.iterate_frames(frame_filter=frame_ids):
                    yield (frame, source_path, None)
            else:
                reader = VideoReader([source_path], allow_threading=False)

                for frame_tuple in reader.iterate_frames(frame_filter=frame_ids):
                    yield frame_tuple
        else:
            yield from MediaCache._read_raw_images(db_task, frame_ids, manifest_path=manifest_path)

    def prepare_segment_chunk(
        self, db_segment: Union[models.Segment, int], chunk_number: int, *, quality: FrameQuality
    ) -> DataWithMime:
        if isinstance(db_segment, int):
            db_segment = models.Segment.objects.get(pk=db_segment)

        if db_segment.type == models.SegmentType.RANGE:
            return self.prepare_range_segment_chunk(db_segment, chunk_number, quality=quality)
        elif db_segment.type == models.SegmentType.SPECIFIC_FRAMES:
            return self.prepare_masked_range_segment_chunk(
                db_segment, chunk_number, quality=quality
            )
        else:
            assert False, f"Unknown segment type {db_segment.type}"

    def prepare_range_segment_chunk(
        self, db_segment: models.Segment, chunk_number: int, *, quality: FrameQuality
    ) -> DataWithMime:
        db_task = db_segment.task
        db_data = db_task.data

        chunk_size = db_data.chunk_size
        chunk_frame_ids = list(db_segment.frame_set)[
            chunk_size * chunk_number : chunk_size * (chunk_number + 1)
        ]

        return self.prepare_custom_range_segment_chunk(db_task, chunk_frame_ids, quality=quality)

    @classmethod
    def prepare_custom_range_segment_chunk(
        cls, db_task: models.Task, frame_ids: Sequence[int], *, quality: FrameQuality
    ) -> DataWithMime:
        with closing(cls._read_raw_frames(db_task, frame_ids=frame_ids)) as frame_iter:
            return prepare_chunk(frame_iter, quality=quality, db_task=db_task)

    def prepare_masked_range_segment_chunk(
        self, db_segment: models.Segment, chunk_number: int, *, quality: FrameQuality
    ) -> DataWithMime:
        db_task = db_segment.task
        db_data = db_task.data

        chunk_size = db_data.chunk_size
        chunk_frame_ids = sorted(db_segment.frame_set)[
            chunk_size * chunk_number : chunk_size * (chunk_number + 1)
        ]

        return self.prepare_custom_masked_range_segment_chunk(
            db_task, chunk_frame_ids, chunk_number, quality=quality
        )

    @classmethod
    def prepare_custom_masked_range_segment_chunk(
        cls,
        db_task: Union[models.Task, int],
        frame_ids: Collection[int],
        chunk_number: int,
        *,
        quality: FrameQuality,
        insert_placeholders: bool = False,
    ) -> DataWithMime:
        if isinstance(db_task, int):
            db_task = models.Task.objects.get(pk=db_task)

        db_data = db_task.data

        frame_step = db_data.get_frame_step()

        image_quality = 100 if quality == FrameQuality.ORIGINAL else db_data.image_quality
        writer = ZipCompressedChunkWriter(image_quality, dimension=db_task.dimension)

        dummy_frame = io.BytesIO()
        PIL.Image.new("RGB", (1, 1)).save(dummy_frame, writer.IMAGE_EXT)

        # Optimize frame access if all the required frames are already cached
        # Otherwise we might need to download files.
        # This is not needed for video tasks, as it will reduce performance,
        # because of reading multiple files (chunks)
        from cvat.apps.engine.frame_provider import FrameOutputType, make_frame_provider

        task_frame_provider = make_frame_provider(db_task)

        use_cached_data = False
        if db_task.mode != "interpolation":
            required_frame_set = set(frame_ids)
            available_chunks = []
            for db_segment in db_task.segment_set.filter(type=models.SegmentType.RANGE).all():
                segment_frame_provider = make_frame_provider(db_segment)

                for i, chunk_frames in groupby(
                    sorted(required_frame_set.intersection(db_segment.frame_set)),
                    key=lambda abs_frame: (
                        segment_frame_provider.validate_frame_number(
                            task_frame_provider.get_rel_frame_number(abs_frame)
                        )[1]
                    ),
                ):
                    if not list(chunk_frames):
                        continue

                    chunk_available = cls._has_key(
                        cls._make_chunk_key(db_segment, i, quality=quality)
                    )
                    available_chunks.append(chunk_available)

            use_cached_data = bool(available_chunks) and all(available_chunks)

        if hasattr(db_data, "video"):
            frame_size = (db_data.video.width, db_data.video.height)
        else:
            frame_size = None

        def get_frames():
            with ExitStack() as es:
                es.callback(task_frame_provider.unload)

                if insert_placeholders:
                    frame_range = (
                        (
                            db_data.start_frame
                            + (chunk_number * db_data.chunk_size + chunk_frame_idx) * frame_step
                        )
                        for chunk_frame_idx in range(db_data.chunk_size)
                    )
                else:
                    frame_range = frame_ids

                if not use_cached_data:
                    frames_gen = cls._read_raw_frames(db_task, frame_ids)
                    frames_iter = iter(es.enter_context(closing(frames_gen)))

                for abs_frame_idx in frame_range:
                    if db_data.stop_frame < abs_frame_idx:
                        break

                    if abs_frame_idx in frame_ids:
                        if use_cached_data:
                            frame_data = task_frame_provider.get_frame(
                                task_frame_provider.get_rel_frame_number(abs_frame_idx),
                                quality=quality,
                                out_type=FrameOutputType.BUFFER,
                            )
                            frame = frame_data.data
                        else:
                            frame, _, _ = next(frames_iter)

                        if hasattr(db_data, "video"):
                            # Decoded video frames can have different size, restore the original one

                            if isinstance(frame, av.VideoFrame):
                                frame = frame.to_image()
                            else:
                                frame = PIL.Image.open(frame)

                            if frame.size != frame_size:
                                frame = frame.resize(frame_size)
                    else:
                        # Populate skipped frames with placeholder data,
                        # this is required for video chunk decoding implementation in UI
                        frame = io.BytesIO(dummy_frame.getvalue())

                    yield (frame, None, None)

        buff = io.BytesIO()
        with closing(get_frames()) as frame_iter:
            writer.save_as_chunk(
                frame_iter,
                buff,
                zip_compress_level=1,
                # there are likely to be many skips with repeated placeholder frames
                # in SPECIFIC_FRAMES segments, it makes sense to compress the archive
            )

        buff.seek(0)
        return buff, get_chunk_mime_type_for_writer(writer)

    def _prepare_segment_preview(self, db_segment: Union[models.Segment, int]) -> DataWithMime:
        if isinstance(db_segment, int):
            db_segment = models.Segment.objects.get(pk=db_segment)

        if db_segment.task.dimension == models.DimensionType.DIM_3D:
            # TODO
            preview = PIL.Image.open(
                os.path.join(os.path.dirname(__file__), "assets/3d_preview.jpeg")
            )
        else:
            from cvat.apps.engine.frame_provider import (  # avoid circular import
                FrameOutputType,
                make_frame_provider,
            )

            task_frame_provider = make_frame_provider(db_segment.task)
            segment_frame_provider = make_frame_provider(db_segment)
            preview = segment_frame_provider.get_frame(
                task_frame_provider.get_rel_frame_number(min(db_segment.frame_set)),
                quality=FrameQuality.COMPRESSED,
                out_type=FrameOutputType.PIL,
            ).data

        return prepare_preview_image(preview)

    def _prepare_cloud_preview(self, db_storage: Union[models.CloudStorage, int]) -> DataWithMime:
        if isinstance(db_storage, int):
            db_storage = models.CloudStorage.objects.get(pk=db_storage)

        storage = db_storage_to_storage_instance(db_storage)
        if not db_storage.manifests.count():
            raise ValidationError("Cannot get the cloud storage preview. There is no manifest file")

        preview_path = None
        for db_manifest in db_storage.manifests.all():
            manifest_prefix = os.path.dirname(db_manifest.filename)

            full_manifest_path = os.path.join(
                db_storage.get_storage_dirname(), db_manifest.filename
            )
            if not os.path.exists(full_manifest_path) or datetime.fromtimestamp(
                os.path.getmtime(full_manifest_path), tz=timezone.utc
            ) < storage.get_file_last_modified(db_manifest.filename):
                storage.download_file(db_manifest.filename, full_manifest_path)

            manifest = ImageManifestManager(
                os.path.join(db_storage.get_storage_dirname(), db_manifest.filename),
                db_storage.get_storage_dirname(),
            )
            # need to update index
            manifest.set_index()
            if not len(manifest):
                continue

            preview_info = manifest[0]
            preview_filename = "".join([preview_info["name"], preview_info["extension"]])
            preview_path = os.path.join(manifest_prefix, preview_filename)
            break

        if not preview_path:
            msg = "Cloud storage {} does not contain any images".format(db_storage.pk)
            slogger.cloud_storage[db_storage.pk].info(msg)
            raise NotFound(msg)

        buff = storage.download_fileobj(preview_path)
        image = PIL.Image.open(buff)
        return prepare_preview_image(image)

    def prepare_context_images_chunk(
        self, db_data: Union[models.Data, int], frame_number: int
    ) -> DataWithMime:
        if isinstance(db_data, int):
            db_data = models.Data.objects.get(pk=db_data)

        zip_buffer = io.BytesIO()

        related_images = db_data.related_files.filter(images__frame=frame_number).all()
        if not related_images:
            return zip_buffer, ""

        with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
            common_path = os.path.commonpath(list(map(lambda x: str(x.path), related_images)))
            for related_image in related_images:
                path = os.path.realpath(str(related_image.path))
                name = os.path.relpath(str(related_image.path), common_path)
                image = cv2.imread(path)
                success, result = cv2.imencode(".JPEG", image)
                if not success:
                    raise Exception('Failed to encode image to ".jpeg" format')
                zip_file.writestr(f"{name}.jpg", result.tobytes())

        zip_buffer.seek(0)
        mime_type = "application/zip"
        return zip_buffer, mime_type


def prepare_preview_image(image: PIL.Image.Image) -> DataWithMime:
    PREVIEW_SIZE = (256, 256)
    PREVIEW_MIME = "image/jpeg"

    image = PIL.ImageOps.exif_transpose(image)
    image.thumbnail(PREVIEW_SIZE)

    output_buf = io.BytesIO()
    image.convert("RGB").save(output_buf, format="JPEG")
    return output_buf, PREVIEW_MIME


def prepare_chunk(
    task_chunk_frames: Iterator[tuple[Any, str, int]],
    *,
    quality: FrameQuality,
    db_task: models.Task,
    dump_unchanged: bool = False,
) -> DataWithMime:
    # TODO: refactor all chunk building into another class

    db_data = db_task.data

    writer_classes: dict[FrameQuality, type[IChunkWriter]] = {
        FrameQuality.COMPRESSED: (
            Mpeg4CompressedChunkWriter
            if db_data.compressed_chunk_type == models.DataChoice.VIDEO
            else ZipCompressedChunkWriter
        ),
        FrameQuality.ORIGINAL: (
            Mpeg4ChunkWriter
            if db_data.original_chunk_type == models.DataChoice.VIDEO
            else ZipChunkWriter
        ),
    }

    writer_class = writer_classes[quality]

    image_quality = 100 if quality == FrameQuality.ORIGINAL else db_data.image_quality

    writer_kwargs = {}
    if db_task.dimension == models.DimensionType.DIM_3D:
        writer_kwargs["dimension"] = models.DimensionType.DIM_3D
    merged_chunk_writer = writer_class(image_quality, **writer_kwargs)

    writer_kwargs = {}
    if dump_unchanged and isinstance(merged_chunk_writer, ZipCompressedChunkWriter):
        writer_kwargs = dict(compress_frames=False, zip_compress_level=1)

    buffer = io.BytesIO()
    merged_chunk_writer.save_as_chunk(task_chunk_frames, buffer, **writer_kwargs)

    buffer.seek(0)
    return buffer, get_chunk_mime_type_for_writer(writer_class)


def get_chunk_mime_type_for_writer(writer: Union[IChunkWriter, type[IChunkWriter]]) -> str:
    if isinstance(writer, IChunkWriter):
        writer_class = type(writer)
    else:
        writer_class = writer

    if issubclass(writer_class, ZipChunkWriter):
        return "application/zip"
    elif issubclass(writer_class, Mpeg4ChunkWriter):
        return "video/mp4"
    else:
        assert False, f"Unknown chunk writer class {writer_class}"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\cloud_provider.py =====
# Copyright (C) 2021-2023 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import functools
import json
import math
import os
from abc import ABC, abstractmethod
from collections.abc import Iterator
from concurrent.futures import FIRST_EXCEPTION, ThreadPoolExecutor, wait
from enum import Enum
from io import BytesIO
from pathlib import Path
from typing import Any, BinaryIO, Callable, Optional, TypeVar

import boto3
from azure.core.exceptions import HttpResponseError, ResourceExistsError
from azure.storage.blob import BlobServiceClient, ContainerClient, PublicAccess
from azure.storage.blob._list_blobs_helper import BlobPrefix
from boto3.s3.transfer import TransferConfig
from botocore.client import Config
from botocore.exceptions import ClientError
from botocore.handlers import disable_signing
from django.conf import settings
from google.cloud import storage
from google.cloud.exceptions import Forbidden as GoogleCloudForbidden
from google.cloud.exceptions import NotFound as GoogleCloudNotFound
from PIL import Image, ImageFile
from rest_framework.exceptions import NotFound, PermissionDenied, ValidationError
from rq import get_current_job

from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import CloudProviderChoice, CredentialsTypeChoice
from cvat.apps.engine.rq import ExportRQMeta
from cvat.apps.engine.utils import get_cpu_number, take_by
from cvat.utils.http import PROXIES_FOR_UNTRUSTED_URLS


class NamedBytesIO(BytesIO):
    @property
    def filename(self) -> Optional[str]:
        return getattr(self, '_filename', None)

    @filename.setter
    def filename(self, value: str) -> None:
        self._filename = value

slogger = ServerLogManager(__name__)

ImageFile.LOAD_TRUNCATED_IMAGES = True

CPU_NUMBER = get_cpu_number()

def normalize_threads_number(
    threads_number: Optional[int], number_of_files: int
) -> int:
    threads_number = (
        min(
            CPU_NUMBER,
            settings.CLOUD_DATA_DOWNLOADING_MAX_THREADS_NUMBER,
            max(
                math.ceil(number_of_files / settings.CLOUD_DATA_DOWNLOADING_NUMBER_OF_FILES_PER_THREAD), 1
            ),
        )
        if threads_number is None
        else min(
            threads_number,
            CPU_NUMBER,
            settings.CLOUD_DATA_DOWNLOADING_MAX_THREADS_NUMBER,
        )
    )
    threads_number = max(threads_number, 1)

    return threads_number

class Status(str, Enum):
    AVAILABLE = 'AVAILABLE'
    NOT_FOUND = 'NOT_FOUND'
    FORBIDDEN = 'FORBIDDEN'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class Permissions(str, Enum):
    READ = 'read'
    WRITE = 'write'

    @classmethod
    def all(cls):
        return {i.value for i in cls}


def validate_bucket_status(func):
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        try:
            res = func(self, *args, **kwargs)
        except Exception as ex:
            # check that cloud storage exists
            storage_status = self.get_status() if self is not None else None
            if storage_status == Status.FORBIDDEN:
                raise PermissionDenied('The resource {} is no longer available. Access forbidden.'.format(self.name))
            elif storage_status == Status.NOT_FOUND:
                raise NotFound('The resource {} not found. It may have been deleted.'.format(self.name))
            elif storage_status == Status.AVAILABLE:
                raise
            raise ValidationError(str(ex))
        return res
    return wrapper

def validate_file_status(func):
    @functools.wraps(func)
    def wrapper(self, key: str, /, *args, **kwargs):
        try:
            res = func(self, key, *args, **kwargs)
        except Exception as ex:
            storage_status = self.get_status() if self is not None else None
            if storage_status == Status.AVAILABLE:
                file_status = self.get_file_status(key)
                if file_status == Status.NOT_FOUND:
                    raise NotFound("The file '{}' not found on the cloud storage '{}'".format(key, self.name))
                elif file_status == Status.FORBIDDEN:
                    raise PermissionDenied("Access to the file '{}' on the '{}' cloud storage is denied".format(key, self.name))
                raise ValidationError(str(ex))
            else:
                raise
        return res
    return wrapper

class _CloudStorage(ABC):
    def __init__(self, prefix: Optional[str] = None):
        self.prefix = prefix

    @property
    @abstractmethod
    def name(self):
        pass

    @abstractmethod
    def create(self):
        pass

    @abstractmethod
    def _head_file(self, key: str, /):
        pass

    @abstractmethod
    def _head(self):
        pass

    @abstractmethod
    def get_status(self):
        pass

    @abstractmethod
    def get_file_status(self, key: str, /):
        pass

    @abstractmethod
    def get_file_last_modified(self, key: str, /):
        pass

    @abstractmethod
    def _download_fileobj_to_stream(self, key: str, stream: BinaryIO, /) -> None:
        pass

    @validate_file_status
    @validate_bucket_status
    def download_fileobj(self, key: str, /) -> NamedBytesIO:
        buf = NamedBytesIO()
        self._download_fileobj_to_stream(key, buf)
        buf.seek(0)
        buf.filename = key
        return buf

    @validate_file_status
    @validate_bucket_status
    def download_file(self, key: str, path: str, /) -> None:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        try:
            with open(path, 'wb') as f:
                self._download_fileobj_to_stream(key, f)
        except Exception:
            Path(path).unlink()
            raise

    @validate_file_status
    @validate_bucket_status
    def download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int = 0) -> bytes:
        """Method downloads the required bytes range of the file.

        Args:
            key (str): File on the bucket
            stop_byte (int): Stop byte
            start_byte (int, optional): Start byte. Defaults to 0.

        Raises:
            ValidationError: If start_byte > stop_byte

        Returns:
            bytes: Range with bytes
        """

        if start_byte > stop_byte:
            raise ValidationError(f'Incorrect bytes range was received: {start_byte}-{stop_byte}')
        return self._download_range_of_bytes(key, stop_byte=stop_byte, start_byte=start_byte)

    @abstractmethod
    def _download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int):
        pass

    def optimally_image_download(self, key: str, /, *, chunk_size: int = 65536) -> NamedBytesIO:
        """
        Method downloads image by the following approach:
        Firstly we try to download the first N bytes of image which will be enough for determining image properties.
        If for some reason we cannot identify the required properties then we will download all file.

        Args:
            key (str): File on the bucket
            chunk_size (int, optional): The number of first bytes to download. Defaults to 65536 (64kB).

        Returns:
            BytesIO: Buffer with image
        """
        image_parser = ImageFile.Parser()

        chunk = self.download_range_of_bytes(key, stop_byte=chunk_size - 1)
        image_parser.feed(chunk)

        if image_parser.image:
            buff = NamedBytesIO(chunk)
            buff.filename = key
        else:
            buff = self.download_fileobj(key)
            image_size_in_bytes = len(buff.getvalue())
            slogger.glob.warning(
                f'The {chunk_size} bytes were not enough to parse "{key}" image. '
                f'Image size was {image_size_in_bytes} bytes. Image resolution was {Image.open(buff).size}. '
                f'Downloaded percent was {round(min(chunk_size, image_size_in_bytes) / image_size_in_bytes * 100)}')

        return buff

    def bulk_download_to_memory(
        self,
        files: list[str],
        *,
        threads_number: Optional[int] = None,
        _use_optimal_downloading: bool = True,
    ) -> Iterator[BytesIO]:
        func = self.optimally_image_download if _use_optimal_downloading else self.download_fileobj
        threads_number = normalize_threads_number(threads_number, len(files))

        with ThreadPoolExecutor(max_workers=threads_number) as executor:
            for batch_links in take_by(files, chunk_size=threads_number):
                yield from executor.map(func, batch_links)

    def bulk_download_to_dir(
        self,
        files: list[str],
        upload_dir: str,
        *,
        threads_number: Optional[int] = None,
    ) -> None:
        threads_number = normalize_threads_number(threads_number, len(files))

        with ThreadPoolExecutor(max_workers=threads_number) as executor:
            futures = [executor.submit(self.download_file, f, os.path.join(upload_dir, f)) for f in files]
            done, _ = wait(futures, return_when=FIRST_EXCEPTION)
            for future in done:
                if ex := future.exception():
                    raise ex

    @abstractmethod
    def upload_fileobj(self, file_obj: BinaryIO, key: str, /):
        pass

    @abstractmethod
    def upload_file(self, file_path: str, key: str | None = None, /):
        pass

    @abstractmethod
    def _list_raw_content_on_one_page(
        self,
        prefix: str = "",
        *,
        next_token: Optional[str] = None,
        page_size: int = settings.BUCKET_CONTENT_MAX_PAGE_SIZE,
    ) -> dict:
        pass

    def list_files_on_one_page(
        self,
        prefix: str = "",
        *,
        next_token: Optional[str] = None,
        page_size: int = settings.BUCKET_CONTENT_MAX_PAGE_SIZE,
        _use_flat_listing: bool = False,
        _use_sort: bool = False,
    ) -> dict:

        if self.prefix and prefix and not (self.prefix.startswith(prefix) or prefix.startswith(self.prefix)):
            return {
                'content': [],
                'next': None,
            }

        search_prefix = prefix
        if self.prefix and (len(prefix) < len(self.prefix)):
            if prefix and '/' in self.prefix[len(prefix):]:
                next_layer_and_tail = self.prefix[prefix.find('/') + 1:].split(
                    "/", maxsplit=1
                )
                if 2 == len(next_layer_and_tail):
                    directory = (
                        next_layer_and_tail[0]
                        if not _use_flat_listing
                        else self.prefix[: prefix.find('/') + 1] + next_layer_and_tail[0] + "/"
                    )
                    return {
                        "content": [{"name": directory, "type": "DIR"}],
                        "next": None,
                    }
                else:
                    search_prefix = self.prefix
            else:
                search_prefix = self.prefix

        result = self._list_raw_content_on_one_page(search_prefix, next_token=next_token, page_size=page_size)

        if not _use_flat_listing:
            result['directories'] = [d.strip('/') for d in result['directories']]
        content = [{'name': f, 'type': 'REG'} for f in result['files']]
        content.extend([{'name': d, 'type': 'DIR'} for d in result['directories']])

        if not _use_flat_listing and search_prefix and '/' in search_prefix:
            last_slash = search_prefix.rindex('/')
            for f in content:
                f['name'] = f['name'][last_slash + 1:]

        if _use_sort:
            content = sorted(content, key=lambda x: x['type'])

        return {
            'content': content,
            'next': result['next'],
        }

    def list_files(
        self,
        prefix: str = "",
        *,
        _use_flat_listing: bool = False,
    ) -> list[str]:
        all_files = []
        next_token = None
        while True:
            batch = self.list_files_on_one_page(prefix, next_token=next_token, _use_flat_listing=_use_flat_listing)
            all_files.extend(batch['content'])
            next_token = batch['next']
            if not next_token:
                break

        return all_files

    @property
    @abstractmethod
    def supported_actions(self):
        pass

    @property
    def read_access(self):
        return Permissions.READ in self.access

    @property
    def write_access(self):
        return Permissions.WRITE in self.access

def get_cloud_storage_instance(
    *,
    cloud_provider: CloudProviderChoice,
    resource: str,
    credentials: Credentials,
    specific_attributes: Optional[dict[str, Any]] = None,
):
    instance = None
    if cloud_provider == CloudProviderChoice.AWS_S3:
        instance = AWS_S3(
            resource,
            access_key_id=credentials.key,
            secret_key=credentials.secret_key,
            session_token=credentials.session_token,
            region=specific_attributes.get('region'),
            endpoint_url=specific_attributes.get('endpoint_url'),
            prefix=specific_attributes.get('prefix'),
        )
    elif cloud_provider == CloudProviderChoice.AZURE_CONTAINER:
        instance = AzureBlobContainer(
            resource,
            account_name=credentials.account_name,
            sas_token=credentials.session_token,
            connection_string=credentials.connection_string,
            prefix=specific_attributes.get('prefix'),
        )
    elif cloud_provider == CloudProviderChoice.GOOGLE_CLOUD_STORAGE:
        instance = GoogleCloudStorage(
            resource,
            service_account_json=credentials.key_file_path,
            anonymous_access = credentials.credentials_type == CredentialsTypeChoice.ANONYMOUS_ACCESS,
            prefix=specific_attributes.get('prefix'),
            location=specific_attributes.get('location'),
            project=specific_attributes.get('project')
        )
    else:
        raise NotImplementedError(f"The {cloud_provider} provider is not supported")
    return instance

class AWS_S3(_CloudStorage):
    transfer_config = {
        'max_io_queue': 10,
    }

    class Effect(str, Enum):
        ALLOW = 'Allow'
        DENY = 'Deny'


    def __init__(self,
                bucket: str,
                *,
                region: Optional[str] = None,
                access_key_id: Optional[str] = None,
                secret_key: Optional[str] = None,
                session_token: Optional[str] = None,
                endpoint_url: Optional[str] = None,
                prefix: Optional[str] = None,
    ):
        super().__init__(prefix=prefix)
        if (
            sum(
                1
                for credential in (access_key_id, secret_key, session_token)
                if credential
            )
            == 1
        ):
            raise Exception("Insufficient data for authentication")

        kwargs = dict()
        for key, arg_v in zip(
            (
                "aws_access_key_id",
                "aws_secret_access_key",
                "aws_session_token",
                "region_name",
            ),
            (access_key_id, secret_key, session_token, region),
        ):
            if arg_v:
                kwargs[key] = arg_v

        session = boto3.Session(**kwargs)
        self._s3 = session.resource("s3", endpoint_url=endpoint_url,
            config=Config(proxies=PROXIES_FOR_UNTRUSTED_URLS or {}),
        )

        # anonymous access
        if not any([access_key_id, secret_key, session_token]):
            self._s3.meta.client.meta.events.register(
                "choose-signer.s3.*", disable_signing
            )

        self._client = self._s3.meta.client
        self._bucket = self._s3.Bucket(bucket)
        self.region = region

    @property
    def bucket(self):
        return self._bucket

    @property
    def name(self):
        return self._bucket.name

    def _head(self):
        return self._client.head_bucket(Bucket=self.name)

    def _head_file(self, key: str, /):
        return self._client.head_object(Bucket=self.name, Key=key)

    def get_status(self):
        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.head_object
        # return only 3 codes: 200, 403, 404
        try:
            self._head()
            return Status.AVAILABLE
        except ClientError as ex:
            code = ex.response['Error']['Code']
            if code == '403':
                return Status.FORBIDDEN
            else:
                return Status.NOT_FOUND

    def get_file_status(self, key: str, /):
        try:
            self._head_file(key)
            return Status.AVAILABLE
        except ClientError as ex:
            code = ex.response['Error']['Code']
            if code == '403':
                return Status.FORBIDDEN
            else:
                return Status.NOT_FOUND

    @validate_file_status
    @validate_bucket_status
    def get_file_last_modified(self, key: str, /):
        return self._head_file(key).get('LastModified')

    @validate_bucket_status
    def upload_fileobj(self, file_obj: BinaryIO, key: str, /):
        self._bucket.upload_fileobj(
            Fileobj=file_obj,
            Key=key,
            Config=TransferConfig(max_io_queue=self.transfer_config['max_io_queue'])
        )

    @validate_bucket_status
    def upload_file(self, file_path: str, key: str | None = None, /):
        try:
            self._bucket.upload_file(
                file_path,
                key or os.path.basename(file_path),
                Config=TransferConfig(max_io_queue=self.transfer_config['max_io_queue'])
            )
        except ClientError as ex:
            msg = str(ex)
            slogger.glob.error(msg)
            raise Exception(msg)

    def _list_raw_content_on_one_page(
        self,
        prefix: str = "",
        *,
        next_token: Optional[str] = None,
        page_size: int = settings.BUCKET_CONTENT_MAX_PAGE_SIZE,
    ) -> dict:
        # The structure of response looks like this:
        # {
        #    'CommonPrefixes': [{'Prefix': 'sub/'}],
        #    'Contents': [{'ETag': '', 'Key': 'test.jpg', ..., 'Size': 1024}],
        #    ...
        #    'NextContinuationToken': 'str'
        # }
        response = self._client.list_objects_v2(
            Bucket=self.name, MaxKeys=page_size, Delimiter='/',
            **({'Prefix': prefix} if prefix else {}),
            **({'ContinuationToken': next_token} if next_token else {}),
        )
        files = [f['Key'] for f in response.get('Contents', []) if not f['Key'].endswith('/')]
        directories = [p['Prefix'] for p in response.get('CommonPrefixes', [])]

        return {
            'files': files,
            'directories': directories,
            'next': response.get('NextContinuationToken', None),
        }

    def _download_fileobj_to_stream(self, key: str, stream: BinaryIO, /) -> None:
        self.bucket.download_fileobj(
            Key=key,
            Fileobj=stream,
            Config=TransferConfig(max_io_queue=self.transfer_config['max_io_queue'])
        )

    def _download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int) -> bytes:
        try:
            return self._client.get_object(Bucket=self.bucket.name, Key=key, Range=f'bytes={start_byte}-{stop_byte}')['Body'].read()
        except ClientError as ex:
            if 'InvalidRange' in str(ex):
                if self._head_file(key).get('ContentLength') == 0:
                    slogger.glob.info(f"Attempt to download empty file '{key}' from the '{self.name}' bucket.")
                    raise ValidationError(f'The {key} file is empty.')
                else:
                    slogger.glob.error(f"{str(ex)}. Key: {key}, bucket: {self.name}")
            raise

    def create(self):
        try:
            response = self._bucket.create(
                ACL='private',
                CreateBucketConfiguration={
                    'LocationConstraint': self.region,
                },
                ObjectLockEnabledForBucket=False
            )
            slogger.glob.info(
                'Bucket {} has been created on {} region'.format(
                    self.name,
                    response['Location']
                ))
        except Exception as ex:
            msg = str(ex)
            slogger.glob.info(msg)
            raise Exception(msg)

    def delete_file(self, file_name: str, /):
        try:
            self._client.delete_object(Bucket=self.name, Key=file_name)
        except Exception as ex:
            msg = str(ex)
            slogger.glob.info(msg)
            raise

    @property
    def supported_actions(self):
        allowed_actions = set()
        try:
            bucket_policy = self._bucket.Policy().policy
        except ClientError as ex:
            if 'NoSuchBucketPolicy' in str(ex):
                return Permissions.all()
            else:
                raise Exception(str(ex))
        bucket_policy = json.loads(bucket_policy) if isinstance(bucket_policy, str) else bucket_policy
        for statement in bucket_policy['Statement']:
            effect = statement.get('Effect') # Allow | Deny
            actions = statement.get('Action', set())
            if effect == self.Effect.ALLOW:
                allowed_actions.update(actions)
        access = {
            's3:GetObject': Permissions.READ,
            's3:PutObject': Permissions.WRITE,
        }
        allowed_actions = Permissions.all() & {access.get(i) for i in allowed_actions}

        return allowed_actions

class AzureBlobContainer(_CloudStorage):
    MAX_CONCURRENCY = 3


    class Effect:
        pass

    def __init__(
        self,
        container: str,
        *,
        account_name: Optional[str] = None,
        sas_token: Optional[str] = None,
        connection_string: Optional[str] = None,
        prefix: Optional[str] = None,
    ):
        super().__init__(prefix=prefix)
        self._account_name = account_name
        if connection_string:
            self._blob_service_client = BlobServiceClient.from_connection_string(
                connection_string, proxies=PROXIES_FOR_UNTRUSTED_URLS)
        elif sas_token:
            self._blob_service_client = BlobServiceClient(
                account_url=self.account_url, credential=sas_token, proxies=PROXIES_FOR_UNTRUSTED_URLS)
        else:
            self._blob_service_client = BlobServiceClient(
                account_url=self.account_url, proxies=PROXIES_FOR_UNTRUSTED_URLS)
        self._client = self._blob_service_client.get_container_client(container)

    @property
    def container(self) -> ContainerClient:
        return self._client

    @property
    def name(self) -> str:
        return self._client.container_name

    @property
    def account_url(self) -> Optional[str]:
        if self._account_name:
            return "{}.blob.core.windows.net".format(self._account_name)
        return None

    def create(self):
        try:
            self._client.create_container(
               metadata={
                   'type' : 'created by CVAT',
               },
               public_access=PublicAccess.OFF
            )
        except ResourceExistsError:
            msg = f"{self._client.container_name} already exists"
            slogger.glob.info(msg)
            raise Exception(msg)

    def _head(self):
        return self._client.get_container_properties()

    def _head_file(self, key: str, /):
        blob_client = self.container.get_blob_client(key)
        return blob_client.get_blob_properties()

    @validate_file_status
    @validate_bucket_status
    def get_file_last_modified(self, key: str, /):
        return self._head_file(key).last_modified

    def get_status(self):
        try:
            self._head()
            return Status.AVAILABLE
        except HttpResponseError as ex:
            if  ex.status_code == 403:
                return Status.FORBIDDEN
            else:
                return Status.NOT_FOUND

    def get_file_status(self, key: str, /):
        try:
            self._head_file(key)
            return Status.AVAILABLE
        except HttpResponseError as ex:
            if  ex.status_code == 403:
                return Status.FORBIDDEN
            else:
                return Status.NOT_FOUND

    @validate_bucket_status
    def upload_fileobj(self, file_obj: BinaryIO, key: str, /):
        self._client.upload_blob(name=key, data=file_obj, overwrite=True)

    def upload_file(self, file_path: str, key: str | None = None, /):
        with open(file_path, 'rb') as f:
            self.upload_fileobj(f, key or os.path.basename(file_path))

    def _list_raw_content_on_one_page(
        self,
        prefix: str = "",
        *,
        next_token: Optional[str] = None,
        page_size: int = settings.BUCKET_CONTENT_MAX_PAGE_SIZE,
    ) -> dict:
        page = self._client.walk_blobs(
            maxresults=page_size, results_per_page=page_size, delimiter='/',
            **({'name_starts_with': prefix} if prefix else {})
        ).by_page(continuation_token=next_token)
        all_files = list(next(page))

        files, directories = [], []
        for f in all_files:
            if not isinstance(f, BlobPrefix):
                files.append(f.name)
            else:
                directories.append(f.prefix)

        return {
            'files': files,
            'directories': directories,
            'next': page.continuation_token,
        }

    def _download_fileobj_to_stream(self, key: str, stream: BinaryIO, /) -> None:
        storage_stream_downloader = self._client.download_blob(
            blob=key,
            offset=None,
            length=None,
            max_concurrency=self.MAX_CONCURRENCY,
        )
        storage_stream_downloader.readinto(stream)

    def _download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int) -> bytes:
        return self._client.download_blob(blob=key, offset=start_byte, length=stop_byte).readall()

    @property
    def supported_actions(self):
        pass

class GOOGLE_DRIVE(_CloudStorage):
    pass

def _define_gcs_status(func):
    def wrapper(self, key=None):
        try:
            if not key:
                func(self)
            else:
                func(self, key)
            return Status.AVAILABLE
        except GoogleCloudNotFound:
            return Status.NOT_FOUND
        except GoogleCloudForbidden:
            return Status.FORBIDDEN
    return wrapper

class GoogleCloudStorage(_CloudStorage):

    class Effect:
        pass

    def __init__(
        self,
        bucket_name: str,
        *,
        prefix: Optional[str] = None,
        service_account_json: Optional[Any] = None,
        anonymous_access: bool = False,
        project: Optional[str] = None,
        location: Optional[str] = None,
    ):
        super().__init__(prefix=prefix)
        if service_account_json:
            self._client = storage.Client.from_service_account_json(service_account_json)
        elif anonymous_access:
            self._client = storage.Client.create_anonymous_client()
        else:
            # If no credentials were provided when constructing the client, the
            # client library will look for credentials in the environment.
            self._client = storage.Client()

        self._bucket = self._client.bucket(bucket_name, user_project=project)
        self._bucket_location = location

    @property
    def bucket(self):
        return self._bucket

    @property
    def name(self):
        return self._bucket.name

    def _head(self):
        return self._client.get_bucket(bucket_or_name=self.name)

    def _head_file(self, key: str, /):
        blob = self.bucket.blob(key)
        return self._client._get_resource(blob.path)

    @_define_gcs_status
    def get_status(self):
        self._head()

    @_define_gcs_status
    def get_file_status(self, key: str, /):
        self._head_file(key)

    def _list_raw_content_on_one_page(
        self,
        prefix: str = "",
        *,
        next_token: Optional[str] = None,
        page_size: int = settings.BUCKET_CONTENT_MAX_PAGE_SIZE,
    ) -> dict:
        iterator = self._client.list_blobs(
            bucket_or_name=self.name, max_results=page_size, page_size=page_size,
            fields='items(name),nextPageToken,prefixes', # https://cloud.google.com/storage/docs/json_api/v1/parameters#fields
            delimiter='/',
            **({'prefix': prefix} if prefix else {}),
            **({'page_token': next_token} if next_token else {}),
        )
        # NOTE: we should firstly iterate and only then we can define common prefixes
        files = [f.name for f in iterator if not f.name.endswith('/')] # skip manually created "directories"
        directories = iterator.prefixes

        return {
            'files': files,
            'directories': directories,
            'next': iterator.next_page_token,
        }

    def _download_fileobj_to_stream(self, key: str, stream: BinaryIO, /) -> None:
        blob = self.bucket.blob(key)
        self._client.download_blob_to_file(blob, stream)

    def _download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int) -> bytes:
        with BytesIO() as buff:
            blob = self.bucket.blob(key)
            self._client.download_blob_to_file(blob, buff, start_byte, stop_byte)
            buff.seek(0)
            return buff.getvalue()

    @validate_bucket_status
    def upload_fileobj(self, file_obj: BinaryIO, key: str, /):
        self.bucket.blob(key).upload_from_file(file_obj)

    @validate_bucket_status
    def upload_file(self, file_path: str, key: str | None = None, /):
        self.bucket.blob(key or os.path.basename(file_path)).upload_from_filename(file_path)

    def create(self):
        try:
            self._bucket = self._client.create_bucket(
                self.bucket,
                location=self._bucket_location
            )
            slogger.glob.info(
                'Bucket {} has been created at {} region for {}'.format(
                    self.name,
                    self.bucket.location,
                    self.bucket.user_project,
                ))
        except Exception as ex:
            msg = str(ex)
            slogger.glob.info(msg)
            raise Exception(msg)

    @validate_file_status
    @validate_bucket_status
    def get_file_last_modified(self, key: str, /):
        blob = self.bucket.blob(key)
        blob.reload()
        return blob.updated

    @property
    def supported_actions(self):
        pass

class Credentials:
    __slots__ = ('key', 'secret_key', 'session_token', 'account_name', 'key_file_path', 'credentials_type', 'connection_string')

    def __init__(self, **credentials):
        self.key = credentials.get('key', '')
        self.secret_key = credentials.get('secret_key', '')
        self.session_token = credentials.get('session_token', '')
        self.account_name = credentials.get('account_name', '')
        self.key_file_path = credentials.get('key_file_path', None)
        self.credentials_type = credentials.get('credentials_type', None)
        self.connection_string = credentials.get('connection_string', None)

    def convert_to_db(self):
        converted_credentials = {
            CredentialsTypeChoice.KEY_SECRET_KEY_PAIR : \
                " ".join([self.key, self.secret_key]),
            CredentialsTypeChoice.ACCOUNT_NAME_TOKEN_PAIR : " ".join([self.account_name, self.session_token]),
            CredentialsTypeChoice.KEY_FILE_PATH: self.key_file_path,
            CredentialsTypeChoice.ANONYMOUS_ACCESS: "" if not self.account_name else self.account_name,
            CredentialsTypeChoice.CONNECTION_STRING: self.connection_string,
        }
        return converted_credentials[self.credentials_type]

    def convert_from_db(self, credentials):
        self.credentials_type = credentials.get('type')
        if self.credentials_type == CredentialsTypeChoice.KEY_SECRET_KEY_PAIR:
            self.key, self.secret_key = credentials.get('value').split()
        elif self.credentials_type == CredentialsTypeChoice.ACCOUNT_NAME_TOKEN_PAIR:
            self.account_name, self.session_token = credentials.get('value').split()
        elif self.credentials_type == CredentialsTypeChoice.ANONYMOUS_ACCESS:
            # account_name will be in [some_value, '']
            self.account_name = credentials.get('value')
        elif self.credentials_type == CredentialsTypeChoice.KEY_FILE_PATH:
            self.key_file_path = credentials.get('value')
        elif self.credentials_type == CredentialsTypeChoice.CONNECTION_STRING:
            self.connection_string = credentials.get('value')
        else:
            raise NotImplementedError('Found {} not supported credentials type'.format(self.credentials_type))

    def reset(self, exclusion):
        for i in set(self.__slots__) - exclusion - {'credentials_type'}:
            self.__setattr__(i, '')

    def mapping_with_new_values(self, credentials):
        self.credentials_type = credentials.get('credentials_type', self.credentials_type)
        if self.credentials_type == CredentialsTypeChoice.ANONYMOUS_ACCESS:
            self.reset(exclusion={'account_name'})
            self.account_name = credentials.get('account_name', self.account_name)
        elif self.credentials_type == CredentialsTypeChoice.KEY_SECRET_KEY_PAIR:
            self.reset(exclusion={'key', 'secret_key'})
            self.key = credentials.get('key', self.key)
            self.secret_key = credentials.get('secret_key', self.secret_key)
        elif self.credentials_type == CredentialsTypeChoice.ACCOUNT_NAME_TOKEN_PAIR:
            self.reset(exclusion={'session_token', 'account_name'})
            self.session_token = credentials.get('session_token', self.session_token)
            self.account_name = credentials.get('account_name', self.account_name)
        elif self.credentials_type == CredentialsTypeChoice.KEY_FILE_PATH:
            self.reset(exclusion={'key_file_path'})
            self.key_file_path = credentials.get('key_file_path', self.key_file_path)
        elif self.credentials_type == CredentialsTypeChoice.CONNECTION_STRING:
            self.reset(exclusion={'connection_string'})
            self.connection_string = credentials.get('connection_string', self.connection_string)
        else:
            raise NotImplementedError('Mapping credentials: unsupported credentials type')


    def values(self):
        return [self.key, self.secret_key, self.session_token, self.account_name, self.key_file_path]

def db_storage_to_storage_instance(db_storage):
    credentials = Credentials()
    credentials.convert_from_db({
        'type': db_storage.credentials_type,
        'value': db_storage.credentials,
    })
    details = {
        'resource': db_storage.resource,
        'credentials': credentials,
        'specific_attributes': db_storage.get_specific_attributes()
    }
    return get_cloud_storage_instance(cloud_provider=db_storage.provider_type, **details)

T = TypeVar('T', Callable[[str, int, int], int], Callable[[str, int, str, bool], None])

def import_resource_from_cloud_storage(
    db_storage: Any,
    key: str,
    cleanup_func: Callable[[T, str,], Any],
    import_func: T,
    filename: str,
    *args,
    **kwargs,
) -> Any:
    storage = db_storage_to_storage_instance(db_storage)
    storage.download_file(key, filename)

    return cleanup_func(import_func, filename, *args, **kwargs)

def export_resource_to_cloud_storage(
    db_storage: Any,
    func: Callable[[int, str | None, str | None], str],
    *args,
    **kwargs,
) -> str:
    rq_job = get_current_job()
    assert rq_job, "func can be executed only from a background job"

    file_path = func(*args, **kwargs)
    rq_job_meta = ExportRQMeta.for_job(rq_job)

    storage = db_storage_to_storage_instance(db_storage)
    storage.upload_file(file_path, rq_job_meta.result_filename)

    return file_path


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\default_settings.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import logging as log
import os

from attrs.converters import to_bool
from django.core.exceptions import ImproperlyConfigured

logger = log.getLogger("cvat")

MEDIA_CACHE_ALLOW_STATIC_CACHE = to_bool(os.getenv("CVAT_ALLOW_STATIC_CACHE", False))
"""
Allow or disallow static media cache.
If disabled, CVAT will only use the dynamic media cache. New tasks requesting static media cache
will be automatically switched to the dynamic cache.
When enabled, this option can increase data access speed and reduce server load,
but significantly increase disk space occupied by tasks.
"""

CVAT_CHUNK_CREATE_TIMEOUT = 50
"""
Sets the chunk preparation timeout in seconds after which the backend will respond with 429 code.
"""

CVAT_CHUNK_CREATE_CHECK_INTERVAL = 0.2
"""
Sets the frequency of checking the readiness of the chunk
"""
default_export_cache_ttl = 60 * 60 * 24
default_export_cache_lock_ttl = 30
default_export_cache_lock_acquisition_timeout = 50
default_export_locked_retry_interval = 60

EXPORT_CACHE_TTL = os.getenv("CVAT_DATASET_CACHE_TTL")
"Base lifetime for cached export files, in seconds"

if EXPORT_CACHE_TTL is not None:
    EXPORT_CACHE_TTL = int(EXPORT_CACHE_TTL)
    logger.warning(
        "The CVAT_DATASET_CACHE_TTL is deprecated, use CVAT_EXPORT_CACHE_TTL instead",
    )
else:
    EXPORT_CACHE_TTL = int(os.getenv("CVAT_EXPORT_CACHE_TTL", default_export_cache_ttl))


EXPORT_CACHE_LOCK_TTL = os.getenv("CVAT_DATASET_EXPORT_LOCK_TTL")
"Default lifetime for the export cache lock, in seconds."

if EXPORT_CACHE_LOCK_TTL is not None:
    EXPORT_CACHE_LOCK_TTL = int(EXPORT_CACHE_LOCK_TTL)
    logger.warning(
        "The CVAT_DATASET_EXPORT_LOCK_TTL is deprecated, use CVAT_EXPORT_CACHE_LOCK_TTL instead",
    )
else:
    EXPORT_CACHE_LOCK_TTL = int(
        os.getenv("CVAT_EXPORT_CACHE_LOCK_TTL", default_export_cache_lock_ttl)
    )

EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT = os.getenv("CVAT_DATASET_CACHE_LOCK_TIMEOUT")
"Timeout for cache lock acquiring, in seconds"

if EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT is not None:
    EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT = int(EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT)
    logger.warning(
        "The CVAT_DATASET_CACHE_LOCK_TIMEOUT is deprecated, "
        "use CVAT_EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT instead",
    )
else:
    EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT = int(
        os.getenv(
            "CVAT_EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT",
            default_export_cache_lock_acquisition_timeout,
        )
    )

if EXPORT_CACHE_LOCK_ACQUISITION_TIMEOUT <= EXPORT_CACHE_LOCK_TTL:
    raise ImproperlyConfigured("Lock acquisition timeout must be more than lock TTL")

EXPORT_LOCKED_RETRY_INTERVAL = os.getenv("CVAT_DATASET_EXPORT_LOCKED_RETRY_INTERVAL")
"Retry interval for cases the export cache lock was unavailable, in seconds"

if EXPORT_LOCKED_RETRY_INTERVAL is not None:
    EXPORT_LOCKED_RETRY_INTERVAL = int(EXPORT_LOCKED_RETRY_INTERVAL)
    logger.warning(
        "The CVAT_DATASET_EXPORT_LOCKED_RETRY_INTERVAL is deprecated, "
        "use CVAT_EXPORT_LOCKED_RETRY_INTERVAL instead",
    )
else:
    EXPORT_LOCKED_RETRY_INTERVAL = int(
        os.getenv("CVAT_EXPORT_LOCKED_RETRY_INTERVAL", default_export_locked_retry_interval)
    )

MAX_CONSENSUS_REPLICAS = int(os.getenv("CVAT_MAX_CONSENSUS_REPLICAS", 11))
if MAX_CONSENSUS_REPLICAS < 1:
    raise ImproperlyConfigured(f"MAX_CONSENSUS_REPLICAS must be >= 1, got {MAX_CONSENSUS_REPLICAS}")

DEFAULT_DB_BULK_CREATE_BATCH_SIZE = int(os.getenv("CVAT_DEFAULT_DB_BULK_CREATE_BATCH_SIZE", 5000))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\field_validation.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from collections.abc import Sequence
from typing import Any

from rest_framework import serializers


def require_one_of_fields(data: dict[str, Any], keys: Sequence[str]) -> None:
    active_count = sum(key in data for key in keys)
    if active_count == 1:
        return

    options = ", ".join(f'"{k}"' for k in keys)

    if not active_count:
        raise serializers.ValidationError(f"One of the fields {options} required")
    else:
        raise serializers.ValidationError(f"Only 1 of the fields {options} can be used")


def require_field(data: dict[str, Any], key: str) -> None:
    if key not in data:
        raise serializers.ValidationError(f'The "{key}" field is required')


def require_one_of_values(data: dict[str, Any], key: str, values: Sequence[Any]) -> None:
    assert values

    if data.get(key) not in values:
        if len(values) == 1:
            raise serializers.ValidationError(
                'The "{}" field must be {}'.format(key, ", ".join(f"{k}" for k in values))
            )
        else:
            raise serializers.ValidationError(
                'The "{}" field must be one of {}'.format(key, ", ".join(f"{k}" for k in values))
            )


def validate_share(value: float) -> float:
    if not 0 <= value <= 1:
        raise serializers.ValidationError("Value must be in the range [0; 1]")

    return value


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\filters.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import operator
from collections.abc import Iterable, Iterator
from functools import reduce
from textwrap import dedent
from typing import Any, Optional

from django.db.models import Q
from django.db.models.query import QuerySet
from django.utils.encoding import force_str
from django.utils.translation import gettext_lazy as _
from django_filters import FilterSet
from django_filters import filters as djf
from django_filters.filterset import BaseFilterSet
from django_filters.rest_framework import DjangoFilterBackend
from rest_framework import filters
from rest_framework.compat import coreapi, coreschema
from rest_framework.exceptions import ValidationError

from cvat.apps.engine.types import ExtendedRequest

DEFAULT_FILTER_FIELDS_ATTR = 'filter_fields'
DEFAULT_LOOKUP_MAP_ATTR = 'lookup_fields'

def get_lookup_fields(view, fields: Optional[Iterator[str]] = None) -> dict[str, str]:
    if fields is None:
        fields = getattr(view, DEFAULT_FILTER_FIELDS_ATTR, None) or []

    lookup_overrides = getattr(view, DEFAULT_LOOKUP_MAP_ATTR, None) or {}
    lookup_fields = {
        field: lookup_overrides.get(field, field)
        for field in fields
    }
    return lookup_fields


class SearchFilter(filters.SearchFilter):
    def get_search_fields(self, view, request: ExtendedRequest):
        search_fields = getattr(view, 'search_fields') or []
        return get_lookup_fields(view, search_fields).values()

    def get_schema_fields(self, view):
        assert coreapi is not None, 'coreapi must be installed to use `get_schema_fields()`'
        assert coreschema is not None, 'coreschema must be installed to use `get_schema_fields()`'

        search_fields = getattr(view, 'search_fields', [])
        full_description = self.search_description + \
            f' Available search_fields: {search_fields}'

        return [
            coreapi.Field(
                name=self.search_param,
                required=False,
                location='query',
                schema=coreschema.String(
                    title=force_str(self.search_title),
                    description=force_str(full_description)
                )
            )
        ] if search_fields else []

    def get_schema_operation_parameters(self, view):
        search_fields = getattr(view, 'search_fields', [])
        full_description = self.search_description + \
            f' Available search_fields: {search_fields}'

        return [{
            'name': self.search_param,
            'required': False,
            'in': 'query',
            'description': force_str(full_description),
            'schema': {
                'type': 'string',
            },
        }] if search_fields else []

class OrderingFilter(filters.OrderingFilter):
    ordering_param = 'sort'
    reverse_flag = "-"

    def get_ordering(self, request: ExtendedRequest, queryset, view):
        ordering = []
        lookup_fields = self._get_lookup_fields(request, queryset, view)
        for term in super().get_ordering(request, queryset, view):
            flag = ''
            if term.startswith(self.reverse_flag):
                flag = self.reverse_flag
                term = term[len(flag):]
            ordering.append(flag + lookup_fields[term])

        return ordering

    def _get_lookup_fields(self, request: ExtendedRequest, queryset: QuerySet, view):
        ordering_fields = self.get_valid_fields(queryset, view, {'request': request})
        ordering_fields = [v[0] for v in ordering_fields]
        return get_lookup_fields(view, ordering_fields)

    def get_schema_fields(self, view):
        assert coreapi is not None, 'coreapi must be installed to use `get_schema_fields()`'
        assert coreschema is not None, 'coreschema must be installed to use `get_schema_fields()`'

        ordering_fields = getattr(view, 'ordering_fields', [])
        full_description = self.ordering_description + \
            f' Available ordering_fields: {ordering_fields}'

        return [
            coreapi.Field(
                name=self.ordering_param,
                required=False,
                location='query',
                schema=coreschema.String(
                    title=force_str(self.ordering_title),
                    description=force_str(full_description)
                )
            )
        ] if ordering_fields else []

    def get_schema_operation_parameters(self, view):
        ordering_fields = getattr(view, 'ordering_fields', [])
        full_description = self.ordering_description + \
            f' Available ordering_fields: {ordering_fields}'

        return [{
            'name': self.ordering_param,
            'required': False,
            'in': 'query',
            'description': force_str(full_description),
            'schema': {
                'type': 'string',
            },
        }] if ordering_fields else []

class JsonLogicFilter(filters.BaseFilterBackend):
    Rules = dict[str, Any]
    filter_param = 'filter'
    filter_title = _('Filter')
    filter_description = _(dedent("""
        JSON Logic filter. This filter can be used to perform complex filtering by grouping rules.\n
        For example, using such a filter you can get all resources created by you:\n
            - {"and":[{"==":[{"var":"owner"},"<user>"]}]}\n
        Details about the syntax used can be found at the link: https://jsonlogic.com/\n
    """))

    def _build_Q(self, rules, lookup_fields):
        op, args = next(iter(rules.items()))
        if op in ['or', 'and']:
            return reduce({
                'or': operator.or_,
                'and': operator.and_
            }[op], [self._build_Q(arg, lookup_fields) for arg in args])
        elif op == '!':
            return ~self._build_Q(args, lookup_fields)
        elif op == '!!':
            return self._build_Q(args, lookup_fields)
        elif op == 'var':
            return Q(**{args + '__isnull': False})
        elif op in ['==', '<', '>', '<=', '>='] and len(args) == 2:
            var = lookup_fields[args[0]['var']]
            q_var = var + {
                '==': '',
                '<': '__lt',
                '<=': '__lte',
                '>': '__gt',
                '>=': '__gte'
            }[op]
            return Q(**{q_var: args[1]})
        elif op == 'in':
            if isinstance(args[0], dict):
                var = lookup_fields[args[0]['var']]
                return Q(**{var + '__in': args[1]})
            else:
                var = lookup_fields[args[1]['var']]
                return Q(**{var + '__contains': args[0]})
        elif op == '<=' and len(args) == 3:
            var = lookup_fields[args[1]['var']]
            return Q(**{var + '__gte': args[0]}) & Q(**{var + '__lte': args[2]})
        else:
            raise ValidationError(f'filter: {op} operation with {args} arguments is not implemented')

    def _parse_query(self, json_rules: str) -> Rules:
        try:
            rules = json.loads(json_rules)
            if not len(rules):
                raise ValidationError(f"filter shouldn't be empty")
        except json.decoder.JSONDecodeError:
            raise ValidationError(f'filter: Json syntax should be used')

        return rules

    def apply_filter(self,
        queryset: QuerySet, parsed_rules: Rules, *, lookup_fields: dict[str, Any]
    ) -> QuerySet:
        try:
            q_object = self._build_Q(parsed_rules, lookup_fields)
        except KeyError as ex:
            raise ValidationError(f'filter: {str(ex)} term is not supported')

        return queryset.filter(q_object)

    def filter_queryset(self, request: ExtendedRequest, queryset: QuerySet, view):
        json_rules = request.query_params.get(self.filter_param)
        if json_rules:
            parsed_rules = self._parse_query(json_rules)
            lookup_fields = self._get_lookup_fields(view)
            queryset = self.apply_filter(queryset, parsed_rules, lookup_fields=lookup_fields)

        return queryset

    def get_schema_fields(self, view):
        assert coreapi is not None, 'coreapi must be installed to use `get_schema_fields()`'
        assert coreschema is not None, 'coreschema must be installed to use `get_schema_fields()`'

        filter_fields = getattr(view, 'filter_fields', [])
        filter_description = getattr(view, 'filter_description', '')
        full_description = self.filter_description + \
            f' Available filter_fields: {filter_fields}.' + filter_description

        return [
            coreapi.Field(
                name=self.filter_param,
                required=False,
                location='query',
                schema=coreschema.String(
                    title=force_str(self.filter_title),
                    description=force_str(full_description)
                )
            )
        ] if filter_fields else []

    def get_schema_operation_parameters(self, view):
        filter_fields = getattr(view, 'filter_fields', [])
        filter_description = getattr(view, 'filter_description', '')
        full_description = self.filter_description + \
            f' Available filter_fields: {filter_fields}.' + filter_description
        return [
            {
                'name': self.filter_param,
                'required': False,
                'in': 'query',
                'description': force_str(full_description),
                'schema': {
                    'type': 'string',
                },
            },
        ] if filter_fields else []

    def _get_lookup_fields(self, view):
        return get_lookup_fields(view)


class SimpleFilter(DjangoFilterBackend):
    """
    A simple filter, useful for small search queries and manually-edited
    requests.

    Argument types are numbers and strings. The only available check is equality.
    Operators are not supported (e.g. or, less, greater, not etc.).
    Multiple filters are joined with '&' as separate query params.
    """

    filter_desc = _('A simple equality filter for the {field_name} field')
    reserved_names = (
        JsonLogicFilter.filter_param,
        OrderingFilter.ordering_param,
        SearchFilter.search_param,
    )

    filter_fields_attr = 'simple_filters'

    class MappingFiltersetBase(BaseFilterSet):
        _filter_name_map_attr = 'filter_names'

        @classmethod
        def get_filter_name(cls, field_name, lookup_expr):
            filter_names = getattr(cls, cls._filter_name_map_attr, {})

            field_name = super().get_filter_name(field_name, lookup_expr)

            if filter_names:
                # Map names after a lookup suffix is applied to allow
                # mapping specific filters with lookups
                field_name = filter_names.get(field_name, field_name)

            if field_name in SimpleFilter.reserved_names:
                raise ValueError(f'Field name {field_name} is reserved')

            return field_name

    filterset_base = MappingFiltersetBase


    def get_filterset_class(self, view, queryset=None):
        lookup_fields = self.get_lookup_fields(view)
        if not lookup_fields or queryset is None:
            return None

        MetaBase = getattr(self.filterset_base, 'Meta', object)

        class AutoFilterSet(self.filterset_base, metaclass=FilterSet.__class__):
            filter_names = { v: k for k, v in lookup_fields.items() }

            class Meta(MetaBase): # pylint: disable=useless-object-inheritance
                model = queryset.model
                fields = list(lookup_fields.values())

        return AutoFilterSet

    def get_lookup_fields(self, view):
        simple_filters = getattr(view, self.filter_fields_attr, None)
        if simple_filters:
            for k in self.reserved_names:
                assert k not in simple_filters, \
                    f"Query parameter '{k}' is reserved, try to change the filter name."

        return get_lookup_fields(view, fields=simple_filters)

    def get_schema_operation_parameters(self, view):
        queryset = view.queryset

        filterset_class = self.get_filterset_class(view, queryset)
        if not filterset_class:
            return []

        parameters = []
        for field_name, filter_ in filterset_class.base_filters.items():
            if isinstance(filter_, djf.BooleanFilter):
                parameter_schema = { 'type': 'boolean' }
            elif isinstance(filter_, (djf.NumberFilter, djf.ModelChoiceFilter)):
                parameter_schema = { 'type': 'integer' }
            elif isinstance(filter_, (djf.CharFilter, djf.ChoiceFilter)):
                # Choices use their labels as filter values
                parameter_schema = { 'type': 'string' }
            else:
                raise Exception("Filter field '{}' type '{}' is not supported".format(
                    '.'.join([view.basename, view.action, field_name]),
                    filter_
                ))

            parameter = {
                'name': field_name,
                'in': 'query',
                'description': force_str(self.filter_desc.format_map({
                    'field_name': filter_.label if filter_.label is not None else field_name
                })),
                'schema': parameter_schema,
            }
            if filter_.extra and 'choices' in filter_.extra:
                parameter['schema']['enum'] = [c[0] for c in filter_.extra['choices']]
            parameters.append(parameter)
        return parameters


class _NestedAttributeHandler:
    nested_attribute_separator = '.'

    class DotDict(dict):
        """recursive dot.notation access to dictionary attributes"""
        __getattr__ = dict.get
        __setattr__ = dict.__setitem__
        __delattr__ = dict.__delitem__

        def __init__(self, dct: dict):
            for key, value in dct.items():
                if isinstance(value, dict):
                    value = self.__class__(value)
                self[key] = value

    def get_nested_attr(self, obj: Any, nested_attr_path: str) -> Any:
        result = obj
        for attribute in nested_attr_path.split(self.nested_attribute_separator):
            if isinstance(result, dict):
                result = self.DotDict(result)
            result = getattr(result, attribute)

        if callable(result):
            result = result()

        return result

class NonModelSimpleFilter(SimpleFilter, _NestedAttributeHandler):
    """
    A simple filter backend for non-model views, useful for small search queries and manually-edited
    requests.

    Argument types are numbers and strings. The only available check is equality.
    Operators are not supported (e.g. or, less, greater, not etc.).
    """

    def get_schema_operation_parameters(self, view):
        simple_filters = getattr(view, self.filter_fields_attr, None)
        simple_filters_schema = getattr(view, 'simple_filters_schema', None)

        parameters = []
        if simple_filters and simple_filters_schema:
            for filter_name in simple_filters:
                filter_type, filter_choices = simple_filters_schema[filter_name]
                parameter = {
                    'name': filter_name,
                    'in': 'query',
                    'description': force_str(self.filter_desc.format_map({
                        'field_name': filter_name
                    })),
                    'schema': {
                        'type': filter_type
                    },
                }
                if filter_choices:
                    parameter['schema']['enum'] = [c[0] for c in filter_choices]
                parameters.append(parameter)
        return parameters

    def filter_queryset(self, request: ExtendedRequest, queryset: Iterable, view):
        filtered_queryset = queryset

        query_params = request.query_params
        filters_to_use = set(query_params.keys())

        simple_filters = getattr(view, self.filter_fields_attr, None)
        lookup_fields = self.get_lookup_fields(view)

        if simple_filters and lookup_fields and (intersection := filters_to_use & set(simple_filters)):
            filtered_queryset = []

            for obj in queryset:
                fits_filter = False
                for field in intersection:
                    query_param = query_params[field]

                    if query_param.isdigit():
                        query_param = int(query_param)

                    # replace empty string with None
                    if field == 'org' and not query_param:
                        query_param = None

                    fits_filter = self.get_nested_attr(obj, lookup_fields[field]) == query_param
                    if not fits_filter:
                        break

                if fits_filter:
                    filtered_queryset.append(obj)

        return filtered_queryset

class NonModelOrderingFilter(OrderingFilter, _NestedAttributeHandler):
    """Ordering filter for non-model views.
    This filter backend supports the following syntaxes:
    ?sort=field
    ?sort=-field
    ?sort=field1,field2
    ?sort=-field1,-field2
    """

    def get_ordering(self, request: ExtendedRequest, queryset: Iterable, view) -> tuple[list[str], bool]:
        ordering = super().get_ordering(request, queryset, view)
        result, reverse = [], False
        for field in ordering:
            if field.startswith(self.reverse_flag):
                reverse = True
                field = field[len(self.reverse_flag):]
            result.append(field)

        return result, reverse

    def filter_queryset(self, request: ExtendedRequest, queryset: Iterable, view) -> Iterable:
        ordering, reverse = self.get_ordering(request, queryset, view)

        if ordering:
            return sorted(queryset, key=lambda obj: [self.get_nested_attr(obj, field) for field in ordering], reverse=reverse)

        return queryset


class NonModelJsonLogicFilter(JsonLogicFilter, _NestedAttributeHandler):
    filter_description = _(dedent("""
        JSON Logic filter. This filter can be used to perform complex filtering by grouping rules.\n
        Details about the syntax used can be found at the link: https://jsonlogic.com/\n
    """))

    def _apply_filter(self, rules, lookup_fields, obj):
        op, args = next(iter(rules.items()))
        if op in ['or', 'and']:
            return reduce({
                'or': any,
                'and': all,
            }[op], [self._apply_filter(arg, lookup_fields, obj) for arg in args])
        elif op == '!':
            return not self._apply_filter(args, lookup_fields, obj)
        elif op == 'var':
            var = lookup_fields[args]
            var_value = self.get_nested_attr(obj, var)
            return var_value is not None
        elif op in ['!=', '==', '<', '>', '<=', '>='] and len(args) == 2:
            var = lookup_fields[args[0]['var']]
            var_value = self.get_nested_attr(obj, var)
            return {
                '!=': operator.ne,
                '==': operator.eq,
                '<': operator.lt,
                '<=': operator.le,
                '>': operator.gt,
                '>=': operator.ge,
            }[op](var_value, args[1])
        elif op == 'in':
            if isinstance(args[0], dict):
                var = lookup_fields[args[0]['var']]
                var_value = self.get_nested_attr(obj, var)
                return operator.contains(args[1], var_value)
            else:
                var = lookup_fields[args[1]['var']]
                var_value = self.get_nested_attr(obj, var)
                return operator.contains(args[0], var_value)
        elif op == '<=' and len(args) == 3:
            var = lookup_fields[args[1]['var']]
            var_value = self.get_nested_attr(obj, var)
            return var_value >= args[0] and var_value <= args[2]
        else:
            raise ValidationError(f'filter: {op} operation with {args} arguments is not implemented')

    def filter_queryset(self, request: ExtendedRequest, queryset: Iterable, view) -> Iterable:
        filtered_queryset = queryset
        json_rules = request.query_params.get(self.filter_param)
        if json_rules:
            filtered_queryset = []
            parsed_rules = self._parse_query(json_rules)
            lookup_fields = self._get_lookup_fields(view)

            for obj in queryset:
                fits_filter = self._apply_filter(parsed_rules, lookup_fields, obj)
                if fits_filter:
                    filtered_queryset.append(obj)

        return filtered_queryset


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\frame_provider.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import itertools
import math
from abc import ABCMeta, abstractmethod
from bisect import bisect
from collections import OrderedDict
from collections.abc import Iterator, Sequence
from dataclasses import dataclass
from enum import Enum, auto
from io import BytesIO
from typing import Any, Callable, Generic, Optional, TypeVar, Union, overload

import av
import cv2
import numpy as np
from django.conf import settings
from PIL import Image
from rest_framework.exceptions import ValidationError

from cvat.apps.engine import models
from cvat.apps.engine.cache import Callback, DataWithMime, MediaCache, prepare_chunk
from cvat.apps.engine.media_extractors import (
    FrameQuality,
    IMediaReader,
    RandomAccessIterator,
    VideoReader,
    ZipReader,
)
from cvat.apps.engine.mime_types import mimetypes
from cvat.apps.engine.utils import take_by

_T = TypeVar("_T")


class _ChunkLoader(metaclass=ABCMeta):
    def __init__(
        self,
        reader_class: type[IMediaReader],
        *,
        reader_params: Optional[dict] = None,
    ) -> None:
        self.chunk_id: Optional[int] = None
        self.chunk_reader: Optional[RandomAccessIterator] = None
        self.reader_class = reader_class
        self.reader_params = reader_params

    def load(self, chunk_id: int) -> RandomAccessIterator[tuple[Any, str, int]]:
        if self.chunk_id != chunk_id:
            self.unload()

            self.chunk_id = chunk_id
            self.chunk_reader = RandomAccessIterator(
                self.reader_class(
                    [self.read_chunk(chunk_id)[0]],
                    **(self.reader_params or {}),
                )
            )
        return self.chunk_reader

    def unload(self):
        self.chunk_id = None
        if self.chunk_reader:
            self.chunk_reader.close()
            self.chunk_reader = None

    @abstractmethod
    def read_chunk(self, chunk_id: int) -> DataWithMime: ...


class _FileChunkLoader(_ChunkLoader):
    def __init__(
        self,
        reader_class: type[IMediaReader],
        get_chunk_path_callback: Callable[[int], str],
        *,
        reader_params: Optional[dict] = None,
    ) -> None:
        super().__init__(reader_class, reader_params=reader_params)
        self.get_chunk_path = get_chunk_path_callback

    def read_chunk(self, chunk_id: int) -> DataWithMime:
        chunk_path = self.get_chunk_path(chunk_id)
        with open(chunk_path, "rb") as f:
            return (
                io.BytesIO(f.read()),
                mimetypes.guess_type(chunk_path)[0],
            )


class _BufferChunkLoader(_ChunkLoader):
    def __init__(
        self,
        reader_class: type[IMediaReader],
        get_chunk_callback: Callable[[int], DataWithMime],
        *,
        reader_params: Optional[dict] = None,
    ) -> None:
        super().__init__(reader_class, reader_params=reader_params)
        self.get_chunk = get_chunk_callback

    def read_chunk(self, chunk_id: int) -> DataWithMime:
        return self.get_chunk(chunk_id)


class FrameOutputType(Enum):
    BUFFER = auto()
    PIL = auto()
    NUMPY_ARRAY = auto()


Frame2d = Union[BytesIO, np.ndarray, Image.Image]
Frame3d = BytesIO
AnyFrame = Union[Frame2d, Frame3d]


@dataclass
class DataWithMeta(Generic[_T]):
    data: _T
    mime: str


class IFrameProvider(metaclass=ABCMeta):
    VIDEO_FRAME_EXT = ".PNG"
    VIDEO_FRAME_MIME = "image/png"

    def unload(self):
        pass

    @classmethod
    def _av_frame_to_png_bytes(cls, av_frame: av.VideoFrame) -> BytesIO:
        ext = cls.VIDEO_FRAME_EXT
        image = av_frame.to_ndarray(format="bgr24")
        success, result = cv2.imencode(ext, image)
        if not success:
            raise RuntimeError(f"Failed to encode image to '{ext}' format")
        return BytesIO(result.tobytes())

    def _convert_frame(
        self, frame: Any, reader_class: type[IMediaReader], out_type: FrameOutputType
    ) -> AnyFrame:
        if out_type == FrameOutputType.BUFFER:
            return (
                self._av_frame_to_png_bytes(frame)
                if issubclass(reader_class, VideoReader)
                else frame
            )
        elif out_type == FrameOutputType.PIL:
            return frame.to_image() if issubclass(reader_class, VideoReader) else Image.open(frame)
        elif out_type == FrameOutputType.NUMPY_ARRAY:
            if issubclass(reader_class, VideoReader):
                image = frame.to_ndarray(format="bgr24")
            else:
                image = np.array(Image.open(frame))
                if len(image.shape) == 3 and image.shape[2] in {3, 4}:
                    image[:, :, :3] = image[:, :, 2::-1]  # RGB to BGR
            return image
        else:
            raise RuntimeError("unsupported output type")

    @abstractmethod
    def validate_frame_number(self, frame_number: int) -> int: ...

    @abstractmethod
    def validate_chunk_number(self, chunk_number: int) -> int: ...

    @abstractmethod
    def get_chunk_number(self, frame_number: int) -> int: ...

    @abstractmethod
    def get_preview(self) -> DataWithMeta[BytesIO]: ...

    @abstractmethod
    def get_chunk(
        self, chunk_number: int, *, quality: FrameQuality = FrameQuality.ORIGINAL
    ) -> DataWithMeta[BytesIO]: ...

    @abstractmethod
    def get_frame(
        self,
        frame_number: int,
        *,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> DataWithMeta[AnyFrame]: ...

    @abstractmethod
    def get_frame_context_images_chunk(
        self,
        frame_number: int,
    ) -> Optional[DataWithMeta[BytesIO]]: ...

    @abstractmethod
    def iterate_frames(
        self,
        *,
        start_frame: Optional[int] = None,
        stop_frame: Optional[int] = None,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> Iterator[DataWithMeta[AnyFrame]]: ...

    def _get_abs_frame_number(self, db_data: models.Data, rel_frame_number: int) -> int:
        return db_data.start_frame + rel_frame_number * db_data.get_frame_step()

    def _get_rel_frame_number(self, db_data: models.Data, abs_frame_number: int) -> int:
        return (abs_frame_number - db_data.start_frame) // db_data.get_frame_step()


class TaskFrameProvider(IFrameProvider):
    def __init__(self, db_task: models.Task) -> None:
        self._db_task = db_task

    def validate_frame_number(self, frame_number: int) -> int:
        if frame_number not in range(0, self._db_task.data.size):
            raise ValidationError(
                f"Invalid frame '{frame_number}'. "
                f"The frame number should be in the [0, {self._db_task.data.size}] range"
            )

        return frame_number

    def validate_chunk_number(self, chunk_number: int) -> int:
        last_chunk = math.ceil(self._db_task.data.size / self._db_task.data.chunk_size) - 1
        if not 0 <= chunk_number <= last_chunk:
            raise ValidationError(
                f"Invalid chunk number '{chunk_number}'. "
                f"The chunk number should be in the [0, {last_chunk}] range"
            )

        return chunk_number

    def get_chunk_number(self, frame_number: int) -> int:
        return int(frame_number) // self._db_task.data.chunk_size

    def get_abs_frame_number(self, rel_frame_number: int) -> int:
        "Returns absolute frame number in the task (in the range [start, stop, step])"
        return super()._get_abs_frame_number(self._db_task.data, rel_frame_number)

    def get_rel_frame_number(self, abs_frame_number: int) -> int:
        """
        Returns relative frame number in the task (in the range [0, task_size - 1]).
        This is the "normal" frame number, expected in other methods.
        """
        return super()._get_rel_frame_number(self._db_task.data, abs_frame_number)

    def get_preview(self) -> DataWithMeta[BytesIO]:
        return self._get_segment_frame_provider(0).get_preview()

    def get_chunk(
        self, chunk_number: int, *, quality: FrameQuality = FrameQuality.ORIGINAL
    ) -> DataWithMeta[BytesIO]:
        return_type = DataWithMeta[BytesIO]
        chunk_number = self.validate_chunk_number(chunk_number)

        cache = MediaCache()
        cached_chunk = cache.get_task_chunk(self._db_task, chunk_number, quality=quality)
        if cached_chunk:
            return return_type(cached_chunk[0], cached_chunk[1])

        db_data = self._db_task.data
        step = db_data.get_frame_step()
        task_chunk_start_frame = chunk_number * db_data.chunk_size
        task_chunk_stop_frame = (chunk_number + 1) * db_data.chunk_size - 1
        task_chunk_frame_set = set(
            range(
                db_data.start_frame + task_chunk_start_frame * step,
                min(db_data.start_frame + task_chunk_stop_frame * step, db_data.stop_frame) + step,
                step,
            )
        )

        matching_segments: list[models.Segment] = sorted(
            [
                s
                for s in self._db_task.segment_set.all()
                if not task_chunk_frame_set.isdisjoint(s.frame_set)
            ],
            key=lambda s: (
                s.type != models.SegmentType.RANGE,  # prioritize RANGE segments,
                s.start_frame,
            ),
        )
        assert matching_segments

        # Don't put this into set_callback to avoid data duplication in the cache

        if len(matching_segments) == 1:
            segment_frame_provider = SegmentFrameProvider(matching_segments[0])
            matching_chunk_index = segment_frame_provider.find_matching_chunk(
                sorted(task_chunk_frame_set)
            )
            if matching_chunk_index is not None:
                # The requested frames match one of the job chunks, we can use it directly
                return segment_frame_provider.get_chunk(matching_chunk_index, quality=quality)

        buffer, mime_type = cache.get_or_set_task_chunk(
            self._db_task,
            chunk_number,
            quality=quality,
            set_callback=Callback(
                callable=self._get_chunk_create_callback,
                args=[
                    self._db_task,
                    matching_segments,
                    {f: self.get_rel_frame_number(f) for f in task_chunk_frame_set},
                    quality,
                ],
            ),
        )

        return return_type(data=buffer, mime=mime_type)

    @staticmethod
    def _get_chunk_create_callback(
        db_task: Union[models.Task, int],
        matching_segments: list[models.Segment],
        task_chunk_frames_with_rel_numbers: dict[int, int],
        quality: FrameQuality,
    ) -> DataWithMime:
        # Create and return a joined / cleaned chunk
        task_chunk_frames = OrderedDict()
        for db_segment in matching_segments:
            if isinstance(db_segment, int):
                db_segment = models.Segment.objects.get(pk=db_segment)
            segment_frame_provider = SegmentFrameProvider(db_segment)
            segment_frame_set = db_segment.frame_set

            for task_chunk_frame_id in sorted(task_chunk_frames_with_rel_numbers.keys()):
                if (
                    task_chunk_frame_id not in segment_frame_set
                    or task_chunk_frame_id in task_chunk_frames
                ):
                    continue

                frame, frame_name, _ = segment_frame_provider._get_raw_frame(
                    task_chunk_frames_with_rel_numbers[task_chunk_frame_id], quality=quality
                )
                task_chunk_frames[task_chunk_frame_id] = (frame, frame_name, None)

        if isinstance(db_task, int):
            db_task = models.Task.objects.get(pk=db_task)

        return prepare_chunk(
            task_chunk_frames.values(),
            quality=quality,
            db_task=db_task,
            dump_unchanged=True,
        )

    def get_frame(
        self,
        frame_number: int,
        *,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> DataWithMeta[AnyFrame]:
        return self._get_segment_frame_provider(frame_number).get_frame(
            frame_number, quality=quality, out_type=out_type
        )

    def get_frame_context_images_chunk(
        self,
        frame_number: int,
    ) -> Optional[DataWithMeta[BytesIO]]:
        return self._get_segment_frame_provider(frame_number).get_frame_context_images_chunk(
            frame_number
        )

    def iterate_frames(
        self,
        *,
        start_frame: Optional[int] = None,
        stop_frame: Optional[int] = None,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> Iterator[DataWithMeta[AnyFrame]]:
        frame_range = itertools.count(start_frame)
        if stop_frame:
            frame_range = itertools.takewhile(lambda x: x <= stop_frame, frame_range)

        db_segment = None
        db_segment_frame_set = None
        db_segment_frame_provider = None
        for idx in frame_range:
            if (
                db_segment
                and self._get_abs_frame_number(self._db_task.data, idx) not in db_segment_frame_set
            ):
                db_segment = None
                db_segment_frame_set = None
                db_segment_frame_provider = None

            if not db_segment:
                db_segment = self._get_segment(idx)
                db_segment_frame_set = set(db_segment.frame_set)
                db_segment_frame_provider = SegmentFrameProvider(db_segment)

            yield db_segment_frame_provider.get_frame(idx, quality=quality, out_type=out_type)

    def _get_segment(self, validated_frame_number: int) -> models.Segment:
        if not self._db_task.data or not self._db_task.data.size:
            raise ValidationError("Task has no data")

        abs_frame_number = self.get_abs_frame_number(validated_frame_number)

        segment = next(
            (
                s
                for s in sorted(
                    self._db_task.segment_set.all(),
                    key=lambda s: s.type != models.SegmentType.RANGE,  # prioritize RANGE segments
                )
                if abs_frame_number in s.frame_set
            ),
            None,
        )
        if segment is None:
            raise AssertionError(
                f"Can't find a segment with frame {validated_frame_number} "
                f"in task {self._db_task.id}"
            )

        return segment

    def _get_segment_frame_provider(self, frame_number: int) -> SegmentFrameProvider:
        return SegmentFrameProvider(self._get_segment(self.validate_frame_number(frame_number)))


class SegmentFrameProvider(IFrameProvider):
    def __init__(self, db_segment: models.Segment) -> None:
        super().__init__()
        self._db_segment = db_segment

        db_data = db_segment.task.data

        reader_class: dict[models.DataChoice, tuple[type[IMediaReader], Optional[dict]]] = {
            models.DataChoice.IMAGESET: (ZipReader, None),
            models.DataChoice.VIDEO: (
                VideoReader,
                {
                    "allow_threading": False
                    # disable threading to avoid unpredictable server
                    # resource consumption during reading in endpoints
                    # can be enabled for other clients
                },
            ),
        }

        self._loaders: dict[FrameQuality, _ChunkLoader] = {}
        if (
            db_data.storage_method == models.StorageMethodChoice.CACHE
            or not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE
            # TODO: separate handling, extract cache creation logic from media cache
        ):
            cache = MediaCache()

            self._loaders[FrameQuality.COMPRESSED] = _BufferChunkLoader(
                reader_class=reader_class[db_data.compressed_chunk_type][0],
                reader_params=reader_class[db_data.compressed_chunk_type][1],
                get_chunk_callback=lambda chunk_idx: cache.get_or_set_segment_chunk(
                    db_segment, chunk_idx, quality=FrameQuality.COMPRESSED
                ),
            )

            self._loaders[FrameQuality.ORIGINAL] = _BufferChunkLoader(
                reader_class=reader_class[db_data.original_chunk_type][0],
                reader_params=reader_class[db_data.original_chunk_type][1],
                get_chunk_callback=lambda chunk_idx: cache.get_or_set_segment_chunk(
                    db_segment, chunk_idx, quality=FrameQuality.ORIGINAL
                ),
            )
        else:
            self._loaders[FrameQuality.COMPRESSED] = _FileChunkLoader(
                reader_class=reader_class[db_data.compressed_chunk_type][0],
                reader_params=reader_class[db_data.compressed_chunk_type][1],
                get_chunk_path_callback=lambda chunk_idx: db_data.get_compressed_segment_chunk_path(
                    chunk_idx, segment_id=db_segment.id
                ),
            )

            self._loaders[FrameQuality.ORIGINAL] = _FileChunkLoader(
                reader_class=reader_class[db_data.original_chunk_type][0],
                reader_params=reader_class[db_data.original_chunk_type][1],
                get_chunk_path_callback=lambda chunk_idx: db_data.get_original_segment_chunk_path(
                    chunk_idx, segment_id=db_segment.id
                ),
            )

    def unload(self):
        for loader in self._loaders.values():
            loader.unload()

    def __len__(self):
        return self._db_segment.frame_count

    def get_frame_index(self, frame_number: int) -> Optional[int]:
        segment_frames = sorted(self._db_segment.frame_set)
        abs_frame_number = self._get_abs_frame_number(self._db_segment.task.data, frame_number)
        frame_index = bisect(segment_frames, abs_frame_number) - 1
        if not (
            0 <= frame_index < len(segment_frames)
            and segment_frames[frame_index] == abs_frame_number
        ):
            return None

        return frame_index

    def validate_frame_number(self, frame_number: int) -> tuple[int, int, int]:
        frame_index = self.get_frame_index(frame_number)
        if frame_index is None:
            raise ValidationError(f"Incorrect requested frame number: {frame_number}")

        chunk_number, frame_position = divmod(frame_index, self._db_segment.task.data.chunk_size)
        return frame_number, chunk_number, frame_position

    def get_chunk_number(self, frame_number: int) -> int:
        return self.get_frame_index(frame_number) // self._db_segment.task.data.chunk_size

    def find_matching_chunk(self, frames: Sequence[int]) -> Optional[int]:
        return next(
            (
                i
                for i, chunk_frames in enumerate(
                    take_by(
                        sorted(self._db_segment.frame_set), self._db_segment.task.data.chunk_size
                    )
                )
                if frames == set(chunk_frames)
            ),
            None,
        )

    def validate_chunk_number(self, chunk_number: int) -> int:
        segment_size = self._db_segment.frame_count
        last_chunk = math.ceil(segment_size / self._db_segment.task.data.chunk_size) - 1
        if not 0 <= chunk_number <= last_chunk:
            raise ValidationError(
                f"Invalid chunk number '{chunk_number}'. "
                f"The chunk number should be in the [0, {last_chunk}] range"
            )

        return chunk_number

    def get_preview(self) -> DataWithMeta[BytesIO]:
        cache = MediaCache()
        preview, mime = cache.get_or_set_segment_preview(self._db_segment)
        return DataWithMeta[BytesIO](preview, mime=mime)

    def get_chunk(
        self, chunk_number: int, *, quality: FrameQuality = FrameQuality.ORIGINAL
    ) -> DataWithMeta[BytesIO]:
        chunk_number = self.validate_chunk_number(chunk_number)
        chunk_data, mime = self._loaders[quality].read_chunk(chunk_number)
        return DataWithMeta[BytesIO](chunk_data, mime=mime)

    def _get_raw_frame(
        self,
        frame_number: int,
        *,
        quality: FrameQuality = FrameQuality.ORIGINAL,
    ) -> tuple[Any, str, type[IMediaReader]]:
        _, chunk_number, frame_offset = self.validate_frame_number(frame_number)
        loader = self._loaders[quality]
        chunk_reader = loader.load(chunk_number)
        frame, frame_name, _ = chunk_reader[frame_offset]
        return frame, frame_name, loader.reader_class

    def get_frame(
        self,
        frame_number: int,
        *,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> DataWithMeta[AnyFrame]:
        return_type = DataWithMeta[AnyFrame]

        frame, frame_name, reader_class = self._get_raw_frame(frame_number, quality=quality)

        frame = self._convert_frame(frame, reader_class, out_type)
        if issubclass(reader_class, VideoReader):
            return return_type(frame, mime=self.VIDEO_FRAME_MIME)

        return return_type(frame, mime=mimetypes.guess_type(frame_name)[0])

    def get_frame_context_images_chunk(
        self,
        frame_number: int,
    ) -> Optional[DataWithMeta[BytesIO]]:
        self.validate_frame_number(frame_number)

        db_data = self._db_segment.task.data

        cache = MediaCache()
        if db_data.storage_method == models.StorageMethodChoice.CACHE:
            data, mime = cache.get_or_set_frame_context_images_chunk(db_data, frame_number)
        else:
            data, mime = cache.prepare_context_images_chunk(db_data, frame_number)

        if not data.getvalue():
            return None

        return DataWithMeta[BytesIO](data, mime=mime)

    def iterate_frames(
        self,
        *,
        start_frame: Optional[int] = None,
        stop_frame: Optional[int] = None,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        out_type: FrameOutputType = FrameOutputType.BUFFER,
    ) -> Iterator[DataWithMeta[AnyFrame]]:
        frame_range = itertools.count(start_frame)
        if stop_frame:
            frame_range = itertools.takewhile(lambda x: x <= stop_frame, frame_range)

        segment_frame_set = set(self._db_segment.frame_set)
        for idx in frame_range:
            if self._get_abs_frame_number(self._db_segment.task.data, idx) in segment_frame_set:
                yield self.get_frame(idx, quality=quality, out_type=out_type)


class JobFrameProvider(SegmentFrameProvider):
    def __init__(self, db_job: models.Job) -> None:
        super().__init__(db_job.segment)

    def get_chunk(
        self,
        chunk_number: int,
        *,
        quality: FrameQuality = FrameQuality.ORIGINAL,
        is_task_chunk: bool = False,
    ) -> DataWithMeta[BytesIO]:
        if not is_task_chunk:
            return super().get_chunk(chunk_number, quality=quality)

        # Backward compatibility for the "number" parameter
        # Reproduce the task chunks, limited by this job
        return_type = DataWithMeta[BytesIO]

        task_frame_provider = TaskFrameProvider(self._db_segment.task)
        segment_start_chunk = task_frame_provider.get_chunk_number(self._db_segment.start_frame)
        segment_stop_chunk = task_frame_provider.get_chunk_number(self._db_segment.stop_frame)
        if not segment_start_chunk <= chunk_number <= segment_stop_chunk:
            raise ValidationError(
                f"Invalid chunk number '{chunk_number}'. "
                "The chunk number should be in the "
                f"[{segment_start_chunk}, {segment_stop_chunk}] range"
            )

        cache = MediaCache()
        cached_chunk = cache.get_segment_task_chunk(self._db_segment, chunk_number, quality=quality)
        if cached_chunk:
            return return_type(cached_chunk[0], cached_chunk[1])

        db_data = self._db_segment.task.data
        step = db_data.get_frame_step()
        task_chunk_start_frame = chunk_number * db_data.chunk_size
        task_chunk_stop_frame = (chunk_number + 1) * db_data.chunk_size - 1
        task_chunk_frame_set = set(
            range(
                db_data.start_frame + task_chunk_start_frame * step,
                min(db_data.start_frame + task_chunk_stop_frame * step, db_data.stop_frame) + step,
                step,
            )
        )

        # Don't put this into set_callback to avoid data duplication in the cache
        matching_chunk = self.find_matching_chunk(sorted(task_chunk_frame_set))
        if matching_chunk is not None:
            return self.get_chunk(matching_chunk, quality=quality)

        segment_chunk_frame_ids = sorted(
            task_chunk_frame_set.intersection(self._db_segment.frame_set)
        )

        buffer, mime_type = cache.get_or_set_segment_task_chunk(
            self._db_segment,
            chunk_number,
            quality=quality,
            set_callback=Callback(
                callable=self._get_chunk_create_callback,
                args=[
                    self._db_segment,
                    segment_chunk_frame_ids,
                    chunk_number,
                    quality,
                ],
            ),
        )

        return return_type(data=buffer, mime=mime_type)

    @staticmethod
    def _get_chunk_create_callback(
        db_segment: Union[models.Segment, int],
        segment_chunk_frame_ids: list[int],
        chunk_number: int,
        quality: FrameQuality,
    ) -> DataWithMime:
        # Create and return a joined / cleaned chunk
        if isinstance(db_segment, int):
            db_segment = models.Segment.objects.get(pk=db_segment)

        if db_segment.type == models.SegmentType.RANGE:
            return MediaCache.prepare_custom_range_segment_chunk(
                db_task=db_segment.task,
                frame_ids=segment_chunk_frame_ids,
                quality=quality,
            )
        elif db_segment.type == models.SegmentType.SPECIFIC_FRAMES:
            return MediaCache.prepare_custom_masked_range_segment_chunk(
                db_task=db_segment.task,
                frame_ids=segment_chunk_frame_ids,
                chunk_number=chunk_number,
                quality=quality,
                insert_placeholders=True,
            )
        else:
            assert False


@overload
def make_frame_provider(data_source: models.Job) -> JobFrameProvider: ...


@overload
def make_frame_provider(data_source: models.Segment) -> SegmentFrameProvider: ...


@overload
def make_frame_provider(data_source: models.Task) -> TaskFrameProvider: ...


def make_frame_provider(
    data_source: Union[models.Job, models.Segment, models.Task, Any],
) -> IFrameProvider:
    if isinstance(data_source, models.Task):
        frame_provider = TaskFrameProvider(data_source)
    elif isinstance(data_source, models.Segment):
        frame_provider = SegmentFrameProvider(data_source)
    elif isinstance(data_source, models.Job):
        frame_provider = JobFrameProvider(data_source)
    else:
        raise TypeError(f"Unexpected data source type {type(data_source)}")

    return frame_provider


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\handlers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from pathlib import Path
from time import time

from django.conf import settings

from cvat.apps.engine.log import ServerLogManager

slogger = ServerLogManager(__name__)


def clear_import_cache(path: Path, creation_time: float) -> None:
    """
    This function checks and removes the import files if they have not been removed from rq import jobs.
    This means that for some reason file was uploaded to CVAT server but rq import job was not created.

    Args:
        path (Path): path to file
        creation_time (float): file creation time
    """
    if (
        path.is_file()
        and (time() - creation_time + 1) >= settings.IMPORT_CACHE_CLEAN_DELAY.total_seconds()
    ):
        path.unlink()
        slogger.glob.warning(f"The file {str(path)} was removed from cleaning job.")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\lazy_list.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from collections.abc import Iterator
from functools import wraps
from itertools import islice
from typing import Any, Callable, TypeVar, overload

import attrs
from attr import field

T = TypeVar("T", bound=int | float | str)


def _parse_self_and_other_before_accessing(list_method: Callable[..., Any]) -> Callable[..., Any]:
    @wraps(list_method)
    def wrapper(self: "LazyList", other: Any) -> "LazyList":
        self._parse_up_to(-1)
        if isinstance(other, LazyList):
            other._parse_up_to(-1)
        if not isinstance(other, list):
            # explicitly calling list.__add__ with
            # np.ndarray raises TypeError instead of it returning NotImplemented
            # this prevents python from executing np.ndarray.__radd__
            return NotImplemented

        return list_method(self, other)

    return wrapper


def _parse_self_before_accessing(list_method: Callable[..., Any]) -> Callable[..., Any]:
    """Wrapper for original list methods. Forces LazyList to parse itself before accessing them."""

    @wraps(list_method)
    def wrapper(self: "LazyList", *args, **kwargs) -> "LazyList":
        self._parse_up_to(-1)

        return list_method(self, *args, **kwargs)

    return wrapper


class LazyListMeta(type):
    def __new__(
        mcs,
        name: str,
        bases: tuple[type, ...],
        namespace: dict[str, Any],
    ):
        # add pre-parse for list methods
        for method_name in [
            "append",
            "copy",
            "insert",
            "pop",
            "remove",
            "reverse",
            "sort",
            "clear",
            "index",
            "count",
            "__setitem__",
            "__delitem__",
            "__contains__",
            "__len__",
            "__reversed__",
            "__mul__",
            "__rmul__",
            "__imul__",
        ]:
            namespace[method_name] = _parse_self_before_accessing(getattr(list, method_name))

        for method_name in [
            "extend",
            "__add__",
            "__iadd__",
            "__eq__",
            "__gt__",
            "__ge__",
            "__lt__",
            "__le__",
        ]:
            namespace[method_name] = _parse_self_and_other_before_accessing(
                getattr(list, method_name)
            )

        return super().__new__(mcs, name, bases, namespace)


@attrs.define(slots=True, repr=False)
class LazyList(list[T], metaclass=LazyListMeta):
    """
    Evaluates elements from the string representation as needed.
    Lazy evaluation is supported for __getitem__ and __iter__ methods.
    Using any other method will result in parsing the whole string.
    Once instance of LazyList is fully parsed (either by accessing list methods
    or by iterating over all elements), it will behave just as a regular python list.
    """

    _string: str = ""
    _separator: str = ","
    _converter: Callable[[str], T] = lambda s: s
    _probable_length: int | None = field(init=False, default=None)
    _parsed: bool = field(init=False, default=False)

    def __repr__(self) -> str:
        if self._parsed:
            return f"LazyList({list.__repr__(self)})"
        current_index = list.__len__(self)
        current_position = 1 if self._string.startswith("[") else 0
        separator_offset = len(self._separator)

        for _ in range(current_index):
            current_position = (
                self._string.find(self._separator, current_position) + separator_offset
            )

        parsed_elements = list.__repr__(self).removesuffix("]")
        unparsed_elements = self._string[current_position:]
        return (
            f"LazyList({parsed_elements}... + {unparsed_elements}', "
            f"({list.__len__(self) / self._compute_max_length(self._string) * 100:.02f}% parsed))"
        )

    def __deepcopy__(self, memodict: Any = None) -> list[T]:
        """
        Since our elements are scalar, this should be sufficient
        Without this, deepcopy would copy the state of the object,
        then would try to append its elements.

        However, since copy will contain initial string,
        it will compute its elements on the first on the first append,
        resulting in value duplication.
        """
        return list(self)

    @overload
    def __getitem__(self, index: int) -> T: ...

    @overload
    def __getitem__(self, index: slice) -> list[T]: ...

    def __getitem__(self, index: int | slice) -> T | list[T]:
        if self._parsed:
            return list.__getitem__(self, index)

        if isinstance(index, slice):
            if (
                index.start is not None
                and index.start < 0
                or index.stop is not None
                and index.stop < 0
                or index.step is not None
                and index.step < 0
            ):
                # to compute negative indices we must know the exact length in advance
                # which is impossible if we take into account missing elements,
                # so we have to parse the full list
                self._parse_up_to(-1)
            else:
                self._parse_up_to(index.indices(self._compute_max_length(self._string))[1] - 1)

            return list.__getitem__(self, index)

        self._parse_up_to(index)
        return list.__getitem__(self, index)

    def __iter__(self) -> Iterator[T]:
        yield from list.__iter__(self)
        yield from self._iter_unparsed()

    def __str__(self) -> str:
        if not self._parsed:
            return self._string.strip("[]")
        return self._separator.join(map(str, self))

    def _parse_up_to(self, index: int) -> None:
        if self._parsed:
            return

        if index == -1:
            # _iter_unparsed is inefficient when we want to parse the whole list
            element_strs = self._string.split(self._separator)
            if len(element_strs):
                element_strs[0] = element_strs[0].removeprefix("[")
                element_strs[-1] = element_strs[-1].removesuffix("]")

            list.clear(self)
            list.extend(self, [self._converter(str_item) for str_item in element_strs if str_item])
            self._mark_parsed()
            return

        if index < 0:
            index += self._compute_max_length(self._string)

        start = list.__len__(self)
        if start > index:
            return
        end = index - start + 1
        for _ in islice(self._iter_unparsed(), end + 1):
            pass

        if index == self._compute_max_length(self._string) - 1:
            self._mark_parsed()

    def _mark_parsed(self):
        self._parsed = True
        self._string = ""  # freeing the memory

    def _iter_unparsed(self):
        if self._parsed:
            return
        string = self._string
        current_index = list.__len__(self)
        current_position = 1 if string.startswith("[") else 0
        string_length = len(string) - 1 if string.endswith("]") else len(string)
        separator_offset = len(self._separator)

        for _ in range(current_index):
            current_position = string.find(self._separator, current_position) + separator_offset

        probable_length = self._compute_max_length(string)
        while current_index < probable_length:
            end = string.find(self._separator, current_position, string_length)
            if end == -1:
                end = string_length
                self._mark_parsed()

            element_str = string[current_position:end]
            current_position = end + separator_offset
            if not element_str:
                probable_length -= 1
                continue
            element = self._converter(element_str)
            if list.__len__(self) <= current_index:
                # We need to handle special case when instance of lazy list becomes parsed after
                # this function is called:
                # ll = LazyList("1,2,3", _converter=int)
                # iterator = iter(ll)
                # next(iterator)  # > 1 (will generate next element and append to self)
                # list(ll)  # > [1, 2, 3]
                # next(iterator)  # > 2 (will generate next element, however will not append it)
                # assert list(ll) == [1, 2, 3]
                list.append(self, element)
            yield element
            current_index += 1

    def _compute_max_length(self, string) -> int:
        if self._probable_length is None:
            if not self._string:
                return 0
            self._probable_length = string.count(self._separator) + 1
        return self._probable_length

    # support pickling

    def __reduce__(self):
        return self.__class__, (self._string, self._separator, self._converter), self.__getstate__()

    def __reduce_ex__(self, protocol: int):
        return self.__reduce__()

    def __getstate__(self):
        return {
            "string": self._string,
            "_separator": self._separator,
            "_converter": self._converter,
            "_probable_length": self._probable_length,
            "parsed": self._parsed,
            "parsed_elements": list(self) if self._parsed else None,
        }

    def __setstate__(self, state):
        self._string = state["string"]
        self._separator = state["_separator"]
        self._converter = state["_converter"]
        self._probable_length = state["_probable_length"]
        self._parsed = state["parsed"]
        if self._parsed:
            self.extend(state["parsed_elements"])

    def lazy_copy(self) -> list[T]:
        """
        Makes a copy without parsing elements.
        Only works if elements have not been parsed yet.
        """
        assert not self._parsed
        return LazyList(
            string=self._string,
            separator=self._separator,
            converter=self._converter,
        )

    @property
    def is_parsed(self):
        return self._parsed


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\location.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from enum import Enum
from typing import Any, Optional, Union

from cvat.apps.engine.models import Job, Location, Project, Task


class StorageType(str, Enum):
    TARGET = "target_storage"
    SOURCE = "source_storage"

    def __str__(self):
        return self.value


def get_location_configuration(
    query_params: dict[str, Any],
    field_name: str,
    *,
    db_instance: Optional[Union[Project, Task, Job]] = None,
) -> dict[str, Any]:
    location = query_params.get("location")

    # handle resource import
    if not location and not db_instance:
        location = Location.LOCAL

    use_default_settings = location is None

    location_conf = {"is_default": use_default_settings}

    if use_default_settings:
        storage = (
            getattr(db_instance, field_name)
            if not isinstance(db_instance, Job)
            else getattr(db_instance.segment.task, field_name)
        )
        if storage is None:
            location_conf["location"] = Location.LOCAL
        else:
            location_conf["location"] = storage.location
            if cloud_storage_id := storage.cloud_storage_id:
                location_conf["storage_id"] = cloud_storage_id
    else:
        if location not in Location.list():
            raise ValueError(f"The specified location {location} is not supported")

        cloud_storage_id = query_params.get("cloud_storage_id")

        if location == Location.CLOUD_STORAGE and not cloud_storage_id:
            raise ValueError(
                "Cloud storage was selected as location but cloud_storage_id was not specified"
            )

        location_conf["location"] = location
        if cloud_storage_id:
            location_conf["storage_id"] = int(cloud_storage_id)

    return location_conf


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\log.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import logging
import os.path as osp
import sys
from contextlib import contextmanager

from django.conf import settings

from cvat.apps.engine.utils import directory_tree


class _LoggerAdapter(logging.LoggerAdapter):
    def process(self, msg: str, kwargs):
        if msg_prefix := self.extra.get("msg_prefix"):
            msg = msg_prefix + msg
        return msg, kwargs


class _LoggerAdapterMapping:
    def __init__(self, logger: logging.Logger, object_type: str) -> None:
        self._logger = logger
        self._object_type = object_type

    def __getitem__(self, id_: int) -> logging.LoggerAdapter:
        return _LoggerAdapter(self._logger, {"msg_prefix": f"[{self._object_type}.id={id_}] "})


class ServerLogManager:
    def __init__(self, logger_name: str) -> None:
        self.glob = logging.getLogger(logger_name)
        self.project = _LoggerAdapterMapping(self.glob, "Project")
        self.task = _LoggerAdapterMapping(self.glob, "Task")
        self.job = _LoggerAdapterMapping(self.glob, "Job")
        self.cloud_storage = _LoggerAdapterMapping(self.glob, "CloudStorage")


class DatasetLogManager:
    def __init__(self, directory_depth=5) -> None:
        self.glob = logging.getLogger("dataset_logger")
        self.directory_depth = directory_depth

    def log_import_error(self, entity, entity_id, format_name, base_error, dir_path) -> None:
        base_info = f"[{entity}.id={entity_id} format.name={format_name} exc={base_error}]"
        dir_tree = directory_tree(
            path=dir_path,
            max_depth=self.directory_depth,
        )
        log_error = f"{base_info} \nDirectory tree:\n{dir_tree}"
        self.glob.error(log_error)


def get_logger(logger_name, log_file):
    logger = logging.getLogger(name=logger_name)
    logger.setLevel(logging.INFO)
    file_handler = logging.FileHandler(log_file)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(logging.StreamHandler(sys.stdout))
    logger.addHandler(logging.StreamHandler(sys.stderr))
    return logger


vlogger = logging.getLogger("vector")


def get_migration_log_dir() -> str:
    return settings.MIGRATIONS_LOGS_ROOT


def get_migration_log_file_path(migration_name: str) -> str:
    return osp.join(get_migration_log_dir(), f"{migration_name}.log")


@contextmanager
def get_migration_logger(migration_name):
    migration_log_file_path = get_migration_log_file_path(migration_name)
    stdout = sys.stdout
    stderr = sys.stderr

    # redirect all stdout to the file
    with open(migration_log_file_path, "w") as log_file_object:
        sys.stdout = log_file_object
        sys.stderr = log_file_object

        log = logging.getLogger(migration_name)
        log.addHandler(logging.StreamHandler(stdout))
        log.addHandler(logging.StreamHandler(log_file_object))
        log.setLevel(logging.INFO)

        try:
            yield log
        finally:
            sys.stdout = stdout
            sys.stderr = stderr


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\media_extractors.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import itertools
import os
import shutil
import struct
import sysconfig
import tempfile
import zipfile
from abc import ABC, abstractmethod
from bisect import bisect
from collections.abc import Generator, Iterable, Iterator, Sequence
from contextlib import AbstractContextManager, ExitStack, closing, contextmanager
from dataclasses import dataclass
from enum import IntEnum
from random import shuffle
from typing import Any, Callable, Optional, Protocol, TypeVar, Union

import av
import av.codec
import av.container
import av.video.stream
import numpy as np
from natsort import os_sorted
from PIL import Image, ImageFile, ImageOps
from pyunpack import Archive
from rest_framework.exceptions import ValidationError

from cvat.apps.engine.models import DimensionType, SortingMethod
from cvat.apps.engine.utils import rotate_image

# fixes: "OSError:broken data stream" when executing line 72 while loading images downloaded from the web
# see: https://stackoverflow.com/questions/42462431/oserror-broken-data-stream-when-reading-image-file
ImageFile.LOAD_TRUNCATED_IMAGES = True

from cvat.apps.engine.mime_types import mimetypes
from utils.dataset_manifest import ImageManifestManager, VideoManifestManager

ORIENTATION_EXIF_TAG = 274

class ORIENTATION(IntEnum):
    NORMAL_HORIZONTAL=1
    MIRROR_HORIZONTAL=2
    NORMAL_180_ROTATED=3
    MIRROR_VERTICAL=4
    MIRROR_HORIZONTAL_270_ROTATED=5
    NORMAL_90_ROTATED=6
    MIRROR_HORIZONTAL_90_ROTATED=7
    NORMAL_270_ROTATED=8

class FrameQuality(IntEnum):
    COMPRESSED = 0
    ORIGINAL = 100

def get_mime(name):
    for type_name, type_def in MEDIA_TYPES.items():
        if type_def['has_mime_type'](name):
            return type_name

    return 'unknown'

def create_tmp_dir():
    return tempfile.mkdtemp(prefix='cvat-', suffix='.data')

def delete_tmp_dir(tmp_dir):
    if tmp_dir:
        shutil.rmtree(tmp_dir)

def files_to_ignore(directory):
    ignore_files = ('__MSOSX', '._.DS_Store', '__MACOSX', '.DS_Store')
    if not any(ignore_file in directory for ignore_file in ignore_files):
        return True
    return False

def sort(images, sorting_method=SortingMethod.LEXICOGRAPHICAL, func=None):
    if sorting_method == SortingMethod.LEXICOGRAPHICAL:
        return sorted(images, key=func)
    elif sorting_method == SortingMethod.NATURAL:
        return os_sorted(images, key=func)
    elif sorting_method == SortingMethod.PREDEFINED:
        return images
    elif sorting_method == SortingMethod.RANDOM:
        shuffle(images) # TODO: support seed to create reproducible results
        return images
    else:
        raise NotImplementedError()

def image_size_within_orientation(img: Image.Image):
    orientation = img.getexif().get(ORIENTATION_EXIF_TAG, ORIENTATION.NORMAL_HORIZONTAL)
    if orientation > 4:
        return img.height, img.width
    return img.width, img.height

def has_exif_rotation(img: Image.Image):
    return img.getexif().get(ORIENTATION_EXIF_TAG, ORIENTATION.NORMAL_HORIZONTAL) != ORIENTATION.NORMAL_HORIZONTAL


def load_image(image: tuple[str, str, str])-> tuple[Image.Image, str, str]:
    with Image.open(image[0]) as pil_img:
        pil_img.load()
        return pil_img, image[1], image[2]

_T = TypeVar("_T")


class RandomAccessIterator(Iterator[_T]):
    def __init__(self, iterable: Iterable[_T]):
        self.iterable: Iterable[_T] = iterable
        self.iterator: Optional[Iterator[_T]] = None
        self.pos: int = -1

    def __iter__(self):
        return self

    def __next__(self):
        return self[self.pos + 1]

    def __getitem__(self, idx: int) -> Optional[_T]:
        assert 0 <= idx
        if self.iterator is None or idx <= self.pos:
            self.reset()
        v = None
        while self.pos < idx:
            # NOTE: don't keep the last item in self, it can be expensive
            v = next(self.iterator)
            self.pos += 1
        return v

    def reset(self):
        self.close()
        self.iterator = iter(self.iterable)

    def close(self):
        if self.iterator is not None:
            if close := getattr(self.iterator, "close", None):
                close()
        self.iterator = None
        self.pos = -1


class Sized(Protocol):
    def get_size(self) -> int: ...

_MediaT = TypeVar("_MediaT", bound=Sized)

class CachingMediaIterator(RandomAccessIterator[_MediaT]):
    @dataclass
    class _CacheItem:
        value: _MediaT
        size: int

    def __init__(
        self,
        iterable: Iterable,
        *,
        max_cache_memory: int,
        max_cache_entries: int,
        object_size_callback: Optional[Callable[[_MediaT], int]] = None,
    ):
        super().__init__(iterable)
        self.max_cache_entries = max_cache_entries
        self.max_cache_memory = max_cache_memory
        self._get_object_size_callback = object_size_callback
        self.used_cache_memory = 0
        self._cache: dict[int, self._CacheItem] = {}

    def _get_object_size(self, obj: _MediaT) -> int:
        if self._get_object_size_callback:
            return self._get_object_size_callback(obj)

        return obj.get_size()

    def _can_put_item_in_cache(self, value_size: int) -> bool:
        return (
            len(self._cache) + 1 <= self.max_cache_entries and
            self.used_cache_memory + value_size <= self.max_cache_memory
        )

    def __getitem__(self, idx: int):
        cache_item = self._cache.get(idx)
        if cache_item:
            return cache_item.value

        value = super().__getitem__(idx)
        value_size = self._get_object_size(value)

        while len(self._cache) > 0 and not self._can_put_item_in_cache(value_size):
            min_key = min(self._cache.keys())
            self._cache.pop(min_key)

        if self._can_put_item_in_cache(value_size):
            self._cache[idx] = self._CacheItem(value, value_size)

        return value


class IMediaReader(ABC):
    def __init__(
        self,
        source_path,
        *,
        start: int = 0,
        stop: Optional[int] = None,
        step: int = 1,
        dimension: DimensionType = DimensionType.DIM_2D
    ):
        self._source_path = source_path

        self._step = step

        self._start = start
        "The first included index"

        self._stop = stop
        "The last included index"

        self._dimension = dimension

    @abstractmethod
    def __iter__(self):
        pass

    @abstractmethod
    def get_preview(self, frame):
        pass

    @abstractmethod
    def get_progress(self, pos):
        pass

    @staticmethod
    def _get_preview(obj):
        PREVIEW_SIZE = (256, 256)

        if isinstance(obj, io.IOBase):
            preview = Image.open(obj)
        else:
            preview = obj
        preview = ImageOps.exif_transpose(preview)
        # TODO - Check if the other formats work. I'm only interested in I;16 for now. Sorry @:-|
        # Summary:
        # Images in the Format I;16 definitely don't work. Most likely I;16B/L/N won't work as well.
        # Simple Conversion from I;16 to I/RGB/L doesn't work as well.
        #   Including any Intermediate Conversions doesn't work either. (eg. I;16 to I to L)
        # Seems like an internal Bug of PIL
        #     See Issue for further details: https://github.com/python-pillow/Pillow/issues/3011
        #     Issue was opened 2018, so don't expect any changes soon and work with manual conversions.
        mode: str = preview.mode
        if mode == "I;16":
            preview = np.array(preview, dtype=np.uint16) # 'I;16' := Unsigned Integer 16, Grayscale
            image = image - image.min()                  # In case the used range lies in [a, 2^16] with a > 0
            preview = preview / preview.max() * 255      # Downscale into real numbers of range [0, 255]
            preview = preview.astype(np.uint8)           # Floor to integers of range [0, 255]
            preview = Image.fromarray(preview, mode="L") # 'L' := Unsigned Integer 8, Grayscale
            preview = ImageOps.equalize(preview)         # The Images need equalization. High resolution with 16-bit but only small range that actually contains information
        preview.thumbnail(PREVIEW_SIZE)

        return preview

    @abstractmethod
    def get_image_size(self, i):
        pass

    @property
    def start(self) -> int:
        return self._start

    @property
    def stop(self) -> Optional[int]:
        return self._stop

    @property
    def step(self) -> int:
        return self._step


class ImageListReader(IMediaReader):
    def __init__(self,
        source_path,
        step: int = 1,
        start: int = 0,
        stop: Optional[int] = None,
        dimension: DimensionType = DimensionType.DIM_2D,
        sorting_method: SortingMethod = SortingMethod.LEXICOGRAPHICAL,
    ):
        if not source_path:
            raise Exception('No image found')

        if not stop:
            stop = len(source_path) - 1
        else:
            stop = min(len(source_path) - 1, stop)

        step = max(step, 1)
        assert stop >= start

        super().__init__(
            source_path=sort(source_path, sorting_method),
            step=step,
            start=start,
            stop=stop,
            dimension=dimension
        )

        self._sorting_method = sorting_method

    def __iter__(self):
        for i in self.frame_range:
            yield (self.get_image(i), self.get_path(i), i)

    def __contains__(self, media_file):
        return media_file in self._source_path

    def filter(self, callback):
        source_path = list(filter(callback, self._source_path))
        ImageListReader.__init__(
            self,
            source_path,
            step=self._step,
            start=self._start,
            stop=self._stop,
            dimension=self._dimension,
            sorting_method=self._sorting_method
        )

    def get_path(self, i):
        return self._source_path[i]

    def get_image(self, i):
        return self._source_path[i]

    def get_progress(self, pos):
        return (pos + 1) / (len(self.frame_range) or 1)

    def get_preview(self, frame):
        if self._dimension == DimensionType.DIM_3D:
            fp = open(os.path.join(os.path.dirname(__file__), 'assets/3d_preview.jpeg'), "rb")
        else:
            fp = open(self._source_path[frame], "rb")
        return self._get_preview(fp)

    def get_image_size(self, i):
        if self._dimension == DimensionType.DIM_3D:
            with open(self.get_path(i), 'rb') as f:
                properties = ValidateDimension.get_pcd_properties(f)
                return int(properties["WIDTH"]),  int(properties["HEIGHT"])
        with Image.open(self._source_path[i]) as img:
            return image_size_within_orientation(img)

    def reconcile(self, source_files, step=1, start=0, stop=None, dimension=DimensionType.DIM_2D, sorting_method=None):
        # FIXME
        ImageListReader.__init__(self,
            source_path=source_files,
            step=step,
            start=start,
            stop=stop,
            sorting_method=sorting_method if sorting_method else self._sorting_method,
        )
        self._dimension = dimension

    @property
    def absolute_source_paths(self):
        return [self.get_path(idx) for idx, _ in enumerate(self._source_path)]

    def __len__(self):
        return len(self.frame_range)

    @property
    def frame_range(self):
        return range(self._start, self._stop + 1, self._step)

class DirectoryReader(ImageListReader):
    def __init__(self,
                source_path,
                step=1,
                start=0,
                stop=None,
                dimension=DimensionType.DIM_2D,
                sorting_method=SortingMethod.LEXICOGRAPHICAL):
        image_paths = []
        for source in source_path:
            for root, _, files in os.walk(source):
                paths = [os.path.join(root, f) for f in files]
                paths = filter(lambda x: get_mime(x) == 'image', paths)
                image_paths.extend(paths)
        super().__init__(
            source_path=image_paths,
            step=step,
            start=start,
            stop=stop,
            dimension=dimension,
            sorting_method=sorting_method,
        )

class ArchiveReader(DirectoryReader):
    def __init__(self,
                source_path,
                step=1,
                start=0,
                stop=None,
                dimension=DimensionType.DIM_2D,
                sorting_method=SortingMethod.LEXICOGRAPHICAL,
                extract_dir=None):

        self._archive_source = source_path[0]
        tmp_dir = extract_dir if extract_dir else os.path.dirname(source_path[0])
        patool_path = os.path.join(sysconfig.get_path('scripts'), 'patool')
        Archive(self._archive_source).extractall(tmp_dir, False, patool_path)
        if not extract_dir:
            os.remove(self._archive_source)
        super().__init__(
            source_path=[tmp_dir],
            step=step,
            start=start,
            stop=stop,
            dimension=dimension,
            sorting_method=sorting_method,
        )

class PdfReader(ImageListReader):
    def __init__(self,
                source_path,
                step=1,
                start=0,
                stop=None,
                dimension=DimensionType.DIM_2D,
                sorting_method=SortingMethod.LEXICOGRAPHICAL,
                extract_dir=None):
        if not source_path:
            raise Exception('No PDF found')

        self._pdf_source = source_path[0]

        _basename = os.path.splitext(os.path.basename(self._pdf_source))[0]
        _counter = itertools.count()
        def _make_name():
            for page_num in _counter:
                yield '{}{:09d}.jpeg'.format(_basename, page_num)

        from pdf2image import convert_from_path
        self._tmp_dir = extract_dir if extract_dir else os.path.dirname(source_path[0])
        os.makedirs(self._tmp_dir, exist_ok=True)

        # Avoid OOM: https://github.com/openvinotoolkit/cvat/issues/940
        paths = convert_from_path(self._pdf_source,
            last_page=stop, paths_only=True,
            output_folder=self._tmp_dir, fmt="jpeg", output_file=_make_name())

        if not extract_dir:
            os.remove(source_path[0])

        super().__init__(
            source_path=paths,
            step=step,
            start=start,
            stop=stop,
            dimension=dimension,
            sorting_method=sorting_method,
        )

class ZipReader(ImageListReader):
    def __init__(self,
                source_path,
                step=1,
                start=0,
                stop=None,
                dimension=DimensionType.DIM_2D,
                sorting_method=SortingMethod.LEXICOGRAPHICAL,
                extract_dir=None):
        self._zip_source = zipfile.ZipFile(source_path[0], mode='r')
        self.extract_dir = extract_dir
        file_list = [f for f in self._zip_source.namelist() if files_to_ignore(f) and get_mime(f) == 'image']
        super().__init__(file_list,
                        step=step,
                        start=start,
                        stop=stop,
                        dimension=dimension,
                        sorting_method=sorting_method)

    def __del__(self):
        self._zip_source.close()

    def get_preview(self, frame):
        if self._dimension == DimensionType.DIM_3D:
            # TODO
            fp = open(os.path.join(os.path.dirname(__file__), 'assets/3d_preview.jpeg'), "rb")
            return self._get_preview(fp)

        io_image = io.BytesIO(self._zip_source.read(self._source_path[frame]))
        return self._get_preview(io_image)

    def get_image_size(self, i):
        if self._dimension == DimensionType.DIM_3D:
            with open(self.get_path(i), 'rb') as f:
                properties = ValidateDimension.get_pcd_properties(f)
                return int(properties["WIDTH"]),  int(properties["HEIGHT"])
        with Image.open(io.BytesIO(self._zip_source.read(self._source_path[i]))) as img:
            return image_size_within_orientation(img)

    def get_image(self, i):
        if self._dimension == DimensionType.DIM_3D:
            return self.get_path(i)
        return io.BytesIO(self._zip_source.read(self._source_path[i]))

    def get_zip_filename(self):
        return self._zip_source.filename

    def get_path(self, i):
        if self._zip_source.filename:
            prefix = self._get_extract_prefix()
            return os.path.join(prefix, self._source_path[i])
        else: # necessary for mime_type definition
            return self._source_path[i]

    def __contains__(self, media_file):
        return super().__contains__(os.path.relpath(media_file, self._get_extract_prefix()))

    def _get_extract_prefix(self):
        return self.extract_dir or os.path.dirname(self._zip_source.filename)

    def reconcile(self, source_files, step=1, start=0, stop=None, dimension=DimensionType.DIM_2D, sorting_method=None):
        if source_files:
            # file list is expected to be a processed output of self.get_path()
            # which returns files with the output directory prefix
            prefix = self._get_extract_prefix()
            source_files = [os.path.relpath(fn, prefix) for fn in source_files]

        super().reconcile(
            source_files=source_files,
            step=step,
            start=start,
            stop=stop,
            dimension=dimension,
            sorting_method=sorting_method
        )

    def extract(self):
        self._zip_source.extractall(self._get_extract_prefix())
        if not self.extract_dir:
            os.remove(self._zip_source.filename)

class _AvVideoReading:
    @contextmanager
    def read_av_container(
        self, source: Union[str, io.BytesIO]
    ) -> Generator[av.container.InputContainer, None, None]:
        if isinstance(source, io.BytesIO):
            source.seek(0) # required for re-reading

        container = av.open(source)
        try:
            yield container
        finally:
            # fixes a memory leak in input container closing
            # https://github.com/PyAV-Org/PyAV/issues/1117
            for stream in container.streams:
                context = stream.codec_context
                if context and context.is_open:
                    # Currently, context closing may get stuck on some videos for an unknown reason,
                    # so the thread_type == 'AUTO' setting is disabled for future investigation
                    context.close()

            if container.open_files:
                container.close()

    def decode_stream(
        self, container: av.container.Container, video_stream: av.video.stream.VideoStream
    ) -> Generator[av.VideoFrame, None, None]:
        demux_iter = container.demux(video_stream)
        try:
            for packet in demux_iter:
                yield from packet.decode()
        finally:
            # av v9.2.0 seems to have a memory corruption or a deadlock
            # in exception handling for demux() in the multithreaded mode.
            # Instead of breaking the iteration, we iterate over packets till the end.
            # Fixed in av v12.2.0.
            if av.__version__ == "9.2.0" and video_stream.thread_type == 'AUTO':
                exhausted = object()
                while next(demux_iter, exhausted) is not exhausted:
                    pass

class VideoReader(IMediaReader):
    def __init__(
        self,
        source_path: Union[str, io.BytesIO],
        step: int = 1,
        start: int = 0,
        stop: Optional[int] = None,
        dimension: DimensionType = DimensionType.DIM_2D,
        *,
        allow_threading: bool = False,
    ):
        super().__init__(
            source_path=source_path,
            step=step,
            start=start,
            stop=stop,
            dimension=dimension,
        )

        self.allow_threading = allow_threading
        self._frame_count: Optional[int] = None
        self._frame_size: Optional[tuple[int, int]] = None # (w, h)

    def iterate_frames(
        self,
        *,
        frame_filter: Union[bool, Iterable[int]] = True,
        video_stream: Optional[av.video.stream.VideoStream] = None,
    ) -> Iterator[tuple[av.VideoFrame, str, int]]:
        """
        If provided, frame_filter must be an ordered sequence in the ascending order.
        'True' means using the frames configured in the reader object.
        'False' or 'None' means returning all the video frames.
        """

        if frame_filter is True:
            frame_filter = itertools.count(self._start, self._step)
            if self._stop:
                frame_filter = itertools.takewhile(lambda x: x <= self._stop, frame_filter)
        elif not frame_filter:
            frame_filter = itertools.count()

        frame_filter_iter = iter(frame_filter)
        next_frame_filter_frame = next(frame_filter_iter, None)
        if next_frame_filter_frame is None:
            return

        es = ExitStack()

        needs_init = video_stream is None
        if needs_init:
            container = es.enter_context(self._read_av_container())
        else:
            container = video_stream.container

        with es:
            if needs_init:
                video_stream = container.streams.video[0]

                if self.allow_threading:
                    video_stream.thread_type = 'AUTO'
                else:
                    video_stream.thread_type = 'NONE'

            frame_counter = itertools.count()
            with closing(self._decode_stream(container, video_stream)) as stream_decoder:
                for frame, frame_number in zip(stream_decoder, frame_counter):
                    if frame_number == next_frame_filter_frame:
                        if video_stream.metadata.get('rotate'):
                            pts = frame.pts
                            frame = av.VideoFrame().from_ndarray(
                                rotate_image(
                                    frame.to_ndarray(format='bgr24'),
                                    360 - int(video_stream.metadata.get('rotate'))
                                ),
                                format ='bgr24'
                            )
                            frame.pts = pts

                        if self._frame_size is None:
                            self._frame_size = (frame.width, frame.height)

                        yield (frame, self._source_path[0], frame.pts)

                        next_frame_filter_frame = next(frame_filter_iter, None)

                    if next_frame_filter_frame is None:
                        return

    def __iter__(self) -> Iterator[tuple[av.VideoFrame, str, int]]:
        return self.iterate_frames()

    def get_progress(self, pos):
        duration = self._get_duration()
        return pos / duration if duration else None

    def _read_av_container(self) -> AbstractContextManager[av.container.InputContainer]:
        return _AvVideoReading().read_av_container(self._source_path[0])

    def _decode_stream(
        self, container: av.container.Container, video_stream: av.video.stream.VideoStream
    ) -> Generator[av.VideoFrame, None, None]:
        return _AvVideoReading().decode_stream(container, video_stream)

    def _get_duration(self):
        with self._read_av_container() as container:
            stream = container.streams.video[0]

            duration = None
            if stream.duration:
                duration = stream.duration
            else:
                # may have a DURATION in format like "01:16:45.935000000"
                duration_str = stream.metadata.get("DURATION", None)
                tb_denominator = stream.time_base.denominator
                if duration_str and tb_denominator:
                    _hour, _min, _sec = duration_str.split(':')
                    duration_sec = 60*60*float(_hour) + 60*float(_min) + float(_sec)
                    duration = duration_sec * tb_denominator
            return duration

    def get_preview(self, frame):
        with self._read_av_container() as container:
            stream = container.streams.video[0]

            tb_denominator = stream.time_base.denominator
            needed_time = int((frame / stream.guessed_rate) * tb_denominator)
            container.seek(offset=needed_time, stream=stream)

            with closing(self.iterate_frames(video_stream=stream)) as frame_iter:
                return self._get_preview(next(frame_iter))

    def get_image_size(self, i):
        if self._frame_size is not None:
            return self._frame_size

        with closing(iter(self)) as frame_iter:
            frame = next(frame_iter)[0]
            self._frame_size = (frame.width, frame.height)

        return self._frame_size

    def get_frame_count(self) -> int:
        """
        Returns total frame count in the video

        Note that not all videos provide length / duration metainfo, so the
        result may require full video decoding.

        The total count is NOT affected by the frame filtering options of the object,
        i.e. start frame, end frame and frame step.
        """
        # It's possible to retrieve frame count from the stream.frames,
        # but the number may be incorrect.
        # https://superuser.com/questions/1512575/why-total-frame-count-is-different-in-ffmpeg-than-ffprobe
        if self._frame_count is not None:
            return self._frame_count

        frame_count = 0
        for _ in self.iterate_frames(frame_filter=False):
            frame_count += 1

        self._frame_count = frame_count

        return frame_count


class ImageReaderWithManifest:
    def __init__(self, manifest_path: str):
        self._manifest = ImageManifestManager(manifest_path)
        self._manifest.init_index()

    def iterate_frames(self, frame_ids: Iterable[int]):
        for idx in frame_ids:
            yield self._manifest[idx]

class VideoReaderWithManifest:
    # TODO: merge this class with VideoReader

    def __init__(self, manifest_path: str, source_path: str, *, allow_threading: bool = False):
        self.source_path = source_path
        self.manifest = VideoManifestManager(manifest_path)
        if self.manifest.exists:
            self.manifest.init_index()

        self.allow_threading = allow_threading

    def _read_av_container(self) -> AbstractContextManager[av.container.InputContainer]:
        return _AvVideoReading().read_av_container(self.source_path)

    def _decode_stream(
        self, container: av.container.Container, video_stream: av.video.stream.VideoStream
    ) -> Generator[av.VideoFrame, None, None]:
        return _AvVideoReading().decode_stream(container, video_stream)

    def _get_nearest_left_key_frame(self, frame_id: int) -> tuple[int, int]:
        nearest_left_keyframe_pos = bisect(
            self.manifest, frame_id, key=lambda entry: entry.get('number')
        )
        if nearest_left_keyframe_pos:
            frame_number = self.manifest[nearest_left_keyframe_pos - 1].get('number')
            timestamp = self.manifest[nearest_left_keyframe_pos - 1].get('pts')
        else:
            frame_number = 0
            timestamp = 0
        return frame_number, timestamp

    def iterate_frames(self, *, frame_filter: Iterable[int]) -> Iterable[av.VideoFrame]:
        "frame_ids must be an ordered sequence in the ascending order"

        frame_filter_iter = iter(frame_filter)
        next_frame_filter_frame = next(frame_filter_iter, None)
        if next_frame_filter_frame is None:
            return

        start_decode_frame_number, start_decode_timestamp = self._get_nearest_left_key_frame(
            next_frame_filter_frame
        )

        with self._read_av_container() as container:
            video_stream = container.streams.video[0]
            if self.allow_threading:
                video_stream.thread_type = 'AUTO'
            else:
                video_stream.thread_type = 'NONE'

            container.seek(offset=start_decode_timestamp, stream=video_stream)

            frame_counter = itertools.count(start_decode_frame_number)
            with closing(self._decode_stream(container, video_stream)) as stream_decoder:
                for frame, frame_number in zip(stream_decoder, frame_counter):
                    if frame_number == next_frame_filter_frame:
                        if video_stream.metadata.get('rotate'):
                            frame = av.VideoFrame().from_ndarray(
                                rotate_image(
                                    frame.to_ndarray(format='bgr24'),
                                    360 - int(video_stream.metadata.get('rotate'))
                                ),
                                format ='bgr24'
                            )

                        yield frame

                        next_frame_filter_frame = next(frame_filter_iter, None)

                    if next_frame_filter_frame is None:
                        return

class IChunkWriter(ABC):
    def __init__(self, quality, dimension=DimensionType.DIM_2D):
        self._image_quality = quality
        self._dimension = dimension

    @staticmethod
    def _compress_image(source_image: av.VideoFrame | io.IOBase | Image.Image, quality: int) -> tuple[int, int, io.BytesIO]:
        image = None
        if isinstance(source_image, av.VideoFrame):
            image = source_image.to_image()
        elif isinstance(source_image, io.IOBase):
            image, _, _ = load_image((source_image, None, None))
        elif isinstance(source_image, Image.Image):
            image = source_image

        assert image is not None

        if has_exif_rotation(image):
            image = ImageOps.exif_transpose(image)

        # Ensure image data fits into 8bit per pixel before RGB conversion as PIL clips values on conversion
        if image.mode == "I":
            # Image mode is 32bit integer pixels.
            # Autoscale pixels by factor 2**8 / im_data.max() to fit into 8bit
            im_data = np.array(image)
            im_data = im_data * (2**8 / im_data.max())
            image = Image.fromarray(im_data.astype(np.int32))

        # TODO - Check if the other formats work. I'm only interested in I;16 for now. Sorry @:-|
        # Summary:
        # Images in the Format I;16 definitely don't work. Most likely I;16B/L/N won't work as well.
        # Simple Conversion from I;16 to I/RGB/L doesn't work as well.
        #   Including any Intermediate Conversions doesn't work either. (eg. I;16 to I to L)
        # Seems like an internal Bug of PIL
        #     See Issue for further details: https://github.com/python-pillow/Pillow/issues/3011
        #     Issue was opened 2018, so don't expect any changes soon and work with manual conversions.
        if image.mode == "I;16":
            image = np.array(image, dtype=np.uint16) # 'I;16' := Unsigned Integer 16, Grayscale
            image = image - image.min()              # In case the used range lies in [a, 2^16] with a > 0
            image = image / image.max() * 255        # Downscale into real numbers of range [0, 255]
            image = image.astype(np.uint8)           # Floor to integers of range [0, 255]
            image = Image.fromarray(image, mode="L") # 'L' := Unsigned Integer 8, Grayscale
            image = ImageOps.equalize(image)         # The Images need equalization. High resolution with 16-bit but only small range that actually contains information

        if image.mode != 'RGB' and image.mode != 'L':
            image = image.convert('RGB')

        buf = io.BytesIO()
        image.save(buf, format='JPEG', quality=quality, optimize=True)
        buf.seek(0)

        return image.width, image.height, buf

    @abstractmethod
    def save_as_chunk(self, images, chunk_path):
        pass

class ZipChunkWriter(IChunkWriter):
    IMAGE_EXT = 'jpeg'
    POINT_CLOUD_EXT = 'pcd'

    def _write_pcd_file(self, image: str|io.BytesIO) -> tuple[io.BytesIO, str, int, int]:
        with ExitStack() as es:
            if isinstance(image, str):
                image_buf = es.enter_context(open(image, "rb"))
            else:
                image_buf = image

            properties = ValidateDimension.get_pcd_properties(image_buf)
            w, h = int(properties["WIDTH"]), int(properties["HEIGHT"])
            image_buf.seek(0, 0)
            return io.BytesIO(image_buf.read()), self.POINT_CLOUD_EXT, w, h

    def save_as_chunk(self, images: Iterator[tuple[Image.Image|io.IOBase|str, str, str]], chunk_path: str):
        with zipfile.ZipFile(chunk_path, 'x') as zip_chunk:
            for idx, (image, path, _) in enumerate(images):
                ext = os.path.splitext(path)[1].replace('.', '')

                if self._dimension == DimensionType.DIM_2D:
                    # current version of Pillow applies exif rotation immediately when TIFF image opened
                    # and it removes rotation tag after that
                    # so, has_exif_rotation(image) will return False for TIFF images even if they were actually rotated
                    # and original files will be added to the archive (without applied rotation)
                    # that is why we need the second part of the condition
                    if isinstance(image, Image.Image) and (
                        has_exif_rotation(image) or image.format == 'TIFF'
                    ):
                        output = io.BytesIO()
                        rot_image = ImageOps.exif_transpose(image)
                        try:
                            if image.format == 'TIFF':
                                # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html
                                # use lossless lzw compression for tiff images
                                rot_image.save(output, format='TIFF', compression='tiff_lzw')
                            else:
                                rot_image.save(
                                    output,
                                    # use format from original image, https://github.com/python-pillow/Pillow/issues/5527
                                    format=image.format if image.format else self.IMAGE_EXT,
                                    quality=100,
                                    subsampling=0
                                )
                        finally:
                            rot_image.close()
                    elif isinstance(image, io.IOBase):
                        output = image
                    else:
                        output = path
                else:
                    if isinstance(image, io.BytesIO):
                        output, ext = self._write_pcd_file(image)[0:2]
                    else:
                        output, ext = self._write_pcd_file(path)[0:2]

                arcname = '{:06d}.{}'.format(idx, ext)
                if isinstance(output, io.BytesIO):
                    zip_chunk.writestr(arcname, output.getvalue())
                else:
                    zip_chunk.write(filename=output, arcname=arcname)

        # return empty list because ZipChunkWriter write files as is
        # and does not decode it to know img size.
        return []

class ZipCompressedChunkWriter(ZipChunkWriter):
    def save_as_chunk(
        self,
        images: Iterator[tuple[Image.Image|io.IOBase|str, str, str]],
        chunk_path: str, *, compress_frames: bool = True, zip_compress_level: int = 0
    ):
        image_sizes = []
        with zipfile.ZipFile(chunk_path, 'x', compresslevel=zip_compress_level) as zip_chunk:
            for idx, (image, path, _) in enumerate(images):
                if self._dimension == DimensionType.DIM_2D:
                    if compress_frames:
                        w, h, image_buf = self._compress_image(image, self._image_quality)
                    else:
                        assert isinstance(image, io.IOBase)
                        image_buf = io.BytesIO(image.read())
                        with Image.open(image_buf) as img:
                            w, h = img.size
                    extension = self.IMAGE_EXT
                else:
                    if isinstance(image, io.BytesIO):
                        image_buf, extension, w, h = self._write_pcd_file(image)
                    else:
                        image_buf, extension, w, h = self._write_pcd_file(path)

                image_sizes.append((w, h))
                arcname = '{:06d}.{}'.format(idx, extension)
                zip_chunk.writestr(arcname, image_buf.getvalue())
        return image_sizes

class Mpeg4ChunkWriter(IChunkWriter):
    FORMAT = 'mp4'
    MAX_MBS_PER_FRAME = 36864

    def __init__(self, quality=67):
        # translate inversed range [1:100] to [0:51]
        quality = round(51 * (100 - quality) / 99)
        super().__init__(quality)
        self._output_fps = 25
        try:
            codec = av.codec.Codec('libopenh264', 'w')
            self._codec_name = codec.name
            self._codec_opts = {
                'profile': 'constrained_baseline',
                'qmin': str(self._image_quality),
                'qmax': str(self._image_quality),
                'rc_mode': 'buffer',
            }
        except av.codec.codec.UnknownCodecError:
            codec = av.codec.Codec('libx264', 'w')
            self._codec_name = codec.name
            self._codec_opts = {
                "crf": str(self._image_quality),
                "preset": "ultrafast",
            }

    def _add_video_stream(self, container: av.container.OutputContainer, w, h, rate, options):
        # x264 requires width and height must be divisible by 2 for yuv420p
        if h % 2:
            h += 1
        if w % 2:
            w += 1

        # libopenh264 has 4K limitations, https://github.com/cvat-ai/cvat/issues/7425
        if h * w > (self.MAX_MBS_PER_FRAME << 8):
            raise ValidationError(
                'The video codec being used does not support such high video resolution, refer https://github.com/cvat-ai/cvat/issues/7425'
            )

        video_stream = container.add_stream(self._codec_name, rate=rate)
        video_stream.pix_fmt = "yuv420p"
        video_stream.width = w
        video_stream.height = h
        video_stream.options = options

        return video_stream

    FrameDescriptor = tuple[av.VideoFrame, Any, Any]

    def _peek_first_frame(
        self, frame_iter: Iterator[FrameDescriptor]
    ) -> tuple[Optional[FrameDescriptor], Iterator[FrameDescriptor]]:
        "Gets the first frame and returns the same full iterator"

        if not hasattr(frame_iter, '__next__'):
            frame_iter = iter(frame_iter)

        first_frame = next(frame_iter, None)
        return first_frame, itertools.chain((first_frame, ), frame_iter)

    def save_as_chunk(
        self, images: Iterator[FrameDescriptor], chunk_path: str
    ) -> Sequence[tuple[int, int]]:
        first_frame, images = self._peek_first_frame(images)
        if not first_frame:
            raise Exception('no images to save')

        input_w = first_frame[0].width
        input_h = first_frame[0].height

        with av.open(chunk_path, 'w', format=self.FORMAT) as output_container:
            output_v_stream = self._add_video_stream(
                container=output_container,
                w=input_w,
                h=input_h,
                rate=self._output_fps,
                options=self._codec_opts,
            )

            with closing(output_v_stream):
                self._encode_images(images, output_container, output_v_stream)

        return [(input_w, input_h)]

    @staticmethod
    def _encode_images(
        images, container: av.container.OutputContainer, stream: av.video.stream.VideoStream
    ):
        for frame, _, _ in images:
            # let libav set the correct pts and time_base
            frame.pts = None
            frame.time_base = None

            for packet in stream.encode(frame):
                container.mux(packet)

        # Flush streams
        for packet in stream.encode():
            container.mux(packet)

class Mpeg4CompressedChunkWriter(Mpeg4ChunkWriter):
    def __init__(self, quality):
        super().__init__(quality)
        if self._codec_name == 'libx264':
            self._codec_opts = {
                'profile': 'baseline',
                'coder': '0',
                'crf': str(self._image_quality),
                'wpredp': '0',
                'flags': '-loop',
            }

    def save_as_chunk(self, images, chunk_path):
        first_frame, images = self._peek_first_frame(images)
        if not first_frame:
            raise Exception('no images to save')

        input_w = first_frame[0].width
        input_h = first_frame[0].height

        downscale_factor = 1
        while input_h / downscale_factor >= 1080:
            downscale_factor *= 2

        output_h = input_h // downscale_factor
        output_w = input_w // downscale_factor

        with av.open(chunk_path, 'w', format=self.FORMAT) as output_container:
            output_v_stream = self._add_video_stream(
                container=output_container,
                w=output_w,
                h=output_h,
                rate=self._output_fps,
                options=self._codec_opts,
            )

            with closing(output_v_stream):
                self._encode_images(images, output_container, output_v_stream)

        return [(input_w, input_h)]

def _is_archive(path):
    mime = mimetypes.guess_type(path)
    mime_type = mime[0]
    encoding = mime[1]
    supportedArchives = ['application/x-rar-compressed',
        'application/x-tar', 'application/x-7z-compressed', 'application/x-cpio',
        'application/gzip', 'application/x-bzip']
    return mime_type in supportedArchives or encoding in supportedArchives

def _is_video(path):
    mime = mimetypes.guess_type(path)
    return mime[0] is not None and mime[0].startswith('video')

def _is_image(path):
    mime = mimetypes.guess_type(path)
    # Exclude vector graphic images because Pillow cannot work with them
    return mime[0] is not None and mime[0].startswith('image') and \
        not mime[0].startswith('image/svg')

def _is_dir(path):
    return os.path.isdir(path)

def _is_pdf(path):
    mime = mimetypes.guess_type(path)
    return mime[0] == 'application/pdf'

def _is_zip(path):
    mime = mimetypes.guess_type(path)
    mime_type = mime[0]
    encoding = mime[1]
    supportedArchives = ['application/zip']
    return mime_type in supportedArchives or encoding in supportedArchives

# 'has_mime_type': function receives 1 argument - path to file.
#                  Should return True if file has specified media type.
# 'extractor': class that extracts images from specified media.
# 'mode': 'annotation' or 'interpolation' - mode of task that should be created.
# 'unique': True or False - describes how the type can be combined with other.
#           True - only one item of this type and no other is allowed
#           False - this media types can be combined with other which have unique is False

MEDIA_TYPES = {
    'image': {
        'has_mime_type': _is_image,
        'extractor': ImageListReader,
        'mode': 'annotation',
        'unique': False,
    },
    'video': {
        'has_mime_type': _is_video,
        'extractor': VideoReader,
        'mode': 'interpolation',
        'unique': True,
    },
    'archive': {
        'has_mime_type': _is_archive,
        'extractor': ArchiveReader,
        'mode': 'annotation',
        'unique': True,
    },
    'directory': {
        'has_mime_type': _is_dir,
        'extractor': DirectoryReader,
        'mode': 'annotation',
        'unique': False,
    },
    'pdf': {
        'has_mime_type': _is_pdf,
        'extractor': PdfReader,
        'mode': 'annotation',
        'unique': True,
    },
    'zip': {
        'has_mime_type': _is_zip,
        'extractor': ZipReader,
        'mode': 'annotation',
        'unique': True,
    }
}

class ValidateDimension:

    def __init__(self, path=None):
        self.dimension = DimensionType.DIM_2D
        self.path = path
        self.related_files = {}
        self.image_files = {}
        self.converted_files = []

    @staticmethod
    def get_pcd_properties(fp, verify_version=False):
        kv = {}
        pcd_version = ["0.7", "0.6", "0.5", "0.4", "0.3", "0.2", "0.1",
                       ".7", ".6", ".5", ".4", ".3", ".2", ".1"]
        try:
            for line in fp:
                line = line.decode("utf-8")
                if line.startswith("#"):
                    continue
                k, v = line.split(" ", maxsplit=1)
                kv[k] = v.strip()
                if "DATA" in line:
                    break
            if verify_version:
                if "VERSION" in kv and kv["VERSION"] in pcd_version:
                    return True
                return None
            return kv
        except AttributeError:
            return None

    @staticmethod
    def convert_bin_to_pcd(path, delete_source=True):
        def write_header(fileObj, width, height):
            fileObj.writelines(f'{line}\n' for line in [
                'VERSION 0.7',
                'FIELDS x y z intensity',
                'SIZE 4 4 4 4',
                'TYPE F F F F',
                'COUNT 1 1 1 1',
                f'WIDTH {width}',
                f'HEIGHT {height}',
                'VIEWPOINT 0 0 0 1 0 0 0',
                f'POINTS {width * height}',
                'DATA binary',
            ])


        list_pcd = []
        with open(path, "rb") as f:
            size_float = 4
            byte = f.read(size_float * 4)
            while byte:
                x, y, z, intensity = struct.unpack("ffff", byte)
                list_pcd.append([x, y, z, intensity])
                byte = f.read(size_float * 4)
        np_pcd = np.asarray(list_pcd)
        pcd_filename = path.replace(".bin", ".pcd")
        with open(pcd_filename, "w") as f:
            write_header(f, np_pcd.shape[0], 1)
        with open(pcd_filename, "ab") as f:
            f.write(np_pcd.astype('float32').tobytes())
        if delete_source:
            os.remove(path)
        return pcd_filename

    def set_path(self, path):
        self.path = path

    def bin_operation(self, file_path, actual_path):
        pcd_path = ValidateDimension.convert_bin_to_pcd(file_path)
        self.converted_files.append(pcd_path)
        return pcd_path.split(actual_path)[-1][1:]

    @staticmethod
    def pcd_operation(file_path, actual_path):
        with open(file_path, "rb") as file:
            is_pcd = ValidateDimension.get_pcd_properties(file, verify_version=True)
        return file_path.split(actual_path)[-1][1:] if is_pcd else file_path

    def process_files(self, root, actual_path, files):
        pcd_files = {}

        for file in files:
            file_name, file_extension = os.path.splitext(file)
            file_path = os.path.abspath(os.path.join(root, file))

            if file_extension == ".bin":
                path = self.bin_operation(file_path, actual_path)
                pcd_files[file_name] = path
                self.related_files[path] = []

            elif file_extension == ".pcd":
                path = ValidateDimension.pcd_operation(file_path, actual_path)
                if path == file_path:
                    self.image_files[file_name] = file_path
                else:
                    pcd_files[file_name] = path
                    self.related_files[path] = []
            else:
                if _is_image(file_path):
                    self.image_files[file_name] = file_path
        return pcd_files

    def validate(self):
        """
            Validate the directory structure for kitty and point cloud format.
        """
        if not self.path:
            return
        actual_path = self.path
        for root, _, files in os.walk(actual_path):
            if not files_to_ignore(root):
                continue

            self.process_files(root, actual_path, files)

        if len(self.related_files.keys()):
            self.dimension = DimensionType.DIM_3D


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\middleware.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import Protocol
from uuid import uuid4


class WithUUID(Protocol):
    uuid: str


class RequestTrackingMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    @staticmethod
    def _generate_id():
        return str(uuid4())

    def __call__(self, request):
        request.uuid = self._generate_id()
        response = self.get_response(request)
        response.headers["X-Request-Id"] = request.uuid

        return response


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\mime_types.py =====
# Copyright (C) 2019-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import mimetypes
import os

_SCRIPT_DIR = os.path.realpath(os.path.dirname(__file__))
MEDIA_MIMETYPES_FILES = [
    os.path.join(_SCRIPT_DIR, "media.mimetypes"),
]
mimetypes.init(files=MEDIA_MIMETYPES_FILES)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\mixins.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import base64
import json
import os
import os.path
import uuid
from dataclasses import asdict, dataclass
from pathlib import Path
from tempfile import NamedTemporaryFile
from textwrap import dedent
from typing import Callable
from unittest import mock
from urllib.parse import urljoin

import django_rq
from attr.converters import to_bool
from django.conf import settings
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import OpenApiParameter, OpenApiResponse, extend_schema
from rest_framework import mixins, status
from rest_framework.decorators import action
from rest_framework.response import Response

from cvat.apps.engine.background import BackupExportManager, DatasetExportManager
from cvat.apps.engine.handlers import clear_import_cache
from cvat.apps.engine.location import StorageType, get_location_configuration
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import (
    Job,
    Location,
    Project,
    RequestAction,
    RequestSubresource,
    RequestTarget,
    Task,
)
from cvat.apps.engine.rq import RQId
from cvat.apps.engine.serializers import DataSerializer, RqIdSerializer
from cvat.apps.engine.types import ExtendedRequest

slogger = ServerLogManager(__name__)

class TusFile:
    @dataclass
    class TusMeta:
        metadata: dict
        filename: str
        file_size: int
        offset: int = 0

    class TusMetaFile():
        def __init__(self, path) -> None:
            self._path = path
            self._meta = None
            if os.path.exists(self._path):
                self._meta = self._read()

        @property
        def meta(self):
            return self._meta

        @meta.setter
        def meta(self, meta):
            self._meta = meta

        def _read(self):
            with open(self._path, "r") as fp:
                data = json.load(fp)
            return TusFile.TusMeta(**data)

        def save(self):
            if self._meta is not None:
                os.makedirs(os.path.dirname(self._path), exist_ok=True)
                with open(self._path, "w") as fp:
                    json.dump(asdict(self._meta), fp)

        def exists(self):
            return os.path.exists(self._path)

        def delete(self):
            os.remove(self._path)

    def __init__(self, file_id, upload_dir, meta=None):
        self.file_id = file_id
        self.upload_dir = upload_dir
        self.file_path = os.path.join(self.upload_dir, self.file_id)
        self.meta_file = self.TusMetaFile(self._get_tus_meta_file_path(file_id, upload_dir))
        if meta is not None:
            self.meta_file.meta = meta
            self.meta_file.save()

    @property
    def filename(self):
        return self.meta_file.meta.filename

    @property
    def file_size(self):
        return self.meta_file.meta.file_size

    @property
    def offset(self):
        return self.meta_file.meta.offset

    def exists(self):
        return self.meta_file.exists()

    @staticmethod
    def _get_tus_meta_file_path(file_id, upload_dir):
        return os.path.join(upload_dir, f"{file_id}.meta")

    def init_file(self):
        os.makedirs(self.upload_dir, exist_ok=True)
        file_path = os.path.join(self.upload_dir, self.file_id)
        with open(file_path, 'wb') as file:
            file.seek(self.file_size - 1)
            file.write(b'\0')

    def write_chunk(self, chunk):
        with open(self.file_path, 'r+b') as file:
            file.seek(chunk.offset)
            file.write(chunk.content)
        self.meta_file.meta.offset += chunk.size
        self.meta_file.save()

    def is_complete(self):
        return self.offset == self.file_size

    def rename(self):
        file_path = os.path.join(self.upload_dir, self.filename)
        if os.path.lexists(file_path):
            original_file_name, extension = os.path.splitext(self.filename)
            file_amount = 1
            while os.path.lexists(os.path.join(self.upload_dir, self.filename)):
                self.meta_file.meta.filename = "{}_{}{}".format(original_file_name, file_amount, extension)
                file_path = os.path.join(self.upload_dir, self.filename)
                file_amount += 1
        os.rename(self.file_path, file_path)

    def clean(self):
        self.meta_file.delete()

    @staticmethod
    def create_file(metadata, file_size, upload_dir):
        file_id = str(uuid.uuid4())
        filename = metadata.get("filename")

        tus_file = TusFile(
            file_id,
            upload_dir,
            TusFile.TusMeta(
                filename=filename,
                file_size=file_size,
                offset=0,
                metadata=metadata,
            ),
        )
        tus_file.init_file()

        return tus_file

class TusChunk:
    def __init__(self, request: ExtendedRequest):
        self.META = request.META
        self.offset = int(request.META.get("HTTP_UPLOAD_OFFSET", 0))
        self.size = int(request.META.get("CONTENT_LENGTH", settings.TUS_DEFAULT_CHUNK_SIZE))
        self.content = request.body

class UploadMixin:
    """
    Implements file uploads to the server. Allows to upload single and multiple files, suspend
    and resume uploading. Uses the TUS open file uploading protocol (https://tus.io/).

    Implements the following protocols:
    a. A single Data request

    and

    b.1. An Upload-Start request
    b.2.a. The regular TUS protocol requests (Upload-Length + Chunks)
    b.2.b. Upload-Multiple requests
    b.3. An Upload-Finish request

    Requests:
    - Data - POST, no extra headers or 'Upload-Start' + 'Upload-Finish' headers
    - Upload-Start - POST, has an 'Upload-Start' header
    - Upload-Length - POST, has an 'Upload-Length' header (read the TUS protocol)
    - Chunk - HEAD/PATCH (read the TUS protocol)
    - Upload-Finish - POST, has an 'Upload-Finish' header
    - Upload-Multiple - POST, has a 'Upload-Multiple' header
    """

    _tus_api_version = '1.0.0'
    _tus_api_version_supported = ['1.0.0']
    _tus_api_extensions = []
    _tus_max_file_size = str(settings.TUS_MAX_FILE_SIZE)
    _base_tus_headers = {
        'Tus-Resumable': _tus_api_version,
        'Tus-Version': ",".join(_tus_api_version_supported),
        'Tus-Extension': ",".join(_tus_api_extensions),
        'Tus-Max-Size': _tus_max_file_size,
        'Access-Control-Allow-Origin': "*",
        'Access-Control-Allow-Methods': "PATCH,HEAD,GET,POST,OPTIONS",
        'Access-Control-Expose-Headers': "Tus-Resumable,upload-length,upload-metadata,Location,Upload-Offset",
        'Access-Control-Allow-Headers': "Tus-Resumable,upload-length,upload-metadata,Location,Upload-Offset,content-type",
        'Cache-Control': 'no-store'
    }
    file_id_regex = r'(?P<file_id>\b[0-9a-f]{8}\b-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-\b[0-9a-f]{12}\b)'

    def _tus_response(self, status, data=None, extra_headers=None):
        response = Response(data, status)
        for key, value in self._base_tus_headers.items():
            response.__setitem__(key, value)
        if extra_headers:
            for key, value in extra_headers.items():
                response.__setitem__(key, value)
        return response

    def _get_metadata(self, request: ExtendedRequest):
        metadata = {}
        if request.META.get("HTTP_UPLOAD_METADATA"):
            for kv in request.META.get("HTTP_UPLOAD_METADATA").split(","):
                splited_metadata = kv.split(" ")
                if len(splited_metadata) == 2:
                    key, value = splited_metadata
                    value = base64.b64decode(value)
                    if isinstance(value, bytes):
                        value = value.decode()
                    metadata[key] = value
                else:
                    metadata[splited_metadata[0]] = ""
        return metadata

    def upload_data(self, request: ExtendedRequest):
        tus_request = request.headers.get('Upload-Length', None) is not None or request.method == 'OPTIONS'
        bulk_file_upload = request.headers.get('Upload-Multiple', None) is not None
        start_upload = request.headers.get('Upload-Start', None) is not None
        finish_upload = request.headers.get('Upload-Finish', None) is not None
        one_request_upload = start_upload and finish_upload
        if one_request_upload or finish_upload:
            return self.upload_finished(request)
        elif start_upload:
            return self.upload_started(request)
        elif tus_request:
            return self.init_tus_upload(request)
        elif bulk_file_upload:
            return self.append_files(request)
        else: # backward compatibility case - no upload headers were found
            return self.upload_finished(request)

    def init_tus_upload(self, request: ExtendedRequest):
        if request.method == 'OPTIONS':
            return self._tus_response(status=status.HTTP_204_NO_CONTENT)
        else:
            metadata = self._get_metadata(request)
            filename = metadata.get('filename', '')
            if not self.is_valid_uploaded_file_name(filename):
                return self._tus_response(status=status.HTTP_400_BAD_REQUEST,
                    data="File name {} is not allowed".format(filename))


            message_id = request.META.get("HTTP_MESSAGE_ID")
            if message_id:
                metadata["message_id"] = base64.b64decode(message_id)

            import_type = request.path.strip('/').split('/')[-1]
            if import_type == 'backup':
                # we need to create unique temp file here because
                # users can try to import backups with the same name at the same time
                with NamedTemporaryFile(prefix=f'cvat-backup-{filename}-by-{request.user}', suffix='.zip', dir=self.get_upload_dir()) as tmp_file:
                    filename = os.path.relpath(tmp_file.name, self.get_upload_dir())
                metadata['filename'] = filename
            file_path = os.path.join(self.get_upload_dir(), filename)
            file_exists = os.path.lexists(file_path) and import_type != 'backup'

            if file_exists:
                # check whether the rq_job is in progress or has been finished/failed
                object_class_name = self._object.__class__.__name__.lower()
                template = RQId(
                    RequestAction.IMPORT, RequestTarget(object_class_name), self._object.pk,
                    subresource=RequestSubresource(import_type)
                ).render()
                queue = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)
                finished_job_ids = queue.finished_job_registry.get_job_ids()
                failed_job_ids = queue.failed_job_registry.get_job_ids()
                if template in finished_job_ids or template in failed_job_ids:
                    os.remove(file_path)
                    file_exists = False

            if file_exists:
                return self._tus_response(status=status.HTTP_409_CONFLICT,
                    data="File with same name already exists")

            file_size = int(request.META.get("HTTP_UPLOAD_LENGTH", "0"))
            if file_size > int(self._tus_max_file_size):
                return self._tus_response(status=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE,
                    data="File size exceeds max limit of {} bytes".format(self._tus_max_file_size))


            tus_file = TusFile.create_file(metadata, file_size, self.get_upload_dir())

            location = request.build_absolute_uri()
            if 'HTTP_X_FORWARDED_HOST' not in request.META:
                location = request.META.get('HTTP_ORIGIN') + request.META.get('PATH_INFO')

            if import_type in ('backup', 'annotations', 'datasets'):
                scheduler = django_rq.get_scheduler(settings.CVAT_QUEUES.CLEANING.value)
                path = Path(self.get_upload_dir()) / tus_file.filename
                cleaning_job = scheduler.enqueue_in(time_delta=settings.IMPORT_CACHE_CLEAN_DELAY,
                    func=clear_import_cache,
                    path=path,
                    creation_time=Path(tus_file.file_path).stat().st_ctime
                )
                slogger.glob.info(
                    f'The cleaning job {cleaning_job.id} is queued.'
                    f'The check that the file {path} is deleted will be carried out after '
                    f'{settings.IMPORT_CACHE_CLEAN_DELAY}.'
                )

            return self._tus_response(
                status=status.HTTP_201_CREATED,
                extra_headers={'Location': urljoin(location, tus_file.file_id),
                               'Upload-Filename': tus_file.filename})

    def append_tus_chunk(self, request: ExtendedRequest, file_id: str):
        tus_file = TusFile(str(file_id), self.get_upload_dir())
        if request.method == 'HEAD':
            if tus_file.exists():
                return self._tus_response(status=status.HTTP_200_OK, extra_headers={
                               'Upload-Offset': tus_file.offset,
                               'Upload-Length': tus_file.file_size})
            return self._tus_response(status=status.HTTP_404_NOT_FOUND)

        chunk = TusChunk(request)

        if chunk.offset != tus_file.offset:
            return self._tus_response(status=status.HTTP_409_CONFLICT)

        if chunk.offset > tus_file.file_size:
            return self._tus_response(status=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE)

        tus_file.write_chunk(chunk)

        if tus_file.is_complete():
            tus_file.rename()
            tus_file.clean()

        return self._tus_response(status=status.HTTP_204_NO_CONTENT,
                                    extra_headers={'Upload-Offset': tus_file.offset,
                                                    'Upload-Filename': tus_file.filename})

    def is_valid_uploaded_file_name(self, filename: str) -> bool:
        """
        Checks the file name to be valid.
        Returns True if the filename is valid, otherwise returns False.
        """

        upload_dir = self.get_upload_dir()
        file_path = os.path.join(upload_dir, filename)
        return os.path.commonprefix((os.path.realpath(file_path), upload_dir)) == upload_dir

    def get_upload_dir(self) -> str:
        return self._object.data.get_upload_dirname()

    def _get_request_client_files(self, request: ExtendedRequest):
        serializer = DataSerializer(self._object, data=request.data)
        serializer.is_valid(raise_exception=True)
        return serializer.validated_data.get('client_files')

    def append_files(self, request: ExtendedRequest):
        """
        Processes a single or multiple files sent in a single request inside
        a file uploading session.
        """

        client_files = self._get_request_client_files(request)
        if client_files:
            upload_dir = self.get_upload_dir()
            for client_file in client_files:
                filename = client_file['file'].name
                if not self.is_valid_uploaded_file_name(filename):
                    return Response(status=status.HTTP_400_BAD_REQUEST,
                        data=f"File name {filename} is not allowed", content_type="text/plain")

                with open(os.path.join(upload_dir, filename), 'ab+') as destination:
                    destination.write(client_file['file'].read())
        return Response(status=status.HTTP_200_OK)

    def upload_started(self, request: ExtendedRequest):
        """
        Allows to do actions before upcoming file uploading.
        """
        return Response(status=status.HTTP_202_ACCEPTED)

    def upload_finished(self, request: ExtendedRequest):
        """
        Allows to process uploaded files.
        """

        raise NotImplementedError('Must be implemented in the derived class')

class PartialUpdateModelMixin:
    """
    Update fields of a model instance.

    Almost the same as UpdateModelMixin, but has no public PUT / update() method.
    """

    def _update(self, request: ExtendedRequest, *args, **kwargs):
        # This method must not be named "update" not to be matched with the PUT method
        return mixins.UpdateModelMixin.update(self, request, *args, **kwargs)

    def perform_update(self, serializer):
        mixins.UpdateModelMixin.perform_update(self, serializer=serializer)

    def partial_update(self, request: ExtendedRequest, *args, **kwargs):
        with mock.patch.object(self, 'update', new=self._update, create=True):
            return mixins.UpdateModelMixin.partial_update(self, request=request, *args, **kwargs)

class DatasetMixin:
    @extend_schema(
        summary='Initialize process to export resource as a dataset in a specific format',
        description=dedent("""\
             The request `POST /api/<projects|tasks|jobs>/id/dataset/export` will initialize
             a background process to export a dataset. To check status of the process
             please, use `GET /api/requests/<rq_id>` where **rq_id** is request ID returned in the response for this endpoint.
         """),
        parameters=[
            OpenApiParameter('format', location=OpenApiParameter.QUERY,
                description='Desired output format name\nYou can get the list of supported formats at:\n/server/annotation/formats',
                type=OpenApiTypes.STR, required=True),
            OpenApiParameter('filename', description='Desired output file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
            OpenApiParameter('location', description='Where need to save downloaded dataset',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list()),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('save_images', description='Include images or not',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.BOOL, required=False, default=False),
        ],
        request=OpenApiTypes.NONE,
        responses={
            '202': OpenApiResponse(response=RqIdSerializer, description='Exporting has been started'),
            '405': OpenApiResponse(description='Format is not available'),
            '409': OpenApiResponse(description='Exporting is already in progress'),
        },
    )
    @action(detail=True, methods=['POST'], serializer_class=None, url_path='dataset/export')
    def initiate_dataset_export(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # force call of check_object_permissions()

        export_manager = DatasetExportManager(self._object, request)
        return export_manager.export()

    @extend_schema(summary='Download a prepared dataset file',
        parameters=[
            OpenApiParameter('rq_id', description='Request ID',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=True),
        ],
        responses={
            '200': OpenApiResponse(description='Download of file started'),
        },
        exclude=True, # private API endpoint that should be used only as result_url
    )
    @action(methods=['GET'], detail=True, url_path='dataset/download')
    def download_dataset(self, request: ExtendedRequest, pk: int):
        obj = self.get_object()  # force to call check_object_permissions
        export_manager = DatasetExportManager(obj, request)
        return export_manager.download_file()

    # FUTURE-TODO: migrate to new API
    def import_annotations(
        self,
        request: ExtendedRequest,
        db_obj: Project | Task | Job,
        import_func: Callable[..., None],
        rq_func: Callable[..., None],
        rq_id_factory: Callable[..., RQId],
    ):
        is_tus_request = request.headers.get('Upload-Length', None) is not None or \
            request.method == 'OPTIONS'
        if is_tus_request:
            return self.init_tus_upload(request)

        conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
        location_conf = get_location_configuration(
            db_instance=db_obj,
            query_params=request.query_params,
            field_name=StorageType.SOURCE,
        )

        if location_conf['location'] == Location.CLOUD_STORAGE:
            format_name = request.query_params.get('format')
            file_name = request.query_params.get('filename')

            return import_func(
                request=request,
                rq_id_factory=rq_id_factory,
                rq_func=rq_func,
                db_obj=self._object,
                format_name=format_name,
                location_conf=location_conf,
                filename=file_name,
                conv_mask_to_poly=conv_mask_to_poly,
            )

        return self.upload_data(request)


class BackupMixin:
    # FUTURE-TODO: migrate to new API
    def import_backup_v1(self, request: ExtendedRequest, import_func: Callable) -> Response:
        location = request.query_params.get("location", Location.LOCAL)
        if location == Location.CLOUD_STORAGE:
            file_name = request.query_params.get("filename", "")
            return import_func(
                request,
                queue_name=settings.CVAT_QUEUES.IMPORT_DATA.value,
                filename=file_name,
            )
        return self.upload_data(request)

    @extend_schema(summary='Initiate process to backup resource',
        description=dedent("""\
             The request `POST /api/<projects|tasks>/id/backup/export` will initialize
             a background process to backup a resource. To check status of the process
             please, use `GET /api/requests/<rq_id>` where **rq_id** is request ID returned in the response for this endpoint.
         """),
        parameters=[
            OpenApiParameter('filename', description='Backup file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
            OpenApiParameter('location', description='Where need to save downloaded backup',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list()),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
        ],
        request=OpenApiTypes.NONE,
        responses={
            '202': OpenApiResponse(response=RqIdSerializer, description='Creating a backup file has been started'),
            '400': OpenApiResponse(description='Wrong query parameters were passed'),
            '409': OpenApiResponse(description='The backup process has already been initiated and is not yet finished'),
        },
    )
    @action(detail=True, methods=['POST'], serializer_class=None, url_path='backup/export')
    def initiate_backup_export(self, request: ExtendedRequest, pk: int):
        db_object = self.get_object() # force to call check_object_permissions
        export_manager = BackupExportManager(db_object, request)
        return export_manager.export()


    @extend_schema(summary='Download a prepared backup file',
        parameters=[
            OpenApiParameter('rq_id', description='Request ID',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=True),
        ],
        responses={
            '200': OpenApiResponse(description='Download of file started'),
        },
        exclude=True, # private API endpoint that should be used only as result_url
    )
    @action(methods=['GET'], detail=True, url_path='backup/download')
    def download_backup(self, request: ExtendedRequest, pk: int):
        obj = self.get_object()  # force to call check_object_permissions
        export_manager = BackupExportManager(obj, request)
        return export_manager.download_file()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\models.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import datetime
import os
import re
import shutil
import uuid
from abc import ABCMeta, abstractmethod
from collections.abc import Collection, Iterable, Sequence
from enum import Enum
from functools import cached_property
from typing import TYPE_CHECKING, Any, ClassVar, Optional

from django.conf import settings
from django.contrib.auth.models import User
from django.core.exceptions import ValidationError
from django.core.files.storage import FileSystemStorage
from django.db import IntegrityError, models, transaction
from django.db.models import Q, TextChoices
from django.db.models.base import ModelBase
from django.db.models.fields import FloatField
from django.utils.translation import gettext_lazy as _
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import extend_schema_field

from cvat.apps.engine.lazy_list import LazyList
from cvat.apps.engine.model_utils import MaybeUndefined
from cvat.apps.engine.utils import parse_specific_attributes, take_by
from cvat.apps.events.utils import cache_deleted

if TYPE_CHECKING:
    from cvat.apps.organizations.models import Organization


class SafeCharField(models.CharField):
    def get_prep_value(self, value):
        value = super().get_prep_value(value)
        if value:
            return value[:self.max_length]
        return value


class DimensionType(str, Enum):
    DIM_3D = '3d'
    DIM_2D = '2d'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class StatusChoice(str, Enum):
    """Deprecated. Use StageChoice and StateChoice instead"""

    ANNOTATION = 'annotation'
    VALIDATION = 'validation'
    COMPLETED = 'completed'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    @classmethod
    def list(cls):
        return list(map(lambda x: x.value, cls))

    def __str__(self):
        return self.value

class LabelType(str, Enum):
    ANY = 'any'
    CUBOID = 'cuboid'
    ELLIPSE = 'ellipse'
    MASK = 'mask'
    POINTS = 'points'
    POLYGON = 'polygon'
    POLYLINE = 'polyline'
    RECTANGLE = 'rectangle'
    SKELETON = 'skeleton'
    TAG = 'tag'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    @classmethod
    def list(cls):
        return list(map(lambda x: x.value, cls))

    def __str__(self):
        return self.value

class StageChoice(str, Enum):
    ANNOTATION = 'annotation'
    VALIDATION = 'validation'
    ACCEPTANCE = 'acceptance'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class StateChoice(str, Enum):
    NEW = 'new'
    IN_PROGRESS = 'in progress'
    COMPLETED = 'completed'
    REJECTED = 'rejected'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class DataChoice(str, Enum):
    VIDEO = 'video'
    IMAGESET = 'imageset'
    LIST = 'list'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class StorageMethodChoice(str, Enum):
    CACHE = 'cache'
    FILE_SYSTEM = 'file_system'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class StorageChoice(str, Enum):
    CLOUD_STORAGE = 'cloud_storage'
    LOCAL = 'local'
    SHARE = 'share'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class SortingMethod(str, Enum):
    LEXICOGRAPHICAL = 'lexicographical'
    NATURAL = 'natural'
    PREDEFINED = 'predefined'
    RANDOM = 'random'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class JobType(str, Enum):
    ANNOTATION = 'annotation'
    GROUND_TRUTH = 'ground_truth'
    CONSENSUS_REPLICA = 'consensus_replica'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class JobFrameSelectionMethod(str, Enum):
    RANDOM_UNIFORM = 'random_uniform'
    RANDOM_PER_JOB = 'random_per_job'
    MANUAL = 'manual'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value


class AbstractArrayField(models.TextField):
    separator = ","
    converter = staticmethod(lambda x: x)

    def __init__(self, *args, store_sorted:Optional[bool]=False, unique_values:Optional[bool]=False, **kwargs):
        self._store_sorted = store_sorted
        self._unique_values = unique_values
        super().__init__(*args,**{'default': '', **kwargs})

    def from_db_value(self, value, expression, connection):
        if not value:
            return []
        return LazyList(string=value, separator=self.separator, converter=self.converter)

    def to_python(self, value):
        if isinstance(value, list | LazyList):
            return value

        return self.from_db_value(value, None, None)

    def get_prep_value(self, value):
        if isinstance(value, LazyList) and not (self._unique_values or self._store_sorted):
            return str(value)

        if self._unique_values:
            value = dict.fromkeys(value)
        if self._store_sorted:
            value = sorted(value)
        return self.separator.join(map(str, value))

class FloatArrayField(AbstractArrayField):
    converter = float

class IntArrayField(AbstractArrayField):
    converter = int

class ValidationMode(str, Enum):
    GT = "gt"
    GT_POOL = "gt_pool"

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class ValidationParams(models.Model):
    task_data = models.OneToOneField(
        'Data', on_delete=models.CASCADE, related_name="validation_params"
    )

    mode = models.CharField(max_length=32, choices=ValidationMode.choices())

    frame_selection_method = models.CharField(
        max_length=32, choices=JobFrameSelectionMethod.choices()
    )
    random_seed = models.IntegerField(null=True)

    frames: models.manager.RelatedManager[ValidationFrame]
    frame_count = models.IntegerField(null=True)
    frame_share = models.FloatField(null=True)
    frames_per_job_count = models.IntegerField(null=True)
    frames_per_job_share = models.FloatField(null=True)

class ValidationFrame(models.Model):
    validation_params = models.ForeignKey(
        ValidationParams, on_delete=models.CASCADE, related_name="frames"
    )
    path = models.CharField(max_length=1024, default='')

class ValidationLayout(models.Model):
    "Represents validation configuration in a task"

    task_data = models.OneToOneField(
        'Data', on_delete=models.CASCADE, related_name="validation_layout"
    )

    mode = models.CharField(max_length=32, choices=ValidationMode.choices())

    frames_per_job_count = models.IntegerField(null=True)

    frames = IntArrayField(store_sorted=True, unique_values=True)
    "Stores task frame numbers of the validation frames"

    disabled_frames = IntArrayField(store_sorted=True, unique_values=True)
    "Stores task frame numbers of the disabled (deleted) validation frames"

    @property
    def active_frames(self) -> Sequence[int]:
        "An ordered sequence of active (non-disabled) validation frames"
        return set(self.frames).difference(self.disabled_frames)

class Data(models.Model):
    MANIFEST_FILENAME: ClassVar[str] = 'manifest.jsonl'

    chunk_size = models.PositiveIntegerField(null=True)
    size = models.PositiveIntegerField(default=0)
    image_quality = models.PositiveSmallIntegerField(default=50)
    start_frame = models.PositiveIntegerField(default=0)
    stop_frame = models.PositiveIntegerField(default=0)
    frame_filter = models.CharField(max_length=256, default="", blank=True)
    compressed_chunk_type = models.CharField(max_length=32, choices=DataChoice.choices(),
        default=DataChoice.IMAGESET)
    original_chunk_type = models.CharField(max_length=32, choices=DataChoice.choices(),
        default=DataChoice.IMAGESET)
    storage_method = models.CharField(max_length=15, choices=StorageMethodChoice.choices(), default=StorageMethodChoice.FILE_SYSTEM)
    storage = models.CharField(max_length=15, choices=StorageChoice.choices(), default=StorageChoice.LOCAL)
    cloud_storage = models.ForeignKey('CloudStorage', on_delete=models.SET_NULL, null=True, related_name='data')
    sorting_method = models.CharField(max_length=15, choices=SortingMethod.choices(), default=SortingMethod.LEXICOGRAPHICAL)
    deleted_frames = IntArrayField(store_sorted=True, unique_values=True)

    images: models.manager.RelatedManager[Image]
    video: MaybeUndefined[Video]
    related_files: models.manager.RelatedManager[RelatedFile]
    validation_layout: MaybeUndefined[ValidationLayout]

    client_files: models.manager.RelatedManager[ClientFile]
    server_files: models.manager.RelatedManager[ServerFile]
    remote_files: models.manager.RelatedManager[RemoteFile]
    validation_params: MaybeUndefined[ValidationParams]
    """
    Represents user-requested validation params before task is created.
    After the task creation, 'validation_layout' is used instead.
    """

    class Meta:
        default_permissions = ()

    def get_frame_step(self):
        match = re.search(r"step\s*=\s*([1-9]\d*)", self.frame_filter)
        return int(match.group(1)) if match else 1

    def get_valid_frame_indices(self):
        return range(self.start_frame, self.stop_frame + 1, self.get_frame_step())

    def get_data_dirname(self):
        return os.path.join(settings.MEDIA_DATA_ROOT, str(self.id))

    def get_upload_dirname(self):
        return os.path.join(self.get_data_dirname(), "raw")

    def get_raw_data_dirname(self) -> str:
        return {
            StorageChoice.LOCAL: self.get_upload_dirname(),
            StorageChoice.SHARE: settings.SHARE_ROOT,
            StorageChoice.CLOUD_STORAGE: self.get_upload_dirname(),
        }[self.storage]

    def get_compressed_cache_dirname(self):
        return os.path.join(self.get_data_dirname(), "compressed")

    def get_original_cache_dirname(self):
        return os.path.join(self.get_data_dirname(), "original")

    @staticmethod
    def _get_chunk_name(segment_id: int, chunk_number: int, chunk_type: DataChoice | str) -> str:
        if chunk_type == DataChoice.VIDEO:
            ext = 'mp4'
        elif chunk_type == DataChoice.IMAGESET:
            ext = 'zip'
        else:
            ext = 'list'

        return 'segment_{}-{}.{}'.format(segment_id, chunk_number, ext)

    def _get_compressed_chunk_name(self, segment_id: int, chunk_number: int) -> str:
        return self._get_chunk_name(segment_id, chunk_number, self.compressed_chunk_type)

    def _get_original_chunk_name(self, segment_id: int, chunk_number: int) -> str:
        return self._get_chunk_name(segment_id, chunk_number, self.original_chunk_type)

    def get_original_segment_chunk_path(self, chunk_number: int, segment_id: int) -> str:
        return os.path.join(self.get_original_cache_dirname(),
            self._get_original_chunk_name(segment_id, chunk_number))

    def get_compressed_segment_chunk_path(self, chunk_number: int, segment_id: int) -> str:
        return os.path.join(self.get_compressed_cache_dirname(),
            self._get_compressed_chunk_name(segment_id, chunk_number))

    def get_manifest_path(self) -> str:
        return os.path.join(self.get_upload_dirname(), self.MANIFEST_FILENAME)

    def make_dirs(self):
        data_path = self.get_data_dirname()
        if os.path.isdir(data_path):
            shutil.rmtree(data_path)
        os.makedirs(self.get_compressed_cache_dirname())
        os.makedirs(self.get_original_cache_dirname())
        os.makedirs(self.get_upload_dirname())

    @transaction.atomic
    def update_validation_layout(
        self, validation_layout: Optional[ValidationLayout]
    ) -> Optional[ValidationLayout]:
        if validation_layout:
            validation_layout.task_data = self
            validation_layout.save()

        ValidationParams.objects.filter(task_data_id=self.id).delete()

        return validation_layout

    @property
    def validation_mode(self) -> Optional[ValidationMode]:
        return getattr(getattr(self, 'validation_layout', None), 'mode', None)


class Video(models.Model):
    data = models.OneToOneField(Data, on_delete=models.CASCADE, related_name="video", null=True)
    path = models.CharField(max_length=1024, default='')
    width = models.PositiveIntegerField()
    height = models.PositiveIntegerField()

    class Meta:
        default_permissions = ()


class Image(models.Model):
    data = models.ForeignKey(Data, on_delete=models.CASCADE, related_name="images", null=True)
    path = models.CharField(max_length=1024, default='')
    frame = models.PositiveIntegerField()
    width = models.PositiveIntegerField()
    height = models.PositiveIntegerField()
    is_placeholder = models.BooleanField(default=False)
    real_frame = models.PositiveIntegerField(default=0)
    related_files: models.manager.RelatedManager[RelatedFile]

    class Meta:
        default_permissions = ()

class TimestampedModel(models.Model):
    created_date = models.DateTimeField(auto_now_add=True)
    updated_date = models.DateTimeField(auto_now=True)

    class Meta:
        abstract = True

    def touch(self) -> None:
        self.save(update_fields=["updated_date"])

class ABCModelMeta(ABCMeta, ModelBase):
    pass

class FileSystemRelatedModel(metaclass=ABCModelMeta):
    @abstractmethod
    def get_dirname(self) -> str:
        ...

    def get_tmp_dirname(self) -> str:
        """
        The method returns a directory that is only used
        to store temporary files or folders related to the object
        """
        dir_path = os.path.join(self.get_dirname(), "tmp")
        os.makedirs(dir_path, exist_ok=True)

        return dir_path


@transaction.atomic(savepoint=False)
def clear_annotations_in_jobs(job_ids: Iterable[int]):
    for job_ids_chunk in take_by(job_ids, chunk_size=1000):
        TrackedShapeAttributeVal.objects.filter(shape__track__job_id__in=job_ids_chunk).delete()
        TrackedShape.objects.filter(track__job_id__in=job_ids_chunk).delete()
        LabeledTrackAttributeVal.objects.filter(track__job_id__in=job_ids_chunk).delete()
        LabeledTrack.objects.filter(job_id__in=job_ids_chunk).delete()
        LabeledShapeAttributeVal.objects.filter(shape__job_id__in=job_ids_chunk).delete()
        LabeledShape.objects.filter(job_id__in=job_ids_chunk).delete()
        LabeledImageAttributeVal.objects.filter(image__job_id__in=job_ids_chunk).delete()
        LabeledImage.objects.filter(job_id__in=job_ids_chunk).delete()


@transaction.atomic(savepoint=False)
def clear_annotations_on_frames_in_honeypot_task(db_task: Task, frames: Sequence[int]):
    if db_task.data.validation_mode != ValidationMode.GT_POOL:
        # Tracks are prohibited in honeypot tasks
        raise AssertionError

    for frames_batch in take_by(frames, chunk_size=1000):
        LabeledShapeAttributeVal.objects.filter(
            shape__job_id__segment__task_id=db_task.id,
            shape__frame__in=frames_batch,
        ).delete()
        LabeledShape.objects.filter(
            job_id__segment__task_id=db_task.id,
            frame__in=frames_batch,
        ).delete()
        LabeledImageAttributeVal.objects.filter(
            image__job_id__segment__task_id=db_task.id,
            image__frame__in=frames_batch,
        ).delete()
        LabeledImage.objects.filter(
            job_id__segment__task_id=db_task.id,
            frame__in=frames_batch,
        ).delete()

class Project(TimestampedModel, FileSystemRelatedModel):
    name = SafeCharField(max_length=256)
    owner = models.ForeignKey(User, null=True, blank=True,
                              on_delete=models.SET_NULL, related_name="+")
    assignee = models.ForeignKey(User, null=True, blank=True,
                                 on_delete=models.SET_NULL, related_name="+")
    assignee_updated_date = models.DateTimeField(null=True, blank=True, default=None)

    bug_tracker = models.CharField(max_length=2000, blank=True, default="")
    status = models.CharField(max_length=32, choices=StatusChoice.choices(),
                              default=StatusChoice.ANNOTATION)
    organization = models.ForeignKey('organizations.Organization', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name="projects")
    source_storage = models.ForeignKey('Storage', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name='+')
    target_storage = models.ForeignKey('Storage', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name='+')

    def get_labels(self, prefetch=False):
        queryset = self.label_set.filter(parent__isnull=True).select_related('skeleton')
        return queryset.prefetch_related(
            'attributespec_set', 'sublabels__attributespec_set',
        ) if prefetch else queryset

    def get_dirname(self) -> str:
        return os.path.join(settings.PROJECTS_ROOT, str(self.id))

    def is_job_staff(self, user_id):
        if self.owner == user_id:
            return True

        if self.assignee == user_id:
            return True

        return self.tasks.prefetch_related('segment_set', 'segment_set__job_set').filter(
            Q(owner=user_id) | Q(assignee=user_id) | Q(segment__job__assignee=user_id)
        ).count() > 0

    @cache_deleted
    @transaction.atomic(savepoint=False)
    def delete(self, using=None, keep_parents=False):
        # quicker way to remove annotations and a way to reduce number of queries
        # is to remove labels and attributes first, it will remove annotations cascadely

        # child objects must be removed first
        if self.label_set.exclude(parent=None).count():
            self.label_set.exclude(parent=None).delete()
        self.label_set.filter(parent=None).delete()
        super().delete(using, keep_parents)

    # Extend default permission model
    class Meta:
        default_permissions = ()

    def __str__(self):
        return self.name

class TaskQuerySet(models.QuerySet):
    def with_job_summary(self):
        return self.prefetch_related(
            'segment_set__job_set',
        ).annotate(
            total_jobs_count=models.Count('segment__job', distinct=True),
            completed_jobs_count=models.Count(
                'segment__job',
                filter=models.Q(segment__job__state=StateChoice.COMPLETED.value) &
                       models.Q(segment__job__stage=StageChoice.ACCEPTANCE.value),
                distinct=True,
            ),
            validation_jobs_count=models.Count(
                'segment__job',
                filter=models.Q(segment__job__stage=StageChoice.VALIDATION.value),
                distinct=True,
            )
        )

class Task(TimestampedModel, FileSystemRelatedModel):
    objects = TaskQuerySet.as_manager()

    project = models.ForeignKey(Project, on_delete=models.CASCADE,
        null=True, blank=True, related_name="tasks",
        related_query_name="task")
    name = SafeCharField(max_length=256)
    mode = models.CharField(max_length=32)
    owner = models.ForeignKey(User, null=True, blank=True,
        on_delete=models.SET_NULL, related_name="owners")
    assignee = models.ForeignKey(User, null=True,  blank=True,
        on_delete=models.SET_NULL, related_name="assignees")
    assignee_updated_date = models.DateTimeField(null=True, blank=True, default=None)
    bug_tracker = models.CharField(max_length=2000, blank=True, default="")
    overlap = models.PositiveIntegerField(null=True)
    # Zero means that there are no limits (default)
    # Note that the files can be split into jobs in a custom way in this case
    segment_size = models.PositiveIntegerField(default=0)
    status = models.CharField(max_length=32, choices=StatusChoice.choices(),
                              default=StatusChoice.ANNOTATION)
    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name="tasks")
    dimension = models.CharField(max_length=2, choices=DimensionType.choices(), default=DimensionType.DIM_2D)
    subset = models.CharField(max_length=64, blank=True, default="")
    organization = models.ForeignKey('organizations.Organization', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name="tasks")
    source_storage = models.ForeignKey('Storage', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name='+')
    target_storage = models.ForeignKey('Storage', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name='+')
    consensus_replicas = models.IntegerField(default=0)
    "Per job consensus replica count"

    segment_set: models.manager.RelatedManager[Segment]

    # Extend default permission model
    class Meta:
        default_permissions = ()

    def get_labels(self, prefetch=False):
        project = self.project
        if project:
            return project.get_labels(prefetch)

        queryset = self.label_set.filter(parent__isnull=True).select_related('skeleton')
        return queryset.prefetch_related(
            'attributespec_set', 'sublabels__attributespec_set',
        ) if prefetch else queryset

    def get_dirname(self) -> str:
        return os.path.join(settings.TASKS_ROOT, str(self.id))

    def is_job_staff(self, user_id):
        if self.owner == user_id:
            return True
        if self.assignee == user_id:
            return True
        return self.segment_set.prefetch_related('job_set').filter(job__assignee=user_id).count() > 0

    @cached_property
    def completed_jobs_count(self) -> Optional[int]:
        # Requires this field to be defined externally,
        # e.g. by calling Task.objects.with_job_summary,
        # to avoid unexpected DB queries on access.
        return None

    @cached_property
    def validation_jobs_count(self) -> Optional[int]:
        # Requires this field to be defined externally,
        # e.g. by calling Task.objects.with_job_summary,
        # to avoid unexpected DB queries on access.
        return None

    @cached_property
    def gt_job(self) -> Optional[Job]:
        try:
            return Job.objects.get(segment__task=self, type=JobType.GROUND_TRUTH)
        except Job.DoesNotExist:
            return None

    def __str__(self):
        return self.name

    @cache_deleted
    @transaction.atomic(savepoint=False)
    def delete(self, using=None, keep_parents=False):
        if not self.project:
            # quicker way to remove annotations and a way to reduce number of queries
            # is to remove labels and attributes first, it will remove annotations cascadely

            # child objects must be removed first
            if self.label_set.exclude(parent=None).count():
                self.label_set.exclude(parent=None).delete()
            self.label_set.filter(parent=None).delete()
        else:
            job_ids = list(self.segment_set.values_list('job__id', flat=True))
            clear_annotations_in_jobs(job_ids)
        super().delete(using, keep_parents)

    def get_chunks_updated_date(self) -> datetime.datetime:
        return self.segment_set.aggregate(
            chunks_updated_date=models.Max('chunks_updated_date')
        )['chunks_updated_date']

# Redefined a couple of operation for FileSystemStorage to avoid renaming
# or other side effects.
class MyFileSystemStorage(FileSystemStorage):
    def get_valid_name(self, name):
        return name

    def get_available_name(self, name, max_length=None):
        if self.exists(name) or (max_length and len(name) > max_length):
            raise IOError('`{}` file already exists or its name is too long'.format(name))
        return name

def upload_path_handler(instance, filename):
    # relative path is required since Django 3.1.11
    return os.path.join(os.path.relpath(instance.data.get_upload_dirname(), settings.BASE_DIR), filename)

# For client files which the user is uploaded
class ClientFile(models.Model):
    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name='client_files')
    file = models.FileField(upload_to=upload_path_handler,
        max_length=1024, storage=MyFileSystemStorage())

    class Meta:
        default_permissions = ()
        unique_together = ("data", "file")

        # Some DBs can shuffle the rows. Here we restore the insertion order.
        # https://github.com/cvat-ai/cvat/pull/5083#discussion_r1038032715
        ordering = ('id', )

# For server files on the mounted share
class ServerFile(models.Model):
    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name='server_files')
    file = models.CharField(max_length=1024)

    class Meta:
        default_permissions = ()
        unique_together = ("data", "file")

        # Some DBs can shuffle the rows. Here we restore the insertion order.
        # https://github.com/cvat-ai/cvat/pull/5083#discussion_r1038032715
        ordering = ('id', )

# For URLs
class RemoteFile(models.Model):
    data = models.ForeignKey(Data, on_delete=models.CASCADE, null=True, related_name='remote_files')
    file = models.CharField(max_length=1024)

    class Meta:
        default_permissions = ()
        unique_together = ("data", "file")

        # Some DBs can shuffle the rows. Here we restore the insertion order.
        # https://github.com/cvat-ai/cvat/pull/5083#discussion_r1038032715
        ordering = ('id', )


class RelatedFile(models.Model):
    data = models.ForeignKey(Data, on_delete=models.CASCADE, related_name="related_files", default=1, null=True)
    path = models.FileField(upload_to=upload_path_handler,
                            max_length=1024, storage=MyFileSystemStorage())
    images = models.ManyToManyField(Image, related_name="related_files")

    class Meta:
        default_permissions = ()
        unique_together = ("data", "path")

        # Some DBs can shuffle the rows. Here we restore the insertion order.
        # https://github.com/cvat-ai/cvat/pull/5083#discussion_r1038032715
        ordering = ('id', )


class SegmentType(str, Enum):
    RANGE = 'range'
    SPECIFIC_FRAMES = 'specific_frames'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value


class Segment(models.Model):
    # Common fields
    task = models.ForeignKey(Task, on_delete=models.CASCADE) # TODO: add related name
    start_frame = models.IntegerField()
    stop_frame = models.IntegerField()
    chunks_updated_date = models.DateTimeField(null=False, auto_now_add=True)
    type = models.CharField(choices=SegmentType.choices(), default=SegmentType.RANGE, max_length=32)

    # SegmentType.SPECIFIC_FRAMES fields
    frames = IntArrayField(store_sorted=True, unique_values=True, default='', blank=True)

    job_set: models.manager.RelatedManager[Job]

    def contains_frame(self, idx: int) -> bool:
        return idx in self.frame_set

    @property
    def frame_count(self) -> int:
        return len(self.frame_set)

    @property
    def frame_set(self) -> Collection[int]:
        data = self.task.data
        data_start_frame = data.start_frame
        data_stop_frame = data.stop_frame
        step = data.get_frame_step()
        frame_range = range(
            data_start_frame + self.start_frame * step,
            min(data_start_frame + self.stop_frame * step, data_stop_frame) + step,
            step
        )

        if self.type == SegmentType.RANGE:
            return frame_range
        elif self.type == SegmentType.SPECIFIC_FRAMES:
            return set(frame_range).intersection(self.frames or [])
        else:
            assert False

    def save(self, *args, **kwargs) -> None:
        self.full_clean()
        return super().save(*args, **kwargs)

    def clean(self) -> None:
        if not (self.type == SegmentType.RANGE) ^ bool(self.frames):
            raise ValidationError(
                f"frames and type == {SegmentType.SPECIFIC_FRAMES} can only be used together"
            )

        if self.stop_frame < self.start_frame:
            raise ValidationError("stop_frame cannot be less than start_frame")

        return super().clean()

    @cache_deleted
    def delete(self, using=None, keep_parents=False):
        super().delete(using, keep_parents)

    class Meta:
        default_permissions = ()



class TaskGroundTruthJobsLimitError(ValidationError):
    def __init__(self) -> None:
        super().__init__("A task can have only 1 ground truth job")



class JobQuerySet(models.QuerySet):
    @transaction.atomic
    def create(self, **kwargs: Any):
        self._validate_constraints(kwargs)

        return super().create(**kwargs)

    @transaction.atomic
    def update(self, **kwargs: Any) -> int:
        self._validate_constraints(kwargs)

        return super().update(**kwargs)

    @transaction.atomic
    def get_or_create(self, *args, **kwargs: Any):
        self._validate_constraints(kwargs)

        return super().get_or_create(*args, **kwargs)

    @transaction.atomic
    def update_or_create(self, *args, **kwargs: Any):
        self._validate_constraints(kwargs)

        return super().update_or_create(*args, **kwargs)

    def _validate_constraints(self, obj: dict[str, Any]):
        if 'type' not in obj:
            return

        # Constraints can't be set on the related model fields
        # This method requires the save operation to be called as a transaction
        if obj['type'] == JobType.GROUND_TRUTH and self.filter(
            segment__task=obj['segment'].task, type=JobType.GROUND_TRUTH.value
        ).count() != 0:
            raise TaskGroundTruthJobsLimitError()

    def with_issue_counts(self):
        return self.annotate(issues__count=models.Count('issues'))



class Job(TimestampedModel, FileSystemRelatedModel):
    objects = JobQuerySet.as_manager()

    segment = models.ForeignKey(Segment, on_delete=models.CASCADE)

    assignee = models.ForeignKey(User, null=True, blank=True, on_delete=models.SET_NULL)
    assignee_updated_date = models.DateTimeField(null=True, blank=True, default=None)

    # TODO: it has to be deleted in Job, Task, Project and replaced by (stage, state)
    # The stage field cannot be changed by an assignee, but state field can be. For
    # now status is read only and it will be updated by (stage, state). Thus we don't
    # need to update Task and Project (all should work as previously).
    status = models.CharField(max_length=32, choices=StatusChoice.choices(),
        default=StatusChoice.ANNOTATION)
    stage = models.CharField(max_length=32, choices=StageChoice.choices(),
        default=StageChoice.ANNOTATION)
    state = models.CharField(max_length=32, choices=StateChoice.choices(),
        default=StateChoice.NEW)
    type = models.CharField(max_length=32, choices=JobType.choices(),
        default=JobType.ANNOTATION)
    parent_job = models.ForeignKey(
        'self', on_delete=models.CASCADE, null=True, blank=True,
        related_name='child_jobs', related_query_name="child_job"
    )

    user_can_view_task: MaybeUndefined[bool]
    "Can be defined by the fetching queryset to avoid extra IAM checks, e.g. in a list serializer"

    issues__count: MaybeUndefined[int]
    "Can be defined by the fetching queryset"

    def get_target_storage(self) -> Optional[Storage]:
        return self.segment.task.target_storage

    def get_source_storage(self) -> Optional[Storage]:
        return self.segment.task.source_storage

    def get_dirname(self) -> str:
        return os.path.join(settings.JOBS_ROOT, str(self.id))

    @extend_schema_field(OpenApiTypes.INT)
    def get_project_id(self):
        return self.segment.task.project_id

    @extend_schema_field(OpenApiTypes.INT)
    def get_guide_id(self):
        source = self.segment.task.project
        if not source:
            source = self.segment.task
        if hasattr(source, 'annotation_guide'):
            return source.annotation_guide.id
        return None

    @extend_schema_field(OpenApiTypes.INT)
    def get_task_id(self):
        return self.segment.task_id

    @property
    def organization_id(self) -> int | None:
        return self.segment.task.organization_id

    @property
    def organization(self) -> Organization:
        return self.segment.task.organization

    def get_organization_slug(self) -> str:
        return self.segment.task.organization.slug

    def get_bug_tracker(self):
        task = self.segment.task
        project = task.project
        return task.bug_tracker or getattr(project, 'bug_tracker', None)

    def get_labels(self, prefetch=False):
        task = self.segment.task
        project = task.project
        return project.get_labels(prefetch) if project else task.get_labels(prefetch)

    class Meta:
        default_permissions = ()

    @cache_deleted
    @transaction.atomic(savepoint=False)
    def delete(self, using=None, keep_parents=False):
        clear_annotations_in_jobs([self.id])
        segment = self.segment
        super().delete(using, keep_parents)
        if segment:
            segment.delete()

    def make_dirs(self):
        job_path = self.get_dirname()
        if os.path.isdir(job_path):
            shutil.rmtree(job_path)
        os.makedirs(job_path)


class InvalidLabel(ValueError):
    pass

class Label(models.Model):
    task = models.ForeignKey(Task, null=True, blank=True, on_delete=models.CASCADE)
    project = models.ForeignKey(Project, null=True, blank=True, on_delete=models.CASCADE)
    name = SafeCharField(max_length=64)
    color = models.CharField(default='', max_length=8)
    type = models.CharField(max_length=32, choices=LabelType.choices(), default=LabelType.ANY)
    parent = models.ForeignKey('self', on_delete=models.CASCADE, null=True, blank=True, related_name='sublabels')

    def __str__(self):
        return self.name

    def has_parent_label(self):
        return bool(self.parent)

    def save(self, *args, **kwargs):
        try:
            super().save(*args, **kwargs)
        except IntegrityError:
            raise InvalidLabel("All label names must be unique")

    @classmethod
    def create(cls, **kwargs):
        try:
            return cls.objects.create(**kwargs)
        except IntegrityError:
            raise InvalidLabel("All label names must be unique")

    @property
    def organization_id(self):
        if self.project is not None:
            return self.project.organization_id
        if self.task is not None:
            return self.task.organization_id
        return None

    class Meta:
        default_permissions = ()
        constraints = [
            models.UniqueConstraint(
                name='project_name_unique',
                fields=('project', 'name'),
                condition=models.Q(task__isnull=True, parent__isnull=True)
            ),
            models.UniqueConstraint(
                name='task_name_unique',
                fields=('task', 'name'),
                condition=models.Q(project__isnull=True, parent__isnull=True)
            ),
            models.UniqueConstraint(
                name='project_name_parent_unique',
                fields=('project', 'name', 'parent'),
                condition=models.Q(task__isnull=True)
            ),
            models.UniqueConstraint(
                name='task_name_parent_unique',
                fields=('task', 'name', 'parent'),
                condition=models.Q(project__isnull=True)
            )
        ]

class Skeleton(models.Model):
    root = models.OneToOneField(Label, on_delete=models.CASCADE)
    svg = models.TextField(null=True, default=None)

    class Meta:
        default_permissions = ()
        unique_together = ('root',)

class AttributeType(str, Enum):
    CHECKBOX = 'checkbox'
    RADIO = 'radio'
    NUMBER = 'number'
    TEXT = 'text'
    SELECT = 'select'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class AttributeSpec(models.Model):
    label = models.ForeignKey(Label, on_delete=models.CASCADE)
    name = models.CharField(max_length=64)
    mutable = models.BooleanField()
    input_type = models.CharField(max_length=16,
        choices=AttributeType.choices())
    default_value = models.CharField(blank=True, max_length=128)
    values = models.CharField(blank=True, max_length=4096)

    class Meta:
        default_permissions = ()
        unique_together = ('label', 'name')

    def __str__(self):
        return self.name

class AttributeVal(models.Model):
    # TODO: add a validator here to be sure that it corresponds to self.label
    id = models.BigAutoField(primary_key=True)
    spec = models.ForeignKey(AttributeSpec, on_delete=models.CASCADE)
    value = SafeCharField(max_length=4096)

    class Meta:
        abstract = True
        default_permissions = ()

class ShapeType(str, Enum):
    RECTANGLE = 'rectangle' # (x0, y0, x1, y1)
    POLYGON = 'polygon'     # (x0, y0, ..., xn, yn)
    POLYLINE = 'polyline'   # (x0, y0, ..., xn, yn)
    POINTS = 'points'       # (x0, y0, ..., xn, yn)
    ELLIPSE = 'ellipse'     # (cx, cy, rx, ty)
    CUBOID = 'cuboid'       # (x0, y0, ..., x7, y7)
    MASK = 'mask'       # (rle mask, left, top, right, bottom)
    SKELETON = 'skeleton'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class SourceType(str, Enum):
    AUTO = 'auto'
    SEMI_AUTO = 'semi-auto'
    MANUAL = 'manual'
    FILE = 'file'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

class Annotation(models.Model):
    id = models.BigAutoField(primary_key=True)
    job = models.ForeignKey(Job, on_delete=models.DO_NOTHING)
    label = models.ForeignKey(Label, on_delete=models.CASCADE)
    frame = models.PositiveIntegerField()
    group = models.PositiveIntegerField(null=True)
    source = models.CharField(max_length=16, choices=SourceType.choices(),
        default=str(SourceType.MANUAL), null=True)

    class Meta:
        abstract = True
        default_permissions = ()

class Shape(models.Model):
    type = models.CharField(max_length=16, choices=ShapeType.choices())
    occluded = models.BooleanField(default=False)
    outside = models.BooleanField(default=False)
    z_order = models.IntegerField(default=0)
    points = FloatArrayField(default=[])
    rotation = FloatField(default=0)

    class Meta:
        abstract = True
        default_permissions = ()

class LabeledImage(Annotation):
    pass

class LabeledImageAttributeVal(AttributeVal):
    image = models.ForeignKey(LabeledImage, on_delete=models.DO_NOTHING,
        related_name='attributes', related_query_name='attribute')

class LabeledShape(Annotation, Shape):
    parent = models.ForeignKey('self', on_delete=models.DO_NOTHING, null=True, related_name='elements')

class LabeledShapeAttributeVal(AttributeVal):
    shape = models.ForeignKey(LabeledShape, on_delete=models.DO_NOTHING,
        related_name='attributes', related_query_name='attribute')

class LabeledTrack(Annotation):
    parent = models.ForeignKey('self', on_delete=models.DO_NOTHING, null=True, related_name='elements')

class LabeledTrackAttributeVal(AttributeVal):
    track = models.ForeignKey(LabeledTrack, on_delete=models.DO_NOTHING,
        related_name='attributes', related_query_name='attribute')

class TrackedShape(Shape):
    id = models.BigAutoField(primary_key=True)
    track = models.ForeignKey(LabeledTrack, on_delete=models.CASCADE,
        related_name='shapes', related_query_name='shape')
    frame = models.PositiveIntegerField()

class TrackedShapeAttributeVal(AttributeVal):
    shape = models.ForeignKey(TrackedShape, on_delete=models.DO_NOTHING,
        related_name='attributes', related_query_name='attribute')


class Profile(models.Model):
    user = models.OneToOneField(User, on_delete=models.CASCADE)
    rating = models.FloatField(default=0.0)
    has_analytics_access = models.BooleanField(
        _("has access to analytics"),
        default=False,
        help_text=_("Designates whether the user can access analytics."),
    )


class Issue(TimestampedModel):
    frame = models.PositiveIntegerField()
    position = FloatArrayField()
    job = models.ForeignKey(Job, related_name='issues', on_delete=models.CASCADE)
    owner = models.ForeignKey(User, null=True, blank=True, related_name='+',
        on_delete=models.SET_NULL)
    assignee = models.ForeignKey(User, null=True, blank=True, related_name='+',
        on_delete=models.SET_NULL)
    resolved = models.BooleanField(default=False)

    def get_project_id(self):
        return self.job.get_project_id()

    @property
    def organization_id(self):
        return self.job.organization_id

    def get_organization_slug(self):
        return self.job.get_organization_slug()

    def get_task_id(self):
        return self.job.get_task_id()

    def get_job_id(self):
        return self.job_id


class Comment(TimestampedModel):
    issue = models.ForeignKey(Issue, related_name='comments', on_delete=models.CASCADE)
    owner = models.ForeignKey(User, null=True, blank=True, on_delete=models.SET_NULL)
    message = models.TextField(default='')

    def get_project_id(self):
        return self.issue.get_project_id()

    @property
    def organization_id(self):
        return self.issue.organization_id

    def get_organization_slug(self):
        return self.issue.get_organization_slug()

    def get_task_id(self):
        return self.issue.get_task_id()

    def get_job_id(self):
        return self.issue.get_job_id()

class CloudProviderChoice(str, Enum):
    AWS_S3 = 'AWS_S3_BUCKET'
    AZURE_CONTAINER = 'AZURE_CONTAINER'
    GOOGLE_DRIVE = 'GOOGLE_DRIVE'
    GOOGLE_CLOUD_STORAGE = 'GOOGLE_CLOUD_STORAGE'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    @classmethod
    def list(cls):
        return list(map(lambda x: x.value, cls))

    def __str__(self):
        return self.value

class CredentialsTypeChoice(str, Enum):
    # ignore bandit issues because false positives
    KEY_SECRET_KEY_PAIR = 'KEY_SECRET_KEY_PAIR' # nosec
    ACCOUNT_NAME_TOKEN_PAIR = 'ACCOUNT_NAME_TOKEN_PAIR' # nosec
    KEY_FILE_PATH = 'KEY_FILE_PATH'
    ANONYMOUS_ACCESS = 'ANONYMOUS_ACCESS'
    CONNECTION_STRING = 'CONNECTION_STRING'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    @classmethod
    def list(cls):
        return list(map(lambda x: x.value, cls))

    def __str__(self):
        return self.value

class Manifest(models.Model):
    filename = models.CharField(max_length=1024, default='manifest.jsonl')
    cloud_storage = models.ForeignKey('CloudStorage', on_delete=models.CASCADE, null=True, related_name='manifests')

    def __str__(self):
        return '{}'.format(self.filename)

class Location(str, Enum):
    CLOUD_STORAGE = 'cloud_storage'
    LOCAL = 'local'

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value

    @classmethod
    def list(cls):
        return [i.value for i in cls]

class CloudStorage(TimestampedModel):
    # restrictions:
    # AWS bucket name, Azure container name - 63, Google bucket name - 63 without dots and 222 with dots
    # https://cloud.google.com/storage/docs/naming-buckets#requirements
    # AWS access key id - 20, Oracle OCI access key id - 40
    # AWS secret access key - 40, Oracle OCI secret access key - 44, Cloudflare R2 secret access key - 64
    # AWS temporary session token - None
    # The size of the security token that AWS STS API operations return is not fixed.
    # We strongly recommend that you make no assumptions about the maximum size.
    # The typical token size is less than 4096 bytes, but that can vary.
    # specific attributes:
    # location - max 23
    # project ID: 6 - 30 (https://cloud.google.com/resource-manager/docs/creating-managing-projects#before_you_begin)
    provider_type = models.CharField(max_length=20, choices=CloudProviderChoice.choices())
    resource = models.CharField(max_length=222)
    display_name = models.CharField(max_length=63)
    owner = models.ForeignKey(User, null=True, blank=True,
        on_delete=models.SET_NULL, related_name="cloud_storages")
    credentials = models.CharField(max_length=1024, null=True, blank=True)
    credentials_type = models.CharField(max_length=29, choices=CredentialsTypeChoice.choices())#auth_type
    specific_attributes = models.CharField(max_length=1024, blank=True)
    description = models.TextField(blank=True)
    organization = models.ForeignKey('organizations.Organization', null=True, default=None,
        blank=True, on_delete=models.SET_NULL, related_name="cloudstorages")

    class Meta:
        default_permissions = ()

    def __str__(self):
        return "{} {} {}".format(self.provider_type, self.display_name, self.id)

    def get_storage_dirname(self):
        return os.path.join(settings.CLOUD_STORAGE_ROOT, str(self.id))

    def get_specific_attributes(self):
        return parse_specific_attributes(self.specific_attributes)

    def get_key_file_path(self):
        return os.path.join(self.get_storage_dirname(), 'key.json')

    @property
    def has_at_least_one_manifest(self) -> bool:
        return bool(self.manifests.count())

class Storage(models.Model):
    location = models.CharField(max_length=16, choices=Location.choices(), default=Location.LOCAL)
    cloud_storage = models.ForeignKey(
        CloudStorage,
        on_delete=models.CASCADE,
        null=True,
        related_name='+',
    )

    class Meta:
        default_permissions = ()

class AnnotationGuide(TimestampedModel):
    task = models.OneToOneField(Task, null=True, blank=True, on_delete=models.CASCADE, related_name="annotation_guide")
    project = models.OneToOneField(Project, null=True, blank=True, on_delete=models.CASCADE, related_name="annotation_guide")
    markdown = models.TextField(blank=True, default='')
    is_public = models.BooleanField(default=False)

    @property
    def target(self) -> Task | Project:
        target = self.project or self.task
        assert target # one of the fields must be set
        return target

    @property
    def organization_id(self):
        return self.target.organization_id

class Asset(models.Model):
    uuid = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    filename = models.CharField(max_length=1024)
    created_date = models.DateTimeField(auto_now_add=True)
    owner = models.ForeignKey(User, null=True, blank=True, on_delete=models.SET_NULL, related_name="assets")
    guide = models.ForeignKey(AnnotationGuide, on_delete=models.CASCADE, related_name="assets")

    @property
    def organization_id(self):
        return self.guide.organization_id

    def get_asset_dir(self):
        return os.path.join(settings.ASSETS_ROOT, str(self.uuid))

class RequestStatus(TextChoices):
    QUEUED = "queued"
    STARTED = "started"
    FAILED = "failed"
    FINISHED = "finished"

class RequestAction(TextChoices):
    AUTOANNOTATE = "autoannotate"
    CREATE = "create"
    IMPORT = "import"
    EXPORT = "export"

class RequestTarget(TextChoices):
    PROJECT = "project"
    TASK = "task"
    JOB = "job"

class RequestSubresource(TextChoices):
    ANNOTATIONS = "annotations"
    DATASET = "dataset"
    BACKUP = "backup"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\model_utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from collections.abc import Iterable
from typing import Sequence, TypeVar, Union

from django.conf import settings
from django.db import models

_T = TypeVar("_T")


class Undefined:
    pass


MaybeUndefined = Union[_T, Undefined]
"""
Can be used to annotate dynamic class members that may be undefined in the object.
Such fields should typically be accessed via hasattr() and getattr().

Common use cases:
- the reverse side of one-to-one relationship
- extra annotations from a model queryset
"""


_ModelT = TypeVar("_ModelT", bound=models.Model)
_unspecified = object()


def bulk_create(
    db_model: type[_ModelT],
    objs: Iterable[_ModelT],
    *,
    batch_size: int | None = _unspecified,
    ignore_conflicts: bool = False,
    update_conflicts: bool | None = False,
    update_fields: Sequence[str] | None = None,
    unique_fields: Sequence[str] | None = None,
) -> list[_ModelT]:
    """
    Like Django's Model.objects.bulk_create(), but applies the default batch size configured by
    the DEFAULT_DB_BULK_CREATE_BATCH_SIZE setting.
    """

    if batch_size is _unspecified:
        batch_size = settings.DEFAULT_DB_BULK_CREATE_BATCH_SIZE

    if not objs:
        return []

    return db_model.objects.bulk_create(
        objs,
        batch_size=batch_size,
        ignore_conflicts=ignore_conflicts,
        update_conflicts=update_conflicts,
        update_fields=update_fields,
        unique_fields=unique_fields,
    )


def is_prefetched(queryset: models.QuerySet, field: str) -> bool:
    "Checks if a field is being prefetched in the queryset"
    return field in queryset._prefetch_related_lookups


_QuerysetT = TypeVar("_QuerysetT", bound=models.QuerySet)


def add_prefetch_fields(queryset: _QuerysetT, fields: Sequence[str]) -> _QuerysetT:
    for field in fields:
        if not is_prefetched(queryset, field):
            queryset = queryset.prefetch_related(field)

    return queryset


def get_cached(queryset: _QuerysetT, pk: int) -> _ModelT:
    """
    Like regular queryset.get(), but checks for the cached values first
    instead of just making a request.
    """

    # Read more about caching insights:
    # https://www.mattduck.com/2021-01-django-orm-result-cache.html
    # The field is initialized on accessing the query results, eg. on iteration
    if getattr(queryset, "_result_cache"):
        result = next((obj for obj in queryset if obj.pk == pk), None)
    else:
        result = None

    if result is None:
        result = queryset.get(id=pk)

    return result


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\pagination.py =====
# Copyright (C) 2019-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import sys

from rest_framework.pagination import PageNumberPagination

from cvat.apps.engine.types import ExtendedRequest


class CustomPagination(PageNumberPagination):
    page_size_query_param = "page_size"

    def get_page_size(self, request: ExtendedRequest):
        page_size = 0
        try:
            value = request.query_params[self.page_size_query_param]
            if value == "all":
                page_size = sys.maxsize
            else:
                page_size = int(value)
        except (KeyError, ValueError):
            pass

        return page_size if page_size > 0 else self.page_size


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\parsers.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework.parsers import BaseParser


class TusUploadParser(BaseParser):
    # The media type is sent by TUS protocol (tus.io) for uploading files
    media_type = 'application/offset+octet-stream'

    def parse(self, stream, media_type=None, parser_context=None):
        # Let's just return empty dictionary which will be used for
        # request.data. Otherwise every access to request.data will lead to
        # exception because a parser for the request with the non-standard
        # content media type isn't defined.
        # https://github.com/imtapps/django-rest-framework/blob/master/docs/api-guide/parsers.md
        return {}


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from collections import namedtuple
from collections.abc import Sequence
from typing import TYPE_CHECKING, Any, Optional, Union, cast

from django.conf import settings
from django.shortcuts import get_object_or_404
from rest_framework.exceptions import PermissionDenied, ValidationError
from rq.job import Job as RQJob

from cvat.apps.engine.rq import RQId, is_rq_job_owner
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import is_dataset_export
from cvat.apps.iam.permissions import (
    OpenPolicyAgentPermission,
    StrEnum,
    get_iam_context,
    get_membership,
)
from cvat.apps.organizations.models import Organization

from .models import AnnotationGuide, CloudStorage, Comment, Issue, Job, Label, Project, Task, User

if TYPE_CHECKING:
    from rest_framework.viewsets import ViewSet

def _get_key(d: dict[str, Any], key_path: Union[str, Sequence[str]]) -> Optional[Any]:
    """
    Like dict.get(), but supports nested fields. If the field is missing, returns None.
    """

    if isinstance(key_path, str):
        key_path = [key_path]
    else:
        assert key_path

    for key_part in key_path:
        d = d.get(key_part)
        if d is None:
            return d

    return d

class DownloadExportedExtension:
    rq_job_id: RQId | None

    class Scopes(StrEnum):
        DOWNLOAD_EXPORTED_FILE = 'download:exported_file'

    @staticmethod
    def extend_params_with_rq_job_details(*, request: ExtendedRequest, params: dict[str, Any]) -> None:
        if rq_id := request.query_params.get("rq_id"):
            try:
                params["rq_job_id"] = RQId.parse(rq_id)
                return
            except Exception:
                raise ValidationError("Unexpected request id format")

        raise ValidationError("Missing request id in the query parameters")

    def extend_resource_with_rq_job_details(self, data: dict[str, Any]) -> None:
        data["rq_job"] = {
            "owner": {
                "id": self.rq_job_id.user_id if self.rq_job_id else None
            }
        }

class ServerPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        VIEW = 'view'
        LIST_CONTENT = 'list:content'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'server':
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/server/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: None):
        Scopes = __class__.Scopes
        return [{
            ('annotation_formats', 'GET'): Scopes.VIEW,
            ('about', 'GET'): Scopes.VIEW,
            ('plugins', 'GET'): Scopes.VIEW,
            ('share', 'GET'): Scopes.LIST_CONTENT,
        }[(view.action, request.method)]]

    def get_resource(self):
        return None

class UserPermission(OpenPolicyAgentPermission):
    obj: Optional[User]

    class Scopes(StrEnum):
        LIST = 'list'
        VIEW = 'view'
        UPDATE = 'update'
        DELETE = 'delete'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: User | None, iam_context: dict[str, Any]):
        permissions = []
        if view.basename == 'user':
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/users/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: User | None):
        Scopes = __class__.Scopes
        return [{
            'list': Scopes.LIST,
            'self': Scopes.VIEW,
            'retrieve': Scopes.VIEW,
            'partial_update': Scopes.UPDATE,
            'destroy': Scopes.DELETE,
        }[view.action]]

    @classmethod
    def create_scope_view(cls, iam_context: dict[str, Any], user_id: int | str):
        obj = namedtuple('User', ['id'])(id=int(user_id))
        return cls(**iam_context, scope=__class__.Scopes.VIEW, obj=obj)

    def get_resource(self):
        data = None
        organization = self.payload['input']['auth']['organization']
        if self.obj:
            data = {
                'id': self.obj.id
            }
        elif self.scope == __class__.Scopes.VIEW: # self
            data = {
                'id': self.user_id
            }

        if data:
            data.update({
                'membership': {
                    'role': organization['user']['role']
                        if organization else None
                }
            })

        return data

class CloudStoragePermission(OpenPolicyAgentPermission):
    obj: Optional[CloudStorage]

    class Scopes(StrEnum):
        LIST = 'list'
        LIST_CONTENT = 'list:content'
        CREATE = 'create'
        VIEW = 'view'
        UPDATE = 'update'
        DELETE = 'delete'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: CloudStorage | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'cloudstorage':
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    @classmethod
    def create_scope_view(cls, iam_context: dict[str, Any], storage_id: int, request: ExtendedRequest | None = None):
        try:
            obj = CloudStorage.objects.get(id=storage_id)
        except CloudStorage.DoesNotExist as ex:
            raise ValidationError(str(ex))

        if not iam_context and request:
            iam_context = get_iam_context(request, obj)

        return cls(**iam_context, obj=obj, scope=__class__.Scopes.VIEW)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/cloudstorages/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: CloudStorage | None):
        Scopes = __class__.Scopes
        return [{
            'list': Scopes.LIST,
            'create': Scopes.CREATE,
            'retrieve': Scopes.VIEW,
            'partial_update': Scopes.UPDATE,
            'destroy': Scopes.DELETE,
            'content': Scopes.LIST_CONTENT,
            'content_v2': Scopes.LIST_CONTENT,
            'preview': Scopes.VIEW,
            'status': Scopes.VIEW,
            'actions': Scopes.VIEW,
        }[view.action]]

    def get_resource(self):
        data = None
        if self.scope.startswith('create'):
            data = {
                'owner': { 'id': self.user_id },
                'organization': {
                    'id': self.org_id,
                } if self.org_id is not None else None,
            }
        elif self.obj:
            data = {
                'id': self.obj.id,
                'owner': { 'id': self.obj.owner_id },
                'organization': {
                    'id': self.obj.organization_id
                } if self.obj.organization_id else None
            }

        return data

class ProjectPermission(OpenPolicyAgentPermission, DownloadExportedExtension):
    obj: Optional[Project]

    class Scopes(StrEnum):
        LIST = 'list'
        CREATE = 'create'
        DELETE = 'delete'
        UPDATE = 'update'
        UPDATE_OWNER = 'update:owner'
        UPDATE_ASSIGNEE = 'update:assignee'
        UPDATE_DESC = 'update:desc'
        UPDATE_ORG = 'update:organization'
        UPDATE_ASSOCIATED_STORAGE = 'update:associated_storage'
        VIEW = 'view'
        IMPORT_DATASET = 'import:dataset'
        EXPORT_ANNOTATIONS = 'export:annotations'
        EXPORT_DATASET = 'export:dataset'
        EXPORT_BACKUP = 'export:backup'
        IMPORT_BACKUP = 'import:backup'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Project | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'project':
            assignee_id = request.data.get('assignee_id') or request.data.get('assignee')

            for scope in cls.get_scopes(request, view, obj):
                scope_params = {}

                if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == scope:
                    cls.extend_params_with_rq_job_details(request=request, params=scope_params)

                self = cls.create_base_perm(request, view, scope, iam_context, obj,
                    assignee_id=assignee_id, **scope_params)
                permissions.append(self)

            if view.action == 'tasks':
                perm = TaskPermission.create_scope_list(request, iam_context)
                permissions.append(perm)

            owner = request.data.get('owner_id') or request.data.get('owner')
            if owner:
                perm = UserPermission.create_scope_view(iam_context, owner)
                permissions.append(perm)

            if assignee_id:
                perm = UserPermission.create_scope_view(iam_context, assignee_id)
                permissions.append(perm)

            for field_source, field in [
                # from ProjectWriteSerializer used in create and partial update endpoints
                (request.data, 'source_storage.cloud_storage_id'),
                (request.data, 'target_storage.cloud_storage_id'),

                # from /backup, /annotations and /dataset endpoints
                (request.query_params, 'cloud_storage_id'),
            ]:
                field_path = field.split('.')
                if cloud_storage_id := _get_key(field_source, field_path):
                    permissions.append(CloudStoragePermission.create_scope_view(
                        iam_context, storage_id=cloud_storage_id))

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/projects/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Project | None):
        Scopes = __class__.Scopes
        scope = {
            ('list', 'GET'): Scopes.LIST,
            ('create', 'POST'): Scopes.CREATE,
            ('destroy', 'DELETE'): Scopes.DELETE,
            ('partial_update', 'PATCH'): Scopes.UPDATE,
            ('retrieve', 'GET'): Scopes.VIEW,
            ('tasks', 'GET'): Scopes.VIEW,
            ('dataset', 'POST'): Scopes.IMPORT_DATASET,
            ('append_dataset_chunk', 'HEAD'): Scopes.IMPORT_DATASET,
            ('append_dataset_chunk', 'PATCH'): Scopes.IMPORT_DATASET,
            ('initiate_dataset_export', 'POST'): Scopes.EXPORT_DATASET if is_dataset_export(request) else Scopes.EXPORT_ANNOTATIONS,
            ('initiate_backup_export', 'POST'): Scopes.EXPORT_BACKUP,
            ('import_backup', 'POST'): Scopes.IMPORT_BACKUP,
            ('append_backup_chunk', 'PATCH'): Scopes.IMPORT_BACKUP,
            ('append_backup_chunk', 'HEAD'): Scopes.IMPORT_BACKUP,
            ('preview', 'GET'): Scopes.VIEW,
            ('download_dataset', 'GET'): DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE,
            ('download_backup', 'GET'): DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE,
            # FUTURE-TODO: delete this after dropping support for deprecated API
            ('annotations', 'GET'): Scopes.EXPORT_ANNOTATIONS,
            ('dataset', 'GET'): Scopes.IMPORT_DATASET if request.query_params.get('action') == 'import_status' else Scopes.EXPORT_DATASET,
            ('export_backup', 'GET'): Scopes.EXPORT_BACKUP,

        }[(view.action, request.method)]

        scopes = []
        if scope == Scopes.UPDATE:
            scopes.extend(__class__.get_per_field_update_scopes(request, {
                'owner_id': Scopes.UPDATE_OWNER,
                'assignee_id': Scopes.UPDATE_ASSIGNEE,
                'name': Scopes.UPDATE_DESC,
                'labels': Scopes.UPDATE_DESC,
                'bug_tracker': Scopes.UPDATE_DESC,
                'organization': Scopes.UPDATE_ORG,
                'source_storage': Scopes.UPDATE_ASSOCIATED_STORAGE,
                'target_storage': Scopes.UPDATE_ASSOCIATED_STORAGE,
            }))
        else:
            scopes.append(scope)

        return scopes

    @classmethod
    def create_scope_view(cls, request: ExtendedRequest, project: int | Project, iam_context: dict[str, Any] | None = None):
        if isinstance(project, int):
            try:
                project = Project.objects.get(id=project)
            except Project.DoesNotExist as ex:
                raise ValidationError(str(ex))

        if not iam_context and request:
            iam_context = get_iam_context(request, project)

        return cls(**iam_context, obj=project, scope=__class__.Scopes.VIEW)

    @classmethod
    def create_scope_create(cls, request: ExtendedRequest, org_id: int | None):
        organization = None
        membership = None
        privilege = request.iam_context['privilege']
        if org_id:
            try:
                organization = Organization.objects.get(id=org_id)
            except Organization.DoesNotExist as ex:
                raise ValidationError(str(ex))

            membership = get_membership(request, organization)

        return cls(
            user_id=request.user.id,
            group_name=getattr(privilege, 'name', None),
            org_id=getattr(organization, 'id', None),
            org_owner_id=getattr(organization.owner, 'id', None)
                if organization else None,
            org_role=getattr(membership, 'role', None),
            scope=__class__.Scopes.CREATE)

    def get_resource(self):
        data = None
        if self.obj:
            data = {
                "id": self.obj.id,
                "owner": { "id": self.obj.owner_id },
                "assignee": { "id": self.obj.assignee_id },
                'organization': { "id": self.obj.organization_id },
            }

            if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == self.scope:
                self.extend_resource_with_rq_job_details(data)

        elif self.scope in [__class__.Scopes.CREATE, __class__.Scopes.IMPORT_BACKUP]:
            data = {
                "id": None,
                "owner": { "id": self.user_id },
                "assignee": {
                    "id": self.assignee_id,
                } if self.assignee_id else None,
                'organization': {
                    "id": self.org_id,
                } if self.org_id else None,
            }

        return data

class TaskPermission(OpenPolicyAgentPermission, DownloadExportedExtension):
    obj: Optional[Task]

    class Scopes(StrEnum):
        LIST = 'list'
        CREATE = 'create'
        CREATE_IN_PROJECT = 'create@project'
        VIEW = 'view'
        UPDATE = 'update'
        UPDATE_DESC = 'update:desc'
        UPDATE_ORGANIZATION = 'update:organization'
        UPDATE_ASSIGNEE = 'update:assignee'
        UPDATE_PROJECT = 'update:project'
        UPDATE_OWNER = 'update:owner'
        UPDATE_ASSOCIATED_STORAGE = 'update:associated_storage'
        DELETE = 'delete'
        VIEW_ANNOTATIONS = 'view:annotations'
        UPDATE_ANNOTATIONS = 'update:annotations'
        DELETE_ANNOTATIONS = 'delete:annotations'
        IMPORT_ANNOTATIONS = 'import:annotations'
        EXPORT_ANNOTATIONS = 'export:annotations'
        EXPORT_DATASET = 'export:dataset'
        VIEW_METADATA = 'view:metadata'
        UPDATE_METADATA = 'update:metadata'
        VIEW_DATA = 'view:data'
        UPLOAD_DATA = 'upload:data'
        IMPORT_BACKUP = 'import:backup'
        EXPORT_BACKUP = 'export:backup'
        VIEW_VALIDATION_LAYOUT = 'view:validation_layout'
        UPDATE_VALIDATION_LAYOUT = 'update:validation_layout'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Task | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'task':
            project_id = request.data.get('project_id') or request.data.get('project')
            assignee_id = request.data.get('assignee_id') or request.data.get('assignee')
            owner = request.data.get('owner_id') or request.data.get('owner')

            for scope in cls.get_scopes(request, view, obj):
                params = { 'project_id': project_id, 'assignee_id': assignee_id }

                if scope == __class__.Scopes.UPDATE_ORGANIZATION:
                    org_id = request.data.get('organization')
                    if obj is not None and obj.project is not None:
                        raise ValidationError('Cannot change the organization for '
                            'a task inside a project')
                    # FIX IT: TaskPermission doesn't have create_scope_create method
                    permissions.append(TaskPermission.create_scope_create(request, org_id))
                elif scope == __class__.Scopes.UPDATE_OWNER:
                    params['owner_id'] = owner

                if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == scope:
                    cls.extend_params_with_rq_job_details(request=request, params=params)

                self = cls.create_base_perm(request, view, scope, iam_context, obj, **params)
                permissions.append(self)

            if view.action == 'jobs':
                perm = JobPermission.create_scope_list(request, iam_context)
                permissions.append(perm)

            if owner:
                perm = UserPermission.create_scope_view(iam_context, owner)
                permissions.append(perm)

            if assignee_id:
                perm = UserPermission.create_scope_view(iam_context, assignee_id)
                permissions.append(perm)

            if project_id:
                perm = ProjectPermission.create_scope_view(request, int(project_id), iam_context)
                permissions.append(perm)

            for field_source, field in [
                # from TaskWriteSerializer being used in the create and partial_update endpoints
                (request.data, 'source_storage.cloud_storage_id'),
                (request.data, 'target_storage.cloud_storage_id'),

                # from DataSerializer being used in the /data endpoint
                (request.data, 'cloud_storage_id'),

                # from /backup, /annotations and /dataset endpoints
                (request.query_params, 'cloud_storage_id'),
            ]:
                field_path = field.split('.')
                if cloud_storage_id := _get_key(field_source, field_path):
                    permissions.append(CloudStoragePermission.create_scope_view(
                        iam_context, storage_id=cloud_storage_id))

        return permissions

    @classmethod
    def create_scope_view(cls, request: ExtendedRequest, task: int | Task, iam_context: dict[str, Any] | None = None):
        if isinstance(task, int):
            try:
                task = Task.objects.get(id=task)
            except Task.DoesNotExist as ex:
                raise ValidationError(str(ex))

        if not iam_context and request:
            iam_context = get_iam_context(request, task)

        return cls(**iam_context, obj=task, scope=__class__.Scopes.VIEW)

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/tasks/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Task | None) -> list[Scopes]:
        Scopes = __class__.Scopes
        scope = {
            ('list', 'GET'): Scopes.LIST,
            ('create', 'POST'): Scopes.CREATE,
            ('retrieve', 'GET'): Scopes.VIEW,
            ('status', 'GET'): Scopes.VIEW,
            ('partial_update', 'PATCH'): Scopes.UPDATE,
            ('update', 'PUT'): Scopes.UPDATE,
            ('destroy', 'DELETE'): Scopes.DELETE,
            ('annotations', 'GET'): Scopes.VIEW_ANNOTATIONS,
            ('annotations', 'PATCH'): Scopes.UPDATE_ANNOTATIONS,
            ('annotations', 'DELETE'): Scopes.DELETE_ANNOTATIONS,
            ('annotations', 'PUT'): Scopes.UPDATE_ANNOTATIONS,
            ('annotations', 'POST'): Scopes.IMPORT_ANNOTATIONS,
            ('append_annotations_chunk', 'PATCH'): Scopes.UPDATE_ANNOTATIONS,
            ('append_annotations_chunk', 'HEAD'): Scopes.UPDATE_ANNOTATIONS,
            ('initiate_dataset_export', 'POST'): Scopes.EXPORT_DATASET if is_dataset_export(request) else Scopes.EXPORT_ANNOTATIONS,
            ('metadata', 'GET'): Scopes.VIEW_METADATA,
            ('metadata', 'PATCH'): Scopes.UPDATE_METADATA,
            ('data', 'GET'): Scopes.VIEW_DATA,
            ('data', 'POST'): Scopes.UPLOAD_DATA,
            ('append_data_chunk', 'PATCH'): Scopes.UPLOAD_DATA,
            ('append_data_chunk', 'HEAD'): Scopes.UPLOAD_DATA,
            ('jobs', 'GET'): Scopes.VIEW,
            ('import_backup', 'POST'): Scopes.IMPORT_BACKUP,
            ('append_backup_chunk', 'PATCH'): Scopes.IMPORT_BACKUP,
            ('append_backup_chunk', 'HEAD'): Scopes.IMPORT_BACKUP,
            ('initiate_backup_export', 'POST'): Scopes.EXPORT_BACKUP,
            ('preview', 'GET'): Scopes.VIEW,
            ('validation_layout', 'GET'): Scopes.VIEW_VALIDATION_LAYOUT,
            ('validation_layout', 'PATCH'): Scopes.UPDATE_VALIDATION_LAYOUT,
            ('download_dataset', 'GET'): DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE,
            ('download_backup', 'GET'): DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE,
            # FUTURE-TODO: deprecated API
            ('dataset_export', 'GET'): Scopes.EXPORT_DATASET,
            ('export_backup', 'GET'): Scopes.EXPORT_BACKUP,
        }[(view.action, request.method)]

        scopes = []
        if scope == Scopes.CREATE:
            project_id = request.data.get('project_id') or request.data.get('project')
            if project_id:
                scope = Scopes.CREATE_IN_PROJECT

            scopes.append(scope)

        elif scope == Scopes.UPDATE:
            scopes.extend(__class__.get_per_field_update_scopes(request, {
                'owner_id': Scopes.UPDATE_OWNER,
                'assignee_id': Scopes.UPDATE_ASSIGNEE,
                'project_id': Scopes.UPDATE_PROJECT,
                'name': Scopes.UPDATE_DESC,
                'labels': Scopes.UPDATE_DESC,
                'bug_tracker': Scopes.UPDATE_DESC,
                'subset': Scopes.UPDATE_DESC,
                'organization': Scopes.UPDATE_ORGANIZATION,
                'source_storage': Scopes.UPDATE_ASSOCIATED_STORAGE,
                'target_storage': Scopes.UPDATE_ASSOCIATED_STORAGE,
            }))

        elif scope == Scopes.VIEW_ANNOTATIONS:
            if 'format' in request.query_params:
                scope = Scopes.EXPORT_ANNOTATIONS

            scopes.append(scope)

        elif scope == Scopes.UPDATE_ANNOTATIONS:
            if 'format' in request.query_params and request.method == 'PUT':
                scope = Scopes.IMPORT_ANNOTATIONS

            scopes.append(scope)

        else:
            scopes.append(scope)

        return scopes

    @classmethod
    def create_scope_view_data(cls, iam_context: dict[str, Any], task_id: int):
        try:
            obj = Task.objects.get(id=task_id)
        except Task.DoesNotExist as ex:
            raise ValidationError(str(ex))
        return cls(**iam_context, obj=obj, scope=__class__.Scopes.VIEW_DATA)

    def get_resource(self):
        data = None
        if self.obj:
            data = {
                "id": self.obj.id,
                "owner": { "id": self.obj.owner_id },
                "assignee": { "id": self.obj.assignee_id },
                'organization': { "id": self.obj.organization_id },
                "project": {
                    "owner": { "id": self.obj.project.owner_id },
                    "assignee": { "id": self.obj.project.assignee_id },
                    'organization': { "id": self.obj.project.organization_id },
                } if self.obj.project else None
            }

            if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == self.scope:
                self.extend_resource_with_rq_job_details(data)

        elif self.scope in [
            __class__.Scopes.CREATE,
            __class__.Scopes.CREATE_IN_PROJECT,
            __class__.Scopes.IMPORT_BACKUP
        ]:
            project = None
            if self.project_id:
                try:
                    project = Project.objects.get(id=self.project_id)
                except Project.DoesNotExist as ex:
                    raise ValidationError(str(ex))

            data = {
                "id": None,
                "owner": { "id": self.user_id },
                "assignee": {
                    "id": self.assignee_id
                },
                'organization': {
                    "id": self.org_id
                },
                "project": {
                    "owner": { "id": project.owner_id },
                    "assignee": { "id": project.assignee_id },
                    'organization': {
                        "id": project.organization_id,
                    } if project.organization_id else None,
                } if project is not None else None,
            }

        return data

class JobPermission(OpenPolicyAgentPermission, DownloadExportedExtension):
    task_id: Optional[int]
    obj: Optional[Job]

    class Scopes(StrEnum):
        CREATE = 'create'
        LIST = 'list'
        VIEW = 'view'
        UPDATE = 'update'
        UPDATE_ASSIGNEE = 'update:assignee'
        UPDATE_STAGE = 'update:stage'
        UPDATE_STATE = 'update:state'
        DELETE = 'delete'
        VIEW_ANNOTATIONS = 'view:annotations'
        UPDATE_ANNOTATIONS = 'update:annotations'
        DELETE_ANNOTATIONS = 'delete:annotations'
        IMPORT_ANNOTATIONS = 'import:annotations'
        EXPORT_ANNOTATIONS = 'export:annotations'
        EXPORT_DATASET = 'export:dataset'
        VIEW_DATA = 'view:data'
        VIEW_METADATA = 'view:metadata'
        UPDATE_METADATA = 'update:metadata'
        VIEW_VALIDATION_LAYOUT = 'view:validation_layout'
        UPDATE_VALIDATION_LAYOUT = 'update:validation_layout'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Job | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'job':
            task_id = request.data.get('task_id')
            for scope in cls.get_scopes(request, view, obj):
                scope_params = {}

                if scope == __class__.Scopes.CREATE:
                    scope_params['task_id'] = task_id

                    if task_id:
                        try:
                            task = Task.objects.get(id=task_id)
                        except Task.DoesNotExist as ex:
                            raise ValidationError(str(ex))

                        iam_context = get_iam_context(request, task)
                        permissions.append(TaskPermission.create_scope_view(
                            request, task, iam_context=iam_context
                        ))

                if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == scope:
                    cls.extend_params_with_rq_job_details(request=request, params=scope_params)

                self = cls.create_base_perm(request, view, scope, iam_context, obj, **scope_params)
                permissions.append(self)

            if view.action == 'issues':
                perm = IssuePermission.create_scope_list(request, iam_context)
                permissions.append(perm)

            assignee_id = request.data.get('assignee')
            if assignee_id:
                perm = UserPermission.create_scope_view(iam_context, assignee_id)
                permissions.append(perm)

            for field_source, field in [
                # from /annotations and /dataset endpoints
                (request.query_params, 'cloud_storage_id'),
            ]:
                field_path = field.split('.')
                if cloud_storage_id := _get_key(field_source, field_path):
                    permissions.append(CloudStoragePermission.create_scope_view(
                        iam_context, storage_id=cloud_storage_id))

        return permissions

    @classmethod
    def create_scope_view_data(cls, iam_context: dict[str, Any], job_id: int):
        try:
            obj = Job.objects.get(id=job_id)
        except Job.DoesNotExist as ex:
            raise ValidationError(str(ex))
        return cls(**iam_context, obj=obj, scope='view:data')

    @classmethod
    def create_scope_view(cls, request: ExtendedRequest, job: int | Job, iam_context: dict[str, Any] | None = None):
        if isinstance(job, int):
            try:
                job = Job.objects.get(id=job)
            except Job.DoesNotExist as ex:
                raise ValidationError(str(ex))

        if not iam_context and request:
            iam_context = get_iam_context(request, job)

        return cls(**iam_context, obj=job, scope=__class__.Scopes.VIEW)

    def __init__(self, **kwargs):
        self.task_id = kwargs.pop('task_id', None)
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/jobs/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Job | None):
        Scopes = __class__.Scopes
        scope = {
            ('list', 'GET'): Scopes.LIST,
            ('create', 'POST'): Scopes.CREATE,
            ('retrieve', 'GET'): Scopes.VIEW,
            ('partial_update', 'PATCH'): Scopes.UPDATE,
            ('destroy', 'DELETE'): Scopes.DELETE,
            ('annotations', 'GET'): Scopes.VIEW_ANNOTATIONS,
            ('annotations', 'PATCH'): Scopes.UPDATE_ANNOTATIONS,
            ('annotations', 'DELETE'): Scopes.DELETE_ANNOTATIONS,
            ('annotations', 'PUT'): Scopes.UPDATE_ANNOTATIONS,
            ('annotations', 'POST'): Scopes.IMPORT_ANNOTATIONS,
            ('append_annotations_chunk', 'PATCH'): Scopes.UPDATE_ANNOTATIONS,
            ('append_annotations_chunk', 'HEAD'): Scopes.UPDATE_ANNOTATIONS,
            ('data', 'GET'): Scopes.VIEW_DATA,
            ('metadata','GET'): Scopes.VIEW_METADATA,
            ('metadata','PATCH'): Scopes.UPDATE_METADATA,
            ('issues', 'GET'): Scopes.VIEW,
            ('initiate_dataset_export', 'POST'): Scopes.EXPORT_DATASET if is_dataset_export(request) else Scopes.EXPORT_ANNOTATIONS,
            ('preview', 'GET'): Scopes.VIEW,
            ('validation_layout', 'GET'): Scopes.VIEW_VALIDATION_LAYOUT,
            ('validation_layout', 'PATCH'): Scopes.UPDATE_VALIDATION_LAYOUT,
            ('download_dataset', 'GET'): DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE,
            # deprecated API
            ('dataset_export', 'GET'): Scopes.EXPORT_DATASET,
        }[(view.action, request.method)]

        scopes = []
        if scope == Scopes.UPDATE:
            scopes.extend(__class__.get_per_field_update_scopes(request, {
                'assignee': Scopes.UPDATE_ASSIGNEE,
                'stage': Scopes.UPDATE_STAGE,
                'state': Scopes.UPDATE_STATE,
            }))
        elif scope == Scopes.VIEW_ANNOTATIONS:
            if 'format' in request.query_params:
                scope = Scopes.EXPORT_ANNOTATIONS

            scopes.append(scope)
        elif scope == Scopes.UPDATE_ANNOTATIONS:
            if 'format' in request.query_params and request.method == 'PUT':
                scope = Scopes.IMPORT_ANNOTATIONS

            scopes.append(scope)
        else:
            scopes.append(scope)

        return scopes

    def get_resource(self):
        data = None
        if self.obj:
            if self.obj.segment.task.project:
                organization_id = self.obj.segment.task.project.organization_id
            else:
                organization_id = self.obj.segment.task.organization_id

            data = {
                "id": self.obj.id,
                "assignee": { "id": self.obj.assignee_id },
                'organization': { "id": organization_id },
                "task": {
                    "owner": { "id": self.obj.segment.task.owner_id },
                    "assignee": { "id": self.obj.segment.task.assignee_id }
                },
                "project": {
                    "owner": { "id": self.obj.segment.task.project.owner_id },
                    "assignee": { "id": self.obj.segment.task.project.assignee_id }
                } if self.obj.segment.task.project else None
            }

            if DownloadExportedExtension.Scopes.DOWNLOAD_EXPORTED_FILE == self.scope:
                self.extend_resource_with_rq_job_details(data)

        elif self.scope == __class__.Scopes.CREATE:
            if self.task_id is None:
                raise ValidationError("task_id is not specified")
            task = Task.objects.get(id=self.task_id)

            if task.project:
                organization_id = task.project.organization_id
            else:
                organization_id = task.organization_id

            data = {
                'organization': { "id": organization_id },
                "task": {
                    "owner": { "id": task.owner_id },
                    "assignee": { "id": task.assignee_id }
                },
                "project": {
                    "owner": { "id": task.project.owner_id },
                    "assignee": { "id": task.project.assignee_id }
                } if task.project else None
            }

        return data

class CommentPermission(OpenPolicyAgentPermission):
    obj: Optional[Comment]

    class Scopes(StrEnum):
        LIST = 'list'
        CREATE  = 'create'
        CREATE_IN_ISSUE  = 'create@issue'
        DELETE = 'delete'
        UPDATE = 'update'
        VIEW = 'view'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Comment | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'comment':
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj,
                    issue_id=request.data.get('issue'))
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/comments/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Comment | None):
        Scopes = __class__.Scopes
        return [{
            'list': Scopes.LIST,
            'create': Scopes.CREATE_IN_ISSUE,
            'destroy': Scopes.DELETE,
            'partial_update': Scopes.UPDATE,
            'retrieve': Scopes.VIEW,
        }[view.action]]

    def get_resource(self):
        data = None
        def get_common_data(db_issue):
            if db_issue.job.segment.task.project:
                organization_id = db_issue.job.segment.task.project.organization_id
            else:
                organization_id = db_issue.job.segment.task.organization_id

            data = {
                "project": {
                    "owner": { "id": db_issue.job.segment.task.project.owner_id },
                    "assignee": { "id": db_issue.job.segment.task.project.assignee_id }
                } if db_issue.job.segment.task.project else None,
                "task": {
                    "owner": { "id": db_issue.job.segment.task.owner_id},
                    "assignee": { "id": db_issue.job.segment.task.assignee_id }
                },
                "job": {
                    "assignee": { "id": db_issue.job.assignee_id }
                },
                "issue": {
                    "owner": { "id": db_issue.owner_id},
                    "assignee": { "id": db_issue.assignee_id }
                },
                'organization': { "id": organization_id }
            }

            return data

        if self.obj:
            db_issue = self.obj.issue
            data = get_common_data(db_issue)
            data.update({
                "id": self.obj.id,
                "owner": { "id": getattr(self.obj.owner, 'id', None) }
            })
        elif self.scope.startswith(__class__.Scopes.CREATE):
            try:
                db_issue = Issue.objects.get(id=self.issue_id)
            except Issue.DoesNotExist as ex:
                raise ValidationError(str(ex))
            data = get_common_data(db_issue)
            data.update({
                "owner": { "id": self.user_id }
            })

        return data

class IssuePermission(OpenPolicyAgentPermission):
    obj: Optional[Issue]

    class Scopes(StrEnum):
        LIST = 'list'
        CREATE  = 'create'
        CREATE_IN_JOB  = 'create@job'
        DELETE = 'delete'
        UPDATE = 'update'
        VIEW = 'view'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Issue | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'issue':
            assignee_id = request.data.get('assignee')
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj,
                    job_id=request.data.get('job'),
                    assignee_id=assignee_id)
                permissions.append(self)

            if assignee_id:
                perm = UserPermission.create_scope_view(iam_context, assignee_id)
                permissions.append(perm)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/issues/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Issue | None):
        Scopes = __class__.Scopes
        return [{
            'list': Scopes.LIST,
            'create': Scopes.CREATE_IN_JOB,
            'destroy': Scopes.DELETE,
            'partial_update': Scopes.UPDATE,
            'retrieve': Scopes.VIEW,
            'comments': Scopes.VIEW,
        }[view.action]]

    def get_resource(self):
        data = None
        def get_common_data(db_job):
            if db_job.segment.task.project:
                organization_id = db_job.segment.task.project.organization_id
            else:
                organization_id = db_job.segment.task.organization_id

            data = {
                "project": {
                    "owner": { "id": db_job.segment.task.project.owner_id },
                    "assignee": { "id": db_job.segment.task.project.assignee_id }
                } if db_job.segment.task.project else None,
                "task": {
                    "owner": { "id": db_job.segment.task.owner_id },
                    "assignee": { "id": db_job.segment.task.assignee_id }
                },
                "job": {
                    "assignee": { "id": db_job.assignee_id }
                },
                'organization': {
                    "id": organization_id
                }
            }

            return data

        if self.obj:
            db_job = self.obj.job
            data = get_common_data(db_job)
            data.update({
                "id": self.obj.id,
                "owner": { "id": self.obj.owner_id },
                "assignee": { "id": self.obj.assignee_id }
            })
        elif self.scope.startswith(__class__.Scopes.CREATE):
            job_id = self.job_id
            try:
                db_job = Job.objects.get(id=job_id)
            except Job.DoesNotExist as ex:
                raise ValidationError(str(ex))
            data = get_common_data(db_job)
            data.update({
                "owner": { "id": self.user_id },
                "assignee": { "id": self.assignee_id },
            })

        return data


class LabelPermission(OpenPolicyAgentPermission):
    obj: Optional[Label]

    class Scopes(StrEnum):
        LIST = 'list'
        DELETE = 'delete'
        UPDATE = 'update'
        VIEW = 'view'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: Label | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        Scopes = __class__.Scopes

        permissions = []
        if view.basename == 'label':
            for scope in cls.get_scopes(request, view, obj):
                if scope in [Scopes.DELETE, Scopes.UPDATE, Scopes.VIEW]:
                    obj = cast(Label, obj)

                    # Access rights are the same as in the owning objects
                    # Job assignees are not supposed to work with separate labels.
                    # They should only use the list operation.
                    if obj.project:
                        if scope == Scopes.VIEW:
                            owning_perm_scope = ProjectPermission.Scopes.VIEW
                        else:
                            owning_perm_scope = ProjectPermission.Scopes.UPDATE_DESC

                        owning_perm = ProjectPermission.create_base_perm(
                            request, view, owning_perm_scope, iam_context, obj=obj.project
                        )
                    else:
                        if scope == Scopes.VIEW:
                            owning_perm_scope = TaskPermission.Scopes.VIEW
                        else:
                            owning_perm_scope = TaskPermission.Scopes.UPDATE_DESC

                        owning_perm = TaskPermission.create_base_perm(
                            request, view, owning_perm_scope, iam_context, obj=obj.task,
                        )

                    # This component doesn't define its own rules for these cases
                    permissions.append(owning_perm)
                elif scope == Scopes.LIST and isinstance(obj, Job):
                    permissions.append(JobPermission.create_base_perm(
                        request, view, JobPermission.Scopes.VIEW, iam_context, obj=obj,
                    ))
                elif scope == Scopes.LIST and isinstance(obj, Task):
                    permissions.append(TaskPermission.create_base_perm(
                        request, view, TaskPermission.Scopes.VIEW, iam_context, obj=obj,
                    ))
                elif scope == Scopes.LIST and isinstance(obj, Project):
                    permissions.append(ProjectPermission.create_base_perm(
                        request, view, ProjectPermission.Scopes.VIEW, iam_context, obj=obj,
                    ))
                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/labels/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: Label | None):
        Scopes = __class__.Scopes
        return [{
            'list': Scopes.LIST,
            'destroy': Scopes.DELETE,
            'partial_update': Scopes.UPDATE,
            'retrieve': Scopes.VIEW,
        }[view.action]]

    def get_resource(self):
        data = None

        if self.obj:
            if self.obj.project:
                organization_id = self.obj.project.organization_id
            else:
                organization_id = self.obj.task.organization_id

            data = {
                "id": self.obj.id,
                'organization': { "id": organization_id },
                "task": {
                    "owner": { "id": self.obj.task.owner_id },
                    "assignee": { "id": self.obj.task.assignee_id }
                } if self.obj.task else None,
                "project": {
                    "owner": { "id": self.obj.project.owner_id },
                    "assignee": { "id": self.obj.project.assignee_id }
                } if self.obj.project else None,
            }

        return data

class AnnotationGuidePermission(OpenPolicyAgentPermission):
    obj: Optional[AnnotationGuide]

    class Scopes(StrEnum):
        VIEW = 'view'
        UPDATE = 'update'
        DELETE = 'delete'
        CREATE  = 'create'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: AnnotationGuide | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        permissions = []

        if view.basename == 'annotationguide':
            project_id = request.data.get('project_id')
            task_id = request.data.get('task_id')
            params = { 'project_id': project_id, 'task_id': task_id }

            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj, **params)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/annotationguides/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: AnnotationGuide | None):
        Scopes = __class__.Scopes
        return [{
            'create': Scopes.CREATE,
            'destroy': Scopes.DELETE,
            'partial_update': Scopes.UPDATE,
            'retrieve': Scopes.VIEW,
        }[view.action]]

    def get_resource(self):
        data = {}
        if self.obj:
            db_target = self.obj.target
            data.update({
                'id': self.obj.id,
                'target': {
                    'owner': { 'id': db_target.owner_id },
                    'assignee': { 'id': db_target.assignee_id },
                    'is_job_staff': db_target.is_job_staff(self.user_id),
                },
                'organization': { 'id': self.obj.organization_id }
            })
        elif self.scope == __class__.Scopes.CREATE:
            db_target = None
            if self.project_id is not None:
                try:
                    db_target = Project.objects.get(id=self.project_id)
                except Project.DoesNotExist as ex:
                    raise ValidationError(str(ex))
            elif self.task_id is not None:
                try:
                    db_target = Task.objects.get(id=self.task_id)
                except Task.DoesNotExist as ex:
                    raise ValidationError(str(ex))

            organization_id = getattr(db_target, 'organization_id', None)
            data.update({
                'target': {
                    'owner': { 'id': getattr(db_target, "owner_id", None) },
                    'assignee': { 'id': getattr(db_target, "assignee_id", None) },
                },
                'organization': { 'id': organization_id }
            })
        return data

class GuideAssetPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        VIEW = 'view'
        DELETE = 'delete'
        CREATE  = 'create'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: AnnotationGuide | None, iam_context: dict[str, Any]) -> list[OpenPolicyAgentPermission]:
        Scopes = __class__.Scopes
        permissions = []

        if view.basename == 'asset':
            for scope in cls.get_scopes(request, view, obj):
                if scope == Scopes.VIEW and isinstance(obj, AnnotationGuide):
                    permissions.append(AnnotationGuidePermission.create_base_perm(
                        request, view, AnnotationGuidePermission.Scopes.VIEW, iam_context, obj=obj)
                    )
                if scope == Scopes.DELETE and isinstance(obj, AnnotationGuide):
                    permissions.append(AnnotationGuidePermission.create_base_perm(
                        request, view, AnnotationGuidePermission.Scopes.UPDATE, iam_context, obj=obj)
                    )
                if scope == Scopes.CREATE:
                    guide_id = request.data.get('guide_id')
                    try:
                        obj = AnnotationGuide.objects.get(id=guide_id)
                        permissions.append(AnnotationGuidePermission.create_base_perm(
                            request, view, AnnotationGuidePermission.Scopes.UPDATE, iam_context, obj=obj)
                        )
                    except AnnotationGuide.DoesNotExist as ex:
                        raise ValidationError(str(ex))

        return permissions

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: AnnotationGuide | None):
        Scopes = __class__.Scopes
        return [{
            'create': Scopes.CREATE,
            'destroy': Scopes.DELETE,
            'retrieve': Scopes.VIEW,
        }[view.action]]


class RequestPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        LIST = 'list'
        VIEW = 'view'
        CANCEL = 'cancel'

    @classmethod
    def create(cls, request: ExtendedRequest, view: ViewSet, obj: RQJob | None, iam_context: dict) -> list[OpenPolicyAgentPermission]:
        permissions = []
        if view.basename == 'request':
            for scope in cls.get_scopes(request, view, obj):
                if scope != cls.Scopes.LIST:
                    user_id = request.user.id
                    if not is_rq_job_owner(obj, user_id):
                        raise PermissionDenied('You don\'t have permission to perform this action')

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + '/requests/allow'

    @staticmethod
    def get_scopes(request: ExtendedRequest, view: ViewSet, obj: RQJob | None) -> list[Scopes]:
        Scopes = __class__.Scopes
        return [{
            ('list', 'GET'): Scopes.LIST,
            ('retrieve', 'GET'): Scopes.VIEW,
            ('cancel', 'POST'): Scopes.CANCEL,
        }[(view.action, request.method)]]


    def get_resource(self):
        return None

def get_cloud_storage_for_import_or_export(
    storage_id: int, *, request: ExtendedRequest, is_default: bool = False
) -> CloudStorage:
    perm = CloudStoragePermission.create_scope_view(None, storage_id=storage_id, request=request)
    result = perm.check_access()
    if not result.allow:
        if is_default:
            # In this case, the user did not specify the location explicitly
            error_message = "A cloud storage is selected as the default location. "
        else:
            error_message = ""
        error_message += "You don't have access to this cloud storage"
        raise PermissionDenied(error_message)

    return get_object_or_404(CloudStorage, pk=storage_id)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\plugins.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from functools import update_wrapper

__plugins = {}


def add_plugin(name, function, order, exc_ok=False):
    if order not in ["before", "after"]:
        raise Exception("Order may be 'before' or 'after' only. Got {}.".format(order))

    if not callable(function):
        raise Exception("'function' argument should be a callable element")

    if not isinstance(name, str):
        raise Exception("'name' argument should be a string. Got {}.".format(type(name)))

    if name not in __plugins:
        __plugins[name] = {"before": [], "after": []}

    if function in __plugins[name][order]:
        raise Exception("plugin has been attached already")

    __plugins[name][order].append(function)

    function.exc_ok = exc_ok


def remove_plugin(name, function):
    if name in __plugins:
        if function in __plugins[name]["before"]:
            __plugins[name]["before"].remove(function)
            del function.exc_ok
        if function in __plugins[name]["after"]:
            __plugins[name]["after"].remove(function)
            del function.exc_ok


def plugin_decorator(function_to_decorate):
    name = function_to_decorate.__name__

    def function_wrapper(*args, **kwargs):
        if name in __plugins:
            for wrapper in __plugins[name]["before"]:
                try:
                    wrapper(*args, **kwargs)
                except Exception as ex:
                    if not wrapper.exc_ok:
                        raise ex

        result = function_to_decorate(*args, **kwargs)

        if name in __plugins:
            for wrapper in __plugins[name]["after"]:
                try:
                    wrapper(*args, **kwargs)
                except Exception as ex:
                    if not wrapper.exc_ok:
                        raise ex

        return result

    # Copy meta info about wrapped function to wrapper function
    update_wrapper(function_wrapper, function_to_decorate)
    return function_wrapper


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\renderers.py =====
# Copyright (C) 2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework.renderers import JSONRenderer


class CVATAPIRenderer(JSONRenderer):
    media_type = "application/vnd.cvat+json"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rq.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from abc import ABCMeta, abstractmethod
from typing import TYPE_CHECKING, Any, Callable, Optional, Protocol, Union
from uuid import UUID

import attrs
from django.conf import settings
from django.db.models import Model
from django.utils import timezone
from django_rq.queues import DjangoRQ
from rq.job import Dependency as RQDependency
from rq.job import Job as RQJob
from rq.registry import BaseRegistry as RQBaseRegistry

from cvat.apps.engine.types import ExtendedRequest

from .models import RequestAction, RequestSubresource, RequestTarget

if TYPE_CHECKING:
    from django.contrib.auth.models import User


class RQJobMetaField:
    class UserField:
        ID = "id"
        USERNAME = "username"
        EMAIL = "email"

    class RequestField:
        UUID = "uuid"
        TIMESTAMP = "timestamp"

    # failure info fields
    FORMATTED_EXCEPTION = "formatted_exception"
    EXCEPTION_TYPE = "exc_type"
    EXCEPTION_ARGS = "exc_args"
    # common fields
    REQUEST = "request"
    USER = "user"
    ORG_ID = "org_id"
    ORG_SLUG = "org_slug"
    PROJECT_ID = "project_id"
    TASK_ID = "task_id"
    JOB_ID = "job_id"
    STATUS = "status"
    PROGRESS = "progress"
    # import fields
    TMP_FILE = "tmp_file"
    TASK_PROGRESS = "task_progress"
    # export fields
    RESULT_URL = "result_url"
    RESULT_FILENAME = "result_filename"
    # lambda fields
    LAMBDA = "lambda"
    FUNCTION_ID = "function_id"


class WithMeta(Protocol):
    meta: dict[str, Any]


class ImmutableRQMetaAttribute:
    def __init__(self, key: str, *, optional: bool = False) -> None:
        self._key = key
        self._optional = optional

    def __get__(self, instance: WithMeta, objtype: type[WithMeta] | None = None):
        if self._optional:
            return instance.meta.get(self._key)

        return instance.meta[self._key]

    def __set__(self, instance: WithMeta, value: Any):
        raise AttributeError("Immutable attributes cannot be set")


class MutableRQMetaAttribute(ImmutableRQMetaAttribute):
    def __init__(
        self, key: str, *, optional: bool = False, validator: Callable | None = None
    ) -> None:
        super().__init__(key, optional=optional)
        assert validator is None or callable(validator), "validator must be callable"
        self._validator = validator

    def validate(self, value):
        if value is None and not self._optional:
            raise ValueError(f"{self._key} is required")
        if value is not None and self._validator and not self._validator(value):
            raise ValueError("Value does not match the attribute validator")

    def __set__(self, instance: WithMeta, value: Any):
        self.validate(value)
        instance.meta[self._key] = value


class UserMeta:
    id: int = ImmutableRQMetaAttribute(RQJobMetaField.UserField.ID)
    username: str = ImmutableRQMetaAttribute(RQJobMetaField.UserField.USERNAME)
    email: str = ImmutableRQMetaAttribute(RQJobMetaField.UserField.EMAIL)

    def __init__(self, meta: dict[str, Any]) -> None:
        self._meta = meta

    @property
    def meta(self) -> dict[str, Any]:
        return self._meta

    def to_dict(self):
        return self.meta


class RequestMeta:
    uuid = ImmutableRQMetaAttribute(RQJobMetaField.RequestField.UUID)
    timestamp = ImmutableRQMetaAttribute(RQJobMetaField.RequestField.TIMESTAMP)

    def __init__(self, meta: dict[str, Any]) -> None:
        self._meta = meta

    @property
    def meta(self) -> dict[str, Any]:
        return self._meta

    def to_dict(self):
        return self.meta


class AbstractRQMeta(metaclass=ABCMeta):
    def __init__(self, *, meta: dict[str, Any], job: RQJob | None = None) -> None:
        if job:
            assert (
                meta is job.meta
            ), "When passed together, job.meta and meta should refer to the same object"

        self._job = job
        self._meta = meta

    @property
    def meta(self) -> dict[str, Any]:
        return self._meta

    @classmethod
    def for_job(cls, job: RQJob):
        return cls(job=job, meta=job.meta)

    @classmethod
    def for_meta(cls, meta: dict[str, Any]):
        return cls(meta=meta)

    def save(self) -> None:
        assert isinstance(self._job, RQJob), "To save meta, rq job must be set"
        self._job.save_meta()

    @staticmethod
    @abstractmethod
    def _get_resettable_fields() -> list[str]:
        """Return a list of fields that must be reset on retry"""

    def get_meta_on_retry(self) -> dict[str, Any]:
        resettable_fields = self._get_resettable_fields()

        return {k: v for k, v in self._meta.items() if k not in resettable_fields}


class RQMetaWithFailureInfo(AbstractRQMeta):
    formatted_exception = MutableRQMetaAttribute(
        RQJobMetaField.FORMATTED_EXCEPTION,
        validator=lambda x: isinstance(x, str),
        optional=True,
    )
    exc_type = MutableRQMetaAttribute(
        RQJobMetaField.EXCEPTION_TYPE,
        validator=lambda x: issubclass(x, BaseException),
        optional=True,
    )
    exc_args = MutableRQMetaAttribute(
        RQJobMetaField.EXCEPTION_ARGS,
        validator=lambda x: isinstance(x, tuple),
        optional=True,
    )

    @staticmethod
    def _get_resettable_fields() -> list[str]:
        return [
            RQJobMetaField.FORMATTED_EXCEPTION,
            RQJobMetaField.EXCEPTION_TYPE,
            RQJobMetaField.EXCEPTION_ARGS,
        ]


class BaseRQMeta(RQMetaWithFailureInfo):
    # immutable fields
    # FUTURE-TODO: change to required fields when each enqueued job
    # regardless of queue type will have these fields
    # Blocked now by:
    # - [annotation queue] Some jobs may have no user/request info
    # - [chunks queue] Each job has no user/request info
    # - [import queue] Jobs running to cleanup uploaded files have no user/request info
    # - [export queue] Jobs preparing events have no user/request info.
    # - [export queue] Jobs running to cleanup csv files with events have no user/request info

    @property
    def user(self):
        if user_info := self.meta.get(RQJobMetaField.USER):
            return UserMeta(user_info)

        return None

    @property
    def request(self):
        if request_info := self.meta.get(RQJobMetaField.REQUEST):
            return RequestMeta(request_info)

        return None

    # immutable && optional fields
    org_id: int | None = ImmutableRQMetaAttribute(RQJobMetaField.ORG_ID, optional=True)
    org_slug: int | None = ImmutableRQMetaAttribute(RQJobMetaField.ORG_SLUG, optional=True)
    project_id: int | None = ImmutableRQMetaAttribute(RQJobMetaField.PROJECT_ID, optional=True)
    task_id: int | None = ImmutableRQMetaAttribute(RQJobMetaField.TASK_ID, optional=True)
    job_id: int | None = ImmutableRQMetaAttribute(RQJobMetaField.JOB_ID, optional=True)

    # mutable && optional fields
    progress: float | None = MutableRQMetaAttribute(
        RQJobMetaField.PROGRESS, validator=lambda x: isinstance(x, float), optional=True
    )
    status: str | None = MutableRQMetaAttribute(
        RQJobMetaField.STATUS, validator=lambda x: isinstance(x, str), optional=True
    )

    @staticmethod
    def _get_resettable_fields() -> list[str]:
        return RQMetaWithFailureInfo._get_resettable_fields() + [
            RQJobMetaField.PROGRESS,
            RQJobMetaField.STATUS,
        ]

    @classmethod
    def build(
        cls,
        *,
        request: ExtendedRequest,
        db_obj: Model | None,
    ):
        # to prevent circular import
        from cvat.apps.events.handlers import job_id, organization_slug, task_id
        from cvat.apps.webhooks.signals import organization_id, project_id

        oid = organization_id(db_obj)
        oslug = organization_slug(db_obj)
        pid = project_id(db_obj)
        tid = task_id(db_obj)
        jid = job_id(db_obj)

        user: User = request.user

        return {
            RQJobMetaField.USER: {
                RQJobMetaField.UserField.ID: user.pk,
                RQJobMetaField.UserField.USERNAME: user.username,
                RQJobMetaField.UserField.EMAIL: user.email,
            },
            RQJobMetaField.REQUEST: {
                RQJobMetaField.RequestField.UUID: request.uuid,
                RQJobMetaField.RequestField.TIMESTAMP: timezone.now(),
            },
            RQJobMetaField.ORG_ID: oid,
            RQJobMetaField.ORG_SLUG: oslug,
            RQJobMetaField.PROJECT_ID: pid,
            RQJobMetaField.TASK_ID: tid,
            RQJobMetaField.JOB_ID: jid,
        }


class ExportRQMeta(BaseRQMeta):
    result_url: str | None = ImmutableRQMetaAttribute(
        RQJobMetaField.RESULT_URL,
        optional=True,
    )
    result_filename: str = ImmutableRQMetaAttribute(RQJobMetaField.RESULT_FILENAME)

    @staticmethod
    def _get_resettable_fields() -> list[str]:
        base_fields = BaseRQMeta._get_resettable_fields()
        return base_fields + [RQJobMetaField.RESULT_URL, RQJobMetaField.RESULT_FILENAME]

    @classmethod
    def build_for(
        cls,
        *,
        request: ExtendedRequest,
        db_obj: Model,
        result_url: str | None,
        result_filename: str,
    ):
        base_meta = BaseRQMeta.build(request=request, db_obj=db_obj)

        return {
            **base_meta,
            RQJobMetaField.RESULT_URL: result_url,
            RQJobMetaField.RESULT_FILENAME: result_filename,
        }


class ImportRQMeta(BaseRQMeta):
    # immutable && optional fields
    tmp_file: str | None = ImmutableRQMetaAttribute(
        RQJobMetaField.TMP_FILE, optional=True
    )  # used only when importing annotations|datasets|backups

    # mutable fields
    task_progress: float | None = MutableRQMetaAttribute(
        RQJobMetaField.TASK_PROGRESS, validator=lambda x: isinstance(x, float), optional=True
    )  # used when importing project dataset

    @staticmethod
    def _get_resettable_fields() -> list[str]:
        base_fields = BaseRQMeta._get_resettable_fields()

        return base_fields + [RQJobMetaField.TASK_PROGRESS]

    @classmethod
    def build_for(
        cls,
        *,
        request: ExtendedRequest,
        db_obj: Model | None,
        tmp_file: str | None = None,
    ):
        base_meta = BaseRQMeta.build(request=request, db_obj=db_obj)

        return {**base_meta, RQJobMetaField.TMP_FILE: tmp_file}


def is_rq_job_owner(rq_job: RQJob, user_id: int) -> bool:
    if user := BaseRQMeta.for_job(rq_job).user:
        return user.id == user_id

    return False


@attrs.frozen()
class RQId:
    action: RequestAction = attrs.field(validator=attrs.validators.instance_of(RequestAction))
    target: RequestTarget = attrs.field(validator=attrs.validators.instance_of(RequestTarget))
    identifier: Union[int, UUID] = attrs.field(validator=attrs.validators.instance_of((int, UUID)))
    subresource: Optional[RequestSubresource] = attrs.field(
        validator=attrs.validators.optional(attrs.validators.instance_of(RequestSubresource)),
        kw_only=True,
        default=None,
    )
    user_id: Optional[int] = attrs.field(
        validator=attrs.validators.optional(attrs.validators.instance_of(int)),
        kw_only=True,
        default=None,
    )
    format: Optional[str] = attrs.field(
        validator=attrs.validators.optional(attrs.validators.instance_of(str)),
        kw_only=True,
        default=None,
    )

    _OPTIONAL_FIELD_REQUIREMENTS = {
        RequestAction.AUTOANNOTATE: {"subresource": False, "format": False, "user_id": False},
        RequestAction.CREATE: {"subresource": False, "format": False, "user_id": False},
        RequestAction.EXPORT: {"subresource": True, "user_id": True},
        RequestAction.IMPORT: {"subresource": True, "format": False, "user_id": False},
    }

    def __attrs_post_init__(self) -> None:
        for field, req in self._OPTIONAL_FIELD_REQUIREMENTS[self.action].items():
            if req:
                if getattr(self, field) is None:
                    raise ValueError(f"{field} is required for the {self.action} action")
            else:
                if getattr(self, field) is not None:
                    raise ValueError(f"{field} is not allowed for the {self.action} action")

    # RQ ID templates:
    # autoannotate:task-<tid>
    # import:<task|project|job>-<id|uuid>-<annotations|dataset|backup>
    # create:task-<tid>
    # export:<project|task|job>-<id>-<annotations|dataset>-in-<format>-format-by-<user_id>
    # export:<project|task>-<id>-backup-by-<user_id>

    def render(
        self,
    ) -> str:
        common_prefix = f"{self.action}:{self.target}-{self.identifier}"

        if RequestAction.IMPORT == self.action:
            return f"{common_prefix}-{self.subresource}"
        elif RequestAction.EXPORT == self.action:
            if self.format is None:
                return f"{common_prefix}-{self.subresource}-by-{self.user_id}"

            format_to_be_used_in_urls = self.format.replace(" ", "_").replace(".", "@")
            return f"{common_prefix}-{self.subresource}-in-{format_to_be_used_in_urls}-format-by-{self.user_id}"
        elif self.action in {RequestAction.CREATE, RequestAction.AUTOANNOTATE}:
            return common_prefix
        else:
            assert False, f"Unsupported action {self.action!r} was found"

    @staticmethod
    def parse(rq_id: str) -> RQId:
        identifier: Optional[Union[UUID, int]] = None
        subresource: Optional[RequestSubresource] = None
        user_id: Optional[int] = None
        anno_format: Optional[str] = None

        try:
            action_and_resource, unparsed = rq_id.split("-", maxsplit=1)
            action_str, target_str = action_and_resource.split(":")
            action = RequestAction(action_str)
            target = RequestTarget(target_str)

            if action in {RequestAction.CREATE, RequestAction.AUTOANNOTATE}:
                identifier = unparsed
            elif RequestAction.IMPORT == action:
                identifier, subresource_str = unparsed.rsplit("-", maxsplit=1)
                subresource = RequestSubresource(subresource_str)
            else:  # action == export
                identifier, subresource_str, unparsed = unparsed.split("-", maxsplit=2)
                subresource = RequestSubresource(subresource_str)

                if RequestSubresource.BACKUP == subresource:
                    _, user_id = unparsed.split("-")
                else:
                    unparsed, _, user_id = unparsed.rsplit("-", maxsplit=2)
                    # remove prefix(in-), suffix(-format) and restore original format name
                    # by replacing special symbols: "_" -> " ", "@" -> "."
                    anno_format = unparsed[3:-7].replace("_", " ").replace("@", ".")

            if identifier is not None:
                if identifier.isdigit():
                    identifier = int(identifier)
                else:
                    identifier = UUID(identifier)

            if user_id is not None:
                user_id = int(user_id)

            return RQId(
                action=action,
                target=target,
                identifier=identifier,
                subresource=subresource,
                user_id=user_id,
                format=anno_format,
            )

        except Exception as ex:
            raise ValueError(f"The {rq_id!r} RQ ID cannot be parsed: {str(ex)}") from ex


def define_dependent_job(
    queue: DjangoRQ,
    user_id: int,
    should_be_dependent: bool = settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER,
    *,
    rq_id: Optional[str] = None,
) -> RQDependency:
    if not should_be_dependent:
        return None

    queues: list[RQBaseRegistry | DjangoRQ] = [
        queue.deferred_job_registry,
        queue,
        queue.started_job_registry,
    ]
    # Since there is no cleanup implementation in DeferredJobRegistry,
    # this registry can contain "outdated" jobs that weren't deleted from it
    # but were added to another registry. Probably such situations can occur
    # if there are active or deferred jobs when restarting the worker container.
    filters = [lambda job: job.is_deferred, lambda _: True, lambda _: True]
    all_user_jobs: list[RQJob] = []
    for q, f in zip(queues, filters):
        job_ids = q.get_job_ids()
        jobs = q.job_class.fetch_many(job_ids, q.connection)
        jobs = filter(
            lambda job: job and is_rq_job_owner(job, user_id) and f(job),
            jobs,
        )
        all_user_jobs.extend(jobs)

    if rq_id:
        # Prevent cases where an RQ job depends on itself.
        # It isn't possible to have multiple RQ jobs with the same ID in Redis.
        # However, a race condition in request processing can lead to self-dependencies
        # when 2 parallel requests attempt to enqueue RQ jobs with the same ID.
        # This happens if an rq_job is fetched without a lock,
        # but a lock is used when defining the dependent job and enqueuing a new one.
        if any(rq_id == job.id for job in all_user_jobs):
            return None

        # prevent possible cyclic dependencies
        all_job_dependency_ids = {
            dep_id.decode() for job in all_user_jobs for dep_id in job.dependency_ids or ()
        }

        if RQJob.redis_job_namespace_prefix + rq_id in all_job_dependency_ids:
            return None

    return (
        RQDependency(
            jobs=[sorted(all_user_jobs, key=lambda job: job.created_at)[-1]], allow_failure=True
        )
        if all_user_jobs
        else None
    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\schema.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap

from drf_spectacular.extensions import OpenApiSerializerExtension
from drf_spectacular.plumbing import build_basic_type, force_instance
from drf_spectacular.serializers import PolymorphicProxySerializerExtension
from drf_spectacular.types import OpenApiTypes
from rest_framework import serializers


def _copy_serializer(
    instance: serializers.Serializer, *, _new_type: type[serializers.Serializer] = None, **kwargs
) -> serializers.Serializer:
    _new_type = _new_type or type(instance)
    instance_kwargs = instance._kwargs
    instance_kwargs["partial"] = instance.partial  # this can be set separately
    instance_kwargs.update(kwargs)
    return _new_type(*instance._args, **instance._kwargs)


class _SerializerTransformer:
    def __init__(self, serializer_instance: serializers.ModelSerializer) -> None:
        self.serializer_instance = serializer_instance

    def _get_field(self, source_name: str, field_name: str) -> serializers.ModelField:
        child_instance = force_instance(self.serializer_instance.fields[source_name].child)
        assert isinstance(child_instance, serializers.ModelSerializer)

        child_fields = child_instance.fields
        assert child_fields.keys() == {field_name}  # protection from implementation changes
        return child_fields[field_name]

    @staticmethod
    def _sanitize_field(field: serializers.ModelField) -> serializers.ModelField:
        field.source = None
        field.source_attrs = []
        return field

    def make_field(self, source_name: str, field_name: str) -> serializers.ModelField:
        return self._sanitize_field(self._get_field(source_name, field_name))


class DataSerializerExtension(OpenApiSerializerExtension):
    # *FileSerializer mimics a FileField
    # but it is mapped as an object with a file field, which
    # is different from what we have for a regular file
    # field - a string of binary format.
    # This class replaces the generated schema as if it was:
    # *_files = serializers.ListField(child=serializers.FileField(allow_empty_file=False))
    #
    # Also, the generator doesn't work with the correct OpenAPI definition,
    # where FileField-like structure (plain type) is referenced in args.
    # So, this class overrides the whole field.

    target_class = "cvat.apps.engine.serializers.DataSerializer"

    def map_serializer(self, auto_schema, direction):
        assert issubclass(self.target_class, serializers.ModelSerializer)

        instance = self.target
        assert isinstance(instance, serializers.ModelSerializer)

        serializer_transformer = _SerializerTransformer(instance)
        source_client_files = instance.fields["client_files"]
        source_server_files = instance.fields["server_files"]
        source_remote_files = instance.fields["remote_files"]

        class _Override(self.target_class):  # pylint: disable=inherit-non-class
            client_files = serializers.ListField(
                child=serializer_transformer.make_field("client_files", "file"),
                default=source_client_files.default,
                help_text=source_client_files.help_text,
            )
            server_files = serializers.ListField(
                child=serializer_transformer.make_field("server_files", "file"),
                default=source_server_files.default,
                help_text=source_server_files.help_text,
            )
            remote_files = serializers.ListField(
                child=serializer_transformer.make_field("remote_files", "file"),
                default=source_remote_files.default,
                help_text=source_remote_files.help_text,
            )

        return auto_schema._map_serializer(
            _copy_serializer(instance, _new_type=_Override, context={"view": auto_schema.view}),
            direction,
            bypass_extensions=False,
        )


class WriteOnceSerializerExtension(OpenApiSerializerExtension):
    """
    Enables support for cvat.apps.engine.serializers.WriteOnceMixin in drf-spectacular.
    Doesn't block other extensions on the target serializer.

    Removes the WriteOnceMixin class docstring from derived class descriptions.
    """

    match_subclasses = True
    target_class = "cvat.apps.engine.serializers.WriteOnceMixin"
    _PROCESSED_INDICATOR_NAME = "write_once_serializer_extension_processed"

    @classmethod
    def _matches(cls, target) -> bool:
        if super()._matches(target):
            # protect from recursive invocations
            assert isinstance(target, serializers.Serializer)
            processed = target.context.get(cls._PROCESSED_INDICATOR_NAME, False)
            return not processed
        return False

    def map_serializer(self, auto_schema, direction):
        from cvat.apps.engine.serializers import WriteOnceMixin

        schema = auto_schema._map_serializer(
            _copy_serializer(
                self.target,
                context={"view": auto_schema.view, self._PROCESSED_INDICATOR_NAME: True},
            ),
            direction,
            bypass_extensions=False,
        )

        if schema.get("description") == textwrap.dedent(WriteOnceMixin.__doc__).strip():
            del schema["description"]

        return schema


class OpenApiTypeProxySerializerExtension(PolymorphicProxySerializerExtension):
    """
    Provides support for OpenApiTypes in the PolymorphicProxySerializer list
    """

    priority = 0  # restore normal priority

    def _process_serializer(self, auto_schema, serializer, direction):
        if isinstance(serializer, OpenApiTypes):
            schema = build_basic_type(serializer)
            return (None, schema)
        else:
            return super()._process_serializer(
                auto_schema=auto_schema, serializer=serializer, direction=direction
            )

    def map_serializer(self, auto_schema, direction):
        """custom handling for @extend_schema's injection of PolymorphicProxySerializer"""
        result = super().map_serializer(auto_schema=auto_schema, direction=direction)

        if isinstance(self.target.serializers, dict):
            required = OpenApiTypes.NONE not in self.target.serializers.values()
        else:
            required = OpenApiTypes.NONE not in self.target.serializers

        if not required:
            result["nullable"] = True

        return result


class ComponentProxySerializerExtension(OpenApiTypeProxySerializerExtension):
    """
    Allows to patch PolymorphicProxySerializer-based component schema.

    Override the "target_component" field in children classes.
    """

    priority = 1  # higher than in the parent class

    target_component: str = ""

    @classmethod
    def _matches(cls, target) -> bool:
        if cls == __class__:
            return False

        if not super()._matches(target):
            return False

        return target.component_name == cls.target_component


class AnyOfProxySerializerExtension(ComponentProxySerializerExtension):
    """
    Replaces oneOf with anyOf in the generated schema. Useful when
    no disciminator field is available, and the options are
    not mutually-exclusive.
    """

    def map_serializer(self, auto_schema, direction):
        schema = super().map_serializer(auto_schema, direction)
        schema["anyOf"] = schema.pop("oneOf")
        return schema


class MetaUserSerializerExtension(AnyOfProxySerializerExtension):
    # Need to replace oneOf to anyOf for MetaUser variants
    # Otherwise, clients cannot distinguish between classes
    # using just input data. Also, we can't use discrimintator
    # field here, because these serializers don't have such.
    target_component = "MetaUser"


class PolymorphicProjectSerializerExtension(AnyOfProxySerializerExtension):
    # Need to replace oneOf to anyOf for PolymorphicProject variants
    # Otherwise, clients cannot distinguish between classes
    # using just input data. Also, we can't use discrimintator
    # field here, because these serializers don't have such.
    target_component = "PolymorphicProject"


class _CloudStorageSerializerExtension(OpenApiSerializerExtension):

    def map_serializer(self, auto_schema, direction):
        assert issubclass(self.target_class, serializers.ModelSerializer)

        instance = self.target
        assert isinstance(instance, serializers.ModelSerializer)

        serializer_transformer = _SerializerTransformer(instance)

        class _Override(self.target_class):  # pylint: disable=inherit-non-class
            manifests = serializers.ListField(
                child=serializer_transformer.make_field("manifests", "filename"), default=[]
            )

        return auto_schema._map_serializer(
            _copy_serializer(instance, _new_type=_Override, context={"view": auto_schema.view}),
            direction,
            bypass_extensions=False,
        )


class CloudStorageReadSerializerExtension(_CloudStorageSerializerExtension):
    target_class = "cvat.apps.engine.serializers.CloudStorageReadSerializer"


class CloudStorageWriteSerializerExtension(_CloudStorageSerializerExtension):
    target_class = "cvat.apps.engine.serializers.CloudStorageWriteSerializer"


__all__ = []  # No public symbols here


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\serializers.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import os
import re
import shutil
import string
import textwrap
import warnings
from collections import OrderedDict
from collections.abc import Iterable, Sequence
from contextlib import closing
from copy import copy
from datetime import timedelta
from decimal import Decimal
from inspect import isclass
from tempfile import NamedTemporaryFile
from typing import Any, Optional, Union

import django_rq
import rq.defaults as rq_defaults
from django.conf import settings
from django.contrib.auth.models import Group, User
from django.db import transaction
from django.db.models import Prefetch, prefetch_related_objects
from django.utils import timezone
from drf_spectacular.utils import OpenApiExample, extend_schema_field, extend_schema_serializer
from numpy import random
from rest_framework import exceptions, serializers
from rq.job import Job as RQJob
from rq.job import JobStatus as RQJobStatus

from cvat.apps.dataset_manager.formats.utils import get_label_color
from cvat.apps.engine import field_validation, models
from cvat.apps.engine.cloud_provider import Credentials, Status, get_cloud_storage_instance
from cvat.apps.engine.frame_provider import FrameQuality, TaskFrameProvider
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.model_utils import bulk_create
from cvat.apps.engine.permissions import TaskPermission
from cvat.apps.engine.rq import BaseRQMeta, ExportRQMeta, ImportRQMeta, RequestAction, RQId
from cvat.apps.engine.task_validation import HoneypotFrameSelector
from cvat.apps.engine.utils import (
    CvatChunkTimestampMismatchError,
    build_field_filter_params,
    format_list,
    get_list_view_name,
    grouped,
    parse_exception_message,
    parse_specific_attributes,
    reverse,
    take_by,
)
from cvat.apps.lambda_manager.rq import LambdaRQMeta
from utils.dataset_manifest import ImageManifestManager

slogger = ServerLogManager(__name__)

class WriteOnceMixin:
    """
    Adds support for write once fields to serializers.

    To use it, specify a list of fields as `write_once_fields` on the
    serializer's Meta:
    ```
    class Meta:
        model = SomeModel
        fields = '__all__'
        write_once_fields = ('collection', )
    ```

    Now the fields in `write_once_fields` can be set during POST (create),
    but cannot be changed afterwards via PUT or PATCH (update).
    Inspired by http://stackoverflow.com/a/37487134/627411.
    """

    def get_fields(self):
        fields = super().get_fields()

        # We're only interested in PATCH and PUT.
        if 'update' in getattr(self.context.get('view'), 'action', ''):
            fields = self._update_write_once_fields(fields)

        return fields

    def _update_write_once_fields(self, fields):
        """
        Set all fields in `Meta.write_once_fields` to read_only.
        """

        write_once_fields = getattr(self.Meta, 'write_once_fields', None)
        if not write_once_fields:
            return fields

        if not isinstance(write_once_fields, (list, tuple)):
            raise TypeError(
                'The `write_once_fields` option must be a list or tuple. '
                'Got {}.'.format(type(write_once_fields).__name__)
            )

        for field_name in write_once_fields:
            fields[field_name].read_only = True

        return fields


@extend_schema_field(serializers.URLField)
class HyperlinkedEndpointSerializer(serializers.Serializer):
    key_field = 'pk'

    def __init__(self, view_name=None, *, filter_key=None, **kwargs):
        if isclass(view_name) and issubclass(view_name, models.models.Model):
            view_name = get_list_view_name(view_name)
        elif not isinstance(view_name, str):
            raise TypeError(view_name)

        kwargs['read_only'] = True
        super().__init__(**kwargs)

        self.view_name = view_name
        self.filter_key = filter_key

    def get_attribute(self, instance):
        return instance

    def to_representation(self, instance):
        request = self.context.get('request')
        if not request:
            return None

        return serializers.Hyperlink(
            reverse(self.view_name, request=request,
                query_params=build_field_filter_params(
                    self.filter_key, getattr(instance, self.key_field)
            )),
            instance
        )


class _CollectionSummarySerializer(serializers.Serializer):
    # This class isn't recommended for direct use in public serializers
    # because it produces too generic description in the schema.
    # Consider creating a dedicated inherited class instead.

    count = serializers.IntegerField(default=0)

    def __init__(self, model, *, url_filter_key, **kwargs):
        super().__init__(**kwargs)
        self._collection_key = self.source
        self._model = model
        self._url_filter_key = url_filter_key

    def bind(self, field_name, parent):
        super().bind(field_name, parent)
        self._collection_key = self._collection_key or self.source
        self._model = self._model or type(self.parent)

    def get_fields(self):
        fields = super().get_fields()
        fields['url'] = HyperlinkedEndpointSerializer(self._model, filter_key=self._url_filter_key)
        if not fields['count'].source:
            fields['count'].source = self._collection_key + '.count'
        return fields

    def get_attribute(self, instance):
        return instance

class JobsSummarySerializer(_CollectionSummarySerializer):
    count = serializers.IntegerField(source='total_jobs_count', default=0)
    completed = serializers.IntegerField(source='completed_jobs_count', allow_null=True)
    validation = serializers.IntegerField(source='validation_jobs_count', allow_null=True)

    def __init__(self, *, model=models.Job, url_filter_key, **kwargs):
        super().__init__(model=model, url_filter_key=url_filter_key, **kwargs)


MAX_FILENAME_LENGTH = 1024


class TasksSummarySerializer(_CollectionSummarySerializer):
    pass


class CommentsSummarySerializer(_CollectionSummarySerializer):
    pass

class LabelsSummarySerializer(serializers.Serializer):
    url = serializers.URLField(read_only=True)

    def get_url(self, request, instance):
        filter_key = instance.__class__.__name__.lower() + '_id'
        return reverse('label-list', request=request,
            query_params={ filter_key: instance.id })

    def to_representation(self, instance):
        request = self.context.get('request')
        if not request:
            return None

        return {
            'url': self.get_url(request, instance),
        }

class IssuesSummarySerializer(serializers.Serializer):
    url = serializers.URLField(read_only=True)
    count = serializers.IntegerField(read_only=True)

    def get_url(self, request, instance):
        return reverse('issue-list', request=request,
            query_params={ 'job_id': instance.id })

    def get_count(self, instance):
        return getattr(instance, 'issues__count', 0)

    def to_representation(self, instance):
        request = self.context.get('request')
        if not request:
            return None

        return {
            'url': self.get_url(request, instance),
            'count': self.get_count(instance)
        }


class BasicUserSerializer(serializers.ModelSerializer):
    def validate(self, attrs):
        if hasattr(self, 'initial_data'):
            unknown_keys = set(self.initial_data.keys()) - set(self.fields.keys())
            if unknown_keys:
                if set(['is_staff', 'is_superuser', 'groups']) & unknown_keys:
                    message = 'You do not have permissions to access some of' + \
                        ' these fields: {}'.format(unknown_keys)
                else:
                    message = 'Got unknown fields: {}'.format(unknown_keys)
                raise serializers.ValidationError(message)
        return attrs

    class Meta:
        model = User
        fields = ('url', 'id', 'username', 'first_name', 'last_name')


class UserSerializer(serializers.ModelSerializer):
    groups = serializers.SlugRelatedField(many=True,
        slug_field='name', queryset=Group.objects.all())
    has_analytics_access = serializers.BooleanField(
        source='profile.has_analytics_access',
        required=False,
        read_only=True,
    )

    class Meta:
        model = User
        fields = ('url', 'id', 'username', 'first_name', 'last_name', 'email',
            'groups', 'is_staff', 'is_superuser', 'is_active', 'last_login',
            'date_joined', 'has_analytics_access')
        read_only_fields = ('last_login', 'date_joined', 'has_analytics_access')
        write_only_fields = ('password', )
        extra_kwargs = {
            'last_login': { 'allow_null': True }
        }


class DelimitedStringListField(serializers.ListField):
    def to_representation(self, value):
        return super().to_representation(value.split('\n'))

    def to_internal_value(self, data):
        return '\n'.join(super().to_internal_value(data))


class AttributeSerializer(serializers.ModelSerializer):
    id = serializers.IntegerField(required=False)
    values = DelimitedStringListField(allow_empty=True,
        child=serializers.CharField(allow_blank=True, max_length=200),
    )

    class Meta:
        model = models.AttributeSpec
        fields = ('id', 'name', 'mutable', 'input_type', 'default_value', 'values')


class SublabelSerializer(serializers.ModelSerializer):
    id = serializers.IntegerField(required=False)
    attributes = AttributeSerializer(many=True, source='attributespec_set', default=[],
        help_text="The list of attributes. "
        "If you want to remove an attribute, you need to recreate the label "
        "and specify the remaining attributes.")
    color = serializers.CharField(allow_blank=True, required=False,
        help_text="The hex value for the RGB color. "
        "Will be generated automatically, unless specified explicitly.")
    type = serializers.ChoiceField(choices=models.LabelType.choices(), required=False,
        help_text="Associated annotation type for this label")
    has_parent = serializers.BooleanField(source='has_parent_label', required=False)

    class Meta:
        model = models.Label
        fields = ('id', 'name', 'color', 'attributes', 'type', 'has_parent', )
        read_only_fields = ('parent',)


class SkeletonSerializer(serializers.ModelSerializer):
    id = serializers.IntegerField(required=False)
    svg = serializers.CharField(allow_blank=True, required=False)

    class Meta:
        model = models.Skeleton
        fields = ('id', 'svg',)


class LabelSerializer(SublabelSerializer):
    deleted = serializers.BooleanField(required=False, write_only=True,
        help_text='Delete the label. Only applicable in the PATCH methods of a project or a task.')
    sublabels = SublabelSerializer(many=True, required=False)
    svg = serializers.CharField(allow_blank=True, required=False)
    has_parent = serializers.BooleanField(read_only=True, source='has_parent_label', required=False)

    class Meta:
        model = models.Label
        fields = (
            'id', 'name', 'color', 'attributes', 'deleted', 'type', 'svg',
            'sublabels', 'project_id', 'task_id', 'parent_id', 'has_parent'
        )
        read_only_fields = ('id', 'svg', 'project_id', 'task_id')
        extra_kwargs = {
            'project_id': { 'required': False, 'allow_null': False },
            'task_id': { 'required': False, 'allow_null': False },
            'parent_id': { 'required': False, },
        }

    def to_representation(self, instance):
        label = super().to_representation(instance)
        if label['type'] == str(models.LabelType.SKELETON):
            label['svg'] = instance.skeleton.svg

        # Clean mutually exclusive fields
        if not label.get('task_id'):
            label.pop('task_id', None)
        if not label.get('project_id'):
            label.pop('project_id', None)

        return label

    def __init__(self, *args, **kwargs):
        self._local = kwargs.pop('local', False)
        """
        Indicates that the operation is called from the dedicated ViewSet
        and not from the parent entity, i.e. a project or task.
        """

        super().__init__(*args, **kwargs)

    def validate(self, attrs):
        if self._local and attrs.get('deleted'):
            # NOTE: Navigate clients to the right method
            raise serializers.ValidationError(
                'Labels cannot be deleted by updating in this endpoint. '
                'Please use the DELETE method instead.'
            )

        if attrs.get('deleted') and attrs.get('id') is None:
            raise serializers.ValidationError('Deleted label must have an ID')

        return attrs

    @staticmethod
    def check_attribute_names_unique(attrs):
        encountered_names = set()
        for attribute in attrs:
            attr_name = attribute.get('name')
            if attr_name in encountered_names:
                raise serializers.ValidationError(f"Duplicate attribute with name '{attr_name}' exists")
            else:
                encountered_names.add(attr_name)

    @classmethod
    @transaction.atomic
    def update_label(
        cls,
        validated_data: dict[str, Any],
        svg: str,
        sublabels: Iterable[dict[str, Any]],
        *,
        parent_instance: Union[models.Project, models.Task],
        parent_label: Optional[models.Label] = None
    ) -> Optional[models.Label]:
        parent_info, logger = cls._get_parent_info(parent_instance)

        attributes = validated_data.pop('attributespec_set', [])

        cls.check_attribute_names_unique(attributes)

        if validated_data.get('id') is not None:
            try:
                db_label = models.Label.objects.get(id=validated_data['id'], **parent_info)
            except models.Label.DoesNotExist as exc:
                raise exceptions.NotFound(
                    detail='Not found label with id #{} to change'.format(validated_data['id'])
                ) from exc

            updated_type = validated_data.get('type') or db_label.type
            if str(models.LabelType.SKELETON) in [db_label.type, updated_type]:
                # do not permit changing types from/to skeleton
                logger.warning("Label id {} ({}): an attempt to change label type from {} to {}. "
                    "Changing from or to '{}' is not allowed, the type won't be changed.".format(
                    db_label.id,
                    db_label.name,
                    db_label.type,
                    updated_type,
                    str(models.LabelType.SKELETON),
                ))
            else:
                db_label.type = updated_type

            db_label.name = validated_data.get('name') or db_label.name

            logger.info("Label id {} ({}) was updated".format(db_label.id, db_label.name))
        else:
            try:
                db_label = models.Label.create(
                    name=validated_data.get('name'),
                    type=validated_data.get('type', models.LabelType.ANY),
                    parent=parent_label,
                    **parent_info
                )
            except models.InvalidLabel as exc:
                raise exceptions.ValidationError(str(exc)) from exc
            logger.info("New {} label was created".format(db_label.name))

            cls.update_labels(sublabels, parent_instance=parent_instance, parent_label=db_label)

            if db_label.type == str(models.LabelType.SKELETON):
                for db_sublabel in list(db_label.sublabels.all()):
                    svg = svg.replace(
                        f'data-label-name="{db_sublabel.name}"',
                        f'data-label-id="{db_sublabel.id}"'
                    )
                db_skeleton = models.Skeleton.objects.create(root=db_label, svg=svg)
                logger.info(
                    f'label:update Skeleton id:{db_skeleton.id} for label_id:{db_label.id}'
                )

        if validated_data.get('deleted'):
            assert validated_data['id'] # must be checked in the validate()
            db_label.delete()
            return None

        if not validated_data.get('color', None):
            other_label_colors = [
                label.color for label in
                parent_instance.label_set.exclude(id=db_label.id).order_by('id')
            ]
            db_label.color = get_label_color(db_label.name, other_label_colors)
        else:
            db_label.color = validated_data.get('color', db_label.color)

        try:
            db_label.save()
        except models.InvalidLabel as exc:
            raise exceptions.ValidationError(str(exc)) from exc

        for attr in attributes:
            attr_id = attr.get('id', None)
            if attr_id is not None:
                try:
                    db_attr = models.AttributeSpec.objects.get(id=attr_id, label=db_label)
                except models.AttributeSpec.DoesNotExist as ex:
                    raise exceptions.NotFound(
                        f'Attribute with id #{attr_id} does not exist'
                    ) from ex
                created = False
            else:
                (db_attr, created) = models.AttributeSpec.objects.get_or_create(
                    label=db_label, name=attr['name'], defaults=attr
                )
            if created:
                logger.info("New {} attribute for {} label was created"
                    .format(db_attr.name, db_label.name))
            else:
                logger.info("{} attribute for {} label was updated"
                    .format(db_attr.name, db_label.name))

                # FIXME: need to update only "safe" fields
                db_attr.name = attr.get('name', db_attr.name)
                db_attr.default_value = attr.get('default_value', db_attr.default_value)
                db_attr.mutable = attr.get('mutable', db_attr.mutable)
                db_attr.input_type = attr.get('input_type', db_attr.input_type)
                db_attr.values = attr.get('values', db_attr.values)
                db_attr.save()

        return db_label

    @classmethod
    @transaction.atomic
    def create_labels(cls,
        labels: Iterable[dict[str, Any]],
        *,
        parent_instance: Union[models.Project, models.Task],
        parent_label: Optional[models.Label] = None
    ):
        parent_info, logger = cls._get_parent_info(parent_instance)

        label_colors = list()

        for label in labels:
            attributes = label.pop('attributespec_set')

            cls.check_attribute_names_unique(attributes)

            if label.get('id', None):
                del label['id']

            if not label.get('color', None):
                label['color'] = get_label_color(label['name'], label_colors)
            label_colors.append(label['color'])

            sublabels = label.pop('sublabels', [])
            svg = label.pop('svg', '')
            try:
                db_label = models.Label.create(**label, **parent_info, parent=parent_label)
            except models.InvalidLabel as exc:
                raise exceptions.ValidationError(str(exc)) from exc
            logger.info(
                f'label:create Label id:{db_label.id} for spec:{label} '
                f'with sublabels:{sublabels}, parent_label:{parent_label}'
            )

            cls.create_labels(sublabels, parent_instance=parent_instance, parent_label=db_label)

            if db_label.type == str(models.LabelType.SKELETON):
                for db_sublabel in list(db_label.sublabels.all()):
                    svg = svg.replace(
                        f'data-label-name="{db_sublabel.name}"',
                        f'data-label-id="{db_sublabel.id}"'
                    )
                db_skeleton = models.Skeleton.objects.create(root=db_label, svg=svg)
                logger.info(f'label:create Skeleton id:{db_skeleton.id} for label_id:{db_label.id}')

            for attr in attributes:
                if attr.get('id', None):
                    del attr['id']
                models.AttributeSpec.objects.create(label=db_label, **attr)

    @classmethod
    @transaction.atomic
    def update_labels(cls,
        labels: Iterable[dict[str, Any]],
        *,
        parent_instance: Union[models.Project, models.Task],
        parent_label: Optional[models.Label] = None
    ):
        _, logger = cls._get_parent_info(parent_instance)

        for label in labels:
            sublabels = label.pop('sublabels', [])
            svg = label.pop('svg', '')
            db_label = cls.update_label(label, svg, sublabels,
                parent_instance=parent_instance, parent_label=parent_label
            )
            if db_label:
                logger.info(
                    f'label:update Label id:{db_label.id} for spec:{label} '
                    f'with sublabels:{sublabels}, parent_label:{parent_label}'
                )
            else:
                logger.info(
                    f'label:delete label:{label} with '
                    f'sublabels:{sublabels}, parent_label:{parent_label}'
                )

    @classmethod
    def _get_parent_info(cls, parent_instance: Union[models.Project, models.Task]):
        parent_info = {}
        if isinstance(parent_instance, models.Project):
            parent_info['project'] = parent_instance
            logger = slogger.project[parent_instance.id]
        elif isinstance(parent_instance, models.Task):
            parent_info['task'] = parent_instance
            logger = slogger.task[parent_instance.id]
        else:
            raise TypeError(f"Unexpected parent instance type {type(parent_instance).__name__}")

        return parent_info, logger

    def update(self, instance, validated_data):
        if not self._local:
            return super().update(instance, validated_data)

        # Here we reuse the parent entity logic to make sure everything is done
        # like these entities expect. Initial data (unprocessed) is used to
        # avoid introducing premature changes.
        data = copy(self.initial_data)
        data['id'] = instance.id
        data.setdefault('name', instance.name)
        parent_query = { 'labels': [data] }

        if isinstance(instance.project, models.Project):
            parent_serializer = ProjectWriteSerializer(
                instance=instance.project, data=parent_query, partial=True,
            )
        elif isinstance(instance.task, models.Task):
            parent_serializer = TaskWriteSerializer(
                instance=instance.task, data=parent_query, partial=True,
            )

        parent_serializer.is_valid(raise_exception=True)
        parent_serializer.save()

        self.instance = models.Label.objects.get(pk=instance.pk)
        return self.instance

class StorageSerializer(serializers.ModelSerializer):
    cloud_storage_id = serializers.IntegerField(required=False, allow_null=True)

    class Meta:
        model = models.Storage
        fields = ('id', 'location', 'cloud_storage_id')

class JobReadListSerializer(serializers.ListSerializer):
    def to_representation(self, data):
        if (request := self.context.get("request")) and isinstance(data, list) and data:
            # Optimized prefetch only for the current page
            page: list[models.Job] = data

            # Annotate page objects
            # We do it explicitly here and not in the LIST queryset to avoid
            # doing the same DB computations twice - one time for the page retrieval
            # and another one for the COUNT(*) request to get the total count
            page_task_ids = set(j.get_task_id() for j in page)
            visible_tasks_perm = TaskPermission.create_scope_list(request)
            visible_tasks_queryset = models.Task.objects.filter(id__in=page_task_ids)
            visible_tasks = set(
                visible_tasks_perm.filter(visible_tasks_queryset).values_list("id", flat=True)
            )

            # Fetching it here removes 1 extra join for all jobs in the COUNT(*) request,
            # limiting in only for the page
            issue_counts = dict(
                models.Job.objects.with_issue_counts().filter(
                    id__in=set(j.id for j in page)
                ).values_list("id", "issues__count")
            )

            for job in page:
                job.user_can_view_task = job.get_task_id() in visible_tasks
                job.issues__count = issue_counts.get(job.id, 0)

        return super().to_representation(data)

class JobReadSerializer(serializers.ModelSerializer):
    task_id = serializers.ReadOnlyField(source="get_task_id")
    project_id = serializers.ReadOnlyField(source="get_project_id", allow_null=True)
    guide_id = serializers.ReadOnlyField(source="get_guide_id", allow_null=True)
    start_frame = serializers.ReadOnlyField(source="segment.start_frame")
    stop_frame = serializers.ReadOnlyField(source="segment.stop_frame")
    frame_count = serializers.ReadOnlyField(source="segment.frame_count")
    assignee = BasicUserSerializer(allow_null=True, read_only=True)
    dimension = serializers.CharField(max_length=2, source='segment.task.dimension', read_only=True)
    data_chunk_size = serializers.ReadOnlyField(source='segment.task.data.chunk_size')
    organization = serializers.ReadOnlyField(source='organization_id', allow_null=True)
    data_original_chunk_type = serializers.ReadOnlyField(source='segment.task.data.original_chunk_type')
    data_compressed_chunk_type = serializers.ReadOnlyField(source='segment.task.data.compressed_chunk_type')
    mode = serializers.ReadOnlyField(source='segment.task.mode')
    bug_tracker = serializers.CharField(max_length=2000, source='get_bug_tracker',
        allow_null=True, read_only=True)
    labels = LabelsSummarySerializer(source='*')
    issues = IssuesSummarySerializer(source='*')
    target_storage = StorageSerializer(required=False, allow_null=True)
    source_storage = StorageSerializer(required=False, allow_null=True)
    parent_job_id = serializers.ReadOnlyField(allow_null=True)
    consensus_replicas = serializers.IntegerField(read_only=True)

    class Meta:
        model = models.Job
        fields = ('url', 'id', 'task_id', 'project_id', 'assignee', 'guide_id',
            'dimension', 'bug_tracker', 'status', 'stage', 'state', 'mode', 'frame_count',
            'start_frame', 'stop_frame',
            'data_chunk_size', 'data_compressed_chunk_type', 'data_original_chunk_type',
            'created_date', 'updated_date', 'issues', 'labels', 'type', 'organization',
            'target_storage', 'source_storage', 'assignee_updated_date', 'parent_job_id',
            'consensus_replicas'
        )
        read_only_fields = fields
        list_serializer_class = JobReadListSerializer

    def to_representation(self, instance):
        data = super().to_representation(instance)

        if instance.segment.type == models.SegmentType.SPECIFIC_FRAMES:
            data['data_compressed_chunk_type'] = models.DataChoice.IMAGESET

        if instance.type == models.JobType.ANNOTATION:
            data['consensus_replicas'] = instance.segment.task.consensus_replicas
        else:
            data['consensus_replicas'] = 0

        if request := self.context.get('request'):
            can_view_task = getattr(instance, "user_can_view_task", None)
            if can_view_task is None:
                perm = TaskPermission.create_scope_view(request, instance.segment.task)
                result = perm.check_access()
                can_view_task = result.allow

            if can_view_task:
                if task_source_storage := instance.get_source_storage():
                    data['source_storage'] = StorageSerializer(task_source_storage).data
                if task_target_storage := instance.get_target_storage():
                    data['target_storage'] = StorageSerializer(task_target_storage).data

        return data

class JobWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    assignee = serializers.IntegerField(allow_null=True, required=False)

    # NOTE: Field sets can be expressed using serializer inheritance, but it is
    # harder to use then: we need to make a manual switch in get_serializer_class()
    # and create an extra serializer type in the API schema.
    # Need to investigate how it can be simplified. It can also be done just internally,
    # (e.g. just on the validation side), but it will complicate the implementation.
    type = serializers.ChoiceField(choices=models.JobType.choices())

    task_id = serializers.IntegerField()
    frame_selection_method = serializers.ChoiceField(
        choices=models.JobFrameSelectionMethod.choices(), required=False
    )
    frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0),
        required=False,
        allow_empty=False,
        help_text=textwrap.dedent("""\
            The list of frame ids. Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.MANUAL))
    )
    frame_count = serializers.IntegerField(
        min_value=1,
        required=False,
        help_text=textwrap.dedent("""\
            The number of frames included in the GT job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_UNIFORM))
    )
    frame_share = serializers.FloatField(
        required=False,
        validators=[field_validation.validate_share],
        help_text=textwrap.dedent("""\
            The share of frames included in the GT job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_UNIFORM))
    )
    frames_per_job_count = serializers.IntegerField(
        min_value=1,
        required=False,
        help_text=textwrap.dedent("""\
            The number of frames included in the GT job from each annotation job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_PER_JOB))
    )
    frames_per_job_share = serializers.FloatField(
        required=False,
        validators=[field_validation.validate_share],
        help_text=textwrap.dedent("""\
            The share of frames included in the GT job from each annotation job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_PER_JOB))
    )
    random_seed = serializers.IntegerField(
        min_value=0,
        required=False,
        help_text=textwrap.dedent("""\
            The seed value for the random number generator.
            The same value will produce the same frame sets.
            Applicable only to random frame selection methods.
            By default, a random value is used.
        """)
    )
    seed = serializers.IntegerField(
        min_value=0, required=False, help_text="Deprecated. Use random_seed instead."
    )

    class Meta:
        model = models.Job
        random_selection_params = (
            'frame_count', 'frame_share', 'frames_per_job_count', 'frames_per_job_share',
            'random_seed', 'seed'
        )
        manual_selection_params = ('frames',)
        write_once_fields = ('type', 'task_id', 'frame_selection_method',) \
            + random_selection_params + manual_selection_params
        fields = ('assignee', 'stage', 'state', ) + write_once_fields

    def to_representation(self, instance):
        serializer = JobReadSerializer(instance, context=self.context)
        return serializer.data

    def validate(self, attrs):
        frame_selection_method = attrs.get('frame_selection_method')
        if frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            field_validation.require_one_of_fields(attrs, ['frame_count', 'frame_share'])

            # 'seed' is a backward compatibility alias
            if attrs.get('seed') is not None or attrs.get('random_seed') is not None:
                field_validation.require_one_of_fields(attrs, ['seed', 'random_seed'])

        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_PER_JOB:
            field_validation.require_one_of_fields(
                attrs, ['frames_per_job_count', 'frames_per_job_share']
            )
        elif frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            field_validation.require_field(attrs, "frames")

        if (
            'frames' in attrs and
            frame_selection_method != models.JobFrameSelectionMethod.MANUAL
        ):
            raise serializers.ValidationError(
                '"frames" can only be used when "frame_selection_method" is "{}"'.format(
                    models.JobFrameSelectionMethod.MANUAL
                )
            )

        return super().validate(attrs)

    @transaction.atomic
    def create(self, validated_data):
        if validated_data["type"] != models.JobType.GROUND_TRUTH:
            raise serializers.ValidationError(f"Unexpected job type '{validated_data['type']}'")

        task_id = validated_data.pop('task_id')
        task = models.Task.objects.select_for_update().get(pk=task_id)

        if not task.data:
            raise serializers.ValidationError(
                "This task has no data attached yet. Please set up task data and try again"
            )
        if task.dimension != models.DimensionType.DIM_2D:
            raise serializers.ValidationError(
                "Ground Truth jobs can only be added in 2d tasks"
            )

        if task.data.validation_mode in (models.ValidationMode.GT_POOL, models.ValidationMode.GT):
            raise serializers.ValidationError(
                f'Task with validation mode "{task.data.validation_mode}" '
                'cannot have more than 1 GT job'
            )

        task_size = task.data.size
        valid_frame_ids = task.data.get_valid_frame_indices()

        # TODO: refactor
        frame_selection_method = validated_data.pop("frame_selection_method")
        if frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            if frame_count := validated_data.pop("frame_count", None):
                if task_size < frame_count:
                    raise serializers.ValidationError(
                        f"The number of frames requested ({frame_count}) "
                        f"must be not be greater than the number of the task frames ({task_size})"
                    )
            elif frame_share := validated_data.pop("frame_share", None):
                frame_count = max(1, int(frame_share * task_size))
            else:
                raise serializers.ValidationError(
                    "The number of validation frames is not specified"
                )

            seed = validated_data.pop("random_seed", None)
            deprecated_seed = validated_data.pop("seed", None)

            # The RNG backend must not change to yield reproducible results,
            # so here we specify it explicitly
            rng = random.Generator(random.MT19937(seed=seed))

            if deprecated_seed is not None and frame_count < task_size:
                # Reproduce the old (a little bit incorrect) behavior that existed before
                # https://github.com/cvat-ai/cvat/pull/7126
                # to make the old seed-based sequences reproducible
                rng = random.Generator(random.MT19937(seed=deprecated_seed))
                valid_frame_ids = [v for v in valid_frame_ids if v != task.data.stop_frame]

            frames = rng.choice(
                list(valid_frame_ids), size=frame_count, shuffle=False, replace=False
            ).tolist()
        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_PER_JOB:
            if frame_count := validated_data.pop("frames_per_job_count", None):
                if task_size < frame_count:
                    raise serializers.ValidationError(
                        f"The number of frames requested ({frame_count}) "
                        f"must be not be greater than the segment size ({task.segment_size})"
                    )
            elif frame_share := validated_data.pop("frames_per_job_share", None):
                frame_count = min(max(1, int(frame_share * task.segment_size)), task_size)
            else:
                raise serializers.ValidationError(
                    "The number of validation frames is not specified"
                )

            task_frame_provider = TaskFrameProvider(task)
            seed = validated_data.pop("random_seed", None)

            # The RNG backend must not change to yield reproducible results,
            # so here we specify it explicitly
            rng = random.Generator(random.MT19937(seed=seed))

            frames: list[int] = []
            overlap = task.overlap
            for segment in task.segment_set.all():
                segment_frames = set(map(task_frame_provider.get_rel_frame_number, segment.frame_set))
                selected_frames = segment_frames.intersection(frames)
                selected_count = len(selected_frames)

                missing_count = min(len(segment_frames), frame_count) - selected_count
                if missing_count <= 0:
                    continue

                selectable_segment_frames = set(
                    sorted(segment_frames)[overlap * (segment.start_frame != 0) : ]
                ).difference(selected_frames)

                frames.extend(rng.choice(
                    tuple(selectable_segment_frames), size=missing_count, replace=False
                ).tolist())

            frames = list(map(task_frame_provider.get_abs_frame_number, frames))
        elif frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            frames = validated_data.pop("frames")

            unique_frames = set(frames)
            if len(unique_frames) != len(frames):
                raise serializers.ValidationError(f"Frames must not repeat")

            invalid_ids = unique_frames.difference(range(task_size))
            if invalid_ids:
                raise serializers.ValidationError(
                    "The following frames do not exist in the task: {}".format(
                        format_list(tuple(map(str, sorted(invalid_ids))))
                    )
                )

            task_frame_provider = TaskFrameProvider(task)
            frames = list(map(task_frame_provider.get_abs_frame_number, frames))
        else:
            raise serializers.ValidationError(
                f"Unexpected frame selection method '{frame_selection_method}'"
            )

        # Save the new job
        segment = models.Segment.objects.create(
            start_frame=0,
            stop_frame=task.data.size - 1,
            frames=frames,
            task=task,
            type=models.SegmentType.SPECIFIC_FRAMES,
        )

        validated_data['segment'] = segment
        validated_data["assignee_id"] = validated_data.pop("assignee", None)

        try:
            job = super().create(validated_data)
        except models.TaskGroundTruthJobsLimitError as ex:
            raise serializers.ValidationError(ex.message) from ex

        if validated_data.get("assignee_id"):
            job.assignee_updated_date = job.updated_date
            job.save(update_fields=["assignee_updated_date"])

        job.make_dirs()

        task.data.update_validation_layout(
            models.ValidationLayout(mode=models.ValidationMode.GT, frames=frames)
        )

        return job

    def update(self, instance, validated_data):
        stage = validated_data.get('stage', instance.stage)
        state = validated_data.get('state', models.StateChoice.NEW if stage != instance.stage else instance.state)

        if 'stage' in validated_data or 'state' in validated_data:
            if stage == models.StageChoice.ANNOTATION:
                validated_data['status'] = models.StatusChoice.ANNOTATION
            elif stage == models.StageChoice.ACCEPTANCE and state == models.StateChoice.COMPLETED:
                validated_data['status'] = models.StatusChoice.COMPLETED
            else:
                validated_data['status'] = models.StatusChoice.VALIDATION

        if state != instance.state:
            validated_data['state'] = state

        if "assignee" in validated_data and (
            (assignee_id := validated_data.pop("assignee")) != instance.assignee_id
        ):
            validated_data["assignee_id"] = assignee_id
            validated_data["assignee_updated_date"] = timezone.now()

        instance = super().update(instance, validated_data)
        return instance

class SimpleJobSerializer(serializers.ModelSerializer):
    assignee = BasicUserSerializer(allow_null=True)

    class Meta:
        model = models.Job
        fields = ('url', 'id', 'assignee', 'status', 'stage', 'state', 'type')
        read_only_fields = fields

class JobValidationLayoutWriteSerializer(serializers.Serializer):
    frame_selection_method = serializers.ChoiceField(
        choices=models.JobFrameSelectionMethod.choices(),
        required=True,
        help_text=textwrap.dedent("""\
            The method to use for frame selection of new real frames for honeypots in the job
        """)
    )
    honeypot_real_frames = serializers.ListSerializer(
        child=serializers.IntegerField(min_value=0),
        required=False,
        allow_empty=False,
        help_text=textwrap.dedent("""\
            The list of frame ids. Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.MANUAL))
    )

    def __init__(
        self, *args, bulk_context: _TaskValidationLayoutBulkUpdateContext | None = None, **kwargs
    ):
        super().__init__(*args, **kwargs)

        self._bulk_context = bulk_context

    def validate(self, attrs):
        frame_selection_method = attrs["frame_selection_method"]
        if frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            field_validation.require_field(attrs, "honeypot_real_frames")
        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            pass
        else:
            assert False

        if (
            'honeypot_real_frames' in attrs and
            frame_selection_method != models.JobFrameSelectionMethod.MANUAL
        ):
            raise serializers.ValidationError(
                '"honeypot_real_frames" can only be used when '
                f'"frame_selection_method" is "{models.JobFrameSelectionMethod.MANUAL}"'
            )

        return super().validate(attrs)

    @transaction.atomic
    def update(self, instance: models.Job, validated_data: dict[str, Any]) -> models.Job:
        from cvat.apps.engine.cache import (
            Callback,
            MediaCache,
            enqueue_create_chunk_job,
            wait_for_rq_job,
        )
        from cvat.apps.engine.frame_provider import JobFrameProvider

        db_job = instance
        db_segment = db_job.segment
        db_task = db_segment.task
        db_data = db_task.data

        if not (
            hasattr(db_job.segment.task.data, 'validation_layout') and
            db_job.segment.task.data.validation_layout.mode == models.ValidationMode.GT_POOL
        ):
            raise serializers.ValidationError(
                "Honeypots can only be modified if the task "
                f"validation mode is '{models.ValidationMode.GT_POOL}'"
            )

        if db_job.type == models.JobType.GROUND_TRUTH:
            raise serializers.ValidationError(
                f"Honeypots cannot exist in {models.JobType.GROUND_TRUTH} jobs"
            )

        assert not hasattr(db_data, 'video')

        frame_step = db_data.get_frame_step()

        def _to_rel_frame(abs_frame: int) -> int:
            return (abs_frame - db_data.start_frame) // frame_step

        def _to_abs_frame(rel_frame: int) -> int:
            return rel_frame * frame_step + db_data.start_frame

        bulk_context = self._bulk_context
        if bulk_context:
            db_frames = bulk_context.all_db_frames
            task_honeypot_frames = set(bulk_context.honeypot_frames)
            task_all_validation_frames = set(bulk_context.all_validation_frames)
            task_active_validation_frames = set(bulk_context.active_validation_frames)
        else:
            db_frames: dict[int, models.Image] = {
                _to_rel_frame(frame.frame): frame
                for frame in db_data.images.all()
            }
            task_honeypot_frames = set(
                _to_rel_frame(frame_id)
                for frame_id, frame in db_frames.items()
                if frame.is_placeholder
            )
            task_all_validation_frames = set(db_data.validation_layout.frames)
            task_active_validation_frames = set(db_data.validation_layout.active_frames)

        segment_frame_set = set(map(_to_rel_frame, db_segment.frame_set))
        segment_honeypots = sorted(segment_frame_set & task_honeypot_frames)
        segment_honeypots_count = len(segment_honeypots)

        frame_selection_method = validated_data['frame_selection_method']
        if frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            requested_frames: list[int] = validated_data['honeypot_real_frames']
            requested_inactive_frames: set[int] = set()
            requested_normal_frames: set[int] = set()
            for requested_validation_frame in requested_frames:
                if requested_validation_frame not in task_all_validation_frames:
                    requested_normal_frames.add(requested_validation_frame)
                    continue

                if requested_validation_frame not in task_active_validation_frames:
                    requested_inactive_frames.add(requested_validation_frame)
                    continue

            if requested_normal_frames:
                raise serializers.ValidationError(
                    "Could not update honeypot frames: "
                    "frames {} are not from the validation pool".format(
                        format_list(tuple(map(str, sorted(requested_normal_frames))))
                    )
                )

            if requested_inactive_frames:
                raise serializers.ValidationError(
                    "Could not update honeypot frames: "
                    "frames {} are disabled. Restore them in the validation pool first".format(
                        format_list(tuple(map(str, sorted(requested_inactive_frames))))
                    )
                )

            if len(requested_frames) != segment_honeypots_count:
                raise serializers.ValidationError(
                    "Could not update honeypot frames: "
                    "the number of honeypots must remain the same. "
                    "Requested {}, current {}".format(
                        len(requested_frames), segment_honeypots_count
                    )
                )

        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            if len(task_active_validation_frames) < segment_honeypots_count:
                raise serializers.ValidationError(
                    "Can't select validation frames: "
                    "the remaining number of validation frames ({}) "
                    "is less than the number of honeypots in a job ({}). "
                    "Try to restore some validation frames".format(
                        len(task_active_validation_frames), segment_honeypots_count
                    )
                )

            if bulk_context:
                frame_selector = bulk_context.honeypot_frame_selector
            else:
                active_validation_frame_counts = {
                    validation_frame: 0 for validation_frame in task_active_validation_frames
                }
                for task_honeypot_frame in task_honeypot_frames:
                    real_frame = _to_rel_frame(db_frames[task_honeypot_frame].real_frame)
                    if real_frame in task_active_validation_frames:
                        active_validation_frame_counts[real_frame] += 1

                frame_selector = HoneypotFrameSelector(active_validation_frame_counts)

            requested_frames = frame_selector.select_next_frames(segment_honeypots_count)
            requested_frames = list(map(_to_abs_frame, requested_frames))
        else:
            assert False

        # Replace validation frames in the job
        updated_honeypots = {}
        for frame, requested_validation_frame in zip(segment_honeypots, requested_frames):
            db_requested_frame = db_frames[requested_validation_frame]
            db_segment_frame = db_frames[frame]
            assert db_segment_frame.is_placeholder

            if db_segment_frame.real_frame == db_requested_frame.frame:
                continue

            # Change image in the current segment honeypot frame
            db_segment_frame.real_frame = db_requested_frame.frame

            db_segment_frame.path = db_requested_frame.path
            db_segment_frame.width = db_requested_frame.width
            db_segment_frame.height = db_requested_frame.height

            updated_honeypots[frame] = db_segment_frame

        if updated_honeypots:
            if bulk_context:
                bulk_context.updated_honeypots.update(updated_honeypots)
            else:
                # Update image infos
                models.Image.objects.bulk_update(
                    updated_honeypots.values(), fields=['path', 'width', 'height', 'real_frame']
                )

                models.RelatedFile.images.through.objects.filter(
                    image_id__in=updated_honeypots
                ).delete()

                for updated_honeypot in updated_honeypots.values():
                    validation_frame = db_frames[_to_rel_frame(updated_honeypot.real_frame)]
                    updated_honeypot.related_files.set(validation_frame.related_files.all())

                # Remove annotations on changed validation frames
                self._clear_annotations_on_frames(db_segment, updated_honeypots)

                # Update manifest
                manifest_path = db_data.get_manifest_path()
                if os.path.isfile(manifest_path):
                    manifest = ImageManifestManager(manifest_path)
                    manifest.reorder([db_frame.path for db_frame in db_frames.values()])

            # Update chunks
            job_frame_provider = JobFrameProvider(db_job)
            updated_segment_chunk_ids = set(
                job_frame_provider.get_chunk_number(updated_segment_frame_id)
                for updated_segment_frame_id in updated_honeypots
            )
            segment_frames = sorted(segment_frame_set)
            segment_frame_map = dict(zip(segment_honeypots, requested_frames))

            chunks_to_be_removed = []
            queue = django_rq.get_queue(settings.CVAT_QUEUES.CHUNKS.value)
            for chunk_id in sorted(updated_segment_chunk_ids):
                chunk_frames = segment_frames[
                    chunk_id * db_data.chunk_size :
                    (chunk_id + 1) * db_data.chunk_size
                ]

                for quality in FrameQuality.__members__.values():
                    if db_data.storage_method == models.StorageMethodChoice.FILE_SYSTEM:
                        rq_id = f"segment_{db_segment.id}_write_chunk_{chunk_id}_{quality}"
                        rq_job = enqueue_create_chunk_job(
                            queue=queue,
                            rq_job_id=rq_id,
                            create_callback=Callback(
                                callable=self._write_updated_static_chunk,
                                args=[
                                    db_segment.id,
                                    chunk_id,
                                    chunk_frames,
                                    quality,
                                    {
                                        chunk_frame: db_frames[chunk_frame].path
                                        for chunk_frame in chunk_frames
                                    },
                                    segment_frame_map,
                                ],
                            ),
                        )
                        wait_for_rq_job(rq_job)

                    chunks_to_be_removed.append(
                        {'db_segment': db_segment, 'chunk_number': chunk_id, 'quality': quality}
                    )

            context_image_chunks_to_be_removed = [
                {"db_data": db_data, "frame_number": f} for f in updated_honeypots
            ]

            if bulk_context:
                bulk_context.chunks_to_be_removed.extend(chunks_to_be_removed)
                bulk_context.context_image_chunks_to_be_removed.extend(
                    context_image_chunks_to_be_removed
                )
                bulk_context.segments_with_updated_chunks.append(db_segment.id)
            else:
                media_cache = MediaCache()
                media_cache.remove_segments_chunks(chunks_to_be_removed)
                media_cache.remove_context_images_chunks(context_image_chunks_to_be_removed)

                db_segment.chunks_updated_date = timezone.now()
                db_segment.save(update_fields=['chunks_updated_date'])

        if updated_honeypots or (
            # even if the randomly selected frames were the same as before, we should still
            # consider it an update to the validation frames and restore them, if they were deleted
            frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM
        ):
            # deleted frames that were updated in the job should be restored, as they are new now
            if set(db_data.deleted_frames).intersection(updated_honeypots):
                db_data.deleted_frames = sorted(
                    set(db_data.deleted_frames).difference(updated_honeypots)
                )
                db_data.save(update_fields=['deleted_frames'])

            new_updated_date = timezone.now()
            db_job.updated_date = new_updated_date

            if bulk_context:
                bulk_context.updated_segments.append(db_segment.id)
            else:
                db_segment.job_set.update(updated_date=new_updated_date)

                db_task.touch()
                if db_task.project:
                    db_task.project.touch()

        return instance

    def _clear_annotations_on_frames(self, segment: models.Segment, frames: Sequence[int]):
        models.clear_annotations_on_frames_in_honeypot_task(segment.task, frames=frames)

    @staticmethod
    def _write_updated_static_chunk(
        db_segment_id: int,
        chunk_id: int,
        chunk_frames: list[int],
        quality: FrameQuality,
        frame_path_map: dict[int, str],
        segment_frame_map: dict[int,int],
    ):
        from cvat.apps.engine.frame_provider import prepare_chunk

        db_segment = models.Segment.objects.select_related("task").get(pk=db_segment_id)
        initial_chunks_updated_date = db_segment.chunks_updated_date
        db_task = db_segment.task
        task_frame_provider = TaskFrameProvider(db_task)
        db_data = db_task.data

        def _iterate_chunk_frames():
            for chunk_frame in chunk_frames:
                db_frame_path = frame_path_map[chunk_frame]
                chunk_real_frame = segment_frame_map.get(chunk_frame, chunk_frame)
                yield (
                    task_frame_provider.get_frame(
                        chunk_real_frame, quality=quality
                    ).data,
                    os.path.basename(db_frame_path),
                    chunk_frame,
                )

        with closing(_iterate_chunk_frames()) as frame_iter:
            chunk, _ = prepare_chunk(
                frame_iter, quality=quality, db_task=db_task, dump_unchanged=True,
            )

            get_chunk_path = {
                FrameQuality.COMPRESSED: db_data.get_compressed_segment_chunk_path,
                FrameQuality.ORIGINAL: db_data.get_original_segment_chunk_path,
            }[quality]

            db_segment.refresh_from_db(fields=["chunks_updated_date"])
            if db_segment.chunks_updated_date > initial_chunks_updated_date:
                raise CvatChunkTimestampMismatchError(
                    "Attempting to write an out of date static chunk, "
                    f"segment.chunks_updated_date: {db_segment.chunks_updated_date}, "
                    f"expected_ts: {initial_chunks_updated_date}"
            )
            with open(get_chunk_path(chunk_id, db_segment_id), 'wb') as f:
                f.write(chunk.getvalue())

class JobValidationLayoutReadSerializer(serializers.Serializer):
    honeypot_count = serializers.IntegerField(min_value=0, required=False)
    honeypot_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids for honeypots in the job
        """)
    )
    honeypot_real_frames = serializers.ListSerializer(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of real (validation) frame ids for honeypots in the job
        """)
    )

    def to_representation(self, instance: models.Job):
        validation_layout = getattr(instance.segment.task.data, 'validation_layout', None)
        if not validation_layout:
            return {}

        data = {}

        if validation_layout.mode == models.ValidationMode.GT_POOL:
            db_segment = instance.segment
            segment_frame_set = db_segment.frame_set

            db_data = db_segment.task.data
            frame_step = db_data.get_frame_step()

            def _to_rel_frame(abs_frame: int) -> int:
                return (abs_frame - db_data.start_frame) // frame_step

            segment_honeypot_frames = []
            for frame in db_segment.task.data.images.all():
                if not frame.is_placeholder:
                    continue

                if not frame.frame in segment_frame_set:
                    continue

                segment_honeypot_frames.append(
                    (_to_rel_frame(frame.frame), _to_rel_frame(frame.real_frame))
                )

            segment_honeypot_frames.sort(key=lambda v: v[0])

            data = {
                'honeypot_count': len(segment_honeypot_frames),
                'honeypot_frames': [v[0] for v in segment_honeypot_frames],
                'honeypot_real_frames': [v[1] for v in segment_honeypot_frames],
            }

        return super().to_representation(data)

class _TaskValidationLayoutBulkUpdateContext:
    def __init__(
        self,
        *,
        all_db_frames: dict[int, models.Image],
        honeypot_frames: list[int],
        all_validation_frames: list[int],
        active_validation_frames: list[int],
        honeypot_frame_selector: HoneypotFrameSelector | None = None
    ):
        self.updated_honeypots: dict[int, models.Image] = {}
        self.updated_segments: list[int] = []
        self.chunks_to_be_removed: list[dict[str, Any]] = []
        self.context_image_chunks_to_be_removed: list[dict[str, Any]] = []
        self.segments_with_updated_chunks: list[int] = []

        self.all_db_frames = all_db_frames
        self.honeypot_frames = honeypot_frames
        self.all_validation_frames = all_validation_frames
        self.active_validation_frames = active_validation_frames
        self.honeypot_frame_selector = honeypot_frame_selector

class TaskValidationLayoutWriteSerializer(serializers.Serializer):
    disabled_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids to be excluded from validation
        """)
    )
    frame_selection_method = serializers.ChoiceField(
        choices=models.JobFrameSelectionMethod.choices(), required=False,
        help_text=textwrap.dedent("""\
            The method to use for frame selection of new real frames for honeypots in the task
        """)
    )
    honeypot_real_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids. Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.MANUAL))
    )

    def validate(self, attrs):
        frame_selection_method = attrs.get("frame_selection_method")
        if frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            field_validation.require_field(attrs, "honeypot_real_frames")
        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            pass

        if (
            'honeypot_real_frames' in attrs and
            frame_selection_method != models.JobFrameSelectionMethod.MANUAL
        ):
            raise serializers.ValidationError(
                '"honeypot_real_frames" can only be used when '
                f'"frame_selection_method" is "{models.JobFrameSelectionMethod.MANUAL}"'
            )

        return super().validate(attrs)

    @transaction.atomic
    def update(self, instance: models.Task, validated_data: dict[str, Any]) -> models.Task:
        # FIXME: this operation is not atomic and it is not protected from race conditions
        # (basically, as many others). Currently, it's up to the user to ensure no parallel
        # calls happen. It also affects any image access, including exports with images, backups,
        # automatic annotation, chunk downloading, etc.

        db_validation_layout: models.ValidationLayout | None = (
            getattr(instance.data, 'validation_layout', None)
        )
        if not db_validation_layout:
            raise serializers.ValidationError("Validation is not configured in the task")

        if 'disabled_frames' in validated_data:
            requested_disabled_frames = validated_data['disabled_frames']
            unknown_requested_disabled_frames = (
                set(requested_disabled_frames).difference(db_validation_layout.frames)
            )
            if unknown_requested_disabled_frames:
                raise serializers.ValidationError(
                    "Unknown frames requested for exclusion from the validation set: {}".format(
                        format_list(tuple(map(str, sorted(unknown_requested_disabled_frames))))
                    )
                )

            gt_job_meta_serializer = JobDataMetaWriteSerializer(instance.gt_job, {
                "deleted_frames": requested_disabled_frames
            })
            gt_job_meta_serializer.is_valid(raise_exception=True)
            gt_job_meta_serializer.save()

            db_validation_layout.refresh_from_db()
            instance.data.refresh_from_db()

        frame_selection_method = validated_data.get('frame_selection_method')
        if frame_selection_method and not (
            db_validation_layout and
            instance.data.validation_layout.mode == models.ValidationMode.GT_POOL
        ):
            raise serializers.ValidationError(
                "Honeypots can only be modified if the task "
                f"validation mode is '{models.ValidationMode.GT_POOL}'"
            )

        if not frame_selection_method:
            return instance

        assert not hasattr(instance.data, 'video')

        # Populate the prefetch cache for required objects
        prefetch_related_objects([instance],
            Prefetch('data__images', queryset=models.Image.objects.order_by('frame')),
            'segment_set',
            'segment_set__job_set',
        )

        frame_provider = TaskFrameProvider(instance)
        db_frames = {
            frame_provider.get_rel_frame_number(db_image.frame): db_image
            for db_image in instance.data.images.all()
        }
        honeypot_frames = sorted(f for f, v in db_frames.items() if v.is_placeholder)
        all_validation_frames = db_validation_layout.frames
        active_validation_frames = db_validation_layout.active_frames

        bulk_context = _TaskValidationLayoutBulkUpdateContext(
            all_db_frames=db_frames,
            honeypot_frames=honeypot_frames,
            all_validation_frames=all_validation_frames,
            active_validation_frames=active_validation_frames,
        )

        if frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
            requested_honeypot_real_frames = validated_data['honeypot_real_frames']
            task_honeypot_frames_count = len(honeypot_frames)
            if task_honeypot_frames_count != len(requested_honeypot_real_frames):
                raise serializers.ValidationError(
                    "Invalid size of 'honeypot_real_frames' array, "
                    f"expected {task_honeypot_frames_count}"
                )
        elif frame_selection_method == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            # Reset distribution for active validation frames
            active_validation_frame_counts = { f: 0 for f in active_validation_frames }
            frame_selector = HoneypotFrameSelector(active_validation_frame_counts)
            bulk_context.honeypot_frame_selector = frame_selector

        # Could be done using Django ORM, but using order_by() and filter()
        # would result in an extra DB request
        db_jobs = sorted(
            (
                db_job
                for db_segment in instance.segment_set.all()
                for db_job in db_segment.job_set.all()
                if db_job.type == models.JobType.ANNOTATION
            ),
            key=lambda j: j.segment.start_frame
        )
        for db_job in db_jobs:
            job_serializer_params = {
                'frame_selection_method': frame_selection_method
            }

            if frame_selection_method == models.JobFrameSelectionMethod.MANUAL:
                segment_frame_set = db_job.segment.frame_set
                job_serializer_params['honeypot_real_frames'] = [
                    requested_frame
                    for rel_frame, requested_frame in zip(
                        honeypot_frames, requested_honeypot_real_frames
                    )
                    if frame_provider.get_abs_frame_number(rel_frame) in segment_frame_set
                ]

            job_validation_layout_serializer = JobValidationLayoutWriteSerializer(
                db_job, job_serializer_params, bulk_context=bulk_context
            )
            job_validation_layout_serializer.is_valid(raise_exception=True)
            job_validation_layout_serializer.save()

        self._perform_bulk_updates(instance, bulk_context=bulk_context)

        return instance

    def _perform_bulk_updates(
        self,
        db_task: models.Task,
        *,
        bulk_context: _TaskValidationLayoutBulkUpdateContext,
    ):
        updated_segments = bulk_context.updated_segments
        if not updated_segments:
            return

        self._update_frames_in_bulk(db_task, bulk_context=bulk_context)

        # Import it here to avoid circular import
        from cvat.apps.engine.cache import MediaCache
        media_cache = MediaCache()
        media_cache.remove_segments_chunks(bulk_context.chunks_to_be_removed)
        media_cache.remove_context_images_chunks(bulk_context.context_image_chunks_to_be_removed)

        # Update segments
        updated_date = timezone.now()
        for updated_segments_batch in take_by(updated_segments, chunk_size=1000):
            models.Job.objects.filter(
                segment_id__in=updated_segments_batch
            ).update(updated_date=updated_date)

        for updated_segment_chunks_batch in take_by(
            bulk_context.segments_with_updated_chunks, chunk_size=1000
        ):
            models.Segment.objects.filter(
                id__in=updated_segment_chunks_batch
            ).update(chunks_updated_date=updated_date)

        # Update parent objects
        db_task.touch()
        if db_task.project:
            db_task.project.touch()

    def _update_frames_in_bulk(
        self,
        db_task: models.Task,
        *,
        bulk_context: _TaskValidationLayoutBulkUpdateContext,
    ):
        self._clear_annotations_on_frames(db_task, bulk_context.updated_honeypots)

        # The django generated bulk_update() query is too slow, so we use bulk_create() instead
        # NOTE: Silk doesn't show these queries in the list of queries
        # for some reason, but they can be seen in the profile
        bulk_create(
            models.Image,
            list(bulk_context.updated_honeypots.values()),
            update_conflicts=True,
            update_fields=['path', 'width', 'height', 'real_frame'],
            unique_fields=[
                # required for Postgres
                # https://docs.djangoproject.com/en/4.2/ref/models/querysets/#bulk-create
                'id'
            ],
        )

        # Update related images in 2 steps: remove all m2m for honeypots, then add (copy) new ones
        # 1. remove
        for updated_honeypots_batch in take_by(
            bulk_context.updated_honeypots.values(), chunk_size=1000
        ):
            models.RelatedFile.images.through.objects.filter(
                image_id__in=(db_honeypot.id for db_honeypot in updated_honeypots_batch)
            ).delete()

        # 2. batched add (copy): collect all the new records and insert
        frame_provider = TaskFrameProvider(db_task)
        honeypots_by_validation_frame = grouped(
            bulk_context.updated_honeypots,
            key=lambda honeypot_frame: frame_provider.get_rel_frame_number(
                bulk_context.updated_honeypots[honeypot_frame].real_frame
            )
        ) # validation frame -> [honeypot_frame, ...]

        new_m2m_objects = []
        m2m_objects_by_validation_image_id = grouped(
            models.RelatedFile.images.through.objects
            .filter(image_id__in=(
                bulk_context.all_db_frames[validation_frame].id
                for validation_frame in honeypots_by_validation_frame
            ))
            .all(),
            key=lambda m2m_obj: m2m_obj.image_id
        )
        for validation_frame, validation_frame_honeypots in honeypots_by_validation_frame.items():
            validation_frame_m2m_objects = m2m_objects_by_validation_image_id.get(
                bulk_context.all_db_frames[validation_frame].id
            )
            if not validation_frame_m2m_objects:
                continue

            # Copy validation frame m2m objects to corresponding honeypots
            for honeypot_frame in validation_frame_honeypots:
                new_m2m_objects.extend(
                    models.RelatedFile.images.through(
                        image_id=bulk_context.all_db_frames[honeypot_frame].id,
                        relatedfile_id=m2m_obj.relatedfile_id
                    )
                    for m2m_obj in validation_frame_m2m_objects
                )

        bulk_create(models.RelatedFile.images.through, new_m2m_objects)

        # Update manifest if present
        manifest_path = db_task.data.get_manifest_path()
        if os.path.isfile(manifest_path):
            manifest = ImageManifestManager(manifest_path)
            manifest.reorder([db_frame.path for db_frame in bulk_context.all_db_frames.values()])

    def _clear_annotations_on_frames(self, db_task: models.Task, frames: Sequence[int]):
        models.clear_annotations_on_frames_in_honeypot_task(db_task, frames=frames)

class TaskValidationLayoutReadSerializer(serializers.ModelSerializer):
    validation_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), source='frames', required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids to be used for validation
        """)
    )
    disabled_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids excluded from validation
        """)
    )
    honeypot_count = serializers.IntegerField(min_value=0, required=False)
    honeypot_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of frame ids for all honeypots in the task
        """)
    )
    honeypot_real_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), required=False,
        help_text=textwrap.dedent("""\
            The list of real (validation) frame ids for all honeypots in the task
        """)
    )

    class Meta:
        model = models.ValidationLayout
        fields = (
            'mode',
            'frames_per_job_count',
            'validation_frames',
            'disabled_frames',
            'honeypot_count',
            'honeypot_frames',
            'honeypot_real_frames',
        )
        read_only_fields = fields
        extra_kwargs = {
            'mode': { 'allow_null': True },
        }

    def to_representation(self, instance: models.ValidationLayout):
        if instance.mode == models.ValidationMode.GT_POOL:
            db_data: models.Data = instance.task_data
            frame_step = db_data.get_frame_step()

            def _to_rel_frame(abs_frame: int) -> int:
                return (abs_frame - db_data.start_frame) // frame_step

            placeholder_queryset = models.Image.objects.filter(
                data_id=instance.task_data_id, is_placeholder=True
            )
            honeypot_count = placeholder_queryset.count()

            instance.honeypot_count = honeypot_count

            # TODO: make this information optional, if there are use cases with too big responses
            instance.honeypot_frames = []
            instance.honeypot_real_frames = []
            for frame, real_frame in (
                placeholder_queryset
                .order_by('frame')
                .values_list('frame', 'real_frame')
                .iterator(chunk_size=10000)
            ):
                instance.honeypot_frames.append(_to_rel_frame(frame))
                instance.honeypot_real_frames.append(_to_rel_frame(real_frame))

        return super().to_representation(instance)

class SegmentSerializer(serializers.ModelSerializer):
    jobs = SimpleJobSerializer(many=True, source='job_set')
    frames = serializers.ListSerializer(child=serializers.IntegerField(), allow_empty=True)

    class Meta:
        model = models.Segment
        fields = ('start_frame', 'stop_frame', 'jobs', 'type', 'frames')
        read_only_fields = fields

class ClientFileSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.ClientFile
        fields = ('file', )

    # pylint: disable=no-self-use
    def to_internal_value(self, data):
        return {'file': data}

    # pylint: disable=no-self-use
    def to_representation(self, instance):
        if instance:
            upload_dir = instance.data.get_upload_dirname()
            return instance.file.path[len(upload_dir) + 1:]
        else:
            return instance

class ServerFileSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.ServerFile
        fields = ('file', )

    # pylint: disable=no-self-use
    def to_internal_value(self, data):
        return {'file': data}

    # pylint: disable=no-self-use
    def to_representation(self, instance):
        return instance.file if instance else instance

class RemoteFileSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.RemoteFile
        fields = ('file', )

    # pylint: disable=no-self-use
    def to_internal_value(self, data):
        return {'file': data}

    # pylint: disable=no-self-use
    def to_representation(self, instance):
        return instance.file if instance else instance

class RqStatusSerializer(serializers.Serializer):
    state = serializers.ChoiceField(choices=[
        "Queued", "Started", "Finished", "Failed"])
    message = serializers.CharField(allow_blank=True, default="")
    progress = serializers.FloatField(max_value=100, default=0)

    def __init__(self, instance=None, data=..., **kwargs):
        warnings.warn("RqStatusSerializer is deprecated, "
                      "use cvat.apps.engine.serializers.RequestSerializer instead", DeprecationWarning)
        super().__init__(instance, data, **kwargs)

class RqIdSerializer(serializers.Serializer):
    rq_id = serializers.CharField(help_text="Request id")


class JobFiles(serializers.ListField):
    """
    Read JobFileMapping docs for more info.
    """

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('child', serializers.CharField(
            allow_blank=False, max_length=MAX_FILENAME_LENGTH
        ))
        kwargs.setdefault('allow_empty', False)
        super().__init__(*args, **kwargs)


class JobFileMapping(serializers.ListField):
    """
    Represents a file-to-job mapping.
    Useful to specify a custom job configuration during task creation.
    This option is not compatible with most other job split-related options.
    Files in the jobs must not overlap or repeat.
    Job file mapping files must be a subset of the input files.
    If directories are specified in server_files, all files obtained by recursive search
    in the specified directories will be used as input files.
    In case of missing items in the input files, an error will be raised.

    Example:
    [

        ["file1.jpg", "file2.jpg"], # job #1 files
        ["file3.png"], # job #2 files
        ["file4.jpg", "file5.png", "file6.bmp"], # job #3 files
    ]
    """

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('child', JobFiles())
        kwargs.setdefault('allow_empty', False)
        kwargs.setdefault('help_text', textwrap.dedent(__class__.__doc__))
        super().__init__(*args, **kwargs)

class ValidationParamsSerializer(serializers.ModelSerializer):
    mode = serializers.ChoiceField(choices=models.ValidationMode.choices(), required=True)
    frame_selection_method = serializers.ChoiceField(
        choices=models.JobFrameSelectionMethod.choices(), required=True
    )
    frames = serializers.ListField(
        write_only=True,
        child=serializers.CharField(max_length=MAX_FILENAME_LENGTH),
        required=False,
        help_text=textwrap.dedent("""\
            The list of file names to be included in the validation set.
            Applicable only to the "{}" frame selection method.
            Can only be used for images.
        """.format(models.JobFrameSelectionMethod.MANUAL))
    )
    frame_count = serializers.IntegerField(
        min_value=1,
        required=False,
        help_text=textwrap.dedent("""\
            The number of frames to be included in the validation set.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_UNIFORM))
    )
    frame_share = serializers.FloatField(
        required=False,
        validators=[field_validation.validate_share],
        help_text=textwrap.dedent("""\
            The share of frames to be included in the validation set.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_UNIFORM))
    )
    frames_per_job_count = serializers.IntegerField(
        min_value=1,
        required=False,
        help_text=textwrap.dedent("""\
            The number of frames to be included in the validation set from each annotation job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_PER_JOB))
    )
    frames_per_job_share = serializers.FloatField(
        required=False,
        validators=[field_validation.validate_share],
        help_text=textwrap.dedent("""\
            The share of frames to be included in the validation set from each annotation job.
            Applicable only to the "{}" frame selection method
        """.format(models.JobFrameSelectionMethod.RANDOM_PER_JOB))
    )
    random_seed = serializers.IntegerField(
        min_value=0,
        required=False,
        help_text=textwrap.dedent("""\
            The seed value for the random number generator.
            The same value will produce the same frame sets.
            Applicable only to random frame selection methods.
            By default, a random value is used.
        """)
    )

    class Meta:
        fields = (
            'mode', 'frame_selection_method', 'random_seed', 'frames',
            'frame_count', 'frame_share', 'frames_per_job_count', 'frames_per_job_share',
        )
        model = models.ValidationParams

    def validate(self, attrs):
        if attrs["mode"] == models.ValidationMode.GT:
            field_validation.require_one_of_values(
                attrs,
                "frame_selection_method",
                [
                    models.JobFrameSelectionMethod.MANUAL,
                    models.JobFrameSelectionMethod.RANDOM_UNIFORM,
                    models.JobFrameSelectionMethod.RANDOM_PER_JOB,
                ]
            )
        elif attrs["mode"] == models.ValidationMode.GT_POOL:
            field_validation.require_one_of_values(
                attrs,
                "frame_selection_method",
                [
                    models.JobFrameSelectionMethod.MANUAL,
                    models.JobFrameSelectionMethod.RANDOM_UNIFORM,
                ]
            )
            field_validation.require_one_of_fields(
                attrs, ['frames_per_job_count', 'frames_per_job_share']
            )
        else:
            assert False, f"Unknown validation mode {attrs['mode']}"

        if attrs['frame_selection_method'] == models.JobFrameSelectionMethod.RANDOM_UNIFORM:
            field_validation.require_one_of_fields(attrs, ['frame_count', 'frame_share'])
        elif attrs['frame_selection_method'] == models.JobFrameSelectionMethod.RANDOM_PER_JOB:
            field_validation.require_one_of_fields(
                attrs, ['frames_per_job_count', 'frames_per_job_share']
            )
        elif attrs['frame_selection_method'] == models.JobFrameSelectionMethod.MANUAL:
            field_validation.require_field(attrs, "frames")

        if (
            'frames' in attrs and
            attrs['frame_selection_method'] != models.JobFrameSelectionMethod.MANUAL
        ):
            raise serializers.ValidationError(
                '"frames" can only be used when "frame_selection_method" is "{}"'.format(
                    models.JobFrameSelectionMethod.MANUAL
                )
            )

        if frames := attrs.get('frames'):
            unique_frames = set(frames)
            if len(unique_frames) != len(frames):
                raise serializers.ValidationError("Frames must not repeat")

        return super().validate(attrs)

    @transaction.atomic
    def create(self, validated_data: dict[str, Any]) -> models.ValidationParams:
        frames = validated_data.pop('frames', None)

        instance = super().create(validated_data)

        if frames:
            bulk_create(
                models.ValidationFrame,
                [models.ValidationFrame(validation_params=instance, path=frame) for frame in frames]
            )

        return instance

    @transaction.atomic
    def update(
        self, instance: models.ValidationParams, validated_data: dict[str, Any]
    ) -> models.ValidationParams:
        frames = validated_data.pop('frames', None)

        instance = super().update(instance, validated_data)

        if frames:
            models.ValidationFrame.objects.filter(validation_params=instance).delete()

            bulk_create(
                models.ValidationFrame,
                [models.ValidationFrame(validation_params=instance, path=frame) for frame in frames]
            )

        return instance

class DataSerializer(serializers.ModelSerializer):
    """
    Read more about parameters here:
    https://docs.cvat.ai/docs/manual/basics/create_an_annotation_task/#advanced-configuration
    """

    image_quality = serializers.IntegerField(min_value=0, max_value=100,
        help_text="Image quality to use during annotation")
    use_zip_chunks = serializers.BooleanField(default=False,
        help_text=textwrap.dedent("""\
            When true, video chunks will be represented as zip archives with decoded video frames.
            When false, video chunks are represented as video segments
        """))
    client_files = ClientFileSerializer(many=True, default=[],
        help_text=textwrap.dedent("""
            Uploaded files.
            Must contain all files from job_file_mapping if job_file_mapping is not empty.
        """))
    server_files = ServerFileSerializer(many=True, default=[],
        help_text=textwrap.dedent("""
            Paths to files from a file share mounted on the server, or from a cloud storage.
            Must contain all files from job_file_mapping if job_file_mapping is not empty.
        """))
    server_files_exclude = serializers.ListField(required=False, default=[],
        child=serializers.CharField(max_length=MAX_FILENAME_LENGTH),
        help_text=textwrap.dedent("""\
            Paths to files and directories from a file share mounted on the server, or from a cloud storage
            that should be excluded from the directories specified in server_files.
            This option cannot be used together with filename_pattern.
            The server_files_exclude parameter cannot be used to exclude a part of dataset from an archive.

            Examples:

            Exclude all files from subfolder 'sub/sub_1/sub_2'and single file 'sub/image.jpg' from specified folder:
            server_files = ['sub/'], server_files_exclude = ['sub/sub_1/sub_2/', 'sub/image.jpg']

            Exclude all cloud storage files with prefix 'sub' from the content of manifest file:
            server_files = ['manifest.jsonl'], server_files_exclude = ['sub/']
        """)
    )
    remote_files = RemoteFileSerializer(many=True, default=[],
        help_text=textwrap.dedent("""
            Direct download URLs for files.
            Must contain all files from job_file_mapping if job_file_mapping is not empty.
        """))
    use_cache = serializers.BooleanField(default=False,
        help_text=textwrap.dedent("""\
            Enable or disable task data chunk caching for the task.
            Read more: https://docs.cvat.ai/docs/manual/advanced/data_on_fly/
        """))
    copy_data = serializers.BooleanField(default=False, help_text=textwrap.dedent("""\
            Copy data from the server file share to CVAT during the task creation.
            This will create a copy of the data, making the server independent from
            the file share availability
        """))
    cloud_storage_id = serializers.IntegerField(write_only=True, allow_null=True, required=False,
        help_text=textwrap.dedent("""\
            If not null, the files referenced by server_files will be retrieved
            from the cloud storage with the specified ID.
            The cloud storages applicable depend on the context.
            In the user sandbox, only the user sandbox cloud storages can be used.
            In an organization, only the organization cloud storages can be used.
        """))
    filename_pattern = serializers.CharField(allow_null=True, required=False,
        help_text=textwrap.dedent("""\
            A filename filter for cloud storage files
            listed in the manifest. Supports fnmatch wildcards.
            Read more: https://docs.python.org/3/library/fnmatch.html
        """))
    job_file_mapping = JobFileMapping(required=False, write_only=True)

    upload_file_order = serializers.ListField(
        child=serializers.CharField(max_length=MAX_FILENAME_LENGTH),
        default=list, allow_empty=True, write_only=True,
        help_text=textwrap.dedent("""\
            Allows to specify file order for client_file uploads.
            Only valid with the "{}" sorting method selected.

            To state that the input files are sent in the correct order,
            pass an empty list.

            If you want to send files in an arbitrary order
            and reorder them afterwards on the server,
            pass the list of file names in the required order.
        """.format(models.SortingMethod.PREDEFINED))
    )
    validation_params = ValidationParamsSerializer(allow_null=True, required=False)

    class Meta:
        model = models.Data
        fields = (
            'chunk_size', 'image_quality', 'start_frame', 'stop_frame', 'frame_filter',
            'client_files', 'server_files', 'remote_files',
            'use_zip_chunks', 'server_files_exclude',
            'cloud_storage_id', 'use_cache', 'copy_data', 'storage_method',
            'storage', 'sorting_method', 'filename_pattern',
            'job_file_mapping', 'upload_file_order', 'validation_params'
        )
        extra_kwargs = {
            'chunk_size': { 'help_text': "Maximum number of frames per chunk" },
            'start_frame': { 'help_text': "First frame index" },
            'stop_frame': { 'help_text': "Last frame index" },
            'frame_filter': { 'help_text': "Frame filter. The only supported syntax is: 'step=N'" },
        }

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('help_text', self.__doc__)
        super().__init__(*args, **kwargs)

    # pylint: disable=no-self-use
    def validate_frame_filter(self, value):
        match = re.search(r"step\s*=\s*([1-9]\d*)", value)
        if not match:
            raise serializers.ValidationError("Invalid frame filter expression")
        return value

    # pylint: disable=no-self-use
    def validate_chunk_size(self, value):
        if not value > 0:
            raise serializers.ValidationError('Chunk size must be a positive integer')
        return value

    def validate_job_file_mapping(self, value):
        existing_files = set()

        for job_files in value:
            for filename in job_files:
                if filename in existing_files:
                    raise serializers.ValidationError(
                        f"The same file '{filename}' cannot be used multiple "
                        "times in the job file mapping"
                    )

                existing_files.add(filename)

        return value

    # pylint: disable=no-self-use
    def validate(self, attrs):
        if 'start_frame' in attrs and 'stop_frame' in attrs \
            and attrs['start_frame'] > attrs['stop_frame']:
            raise serializers.ValidationError('Stop frame must be more or equal start frame')

        filename_pattern = attrs.get('filename_pattern')
        server_files_exclude = attrs.get('server_files_exclude')
        server_files = attrs.get('server_files', [])

        if filename_pattern and len(list(filter(lambda x: not x['file'].endswith('.jsonl'), server_files))):
            raise serializers.ValidationError('The filename_pattern can only be used with specified manifest or without server_files')

        if filename_pattern and server_files_exclude:
            raise serializers.ValidationError('The filename_pattern and server_files_exclude cannot be used together')

        validation_params = attrs.pop('validation_params', None)
        if validation_params:
            validation_params_serializer = ValidationParamsSerializer(data=validation_params)
            validation_params_serializer.is_valid(raise_exception=True)
            attrs['validation_params'] = validation_params_serializer.validated_data

        return attrs

    @transaction.atomic
    def create(self, validated_data):
        files = self._pop_data(validated_data)
        validation_params = validated_data.pop('validation_params', None)

        db_data = models.Data.objects.create(**validated_data)
        db_data.make_dirs()

        self._create_files(db_data, files)

        db_data.save()

        if validation_params:
            validation_params_serializer = ValidationParamsSerializer(data=validation_params)
            validation_params_serializer.is_valid(raise_exception=True)
            db_data.validation_params = validation_params_serializer.save(task_data=db_data)

        return db_data

    @transaction.atomic
    def update(self, instance, validated_data):
        validation_params = validated_data.pop('validation_params', None)

        files = self._pop_data(validated_data)
        for key, value in validated_data.items():
            setattr(instance, key, value)
        self._create_files(instance, files)

        instance.save()

        if validation_params:
            validation_params_serializer = ValidationParamsSerializer(
                instance=getattr(instance, "validation_params", None), data=validation_params
            )
            validation_params_serializer.is_valid(raise_exception=True)
            instance.validation_params = validation_params_serializer.save(task_data=instance)

        return instance

    # pylint: disable=no-self-use
    def _pop_data(self, validated_data):
        client_files = validated_data.pop('client_files')
        server_files = validated_data.pop('server_files')
        remote_files = validated_data.pop('remote_files')

        validated_data.pop('job_file_mapping', None) # optional, not present in Data
        validated_data.pop('upload_file_order', None) # optional, not present in Data
        validated_data.pop('server_files_exclude', None) # optional, not present in Data

        for extra_key in { 'use_zip_chunks', 'use_cache', 'copy_data' }:
            validated_data.pop(extra_key)

        files = {'client_files': client_files, 'server_files': server_files, 'remote_files': remote_files}
        return files


    # pylint: disable=no-self-use
    @transaction.atomic
    def _create_files(self, instance, files):
        for files_type, files_model in zip(
            ('client_files', 'server_files', 'remote_files'),
            (models.ClientFile, models.ServerFile, models.RemoteFile),
        ):
            if files_type in files:
                bulk_create(
                    files_model,
                    [files_model(data=instance, **f) for f in files[files_type]]
                )

class TaskReadSerializer(serializers.ModelSerializer):
    data_chunk_size = serializers.ReadOnlyField(source='data.chunk_size', required=False)
    data_compressed_chunk_type = serializers.ReadOnlyField(source='data.compressed_chunk_type', required=False)
    data_original_chunk_type = serializers.ReadOnlyField(source='data.original_chunk_type', required=False)
    size = serializers.ReadOnlyField(source='data.size', required=False)
    image_quality = serializers.ReadOnlyField(source='data.image_quality', required=False)
    data = serializers.ReadOnlyField(source='data.id', required=False)
    owner = BasicUserSerializer(required=False, allow_null=True)
    assignee = BasicUserSerializer(allow_null=True, required=False)
    project_id = serializers.IntegerField(required=False, allow_null=True)
    guide_id = serializers.IntegerField(source='annotation_guide.id', required=False, allow_null=True)
    dimension = serializers.CharField(allow_blank=True, required=False)
    target_storage = StorageSerializer(required=False, allow_null=True)
    source_storage = StorageSerializer(required=False, allow_null=True)
    jobs = JobsSummarySerializer(url_filter_key='task_id', source='segment_set')
    labels = LabelsSummarySerializer(source='*')
    validation_mode = serializers.CharField(
        source='data.validation_mode', required=False, allow_null=True,
        help_text="Describes how the task validation is performed. Configured at task creation"
    )
    consensus_enabled = serializers.BooleanField(
        source='get_consensus_enabled', required=False, read_only=True
    )

    class Meta:
        model = models.Task
        fields = ('url', 'id', 'name', 'project_id', 'mode', 'owner', 'assignee',
            'bug_tracker', 'created_date', 'updated_date', 'overlap', 'segment_size',
            'status', 'data_chunk_size', 'data_compressed_chunk_type', 'guide_id',
            'data_original_chunk_type', 'size', 'image_quality', 'data', 'dimension',
            'subset', 'organization', 'target_storage', 'source_storage', 'jobs', 'labels',
            'assignee_updated_date', 'validation_mode', 'consensus_enabled',
        )
        read_only_fields = fields
        extra_kwargs = {
            'organization': { 'allow_null': True },
            'overlap': { 'allow_null': True },
        }

    def get_consensus_enabled(self, instance: models.Task) -> bool:
        return instance.consensus_replicas > 0

    def to_representation(self, instance):
        representation = super().to_representation(instance)
        representation['consensus_enabled'] = self.get_consensus_enabled(instance)
        return representation


class TaskWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    labels = LabelSerializer(many=True, source='label_set', partial=True, required=False)
    owner_id = serializers.IntegerField(write_only=True, allow_null=True, required=False)
    assignee_id = serializers.IntegerField(write_only=True, allow_null=True, required=False)
    project_id = serializers.IntegerField(required=False, allow_null=True)
    target_storage = StorageSerializer(required=False, allow_null=True)
    source_storage = StorageSerializer(required=False, allow_null=True)
    consensus_replicas = serializers.IntegerField(
        required=False, default=0, min_value=0,
        help_text=textwrap.dedent("""\
            The number of consensus replica jobs for each annotation job.
            Configured at task creation
        """)
    )

    class Meta:
        model = models.Task
        fields = (
            'url', 'id', 'name', 'project_id', 'owner_id', 'assignee_id',
            'bug_tracker', 'overlap', 'segment_size', 'labels', 'subset',
            'target_storage', 'source_storage', 'consensus_replicas',
        )
        write_once_fields = ('overlap', 'segment_size', 'consensus_replicas')

    def to_representation(self, instance):
        serializer = TaskReadSerializer(instance, context=self.context)
        return serializer.data

    def validate_consensus_replicas(self, value):
        max_replicas = settings.MAX_CONSENSUS_REPLICAS
        if value and (value == 1 or value < 0 or value > max_replicas):
            raise serializers.ValidationError(
                f"Consensus replicas must be 0 "
                f"or a positive number more than 1 and less than {max_replicas + 1}, "
                f"got {value}"
            )

        return value or 0

    # pylint: disable=no-self-use
    @transaction.atomic
    def create(self, validated_data):
        project_id = validated_data.get("project_id")
        if not (validated_data.get("label_set") or project_id):
            raise serializers.ValidationError('Label set or project_id must be present')
        if validated_data.get("label_set") and project_id:
            raise serializers.ValidationError('Project must have only one of Label set or project_id')

        project = None
        if project_id:
            try:
                project = models.Project.objects.get(id=project_id)
            except models.Project.DoesNotExist:
                raise serializers.ValidationError(f'The specified project #{project_id} does not exist.')

            if project.organization != validated_data.get('organization'):
                raise serializers.ValidationError(f'The task and its project should be in the same organization.')

        labels = validated_data.pop('label_set', [])

        # configure source/target storages for import/export
        storages = _configure_related_storages({
            'source_storage': validated_data.pop('source_storage', None),
            'target_storage': validated_data.pop('target_storage', None),
        })

        db_task = models.Task.objects.create(
            **storages,
            **validated_data)

        task_path = db_task.get_dirname()
        if os.path.isdir(task_path):
            shutil.rmtree(task_path)

        os.makedirs(task_path)

        LabelSerializer.create_labels(labels, parent_instance=db_task)

        if validated_data.get('assignee_id'):
            db_task.assignee_updated_date = db_task.updated_date
            db_task.save(update_fields=["assignee_updated_date"])

        return db_task

    # pylint: disable=no-self-use
    @transaction.atomic
    def update(self, instance, validated_data):
        instance.name = validated_data.get('name', instance.name)
        instance.owner_id = validated_data.get('owner_id', instance.owner_id)
        instance.bug_tracker = validated_data.get('bug_tracker', instance.bug_tracker)
        instance.subset = validated_data.get('subset', instance.subset)
        labels = validated_data.get('label_set', [])

        if (
            "assignee_id" in validated_data and
            validated_data["assignee_id"] != instance.assignee_id
        ):
            instance.assignee_id = validated_data.pop('assignee_id')
            instance.assignee_updated_date = timezone.now()

        if instance.project_id is None:
            LabelSerializer.update_labels(labels, parent_instance=instance)

        validated_project_id = validated_data.get('project_id')
        if validated_project_id is not None and validated_project_id != instance.project_id:
            project = models.Project.objects.get(id=validated_project_id)
            if project.tasks.count() and project.tasks.first().dimension != instance.dimension:
                raise serializers.ValidationError(f'Dimension ({instance.dimension}) of the task must be the same as other tasks in project ({project.tasks.first().dimension})')

            if instance.project_id is None:
                label_set = instance.label_set.all()
            else:
                label_set = instance.project.label_set.all()

            for old_label in label_set:
                new_label_for_name = list(filter(lambda x: x.get('id', None) == old_label.id, labels))
                if len(new_label_for_name):
                    old_label.name = new_label_for_name[0].get('name', old_label.name)
                try:
                    if old_label.parent:
                        new_label = project.label_set.filter(name=old_label.name, parent__name=old_label.parent.name).first()
                    else:
                        new_label = project.label_set.filter(name=old_label.name).first()
                except ValueError:
                    raise serializers.ValidationError(f'Target project does not have label with name "{old_label.name}"')

                for old_attr in old_label.attributespec_set.all():
                    new_attr = new_label.attributespec_set.filter(name=old_attr.name,
                                                                  values=old_attr.values,
                                                                  input_type=old_attr.input_type).first()
                    if new_attr is None:
                        raise serializers.ValidationError('Target project does not have ' \
                            f'"{old_label.name}" label with "{old_attr.name}" attribute')

                    for (model, model_name) in (
                        (models.LabeledTrackAttributeVal, 'track'),
                        (models.LabeledShapeAttributeVal, 'shape'),
                        (models.LabeledImageAttributeVal, 'image'),
                        (models.TrackedShapeAttributeVal, 'shape__track')
                    ):
                        model.objects.filter(**{
                            f'{model_name}__job__segment__task': instance,
                            f'{model_name}__label': old_label,
                            'spec': old_attr
                        }).update(spec=new_attr)

                for model in (models.LabeledTrack, models.LabeledShape, models.LabeledImage):
                    model.objects.filter(job__segment__task=instance, label=old_label).update(
                        label=new_label
                    )

            if instance.project_id is None:
                instance.label_set.all().delete()

            instance.project = project

        # update source and target storages
        _update_related_storages(instance, validated_data)

        instance.save()

        if 'label_set' in validated_data and not instance.project_id:
            self.update_child_objects_on_labels_update(instance)

        return instance

    def update_child_objects_on_labels_update(self, instance: models.Task):
        models.Job.objects.filter(
            updated_date__lt=instance.updated_date, segment__task=instance
        ).update(updated_date=instance.updated_date)

    def validate(self, attrs):
        # When moving task labels can be mapped to one, but when not names must be unique
        if 'project_id' in attrs.keys() and self.instance is not None:
            project_id = attrs.get('project_id')
            if project_id is not None:
                project = models.Project.objects.filter(id=project_id).first()
                if project is None:
                    raise serializers.ValidationError(f'Cannot find project with ID {project_id}')

            # Check that all labels can be mapped
            new_label_names = set()
            old_labels = self.instance.project.label_set.all() if self.instance.project_id else self.instance.label_set.all()
            new_sublabel_names = {}
            for old_label in old_labels:
                new_labels = tuple(filter(lambda x: x.get('id') == old_label.id, attrs.get('label_set', [])))
                if len(new_labels):
                    parent = new_labels[0].get('parent', old_label.parent)
                    if parent:
                        if parent.name not in new_sublabel_names:
                            new_sublabel_names[parent.name] = set()
                        new_sublabel_names[parent.name].add(new_labels[0].get('name', old_label.name))
                    else:
                        new_label_names.add(new_labels[0].get('name', old_label.name))
                else:
                    parent = old_label.parent
                    if parent:
                        if parent.name not in new_sublabel_names:
                            new_sublabel_names[parent.name] = set()
                        new_sublabel_names[parent.name].add(old_label.name)
                    else:
                        new_label_names.add(old_label.name)
            target_project = models.Project.objects.get(id=project_id)
            target_project_label_names = set()
            target_project_sublabel_names = {}
            for label in target_project.label_set.all():
                parent = label.parent
                if parent:
                    if parent.name not in target_project_sublabel_names:
                        target_project_sublabel_names[parent.name] = set()
                    target_project_sublabel_names[parent.name].add(label.name)
                else:
                    target_project_label_names.add(label.name)
            if not new_label_names.issubset(target_project_label_names):
                raise serializers.ValidationError('All task or project label names must be mapped to the target project')

            for label, sublabels in new_sublabel_names.items():
                if sublabels != target_project_sublabel_names.get(label):
                    raise serializers.ValidationError('All task or project label names must be mapped to the target project')

        return attrs

class ProjectReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(allow_null=True, required=False, read_only=True)
    assignee = BasicUserSerializer(allow_null=True, required=False, read_only=True)
    guide_id = serializers.IntegerField(source='annotation_guide.id', required=False, allow_null=True)
    task_subsets = serializers.ListField(child=serializers.CharField(), required=False, read_only=True)
    dimension = serializers.CharField(max_length=16, required=False, read_only=True, allow_null=True)
    target_storage = StorageSerializer(required=False, allow_null=True, read_only=True)
    source_storage = StorageSerializer(required=False, allow_null=True, read_only=True)
    tasks = TasksSummarySerializer(models.Task, url_filter_key='project_id')
    labels = LabelsSummarySerializer(source='*')

    class Meta:
        model = models.Project
        fields = ('url', 'id', 'name', 'owner', 'assignee', 'guide_id',
            'bug_tracker', 'task_subsets', 'created_date', 'updated_date', 'status',
            'dimension', 'organization', 'target_storage', 'source_storage',
            'tasks', 'labels', 'assignee_updated_date'
        )
        read_only_fields = fields
        extra_kwargs = { 'organization': { 'allow_null': True } }

    def to_representation(self, instance):
        response = super().to_representation(instance)

        task_subsets = {task.subset for task in instance.tasks.all() if task.subset}
        task_dimension = next(
            (task.dimension for task in instance.tasks.all() if task.dimension),
            None
        )
        response['task_subsets'] = list(task_subsets)
        response['dimension'] = task_dimension
        return response

class ProjectWriteSerializer(serializers.ModelSerializer):
    labels = LabelSerializer(write_only=True, many=True, source='label_set', partial=True, default=[])
    owner_id = serializers.IntegerField(write_only=True, allow_null=True, required=False)
    assignee_id = serializers.IntegerField(write_only=True, allow_null=True, required=False)

    target_storage = StorageSerializer(write_only=True, required=False)
    source_storage = StorageSerializer(write_only=True, required=False)

    class Meta:
        model = models.Project
        fields = ('name', 'labels', 'owner_id', 'assignee_id', 'bug_tracker',
            'target_storage', 'source_storage',
        )

    def to_representation(self, instance):
        serializer = ProjectReadSerializer(instance, context=self.context)
        return serializer.data

    # pylint: disable=no-self-use
    @transaction.atomic
    def create(self, validated_data):
        labels = validated_data.pop('label_set')

        # configure source/target storages for import/export
        storages = _configure_related_storages({
            'source_storage': validated_data.pop('source_storage', None),
            'target_storage': validated_data.pop('target_storage', None),
        })

        db_project = models.Project.objects.create(
            **storages,
            **validated_data)

        project_path = db_project.get_dirname()
        if os.path.isdir(project_path):
            shutil.rmtree(project_path)
        os.makedirs(project_path)

        LabelSerializer.create_labels(labels, parent_instance=db_project)

        if validated_data.get("assignee_id"):
            db_project.assignee_updated_date = db_project.updated_date
            db_project.save(update_fields=["assignee_updated_date"])

        return db_project

    # pylint: disable=no-self-use
    @transaction.atomic
    def update(self, instance, validated_data):
        instance.name = validated_data.get('name', instance.name)
        instance.owner_id = validated_data.get('owner_id', instance.owner_id)
        instance.bug_tracker = validated_data.get('bug_tracker', instance.bug_tracker)

        if (
            "assignee_id" in validated_data and
            validated_data['assignee_id'] != instance.assignee_id
        ):
            instance.assignee_id = validated_data.pop('assignee_id')
            instance.assignee_updated_date = timezone.now()

        labels = validated_data.get('label_set', [])
        LabelSerializer.update_labels(labels, parent_instance=instance)

        # update source and target storages
        _update_related_storages(instance, validated_data)

        instance.save()

        if 'label_set' in validated_data:
            self.update_child_objects_on_labels_update(instance)

        return instance

    @transaction.atomic
    def update_child_objects_on_labels_update(self, instance: models.Project):
        models.Task.objects.filter(
            updated_date__lt=instance.updated_date, project=instance
        ).update(updated_date=instance.updated_date)

        models.Job.objects.filter(
            updated_date__lt=instance.updated_date, segment__task__project=instance
        ).update(updated_date=instance.updated_date)


class AboutSerializer(serializers.Serializer):
    name = serializers.CharField(max_length=128)
    description = serializers.CharField(max_length=2048)
    version = serializers.CharField(max_length=64)
    logo_url = serializers.CharField()
    subtitle = serializers.CharField(max_length=1024)

class FrameMetaSerializer(serializers.Serializer):
    width = serializers.IntegerField()
    height = serializers.IntegerField()
    name = serializers.CharField(max_length=MAX_FILENAME_LENGTH)
    related_files = serializers.IntegerField()

    # for compatibility with version 2.3.0
    has_related_context = serializers.SerializerMethodField()

    @extend_schema_field(serializers.BooleanField)
    def get_has_related_context(self, obj: dict) -> bool:
        return obj['related_files'] != 0

class PluginsSerializer(serializers.Serializer):
    GIT_INTEGRATION = serializers.BooleanField()
    ANALYTICS = serializers.BooleanField()
    MODELS = serializers.BooleanField()
    PREDICT = serializers.BooleanField()

class DataMetaReadSerializer(serializers.ModelSerializer):
    frames = FrameMetaSerializer(many=True, allow_null=True)
    image_quality = serializers.IntegerField(min_value=0, max_value=100)
    deleted_frames = serializers.ListField(child=serializers.IntegerField(min_value=0))
    included_frames = serializers.ListField(
        child=serializers.IntegerField(min_value=0), allow_null=True, required=False,
        help_text=textwrap.dedent("""\
        A list of valid frame ids. The None value means all frames are included.
        """))
    chunks_updated_date = serializers.DateTimeField()

    class Meta:
        model = models.Data
        fields = (
            'chunks_updated_date',
            'chunk_size',
            'size',
            'image_quality',
            'start_frame',
            'stop_frame',
            'frame_filter',
            'frames',
            'deleted_frames',
            'included_frames',
        )
        read_only_fields = fields
        extra_kwargs = {
            'chunks_updated_date': {
                'help_text': textwrap.dedent("""\
                    The date of the last chunk data update.
                    Chunks downloaded before this date are outdated and should be redownloaded.
                """)
            },
            'size': {
                'help_text': textwrap.dedent("""\
                    The number of frames included. Deleted frames do not affect this value.
                """)
            },
        }

class DataMetaWriteSerializer(serializers.ModelSerializer):
    deleted_frames = serializers.ListField(child=serializers.IntegerField(min_value=0))

    class Meta:
        model = models.Data
        fields = ('deleted_frames',)

    def update(self, instance: models.Data, validated_data: dict[str, Any]) -> models.Data:
        requested_deleted_frames = validated_data['deleted_frames']

        requested_deleted_frames_set = set(requested_deleted_frames)
        if len(requested_deleted_frames_set) != len(requested_deleted_frames):
            raise serializers.ValidationError("Deleted frames cannot repeat")

        unknown_requested_deleted_frames = (
            requested_deleted_frames_set.difference(range(instance.size))
        )
        if unknown_requested_deleted_frames:
            raise serializers.ValidationError(
                "Unknown frames {} requested for removal".format(
                    format_list(tuple(map(str, sorted(unknown_requested_deleted_frames))))
                )
            )

        validation_layout = getattr(instance, 'validation_layout', None)
        if validation_layout and validation_layout.mode == models.ValidationMode.GT_POOL:
            gt_frame_set = set(validation_layout.frames)
            changed_deleted_frames = requested_deleted_frames_set.difference(instance.deleted_frames)
            if not gt_frame_set.isdisjoint(changed_deleted_frames):
                raise serializers.ValidationError(
                    f"When task validation mode is {models.ValidationMode.GT_POOL}, "
                    "GT frames can only be deleted and restored via the "
                    "GT job's api/jobs/{id}/data/meta endpoint"
                )

        return super().update(instance, validated_data)

class JobDataMetaWriteSerializer(serializers.ModelSerializer):
    deleted_frames = serializers.ListField(child=serializers.IntegerField(min_value=0))

    class Meta:
        model = models.Job
        fields = ('deleted_frames',)

    @transaction.atomic
    def update(self, instance: models.Job, validated_data: dict[str, Any]) -> models.Job:
        db_segment = instance.segment
        db_task = db_segment.task
        db_data = db_task.data

        deleted_frames = validated_data['deleted_frames']

        task_frame_provider = TaskFrameProvider(db_task)
        segment_rel_frame_set = set(
            map(task_frame_provider.get_rel_frame_number, db_segment.frame_set)
        )

        unknown_deleted_frames = set(deleted_frames) - segment_rel_frame_set
        if unknown_deleted_frames:
            raise serializers.ValidationError("Frames {} do not belong to the job".format(
                format_list(list(map(str, unknown_deleted_frames)))
            ))

        updated_deleted_validation_frames = None
        updated_deleted_task_frames = None

        if instance.type == models.JobType.GROUND_TRUTH:
            updated_deleted_validation_frames = deleted_frames + [
                f
                for f in db_data.validation_layout.disabled_frames
                if f not in segment_rel_frame_set
            ]

            if db_data.validation_layout.mode == models.ValidationMode.GT_POOL:
                # GT pool owns its frames, so we exclude them from the task
                # Them and the related honeypots in jobs
                updated_validation_abs_frame_set = set(
                    map(task_frame_provider.get_abs_frame_number, updated_deleted_validation_frames)
                )

                excluded_placeholder_frames = [
                    task_frame_provider.get_rel_frame_number(frame)
                    for frame, real_frame in (
                        models.Image.objects
                        .filter(data=db_data, is_placeholder=True)
                        .values_list('frame', 'real_frame')
                        .iterator(chunk_size=10000)
                    )
                    if real_frame in updated_validation_abs_frame_set
                ]
                updated_deleted_task_frames = deleted_frames + excluded_placeholder_frames
            elif db_data.validation_layout.mode == models.ValidationMode.GT:
                # Regular GT jobs only refer to the task frames, without data ownership
                pass
            else:
                assert False
        else:
            updated_deleted_task_frames = deleted_frames + [
                f
                for f in db_data.deleted_frames
                if f not in segment_rel_frame_set
            ]

        if updated_deleted_validation_frames is not None:
            db_data.validation_layout.disabled_frames = updated_deleted_validation_frames
            db_data.validation_layout.save(update_fields=['disabled_frames'])

        if updated_deleted_task_frames is not None:
            db_data.deleted_frames = updated_deleted_task_frames
            db_data.save(update_fields=['deleted_frames'])

        db_task.touch()
        if db_task.project:
            db_task.project.touch()

        return instance

class AttributeValSerializer(serializers.Serializer):
    spec_id = serializers.IntegerField()
    value = serializers.CharField(max_length=4096, allow_blank=True)

    def to_internal_value(self, data):
        data['value'] = str(data['value'])
        return super().to_internal_value(data)

class AnnotationSerializer(serializers.Serializer):
    id = serializers.IntegerField(default=None, allow_null=True)
    frame = serializers.IntegerField(min_value=0)
    label_id = serializers.IntegerField(min_value=0)
    group = serializers.IntegerField(min_value=0, allow_null=True, default=None)
    source = serializers.CharField(default='manual')

class LabeledImageSerializer(AnnotationSerializer):
    attributes = AttributeValSerializer(many=True, default=[])

class OptimizedFloatListField(serializers.ListField):
    '''Default ListField is extremely slow when try to process long lists of points'''

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs, child=serializers.FloatField())

    def to_internal_value(self, data):
        return self.run_child_validation(data)

    def to_representation(self, data):
        return data

    def run_child_validation(self, data):
        errors = OrderedDict()
        for idx, item in enumerate(data):
            if type(item) not in [int, float]:
                errors[idx] = exceptions.ValidationError('Value must be a float or an integer')

        if not errors:
            return data

        raise exceptions.ValidationError(errors)

class ShapeSerializer(serializers.Serializer):
    type = serializers.ChoiceField(choices=models.ShapeType.choices())
    occluded = serializers.BooleanField(default=False)
    outside = serializers.BooleanField(default=False, required=False)
    z_order = serializers.IntegerField(default=0)
    rotation = serializers.FloatField(default=0, min_value=0, max_value=360)
    points = OptimizedFloatListField(
        allow_empty=True, required=False
    )

class SubLabeledShapeSerializer(ShapeSerializer, AnnotationSerializer):
    attributes = AttributeValSerializer(many=True, default=[])

class LabeledShapeSerializer(SubLabeledShapeSerializer):
    elements = SubLabeledShapeSerializer(many=True, required=False)

def _convert_annotation(obj, keys):
    return OrderedDict([(key, obj[key]) for key in keys])

def _convert_attributes(attr_set):
    attr_keys = ['spec_id', 'value']
    return [
        OrderedDict([(key, attr[key]) for key in attr_keys]) for attr in attr_set
    ]

class LabeledImageSerializerFromDB(serializers.BaseSerializer):
    # Use this serializer to export data from the database
    # Because default DRF serializer is too slow on huge collections
    def to_representation(self, instance):
        def convert_tag(tag):
            result = _convert_annotation(tag, ['id', 'label_id', 'frame', 'group', 'source'])
            result['attributes'] = _convert_attributes(tag['attributes'])
            return result

        return convert_tag(instance)

class LabeledShapeSerializerFromDB(serializers.BaseSerializer):
    # Use this serializer to export data from the database
    # Because default DRF serializer is too slow on huge collections
    def to_representation(self, instance):
        def convert_shape(shape):
            result = _convert_annotation(shape, [
                'id', 'label_id', 'type', 'frame', 'group', 'source',
                'occluded', 'outside', 'z_order', 'rotation', 'points',
            ])
            result['attributes'] = _convert_attributes(shape['attributes'])
            if shape.get('elements', None) is not None and shape['parent'] is None:
                result['elements'] = [convert_shape(element) for element in shape['elements']]
            return result

        return convert_shape(instance)

class LabeledTrackSerializerFromDB(serializers.BaseSerializer):
    # Use this serializer to export data from the database
    # Because default DRF serializer is too slow on huge collections
    def to_representation(self, instance):
        def convert_track(track):
            shape_keys = [
                'id', 'type', 'frame', 'occluded', 'outside', 'z_order',
                'rotation', 'points', 'attributes',
            ]
            result = _convert_annotation(track, ['id', 'label_id', 'frame', 'group', 'source'])
            result['shapes'] = [_convert_annotation(shape, shape_keys) for shape in track['shapes']]
            result['attributes'] = _convert_attributes(track['attributes'])
            for shape in result['shapes']:
                shape['attributes'] = _convert_attributes(shape['attributes'])
            if track.get('elements', None) is not None and track['parent'] is None:
                result['elements'] = [convert_track(element) for element in track['elements']]
            return result

        return convert_track(instance)

class TrackedShapeSerializer(ShapeSerializer):
    id = serializers.IntegerField(default=None, allow_null=True)
    frame = serializers.IntegerField(min_value=0)
    attributes = AttributeValSerializer(many=True, default=[])

class SubLabeledTrackSerializer(AnnotationSerializer):
    shapes = TrackedShapeSerializer(many=True, allow_empty=True)
    attributes = AttributeValSerializer(many=True, default=[])

class LabeledTrackSerializer(SubLabeledTrackSerializer):
    elements = SubLabeledTrackSerializer(many=True, required=False)

class LabeledDataSerializer(serializers.Serializer):
    version = serializers.IntegerField(default=0) # TODO: remove
    tags   = LabeledImageSerializer(many=True, default=[])
    shapes = LabeledShapeSerializer(many=True, default=[])
    tracks = LabeledTrackSerializer(many=True, default=[])

class FileInfoSerializer(serializers.Serializer):
    name = serializers.CharField(max_length=MAX_FILENAME_LENGTH)
    type = serializers.ChoiceField(choices=["REG", "DIR"])
    mime_type = serializers.CharField(max_length=255)

class AnnotationFileSerializer(serializers.Serializer):
    annotation_file = serializers.FileField()

class DatasetFileSerializer(serializers.Serializer):
    dataset_file = serializers.FileField()

    @staticmethod
    def validate_dataset_file(value):
        if os.path.splitext(value.name)[1] != '.zip':
            raise serializers.ValidationError('Dataset file should be zip archive')
        return value

class TaskFileSerializer(serializers.Serializer):
    task_file = serializers.FileField()

class ProjectFileSerializer(serializers.Serializer):
    project_file = serializers.FileField()

class CommentReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(allow_null=True, required=False)

    class Meta:
        model = models.Comment
        fields = ('id', 'issue', 'owner', 'message', 'created_date',
            'updated_date')
        read_only_fields = fields

class CommentWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    def to_representation(self, instance):
        serializer = CommentReadSerializer(instance, context=self.context)
        return serializer.data

    class Meta:
        model = models.Comment
        fields = ('issue', 'message')
        write_once_fields = ('issue', )


class IssueReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(allow_null=True, required=False)
    assignee = BasicUserSerializer(allow_null=True, required=False)
    position = serializers.ListField(
        child=serializers.FloatField(), allow_empty=False
    )
    comments = CommentsSummarySerializer(models.Comment, url_filter_key='issue_id')

    class Meta:
        model = models.Issue
        fields = ('id', 'frame', 'position', 'job', 'owner', 'assignee',
            'created_date', 'updated_date', 'resolved', 'comments')
        read_only_fields = fields
        extra_kwargs = {
            'created_date': { 'allow_null': True },
            'updated_date': { 'allow_null': True },
        }


class IssueWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    position = serializers.ListField(
        child=serializers.FloatField(), allow_empty=False,
    )
    message = serializers.CharField(style={'base_template': 'textarea.html'})

    def to_representation(self, instance):
        serializer = IssueReadSerializer(instance, context=self.context)
        return serializer.data

    def create(self, validated_data):
        message = validated_data.pop('message')
        db_issue = super().create(validated_data)
        models.Comment.objects.create(issue=db_issue,
            message=message, owner=db_issue.owner)
        return db_issue

    class Meta:
        model = models.Issue
        fields = ('frame', 'position', 'job', 'assignee', 'message', 'resolved')
        write_once_fields = ('frame', 'job', 'message')

class ManifestSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.Manifest
        fields = ('filename', )

    # pylint: disable=no-self-use
    def to_internal_value(self, data):
        return {'filename': data }

    # pylint: disable=no-self-use
    def to_representation(self, instance):
        return instance.filename if instance else instance

class CloudStorageReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(required=False, allow_null=True)
    manifests = ManifestSerializer(many=True, default=[])
    class Meta:
        model = models.CloudStorage
        exclude = ['credentials']
        read_only_fields = ('created_date', 'updated_date', 'owner', 'organization')
        extra_kwargs = { 'organization': { 'allow_null': True } }

@extend_schema_serializer(
    examples=[
        OpenApiExample(
            'Create AWS S3 cloud storage with credentials',
            description='',
            value={
                'provider_type': models.CloudProviderChoice.AWS_S3,
                'resource': 'somebucket',
                'display_name': 'Bucket',
                'credentials_type': models.CredentialsTypeChoice.KEY_SECRET_KEY_PAIR,
                'key': 'XXX',
                'secret_key': 'XXX',
                'specific_attributes': 'region=eu-central-1',
                'description': 'Some description',
                'manifests': [
                    'manifest.jsonl'
                ],

            },
            request_only=True,
        ),
        OpenApiExample(
            'Create AWS S3 cloud storage without credentials',
            value={
                'provider_type': models.CloudProviderChoice.AWS_S3,
                'resource': 'somebucket',
                'display_name': 'Bucket',
                'credentials_type': models.CredentialsTypeChoice.ANONYMOUS_ACCESS,
                'manifests': [
                    'manifest.jsonl'
                ],
            },
            request_only=True,
        ),
        OpenApiExample(
            'Create Azure cloud storage',
            value={
                'provider_type': models.CloudProviderChoice.AZURE_CONTAINER,
                'resource': 'sonecontainer',
                'display_name': 'Container',
                'credentials_type': models.CredentialsTypeChoice.ACCOUNT_NAME_TOKEN_PAIR,
                'account_name': 'someaccount',
                'session_token': 'xxx',
                'manifests': [
                    'manifest.jsonl'
                ],
            },
            request_only=True,
        ),
        OpenApiExample(
            'Create GCS',
            value={
                'provider_type': models.CloudProviderChoice.GOOGLE_CLOUD_STORAGE,
                'resource': 'somebucket',
                'display_name': 'Bucket',
                'credentials_type': models.CredentialsTypeChoice.KEY_FILE_PATH,
                'key_file': 'file',
                'manifests': [
                    'manifest.jsonl'
                ],
            },
            request_only=True,
        )
    ]
)
class CloudStorageWriteSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(required=False)
    session_token = serializers.CharField(max_length=440, allow_blank=True, required=False)
    key = serializers.CharField(max_length=40, allow_blank=True, required=False)
    secret_key = serializers.CharField(max_length=64, allow_blank=True, required=False)
    key_file = serializers.FileField(required=False)
    account_name = serializers.CharField(max_length=24, allow_blank=True, required=False)
    manifests = ManifestSerializer(many=True, default=[])
    connection_string = serializers.CharField(max_length=1024, allow_blank=True, required=False)

    class Meta:
        model = models.CloudStorage
        fields = (
            'provider_type', 'resource', 'display_name', 'owner', 'credentials_type',
            'created_date', 'updated_date', 'session_token', 'account_name', 'key',
            'secret_key', 'connection_string', 'key_file', 'specific_attributes', 'description', 'id',
            'manifests', 'organization'
        )
        read_only_fields = ('created_date', 'updated_date', 'owner', 'organization')
        extra_kwargs = { 'organization': { 'allow_null': True } }

    # pylint: disable=no-self-use
    def validate_specific_attributes(self, value):
        if value:
            attributes = value.split('&')
            for attribute in attributes:
                if not len(attribute.split('=')) == 2:
                    raise serializers.ValidationError('Invalid specific attributes')
        return value

    def validate(self, attrs):
        provider_type = attrs.get('provider_type')
        if provider_type == models.CloudProviderChoice.AZURE_CONTAINER:
            if not attrs.get('account_name', '') and not attrs.get('connection_string', ''):
                raise serializers.ValidationError('Account name or connection string for Azure container was not specified')

        # AWS S3: https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html?icmpid=docs_amazons3_console
        # Azure Container: https://learn.microsoft.com/en-us/rest/api/storageservices/naming-and-referencing-containers--blobs--and-metadata#container-names
        # GCS: https://cloud.google.com/storage/docs/buckets#naming
        ALLOWED_RESOURCE_NAME_SYMBOLS = (
            string.ascii_lowercase + string.digits + "-"
        )

        if provider_type == models.CloudProviderChoice.GOOGLE_CLOUD_STORAGE:
            ALLOWED_RESOURCE_NAME_SYMBOLS += "_."
        elif provider_type == models.CloudProviderChoice.AWS_S3:
            ALLOWED_RESOURCE_NAME_SYMBOLS += "."

        # We need to check only basic naming rule
        if (resource := attrs.get("resource")) and (
            diff := (set(resource) - set(ALLOWED_RESOURCE_NAME_SYMBOLS))
        ):
            raise serializers.ValidationError({
                'resource': f"Invalid characters ({','.join(diff)}) were found.",
            })

        return attrs

    def _validate_prefix(self, value: str) -> None:
        if value.startswith('/'):
            raise serializers.ValidationError('Prefix cannot start with forward slash ("/").')
        if '' in value.strip('/').split('/'):
            raise serializers.ValidationError('Prefix cannot contain multiple slashes in a row.')

    @staticmethod
    def _manifests_validation(storage, manifests):
        # check manifest files availability
        for manifest in manifests:
            file_status = storage.get_file_status(manifest)
            if file_status == Status.NOT_FOUND:
                raise serializers.ValidationError({
                    'manifests': "The '{}' file does not exist on '{}' cloud storage" \
                        .format(manifest, storage.name)
                })
            elif file_status == Status.FORBIDDEN:
                raise serializers.ValidationError({
                    'manifests': "The '{}' file does not available on '{}' cloud storage. Access denied" \
                        .format(manifest, storage.name)
                })

    def create(self, validated_data):
        provider_type = validated_data.get('provider_type')
        should_be_created = validated_data.pop('should_be_created', None)

        key_file = validated_data.pop('key_file', None)
        # we need to save it to temporary file to check the granted permissions
        temporary_file = None
        if key_file:
            with NamedTemporaryFile(mode='wb', prefix='cvat', delete=False) as temp_key:
                temp_key.write(key_file.read())
                temporary_file = temp_key.name
            key_file.close()
            del key_file
        credentials = Credentials(
            account_name=validated_data.pop('account_name', ''),
            key=validated_data.pop('key', ''),
            secret_key=validated_data.pop('secret_key', ''),
            session_token=validated_data.pop('session_token', ''),
            key_file_path=temporary_file,
            credentials_type = validated_data.get('credentials_type'),
            connection_string = validated_data.pop('connection_string', '')
        )
        details = {
            'resource': validated_data.get('resource'),
            'credentials': credentials,
            'specific_attributes': parse_specific_attributes(validated_data.get('specific_attributes', ''))
        }

        if (prefix := details['specific_attributes'].get('prefix')):
            self._validate_prefix(prefix)

        storage = get_cloud_storage_instance(cloud_provider=provider_type, **details)
        if should_be_created:
            try:
                storage.create()
            except Exception as ex:
                slogger.glob.warning("Failed with creating storage\n{}".format(str(ex)))
                raise

        storage_status = storage.get_status()
        if storage_status == Status.AVAILABLE:
            manifests = [m.get('filename') for m in validated_data.pop('manifests')]
            self._manifests_validation(storage, manifests)

            db_storage = models.CloudStorage.objects.create(
                credentials=credentials.convert_to_db(),
                **validated_data
            )
            db_storage.save()

            manifest_file_instances = [models.Manifest(filename=manifest, cloud_storage=db_storage) for manifest in manifests]
            bulk_create(models.Manifest, manifest_file_instances)

            cloud_storage_path = db_storage.get_storage_dirname()
            if os.path.isdir(cloud_storage_path):
                shutil.rmtree(cloud_storage_path)
            os.makedirs(cloud_storage_path)

            if temporary_file:
                # so, gcs key file is valid and we need to set correct path to the file
                real_path_to_key_file = db_storage.get_key_file_path()
                shutil.copyfile(temporary_file, real_path_to_key_file)
                os.remove(temporary_file)

                credentials.key_file_path = real_path_to_key_file
                db_storage.credentials = credentials.convert_to_db()
                db_storage.save()
            return db_storage
        elif storage_status == Status.FORBIDDEN:
            field = 'credentials'
            message = 'Cannot create resource {} with specified credentials. Access forbidden.'.format(storage.name)
        else:
            field = 'resource'
            message = 'The resource {} not found. It may have been deleted.'.format(storage.name)
        if temporary_file:
            os.remove(temporary_file)
        slogger.glob.error(message)
        raise serializers.ValidationError({field: message})

    @transaction.atomic
    def update(self, instance, validated_data):
        credentials = Credentials()
        credentials.convert_from_db({
            'type': instance.credentials_type,
            'value': instance.credentials,
        })
        credentials_dict = {k:v for k,v in validated_data.items() if k in {
            'key','secret_key', 'account_name', 'session_token', 'key_file_path',
            'credentials_type', 'connection_string'
        }}

        key_file = validated_data.pop('key_file', None)
        temporary_file = None
        if key_file:
            with NamedTemporaryFile(mode='wb', prefix='cvat', delete=False) as temp_key:
                temp_key.write(key_file.read())
                temporary_file = temp_key.name
            credentials_dict['key_file_path'] = temporary_file
            key_file.close()
            del key_file

        if (prefix := parse_specific_attributes(validated_data.get('specific_attributes', '')).get('prefix')):
            self._validate_prefix(prefix)

        credentials.mapping_with_new_values(credentials_dict)
        instance.credentials = credentials.convert_to_db()

        for field in ('credentials_type', 'resource', 'display_name', 'description', 'specific_attributes'):
            if field in validated_data:
                setattr(instance, field, validated_data[field])

        # check cloud storage existing
        details = {
            'resource': instance.resource,
            'credentials': credentials,
            'specific_attributes': parse_specific_attributes(instance.specific_attributes)
        }
        storage = get_cloud_storage_instance(cloud_provider=instance.provider_type, **details)
        storage_status = storage.get_status()
        if storage_status == Status.AVAILABLE:
            new_manifest_names = set(i.get('filename') for i in validated_data.get('manifests', []))
            previous_manifest_names = set(i.filename for i in instance.manifests.all())
            delta_to_delete = tuple(previous_manifest_names - new_manifest_names)
            delta_to_create = tuple(new_manifest_names - previous_manifest_names)
            if delta_to_delete:
                instance.manifests.filter(filename__in=delta_to_delete).delete()
            if delta_to_create:
                # check manifest files existing
                self._manifests_validation(storage, delta_to_create)
                manifest_instances = [models.Manifest(filename=f, cloud_storage=instance) for f in delta_to_create]
                bulk_create(models.Manifest, manifest_instances)
            if temporary_file:
                # so, gcs key file is valid and we need to set correct path to the file
                real_path_to_key_file = instance.get_key_file_path()
                shutil.copyfile(temporary_file, real_path_to_key_file)
                os.remove(temporary_file)

                instance.credentials = real_path_to_key_file
            instance.save()
            return instance
        elif storage_status == Status.FORBIDDEN:
            field = 'credentials'
            message = 'Cannot update resource {} with specified credentials. Access forbidden.'.format(storage.name)
        else:
            field = 'resource'
            message = 'The resource {} not found. It may have been deleted.'.format(storage.name)
        if temporary_file:
            os.remove(temporary_file)
        slogger.glob.error(message)
        raise serializers.ValidationError({field: message})


class CloudStorageContentSerializer(serializers.Serializer):
    next = serializers.CharField(required=False, allow_null=True, allow_blank=True,
        help_text="This token is used to continue listing files in the bucket.")
    content = FileInfoSerializer(many=True)

class RelatedFileSerializer(serializers.ModelSerializer):

    class Meta:
        model = models.RelatedFile
        fields = '__all__'
        read_only_fields = ('path',)


def _update_related_storages(
    instance: Union[models.Project, models.Task],
    validated_data: dict[str, Any],
) -> None:
    for storage_type in ('source_storage', 'target_storage'):
        new_conf = validated_data.pop(storage_type, None)

        if not new_conf:
            continue

        new_cloud_storage_id = new_conf.get('cloud_storage_id')
        new_location = new_conf.get('location')

        # storage_instance maybe None
        storage_instance = getattr(instance, storage_type)

        if new_cloud_storage_id:
            if new_location and new_location != models.Location.CLOUD_STORAGE:
                raise serializers.ValidationError(
                    f"It is not allowed to specify '{new_location}' location together with cloud storage id"
                )
            elif (
                not new_location
                and getattr(storage_instance, "location", None) != models.Location.CLOUD_STORAGE
            ):
                raise serializers.ValidationError(
                    f"The configuration of {storage_type} is not full"
                )

            if not models.CloudStorage.objects.filter(id=new_cloud_storage_id).exists():
                raise serializers.ValidationError(
                    f"The specified cloud storage {new_cloud_storage_id} does not exist."
                )
        else:
            if new_location == models.Location.CLOUD_STORAGE:
                raise serializers.ValidationError(
                    "Cloud storage was selected as location but its id was not specified"
                )
            elif (
                not new_location
                and getattr(storage_instance, "location", None) == models.Location.CLOUD_STORAGE
                and "cloud_storage_id" in new_conf
            ):
                raise serializers.ValidationError(
                    "It is not allowed to reset a cloud storage id without explicitly resetting a location"
                )

        if not storage_instance:
            storage_instance = models.Storage(**new_conf)
            storage_instance.save()
            setattr(instance, storage_type, storage_instance)
            continue

        storage_instance.location = new_location or storage_instance.location
        storage_instance.cloud_storage_id = new_cloud_storage_id
        storage_instance.save()

def _configure_related_storages(validated_data: dict[str, Any]) -> dict[str, Optional[models.Storage]]:
    storages = {
        'source_storage': None,
        'target_storage': None,
    }

    for i in storages:
        if storage_conf := validated_data.get(i):
            if (
                (cloud_storage_id := storage_conf.get('cloud_storage_id')) and
                not models.CloudStorage.objects.filter(id=cloud_storage_id).exists()
            ):
                raise serializers.ValidationError(f'The specified cloud storage {cloud_storage_id} does not exist.')
            storage_instance = models.Storage(**storage_conf)
            storage_instance.save()
            storages[i] = storage_instance
    return storages

class AssetReadSerializer(WriteOnceMixin, serializers.ModelSerializer):
    filename = serializers.CharField(required=True, max_length=MAX_FILENAME_LENGTH)
    owner = BasicUserSerializer(required=False)

    class Meta:
        model = models.Asset
        fields = ('uuid', 'filename', 'created_date', 'owner', 'guide_id', )
        read_only_fields = fields

class AssetWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    uuid = serializers.CharField(required=False)
    filename = serializers.CharField(required=True, max_length=MAX_FILENAME_LENGTH)
    guide_id = serializers.IntegerField(required=True)

    class Meta:
        model = models.Asset
        fields = ('uuid', 'filename', 'created_date', 'guide_id', )
        write_once_fields = ('uuid', 'filename', 'created_date', 'guide_id', )

class AnnotationGuideReadSerializer(WriteOnceMixin, serializers.ModelSerializer):
    class Meta:
        model = models.AnnotationGuide
        fields = ('id', 'task_id', 'project_id', 'created_date', 'updated_date', 'markdown', )
        read_only_fields = fields

class AnnotationGuideWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    project_id = serializers.IntegerField(required=False, allow_null=True)
    task_id = serializers.IntegerField(required=False, allow_null=True)

    @transaction.atomic
    def create(self, validated_data):
        project_id = validated_data.get("project_id", None)
        task_id = validated_data.get("task_id", None)
        if project_id is None and task_id is None:
            raise serializers.ValidationError('One of project_id or task_id must be specified')
        if project_id is not None and task_id is not None:
            raise serializers.ValidationError('Both project_id and task_id must not be specified')

        project = None
        task = None
        if project_id is not None:
            try:
                project = models.Project.objects.get(id=project_id)
            except models.Project.DoesNotExist:
                raise serializers.ValidationError(f'The specified project #{project_id} does not exist.')

        if task_id is not None:
            try:
                task = models.Task.objects.get(id=task_id)
            except models.Task.DoesNotExist:
                raise serializers.ValidationError(f'The specified task #{task_id} does not exist.')
        db_data = models.AnnotationGuide.objects.create(**validated_data, project = project, task = task)
        return db_data

    class Meta:
        model = models.AnnotationGuide
        fields = ('id', 'task_id', 'project_id', 'markdown', )

class UserIdentifiersSerializer(BasicUserSerializer):
    class Meta(BasicUserSerializer.Meta):
        fields = (
            "id",
            "username",
        )


class RequestDataOperationSerializer(serializers.Serializer):
    type = serializers.CharField()
    target = serializers.ChoiceField(choices=models.RequestTarget.choices)
    project_id = serializers.IntegerField(required=False, allow_null=True)
    task_id = serializers.IntegerField(required=False, allow_null=True)
    job_id = serializers.IntegerField(required=False, allow_null=True)
    format = serializers.CharField(required=False, allow_null=True)
    function_id = serializers.CharField(required=False, allow_null=True)

    def to_representation(self, rq_job: RQJob) -> dict[str, Any]:
        parsed_rq_id: RQId = rq_job.parsed_rq_id

        base_rq_job_meta = BaseRQMeta.for_job(rq_job)
        representation = {
            "type": ":".join(
                [
                    parsed_rq_id.action,
                    parsed_rq_id.subresource or parsed_rq_id.target,
                ]
            ),
            "target": parsed_rq_id.target,
            "project_id": base_rq_job_meta.project_id,
            "task_id": base_rq_job_meta.task_id,
            "job_id": base_rq_job_meta.job_id,
        }
        if parsed_rq_id.action == RequestAction.AUTOANNOTATE:
            representation["function_id"] = LambdaRQMeta.for_job(rq_job).function_id
        elif parsed_rq_id.action in (RequestAction.IMPORT, RequestAction.EXPORT):
            representation["format"] = parsed_rq_id.format

        return representation

class RequestSerializer(serializers.Serializer):
    # SerializerMethodField is not used here to mark "status" field as required and fix schema generation.
    # Marking them as read_only leads to generating type as allOf with one reference to RequestStatus component.
    # The client generated using openapi-generator from such a schema contains wrong type like:
    # status (bool, date, datetime, dict, float, int, list, str, none_type): [optional]
    status = serializers.ChoiceField(source="get_status", choices=models.RequestStatus.choices)
    message = serializers.SerializerMethodField()
    id = serializers.CharField()
    operation = RequestDataOperationSerializer(source="*")
    progress = serializers.SerializerMethodField()
    created_date = serializers.DateTimeField(source="created_at")
    started_date = serializers.DateTimeField(
        required=False, allow_null=True, source="started_at",
    )
    finished_date = serializers.DateTimeField(
        required=False, allow_null=True, source="ended_at",
    )
    expiry_date = serializers.SerializerMethodField()
    owner = serializers.SerializerMethodField()
    result_url = serializers.URLField(required=False, allow_null=True)
    result_id = serializers.IntegerField(required=False, allow_null=True)

    def __init__(self, *args, **kwargs):
        self._base_rq_job_meta: BaseRQMeta | None = None
        super().__init__(*args, **kwargs)

    @extend_schema_field(UserIdentifiersSerializer())
    def get_owner(self, rq_job: RQJob) -> dict[str, Any]:
        assert self._base_rq_job_meta
        return UserIdentifiersSerializer(self._base_rq_job_meta.user).data

    @extend_schema_field(
        serializers.FloatField(min_value=0, max_value=1, required=False, allow_null=True)
    )
    def get_progress(self, rq_job: RQJob) -> Decimal:
        rq_job_meta = ImportRQMeta.for_job(rq_job)
        # progress of task creation is stored in "task_progress" field
        # progress of project import is stored in "progress" field
        return Decimal(rq_job_meta.progress or rq_job_meta.task_progress or 0.)

    @extend_schema_field(serializers.DateTimeField(required=False, allow_null=True))
    def get_expiry_date(self, rq_job: RQJob) -> Optional[str]:
        delta = None
        if rq_job.is_finished:
            delta = rq_job.result_ttl or rq_defaults.DEFAULT_RESULT_TTL
        elif rq_job.is_failed:
            delta = rq_job.failure_ttl or rq_defaults.DEFAULT_FAILURE_TTL

        if rq_job.ended_at and delta:
            expiry_date = rq_job.ended_at + timedelta(seconds=delta)
            return expiry_date.replace(tzinfo=timezone.utc)

        return None

    @extend_schema_field(serializers.CharField(allow_blank=True))
    def get_message(self, rq_job: RQJob) -> str:
        assert self._base_rq_job_meta
        rq_job_status = rq_job.get_status()
        message = ''

        if RQJobStatus.STARTED == rq_job_status:
            message = self._base_rq_job_meta.status or message
        elif RQJobStatus.FAILED == rq_job_status:
            message = self._base_rq_job_meta.formatted_exception or parse_exception_message(str(rq_job.exc_info or "Unknown error"))

        return message

    def to_representation(self, rq_job: RQJob) -> dict[str, Any]:
        self._base_rq_job_meta = BaseRQMeta.for_job(rq_job)
        representation = super().to_representation(rq_job)

        # FUTURE-TODO: support such statuses on UI
        if representation["status"] in (RQJobStatus.DEFERRED, RQJobStatus.SCHEDULED):
            representation["status"] = RQJobStatus.QUEUED

        if representation["status"] == RQJobStatus.FINISHED:
            if rq_job.parsed_rq_id.action == models.RequestAction.EXPORT:
                representation["result_url"] = ExportRQMeta.for_job(rq_job).result_url

            if (
                rq_job.parsed_rq_id.action == models.RequestAction.IMPORT
                and rq_job.parsed_rq_id.subresource == models.RequestSubresource.BACKUP
            ):
                representation["result_id"] = rq_job.return_value()

        return representation


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\signals.py =====
# Copyright (C) 2019-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT
import functools
import shutil

from django.conf import settings
from django.contrib.auth.models import User
from django.db import transaction
from django.db.models.signals import m2m_changed, post_delete, post_save
from django.dispatch import receiver

from .models import Asset, CloudStorage, Data, Job, Profile, Project, StatusChoice, Task

# TODO: need to log any problems reported by shutil.rmtree when the new
# analytics feature is available. Now the log system can write information
# into a file inside removed directory.


@receiver(post_save, sender=Job, dispatch_uid=__name__ + ".save_job_handler")
def __save_job_handler(instance, created, raw: bool, **kwargs):
    # no need to update task status for newly created jobs
    if created:
        return

    db_task = instance.segment.task
    db_jobs = list(Job.objects.filter(segment__task_id=db_task.id))
    status = StatusChoice.COMPLETED
    if any(db_job.status == StatusChoice.ANNOTATION for db_job in db_jobs):
        status = StatusChoice.ANNOTATION
    elif any(db_job.status == StatusChoice.VALIDATION for db_job in db_jobs):
        status = StatusChoice.VALIDATION

    if status != db_task.status:
        db_task.status = status
        db_task.save(update_fields=["status", "updated_date"])


@receiver(post_save, sender=User, dispatch_uid=__name__ + ".save_user_handler")
def __save_user_handler(instance: User, created: bool, raw: bool, **kwargs):
    if created and raw:
        return

    should_access_analytics = (
        instance.is_superuser or instance.groups.filter(name=settings.IAM_ADMIN_ROLE).exists()
    )
    if not hasattr(instance, "profile"):
        profile = Profile()
        profile.user = instance
        profile.has_analytics_access = should_access_analytics
        profile.save()
    elif should_access_analytics and not instance.profile.has_analytics_access:
        instance.profile.has_analytics_access = True
        instance.profile.save()


@receiver(
    m2m_changed,
    sender=User.groups.through,
    dispatch_uid=__name__ + ".m2m_user_groups_change_handler",
)
def __m2m_user_groups_change_handler(sender, instance: User, action: str, **kwargs):
    if action == "post_add":
        is_admin = instance.groups.filter(name=settings.IAM_ADMIN_ROLE).exists()
        if is_admin and hasattr(instance, "profile") and not instance.profile.has_analytics_access:
            instance.profile.has_analytics_access = True
            instance.profile.save()


@receiver(post_delete, sender=Project, dispatch_uid=__name__ + ".delete_project_handler")
def __delete_project_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_dirname(), ignore_errors=True)
    )


@receiver(post_delete, sender=Asset, dispatch_uid=__name__ + ".__delete_asset_handler")
def __delete_asset_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_asset_dir(), ignore_errors=True)
    )


@receiver(post_delete, sender=Task, dispatch_uid=__name__ + ".delete_task_handler")
def __delete_task_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_dirname(), ignore_errors=True)
    )

    if instance.data and not instance.data.tasks.exists():
        instance.data.delete()

    try:
        if db_project := instance.project:  # update project
            db_project.touch()
    except Project.DoesNotExist:
        pass  # probably the project has been deleted


@receiver(post_delete, sender=Job, dispatch_uid=__name__ + ".delete_job_handler")
def __delete_job_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_dirname(), ignore_errors=True)
    )


@receiver(post_delete, sender=Data, dispatch_uid=__name__ + ".delete_data_handler")
def __delete_data_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_data_dirname(), ignore_errors=True)
    )


@receiver(post_delete, sender=CloudStorage, dispatch_uid=__name__ + ".delete_cloudstorage_handler")
def __delete_cloudstorage_handler(instance, **kwargs):
    transaction.on_commit(
        functools.partial(shutil.rmtree, instance.get_storage_dirname(), ignore_errors=True)
    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\task.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import concurrent.futures
import fnmatch
import itertools
import os
import re
import shutil
from collections.abc import Iterable, Iterator, Sequence
from contextlib import closing
from copy import deepcopy
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Callable, NamedTuple, Optional, Union
from urllib import parse as urlparse
from urllib import request as urlrequest

import attrs
import av
import django_rq
import rq
from django.conf import settings
from django.db import transaction
from django.forms.models import model_to_dict
from rest_framework.serializers import ValidationError

from cvat.apps.engine import models
from cvat.apps.engine.frame_provider import TaskFrameProvider
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.media_extractors import (
    MEDIA_TYPES,
    CachingMediaIterator,
    ImageListReader,
    IMediaReader,
    Mpeg4ChunkWriter,
    Mpeg4CompressedChunkWriter,
    RandomAccessIterator,
    ValidateDimension,
    ZipChunkWriter,
    ZipCompressedChunkWriter,
    get_mime,
    load_image,
    sort,
)
from cvat.apps.engine.model_utils import bulk_create
from cvat.apps.engine.models import RequestAction, RequestTarget
from cvat.apps.engine.rq import ImportRQMeta, RQId, define_dependent_job
from cvat.apps.engine.task_validation import HoneypotFrameSelector
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import av_scan_paths, format_list, get_rq_lock_by_user, take_by
from cvat.utils.http import PROXIES_FOR_UNTRUSTED_URLS, make_requests_session
from utils.dataset_manifest import ImageManifestManager, VideoManifestManager, is_manifest
from utils.dataset_manifest.core import VideoManifestValidator, is_dataset_manifest
from utils.dataset_manifest.utils import detect_related_images

from .cloud_provider import db_storage_to_storage_instance

slogger = ServerLogManager(__name__)

############################# Low Level server API

def create(
    db_task: models.Task,
    data: models.Data,
    request: ExtendedRequest,
) -> str:
    """Schedule a background job to create a task and return that job's identifier"""
    q = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)
    user_id = request.user.id
    rq_id = RQId(RequestAction.CREATE, RequestTarget.TASK, db_task.pk).render()

    with get_rq_lock_by_user(q, user_id):
        q.enqueue_call(
            func=_create_thread,
            args=(db_task.pk, data),
            job_id=rq_id,
            meta=ImportRQMeta.build_for(request=request, db_obj=db_task),
            depends_on=define_dependent_job(q, user_id),
            failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds(),
        )

    return rq_id

############################# Internal implementation for server API

JobFileMapping = list[list[str]]

class SegmentParams(NamedTuple):
    start_frame: int
    stop_frame: int
    type: models.SegmentType = models.SegmentType.RANGE
    frames: Optional[Sequence[int]] = []

class SegmentsParams(NamedTuple):
    segments: Iterator[SegmentParams]
    segment_size: int
    overlap: int

def _copy_data_from_share_point(
    server_files: list[str],
    *,
    update_status_callback: Callable[[str], None],
    upload_dir: str,
    server_dir: str | None = None,
    server_files_exclude: list[str] | None = None,
):
    update_status_callback('Data are being copied from source..')

    filtered_server_files = server_files.copy()

    # filter data from files/directories that should be excluded
    if server_files_exclude:
        for f in server_files:
            path = Path(server_dir or settings.SHARE_ROOT) / f
            if path.is_dir():
                filtered_server_files.remove(f)
                filtered_server_files.extend([str(f / i.relative_to(path)) for i in path.glob('**/*') if i.is_file()])

        filtered_server_files = list(filter(
            lambda x: x not in server_files_exclude and all([f'{i}/' not in server_files_exclude for i in Path(x).parents]),
            filtered_server_files
        ))

    for path in filtered_server_files:
        if server_dir is None:
            source_path = os.path.join(settings.SHARE_ROOT, os.path.normpath(path))
        else:
            source_path = os.path.join(server_dir, os.path.normpath(path))
        target_path = os.path.join(upload_dir, path)
        if os.path.isdir(source_path):
            shutil.copytree(source_path, target_path)
        else:
            target_dir = os.path.dirname(target_path)
            if not os.path.exists(target_dir):
                os.makedirs(target_dir)
            shutil.copyfile(source_path, target_path)

def _generate_segment_params(
    db_task: models.Task,
    *,
    data_size: Optional[int] = None,
    job_file_mapping: Optional[JobFileMapping] = None,
) -> SegmentsParams:
    if job_file_mapping is not None:
        def _segments():
            # It is assumed here that files are already saved ordered in the task
            # Here we just need to create segments by the job sizes
            start_frame = 0
            for job_files in job_file_mapping:
                segment_size = len(job_files)
                stop_frame = start_frame + segment_size - 1
                yield SegmentParams(
                    start_frame=start_frame,
                    stop_frame=stop_frame,
                    type=models.SegmentType.RANGE,
                )

                start_frame = stop_frame + 1

        segments = _segments()
        segment_size = 0
        overlap = 0
    else:
        # The segments have equal parameters
        if data_size is None:
            data_size = db_task.data.size

        segment_size = db_task.segment_size
        if segment_size == 0 or segment_size > data_size:
            segment_size = data_size

        overlap = min(
            db_task.overlap if db_task.overlap is not None
                else 5 if db_task.mode == 'interpolation' else 0,
            segment_size // 2,
        )

        segments = (
            SegmentParams(
                start_frame=start_frame,
                stop_frame=min(start_frame + segment_size - 1, data_size - 1),
                type=models.SegmentType.RANGE
            )
            for start_frame in range(0, data_size - overlap, segment_size - overlap)
        )

    return SegmentsParams(segments, segment_size, overlap)

def _create_segments_and_jobs(
    db_task: models.Task,
    *,
    update_status_callback: Callable[[str], None],
    job_file_mapping: Optional[JobFileMapping] = None,
):
    update_status_callback('Task is being saved in database')

    segments, segment_size, overlap = _generate_segment_params(
        db_task=db_task, job_file_mapping=job_file_mapping,
    )
    db_task.segment_size = segment_size
    db_task.overlap = overlap

    for segment_idx, segment_params in enumerate(segments):
        slogger.glob.info(
            "New segment for task #{task_id}: idx = {segment_idx}, start_frame = {start_frame}, "
            "stop_frame = {stop_frame}".format(
                task_id=db_task.id, segment_idx=segment_idx, **segment_params._asdict()
            ))

        db_segment = models.Segment(task=db_task, **segment_params._asdict())
        db_segment.save()

        db_job = models.Job(segment=db_segment)
        db_job.save()
        db_job.make_dirs()

        # consensus jobs use the same `db_segment` as the regular job, thus data not duplicated in backups, exports
        for _ in range(db_task.consensus_replicas):
            consensus_db_job = models.Job(
                segment=db_segment, parent_job_id=db_job.id, type=models.JobType.CONSENSUS_REPLICA
            )
            consensus_db_job.save()
            consensus_db_job.make_dirs()

    db_task.data.save()
    db_task.save()

def _count_files(data):
    share_root = settings.SHARE_ROOT
    server_files = []

    for path in data["server_files"]:
        path = os.path.normpath(path).lstrip('/')
        if '..' in path.split(os.path.sep):
            raise ValueError("Don't use '..' inside file paths")
        full_path = os.path.abspath(os.path.join(share_root, path))
        if os.path.commonprefix([share_root, full_path]) != share_root:
            raise ValueError("Bad file path: " + path)
        server_files.append(path)

    sorted_server_files = sorted(server_files, reverse=True)
    # The idea of the code is trivial. After sort we will have files in the
    # following order: 'a/b/c/d/2.txt', 'a/b/c/d/1.txt', 'a/b/c/d', 'a/b/c'
    # Let's keep all items which aren't substrings of the previous item. In
    # the example above only 2.txt and 1.txt files will be in the final list.
    # Also need to correctly handle 'a/b/c0', 'a/b/c' case.
    without_extra_dirs = [v[1] for v in zip([""] + sorted_server_files, sorted_server_files)
        if not os.path.dirname(v[0]).startswith(v[1])]

    # we need to keep the original sequence of files
    data['server_files'] = [f for f in server_files if f in without_extra_dirs]

    def count_files(file_mapping, counter):
        for rel_path, full_path in file_mapping.items():
            mime = get_mime(full_path)
            if mime in counter:
                counter[mime].append(rel_path)
            elif rel_path.endswith('.jsonl'):
                continue
            else:
                slogger.glob.warn("Skip '{}' file (its mime type doesn't "
                    "correspond to supported MIME file type)".format(full_path))

    counter = { media_type: [] for media_type in MEDIA_TYPES.keys() }

    count_files(
        file_mapping={ f:f for f in data['remote_files'] or data['client_files']},
        counter=counter,
    )

    count_files(
        file_mapping={ f:os.path.abspath(os.path.join(share_root, f)) for f in data['server_files']},
        counter=counter,
    )

    return counter

def _find_manifest_files(data):
    manifest_files = []
    for files in ['client_files', 'server_files', 'remote_files']:
        current_manifest_files = list(filter(lambda x: x.endswith('.jsonl'), data[files]))
        if current_manifest_files:
            manifest_files.extend(current_manifest_files)
            data[files] = [f for f in data[files] if f not in current_manifest_files]
    return manifest_files

def _validate_data(counter, manifest_files=None):
    unique_entries = 0
    multiple_entries = 0
    for media_type, media_config in MEDIA_TYPES.items():
        if counter[media_type]:
            if media_config['unique']:
                unique_entries += len(counter[media_type])
            else:
                multiple_entries += len(counter[media_type])

            if manifest_files and media_type not in ('video', 'image', 'zip', 'archive'):
                raise Exception(
                    'File with meta information can only be uploaded with video/images/archives'
                )

    if unique_entries == 1 and multiple_entries > 0 or unique_entries > 1:
        unique_types = ', '.join([k for k, v in MEDIA_TYPES.items() if v['unique']])
        multiply_types = ', '.join([k for k, v in MEDIA_TYPES.items() if not v['unique']])
        count = ', '.join(['{} {}(s)'.format(len(v), k) for k, v in counter.items()])
        raise ValueError('Only one {} or many {} can be used simultaneously, \
            but {} found.'.format(unique_types, multiply_types, count))

    if unique_entries == 0 and multiple_entries == 0:
        raise ValueError('No media data found')

    task_modes = [MEDIA_TYPES[media_type]['mode'] for media_type, media_files in counter.items() if media_files]

    if not all(mode == task_modes[0] for mode in task_modes):
        raise Exception('Could not combine different task modes for data')

    return counter, task_modes[0]

def _validate_job_file_mapping(
    db_task: models.Task, data: dict[str, Any]
) -> Optional[JobFileMapping]:
    job_file_mapping = data.get('job_file_mapping', None)

    if job_file_mapping is None:
        return None

    if not list(itertools.chain.from_iterable(job_file_mapping)):
        raise ValidationError("job_file_mapping cannot be empty")

    if db_task.segment_size:
        raise ValidationError("job_file_mapping cannot be used with segment_size")

    if (data.get('sorting_method', db_task.data.sorting_method)
        != models.SortingMethod.LEXICOGRAPHICAL
    ):
        raise ValidationError("job_file_mapping cannot be used with sorting_method")

    if data.get('start_frame', db_task.data.start_frame):
        raise ValidationError("job_file_mapping cannot be used with start_frame")

    if data.get('stop_frame', db_task.data.stop_frame):
        raise ValidationError("job_file_mapping cannot be used with stop_frame")

    if data.get('frame_filter', db_task.data.frame_filter):
        raise ValidationError("job_file_mapping cannot be used with frame_filter")

    if db_task.data.get_frame_step() != 1:
        raise ValidationError("job_file_mapping cannot be used with frame step")

    if data.get('filename_pattern'):
        raise ValidationError("job_file_mapping cannot be used with filename_pattern")

    if data.get('server_files_exclude'):
        raise ValidationError("job_file_mapping cannot be used with server_files_exclude")

    return job_file_mapping

def _validate_validation_params(
    db_task: models.Task, data: dict[str, Any], *, is_backup_restore: bool = False
) -> Optional[dict[str, Any]]:
    params = data.get('validation_params', {})
    if not params:
        return None

    if (
        params['mode'] == models.ValidationMode.GT and
        params['frame_selection_method'] == models.JobFrameSelectionMethod.RANDOM_PER_JOB and
        (frames_per_job := params.get('frames_per_job_count')) and
        db_task.segment_size <= frames_per_job
    ):
        raise ValidationError("Validation frame count per job cannot be greater than segment size")

    if params['mode'] != models.ValidationMode.GT_POOL:
        return params

    if (
        data.get('sorting_method', db_task.data.sorting_method) != models.SortingMethod.RANDOM and
        not is_backup_restore
    ):
        raise ValidationError('validation mode "{}" can only be used with "{}" sorting'.format(
            models.ValidationMode.GT_POOL.value,
            models.SortingMethod.RANDOM.value,
        ))

    for incompatible_key in ['job_file_mapping', 'overlap']:
        if incompatible_key == 'job_file_mapping' and is_backup_restore:
            continue

        if data.get(incompatible_key):
            raise ValidationError('validation mode "{}" cannot be used with "{}"'.format(
                models.ValidationMode.GT_POOL.value,
                incompatible_key,
            ))

    return params

def _validate_manifest(
    manifests: list[str],
    root_dir: Optional[str],
    *,
    is_in_cloud: bool,
    db_cloud_storage: Optional[Any],
) -> Optional[str]:
    if not manifests:
        return None

    if len(manifests) != 1:
        raise ValidationError('Only one manifest file can be attached to data')
    manifest_file = manifests[0]
    full_manifest_path = os.path.join(root_dir, manifests[0])

    if is_in_cloud:
        cloud_storage_instance = db_storage_to_storage_instance(db_cloud_storage)
        # check that cloud storage manifest file exists and is up to date
        if not os.path.exists(full_manifest_path) or (
            datetime.fromtimestamp(os.path.getmtime(full_manifest_path), tz=timezone.utc) \
                < cloud_storage_instance.get_file_last_modified(manifest_file)
        ):
            cloud_storage_instance.download_file(manifest_file, full_manifest_path)

    if not is_manifest(full_manifest_path):
        raise ValidationError('Invalid manifest was uploaded')

    return manifest_file

def _validate_scheme(url):
    ALLOWED_SCHEMES = ['http', 'https']

    parsed_url = urlparse.urlparse(url)

    if parsed_url.scheme not in ALLOWED_SCHEMES:
        raise ValueError('Unsupported URL scheme: {}. Only http and https are supported'.format(parsed_url.scheme))

def _download_data(
    urls: Iterable[str],
    upload_dir: str,
    *,
    update_status_callback: Callable[[str], None],
):
    local_files = {}

    with make_requests_session() as session:
        for url in urls:
            name = os.path.basename(urlrequest.url2pathname(urlparse.urlparse(url).path))
            if name in local_files:
                raise Exception("filename collision: {}".format(name))
            _validate_scheme(url)
            slogger.glob.info("Downloading: {}".format(url))
            update_status_callback('{} is being downloaded..'.format(url))

            response = session.get(url, stream=True, proxies=PROXIES_FOR_UNTRUSTED_URLS)
            if response.status_code == 200:
                response.raw.decode_content = True
                with open(os.path.join(upload_dir, name), 'wb') as output_file:
                    shutil.copyfileobj(response.raw, output_file)
            else:
                error_message = f"Failed to download {response.url}"
                if url != response.url:
                    error_message += f" (redirected from {url})"

                if response.status_code == 407:
                    error_message += "; likely attempt to access internal host"
                elif response.status_code:
                    error_message += f"; HTTP error {response.status_code}"

                raise Exception(error_message)

            local_files[name] = True

    return list(local_files.keys())

def _download_data_from_cloud_storage(
    db_storage: models.CloudStorage,
    files: list[str],
    upload_dir: str,
):
    cloud_storage_instance = db_storage_to_storage_instance(db_storage)
    cloud_storage_instance.bulk_download_to_dir(files, upload_dir)

def _get_manifest_frame_indexer(start_frame=0, frame_step=1):
    return lambda frame_id: start_frame + frame_id * frame_step

def _read_dataset_manifest(path: str, *, create_index: bool = False) -> ImageManifestManager:
    """
    Reads an upload manifest file
    """

    if not is_dataset_manifest(path):
        raise ValidationError(
            "Can't recognize a dataset manifest file in "
            "the uploaded file '{}'".format(os.path.basename(path))
        )

    return ImageManifestManager(path, create_index=create_index)

def _restore_file_order_from_manifest(
    extractor: ImageListReader, manifest: ImageManifestManager, upload_dir: str
) -> list[str]:
    """
    Restores file ordering for the "predefined" file sorting method of the task creation.
    Checks for extra files in the input.
    Read more: https://github.com/cvat-ai/cvat/issues/5061
    """

    input_files = {os.path.relpath(p, upload_dir): p for p in extractor.absolute_source_paths}
    manifest_files = list(manifest.data)

    mismatching_files = list(input_files.keys() ^ manifest_files)
    if mismatching_files:
        DISPLAY_ENTRIES_COUNT = 5
        mismatching_display = [
            fn + (" (upload)" if fn in input_files else " (manifest)")
            for fn in mismatching_files[:DISPLAY_ENTRIES_COUNT]
        ]
        remaining_count = len(mismatching_files) - DISPLAY_ENTRIES_COUNT
        raise FileNotFoundError(
            "Uploaded files do no match the upload manifest file contents. "
            "Please check the upload manifest file contents and the list of uploaded files. "
            "Mismatching files: {}{}. "
            "Read more: https://docs.cvat.ai/docs/manual/advanced/dataset_manifest/"
            .format(
                ", ".join(mismatching_display),
                f" (and {remaining_count} more). " if 0 < remaining_count else ""
            )
        )

    return [input_files[fn] for fn in manifest_files]

def _create_task_manifest_based_on_cloud_storage_manifest(
    sorted_media: list[str],
    cloud_storage_manifest_prefix: str,
    cloud_storage_manifest: ImageManifestManager,
    manifest: ImageManifestManager,
) -> None:
    if cloud_storage_manifest_prefix:
        sorted_media_without_manifest_prefix = [
            os.path.relpath(i, cloud_storage_manifest_prefix) for i in sorted_media
        ]
        sequence, raw_content = cloud_storage_manifest.get_subset(sorted_media_without_manifest_prefix)
        def _add_prefix(properties):
            file_name = properties['name']
            properties['name'] = os.path.join(cloud_storage_manifest_prefix, file_name)
            return properties
        content = list(map(_add_prefix, raw_content))
    else:
        sequence, content = cloud_storage_manifest.get_subset(sorted_media)
    if not content:
        raise ValidationError('There is no intersection of the files specified'
                            'in the request with the contents of the bucket')
    sorted_content = (i[1] for i in sorted(zip(sequence, content)))
    manifest.create(sorted_content)

def _create_task_manifest_from_cloud_data(
    db_storage: models.CloudStorage,
    sorted_media: list[str],
    manifest: ImageManifestManager,
    dimension: models.DimensionType = models.DimensionType.DIM_2D,
    *,
    stop_frame: Optional[int] = None,
) -> None:
    if stop_frame is None:
        stop_frame = len(sorted_media) - 1
    cloud_storage_instance = db_storage_to_storage_instance(db_storage)
    content_generator = cloud_storage_instance.bulk_download_to_memory(sorted_media)

    manifest.link(
        sources=content_generator,
        DIM_3D=dimension == models.DimensionType.DIM_3D,
        stop=stop_frame,
    )
    manifest.create()

@transaction.atomic
def _create_thread(
    db_task: Union[int, models.Task],
    data: dict[str, Any],
    *,
    is_backup_restore: bool = False,
    is_dataset_import: bool = False,
) -> None:
    if isinstance(db_task, int):
        db_task = models.Task.objects.select_for_update().get(pk=db_task)

    slogger.glob.info("create task #{}".format(db_task.id))

    job = rq.get_current_job()
    rq_job_meta = ImportRQMeta.for_job(job)

    def update_status(msg: str) -> None:
        rq_job_meta.status = msg
        rq_job_meta.save()

    job_file_mapping = _validate_job_file_mapping(db_task, data)

    validation_params = _validate_validation_params(
        db_task, data, is_backup_restore=is_backup_restore
    )

    db_data = db_task.data
    upload_dir = db_data.get_upload_dirname() if db_data.storage != models.StorageChoice.SHARE else settings.SHARE_ROOT
    is_data_in_cloud = db_data.storage == models.StorageChoice.CLOUD_STORAGE

    if data['remote_files'] and not is_dataset_import:
        data['remote_files'] = _download_data(data['remote_files'], upload_dir, update_status_callback=update_status)

    # find and validate manifest file
    manifest_files = _find_manifest_files(data)
    manifest_root = None

    # we should also handle this case because files from the share source have not been downloaded yet
    if data['copy_data']:
        manifest_root = settings.SHARE_ROOT
    elif db_data.storage in {models.StorageChoice.LOCAL, models.StorageChoice.SHARE}:
        manifest_root = upload_dir
    elif is_data_in_cloud:
        manifest_root = db_data.cloud_storage.get_storage_dirname()
    else:
        assert False, f"Unknown file storage {db_data.storage}"

    if (
        db_data.storage_method == models.StorageMethodChoice.FILE_SYSTEM and
        not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE
    ):
        db_data.storage_method = models.StorageMethodChoice.CACHE

    manifest_file = _validate_manifest(
        manifest_files,
        manifest_root,
        is_in_cloud=is_data_in_cloud,
        db_cloud_storage=db_data.cloud_storage if is_data_in_cloud else None,
    )

    manifest = None
    if is_data_in_cloud:
        cloud_storage_instance = db_storage_to_storage_instance(db_data.cloud_storage)

        if manifest_file:
            cloud_storage_manifest = ImageManifestManager(
                os.path.join(db_data.cloud_storage.get_storage_dirname(), manifest_file),
                db_data.cloud_storage.get_storage_dirname()
            )
            cloud_storage_manifest.set_index()
            cloud_storage_manifest_prefix = os.path.dirname(manifest_file)

        if manifest_file and not data['server_files'] and not data['filename_pattern']: # only manifest file was specified in server files by the user
            data['filename_pattern'] = '*'

        # update the server_files list with files from the specified directories
        if (dirs:= list(filter(lambda x: x.endswith('/'), data['server_files']))):
            copy_of_server_files = data['server_files'].copy()
            copy_of_dirs = dirs.copy()
            additional_files = []
            if manifest_file:
                for directory in dirs:
                    if cloud_storage_manifest_prefix:
                        # cloud_storage_manifest_prefix is a dirname of manifest, it doesn't end with a slash
                        directory = directory[len(cloud_storage_manifest_prefix) + 1:]
                    additional_files.extend(
                        list(
                            map(
                                lambda x: x[1].full_name,
                                filter(lambda x: x[1].full_name.startswith(directory), cloud_storage_manifest)
                            )
                        ) if directory else [x[1].full_name for x in cloud_storage_manifest]
                    )
                if cloud_storage_manifest_prefix:
                    additional_files = [os.path.join(cloud_storage_manifest_prefix, f) for f in additional_files]
            else:
                while len(dirs):
                    directory = dirs.pop()
                    for f in cloud_storage_instance.list_files(prefix=directory, _use_flat_listing=True):
                        if f['type'] == 'REG':
                            additional_files.append(f['name'])
                        else:
                            dirs.append(f['name'])

            data['server_files'] = []
            for f in copy_of_server_files:
                if f not in copy_of_dirs:
                    data['server_files'].append(f)
                else:
                    data['server_files'].extend(list(filter(lambda x: x.startswith(f), additional_files)))

            del additional_files

        if server_files_exclude := data.get('server_files_exclude'):
            data['server_files'] = list(filter(
                lambda x: x not in server_files_exclude and all([f'{i}/' not in server_files_exclude for i in Path(x).parents]),
                data['server_files']
            ))

        # update list with server files if task creation approach with pattern and manifest file is used
        if data['filename_pattern']:
            additional_files = []

            if not manifest_file:
                # NOTE: we cannot list files with specified pattern on the providers page because they don't provide such function
                dirs = []
                prefix = ""

                while True:
                    for f in cloud_storage_instance.list_files(prefix=prefix, _use_flat_listing=True):
                        if f['type'] == 'REG':
                            additional_files.append(f['name'])
                        else:
                            dirs.append(f['name'])
                    if not dirs:
                        break
                    prefix = dirs.pop()

                if not data['filename_pattern'] == '*':
                    additional_files = fnmatch.filter(additional_files, data['filename_pattern'])
            else:
                additional_files = list(cloud_storage_manifest.data) if not cloud_storage_manifest_prefix \
                    else [os.path.join(cloud_storage_manifest_prefix, f) for f in cloud_storage_manifest.data]
                if not data['filename_pattern'] == '*':
                    additional_files = fnmatch.filter(additional_files, data['filename_pattern'])

            data['server_files'].extend(additional_files)

        if cloud_storage_instance.prefix:
            # filter server_files based on default prefix
            data['server_files'] = list(filter(lambda x: x.startswith(cloud_storage_instance.prefix), data['server_files']))

        # We only need to process the files specified in job_file_mapping
        if job_file_mapping is not None:
            filtered_files = []
            for f in itertools.chain.from_iterable(job_file_mapping):
                if f not in data['server_files']:
                    raise ValidationError(f"Job mapping file {f} is not specified in input files")
                filtered_files.append(f)
            data['server_files'] = filtered_files

    # count and validate uploaded files
    media = _count_files(data)
    media, task_mode = _validate_data(media, manifest_files)
    is_media_sorted = False

    if is_data_in_cloud:
        if (
            # Download remote data if local storage is requested
            # TODO: maybe move into cache building to fail faster on invalid task configurations
            db_data.storage_method == models.StorageMethodChoice.FILE_SYSTEM or

            # Packed media must be downloaded for task creation
            any(v for k, v in media.items() if k != 'image')
        ):
            update_status("Downloading input media")

            filtered_data = []
            for files in (i for i in media.values() if i):
                filtered_data.extend(files)
            media_to_download = filtered_data

            if media['image']:
                start_frame = db_data.start_frame
                stop_frame = len(filtered_data) - 1
                if data['stop_frame'] is not None:
                    stop_frame = min(stop_frame, data['stop_frame'])

                step = db_data.get_frame_step()
                if start_frame or step != 1 or stop_frame != len(filtered_data) - 1:
                    media_to_download = filtered_data[start_frame : stop_frame + 1: step]

            _download_data_from_cloud_storage(db_data.cloud_storage, media_to_download, upload_dir)
            del media_to_download
            del filtered_data

            is_data_in_cloud = False
            db_data.storage = models.StorageChoice.LOCAL
        else:
            manifest = ImageManifestManager(db_data.get_manifest_path())

    if job_file_mapping is not None and task_mode != 'annotation':
        raise ValidationError("job_file_mapping can't be used with sequence-based data like videos")

    if data['server_files']:
        if db_data.storage == models.StorageChoice.LOCAL and not db_data.cloud_storage:
            # this means that the data has not been downloaded from the storage to the host
            _copy_data_from_share_point(
                (data['server_files'] + [manifest_file]) if manifest_file else data['server_files'],
                upload_dir=upload_dir,
                server_dir=data.get('server_files_path'),
                server_files_exclude=data.get('server_files_exclude'),
                update_status_callback=update_status,
            )
            manifest_root = upload_dir
        elif is_data_in_cloud:
            # we should sort media before sorting in the extractor because the manifest structure should match to the sorted media
            if job_file_mapping is not None:
                sorted_media = list(itertools.chain.from_iterable(job_file_mapping))
            else:
                sorted_media = sort(media['image'], data['sorting_method'])
                media['image'] = sorted_media
            is_media_sorted = True

            if manifest_file:
                # Define task manifest content based on cloud storage manifest content and uploaded files
                _create_task_manifest_based_on_cloud_storage_manifest(
                    sorted_media, cloud_storage_manifest_prefix,
                    cloud_storage_manifest, manifest)
            else: # without manifest file but with use_cache option
                # Define task manifest content based on list with uploaded files
                _create_task_manifest_from_cloud_data(db_data.cloud_storage, sorted_media, manifest)

    av_scan_paths(upload_dir)

    update_status('Media files are being extracted...')

    # If upload from server_files image and directories
    # need to update images list by all found images in directories
    if (data['server_files']) and len(media['directory']) and len(media['image']):
        media['image'].extend(
            [os.path.relpath(image, upload_dir) for image in
                MEDIA_TYPES['directory']['extractor'](
                    source_path=[os.path.join(upload_dir, f) for f in media['directory']],
                ).absolute_source_paths
            ]
        )
        media['directory'] = []

    if (not is_backup_restore and manifest_file and
        data['sorting_method'] == models.SortingMethod.RANDOM
    ):
        raise ValidationError("It isn't supported to upload manifest file and use random sorting")

    if (is_backup_restore and db_data.storage_method == models.StorageMethodChoice.FILE_SYSTEM and
        data['sorting_method'] in {models.SortingMethod.RANDOM, models.SortingMethod.PREDEFINED}
    ):
        raise ValidationError(
            "It isn't supported to import the task that was created "
            "without cache but with random/predefined sorting"
        )

    # Extract input data
    extractor: Optional[IMediaReader] = None
    manifest_index = _get_manifest_frame_indexer()
    for media_type, media_files in media.items():
        if not media_files:
            continue

        if extractor is not None:
            raise ValidationError('Combined data types are not supported')

        if (is_dataset_import or is_backup_restore) and media_type == 'image' and db_data.storage == models.StorageChoice.SHARE:
            manifest_index = _get_manifest_frame_indexer(db_data.start_frame, db_data.get_frame_step())
            db_data.start_frame = 0
            data['stop_frame'] = None
            db_data.frame_filter = ''

        source_paths = [os.path.join(upload_dir, f) for f in media_files]

        details = {
            'source_path': source_paths,
            'step': db_data.get_frame_step(),
            'start': db_data.start_frame,
            'stop': data['stop_frame'],
        }
        if media_type in {'archive', 'zip', 'pdf'} and db_data.storage == models.StorageChoice.SHARE:
            details['extract_dir'] = db_data.get_upload_dirname()
            upload_dir = db_data.get_upload_dirname()
            db_data.storage = models.StorageChoice.LOCAL
        if media_type != 'video':
            details['sorting_method'] = data['sorting_method'] if not is_media_sorted else models.SortingMethod.PREDEFINED

        extractor = MEDIA_TYPES[media_type]['extractor'](**details)

    if extractor is None:
        raise ValidationError("Can't create a task without data")

    # filter server_files from server_files_exclude when share point is used and files are not copied to CVAT.
    # here we exclude the case when the files are copied to CVAT because files are already filtered out.
    if (
        (server_files_exclude := data.get('server_files_exclude')) and
        data['server_files'] and
        not is_data_in_cloud and
        not data['copy_data'] and
        isinstance(extractor, MEDIA_TYPES['image']['extractor'])
    ):
        extractor.filter(
            lambda x: os.path.relpath(x, upload_dir) not in server_files_exclude and \
                all([f'{i}/' not in server_files_exclude for i in Path(x).relative_to(upload_dir).parents])
        )

    validate_dimension = ValidateDimension()
    if isinstance(extractor, MEDIA_TYPES['zip']['extractor']):
        extractor.extract()

    validate_dimension = ValidateDimension()
    if db_data.storage == models.StorageChoice.LOCAL or (
        db_data.storage == models.StorageChoice.SHARE and
        isinstance(extractor, MEDIA_TYPES['zip']['extractor'])
    ):
        validate_dimension.set_path(upload_dir)
        validate_dimension.validate()

    if (db_task.project is not None and
        db_task.project.tasks.count() > 1 and
        db_task.project.tasks.first().dimension != validate_dimension.dimension
    ):
        raise ValidationError(
            f"Dimension ({validate_dimension.dimension}) of the task must be the "
            f"same as other tasks in project ({db_task.project.tasks.first().dimension})"
        )

    if validate_dimension.dimension == models.DimensionType.DIM_3D:
        db_task.dimension = models.DimensionType.DIM_3D

        keys_of_related_files = validate_dimension.related_files.keys()
        absolute_keys_of_related_files = [os.path.join(upload_dir, f) for f in keys_of_related_files]
        # When a task is created, the sorting method can be random and in this case, reinitialization will be with correct sorting
        # but when a task is restored from a backup, a random sorting is changed to predefined and we need to manually sort files
        # in the correct order.
        source_files = absolute_keys_of_related_files if not is_backup_restore else \
            [item for item in extractor.absolute_source_paths if item in absolute_keys_of_related_files]
        extractor.reconcile(
            source_files=source_files,
            step=db_data.get_frame_step(),
            start=db_data.start_frame,
            stop=data['stop_frame'],
            dimension=models.DimensionType.DIM_3D,
        )

    related_images = {}
    if isinstance(extractor, MEDIA_TYPES['image']['extractor']):
        extractor.filter(lambda x: not re.search(r'(^|{0})related_images{0}'.format(os.sep), x))
        related_images = detect_related_images(extractor.absolute_source_paths, upload_dir)

    if validate_dimension.dimension != models.DimensionType.DIM_3D and (
        (
            not isinstance(extractor, MEDIA_TYPES['video']['extractor']) and
            is_backup_restore and
            db_data.storage_method == models.StorageMethodChoice.CACHE and
            db_data.sorting_method in {models.SortingMethod.RANDOM, models.SortingMethod.PREDEFINED}
        ) or (
            not is_dataset_import and
            not is_backup_restore and
            data['sorting_method'] == models.SortingMethod.PREDEFINED and (
                # Sorting with manifest is required for zip
                isinstance(extractor, MEDIA_TYPES['zip']['extractor']) or

                # Sorting with manifest is optional for non-video
                (manifest_file or manifest) and
                not isinstance(extractor, MEDIA_TYPES['video']['extractor'])
            )
        )
    ) or job_file_mapping:
        # We should sort media_files according to the manifest content sequence
        # and we should do this in general after validation step for 3D data
        # and after filtering from related_images
        if job_file_mapping:
            sorted_media_files = itertools.chain.from_iterable(job_file_mapping)

        else:
            if manifest is None:
                if not manifest_file or not os.path.isfile(os.path.join(manifest_root, manifest_file)):
                    raise FileNotFoundError(
                        "Can't find upload manifest file '{}' "
                        "in the uploaded files. When the 'predefined' sorting method is used, "
                        "this file is required in the input files. "
                        "Read more: https://docs.cvat.ai/docs/manual/advanced/dataset_manifest/"
                        .format(manifest_file or os.path.basename(db_data.get_manifest_path()))
                    )

                manifest = _read_dataset_manifest(os.path.join(manifest_root, manifest_file),
                    create_index=manifest_root.startswith(db_data.get_upload_dirname())
                )

            sorted_media_files = _restore_file_order_from_manifest(extractor, manifest, upload_dir)

        sorted_media_files = [os.path.join(upload_dir, fn) for fn in sorted_media_files]

        # validate the sorting
        for file_path in sorted_media_files:
            if not file_path in extractor:
                raise ValidationError(
                    f"Can't find file '{os.path.basename(file_path)}' in the input files"
                )

        media_files = sorted_media_files.copy()
        del sorted_media_files

        data['sorting_method'] = models.SortingMethod.PREDEFINED
        extractor.reconcile(
            source_files=media_files,
            step=db_data.get_frame_step(),
            start=db_data.start_frame,
            stop=data['stop_frame'],
            sorting_method=data['sorting_method'],
        )

    db_task.mode = task_mode
    db_data.compressed_chunk_type = models.DataChoice.VIDEO if task_mode == 'interpolation' and not data['use_zip_chunks'] else models.DataChoice.IMAGESET
    db_data.original_chunk_type = models.DataChoice.VIDEO if task_mode == 'interpolation' else models.DataChoice.IMAGESET

    # calculate chunk size if it isn't specified
    if db_data.chunk_size is None:
        if db_data.compressed_chunk_type == models.DataChoice.IMAGESET:
            first_image_idx = db_data.start_frame
            if not is_data_in_cloud:
                w, h = extractor.get_image_size(first_image_idx)
            else:
                img_properties = manifest[first_image_idx]
                w, h = img_properties['width'], img_properties['height']
            area = h * w
            db_data.chunk_size = max(2, min(72, 36 * 1920 * 1080 // area))
        else:
            db_data.chunk_size = 36

    # TODO: try to pull up
    # replace manifest file (e.g was uploaded 'subdir/manifest.jsonl' or 'some_manifest.jsonl')
    if (manifest_file and not os.path.exists(db_data.get_manifest_path())):
        shutil.copyfile(os.path.join(manifest_root, manifest_file),
            db_data.get_manifest_path())
        if manifest_root and manifest_root.startswith(db_data.get_upload_dirname()):
            os.remove(os.path.join(manifest_root, manifest_file))
        manifest_file = os.path.relpath(db_data.get_manifest_path(), upload_dir)

    # Create task frames from the metadata collected
    video_path: str = ""
    video_frame_size: tuple[int, int] = (0, 0)

    images: list[models.Image] = []

    for media_type, media_files in media.items():
        if not media_files:
            continue

        if task_mode == MEDIA_TYPES['video']['mode']:
            if manifest_file:
                try:
                    update_status('Validating the input manifest file')

                    manifest = VideoManifestValidator(
                        source_path=os.path.join(upload_dir, media_files[0]),
                        manifest_path=db_data.get_manifest_path()
                    )
                    manifest.init_index()
                    manifest.validate_seek_key_frames()

                    if not len(manifest):
                        raise ValidationError("No key frames found in the manifest")

                except Exception as ex:
                    manifest.remove()
                    manifest = None

                    if isinstance(ex, (ValidationError, AssertionError)):
                        base_msg = f"Invalid manifest file was uploaded: {ex}"
                    else:
                        base_msg = "Failed to parse the uploaded manifest file"
                        slogger.glob.warning(ex, exc_info=True)

                    update_status(base_msg)
            else:
                manifest = None

            if not manifest:
                try:
                    update_status('Preparing a manifest file')

                    # TODO: maybe generate manifest in a temp directory
                    manifest = VideoManifestManager(db_data.get_manifest_path())
                    manifest.link(
                        media_file=media_files[0],
                        upload_dir=upload_dir,
                        chunk_size=db_data.chunk_size, # TODO: why it's needed here?
                        force=True
                    )
                    manifest.create()

                    update_status('A manifest has been created')

                except Exception as ex:
                    manifest.remove()
                    manifest = None

                    if isinstance(ex, AssertionError):
                        base_msg = f": {ex}"
                    else:
                        base_msg = ""
                        slogger.glob.warning(ex, exc_info=True)

                    update_status(
                        f"Failed to create manifest for the uploaded video{base_msg}. "
                        "A manifest will not be used in this task"
                    )

            if manifest:
                video_frame_count = manifest.video_length
                video_frame_size = manifest.video_resolution
            else:
                video_frame_count = extractor.get_frame_count()
                video_frame_size = extractor.get_image_size(0)

            db_data.size = len(range(
                db_data.start_frame,
                min(
                    data['stop_frame'] + 1 if data['stop_frame'] else video_frame_count,
                    video_frame_count,
                ),
                db_data.get_frame_step()
            ))
            video_path = os.path.join(upload_dir, media_files[0])
        else: # images, archive, pdf
            db_data.size = len(extractor)

            manifest = ImageManifestManager(db_data.get_manifest_path())
            if not manifest.exists:
                manifest.link(
                    sources=extractor.absolute_source_paths,
                    meta={
                        k: {'related_images': related_images[k] }
                        for k in related_images
                    },
                    data_dir=upload_dir,
                    DIM_3D=(db_task.dimension == models.DimensionType.DIM_3D),
                )
                manifest.create()
            else:
                manifest.init_index()

            for frame_id in extractor.frame_range:
                image_path = extractor.get_path(frame_id)
                image_size = None

                if manifest:
                    image_info = manifest[manifest_index(frame_id)]

                    # check mapping
                    if not image_path.endswith(f"{image_info['name']}{image_info['extension']}"):
                        raise ValidationError('Incorrect file mapping to manifest content')

                    if db_task.dimension == models.DimensionType.DIM_2D and (
                        image_info.get('width') is not None and
                        image_info.get('height') is not None
                    ):
                        image_size = (image_info['width'], image_info['height'])
                    elif is_data_in_cloud:
                        raise ValidationError(
                            "Can't find image '{}' width or height info in the manifest"
                            .format(f"{image_info['name']}{image_info['extension']}")
                        )

                if not image_size:
                    image_size = extractor.get_image_size(frame_id)

                images.append(
                    models.Image(
                        data=db_data,
                        path=os.path.relpath(image_path, upload_dir),
                        frame=frame_id,
                        width=image_size[0],
                        height=image_size[1],
                    )
                )

    # TODO: refactor
    # Prepare jobs
    if validation_params and (
        validation_params['mode'] == models.ValidationMode.GT_POOL and is_backup_restore
    ):
        # Validation frames must be in the end of the images list. Collect their ids
        frame_idx_map: dict[str, int] = {}
        for i, frame_filename in enumerate(validation_params['frames']):
            image = images[-len(validation_params['frames']) + i]
            assert frame_filename == image.path
            frame_idx_map[image.path] = image.frame

        # Store information about the real frame placement in validation frames in jobs
        for image in images[:-len(validation_params['frames'])]:
            real_frame = frame_idx_map.get(image.path)
            if real_frame is not None:
                image.is_placeholder = True
                image.real_frame = real_frame

        # Exclude the previous GT job from the list of jobs to be created with normal segments
        # It must be the last one
        assert job_file_mapping[-1] == validation_params['frames']
        job_file_mapping.pop(-1)

        db_data.update_validation_layout(models.ValidationLayout(
            mode=models.ValidationMode.GT_POOL,
            frames=list(frame_idx_map.values()),
            frames_per_job_count=validation_params["frames_per_job_count"],
        ))
    elif validation_params and validation_params['mode'] == models.ValidationMode.GT_POOL:
        if db_task.mode != 'annotation':
            raise ValidationError(
                f"validation mode '{models.ValidationMode.GT_POOL}' can only be used "
                "with 'annotation' mode tasks"
            )

        # 1. select pool frames
        all_frames = range(len(images))

        # The RNG backend must not change to yield reproducible frame picks,
        # so here we specify it explicitly
        from numpy import random
        seed = validation_params.get("random_seed")
        rng = random.Generator(random.MT19937(seed=seed))

        # Sort the images to be able to create reproducible results
        images = sort(images, sorting_method=models.SortingMethod.NATURAL, func=lambda i: i.path)
        for i, image in enumerate(images):
            image.frame = i

        pool_frames: list[int] = []
        match validation_params["frame_selection_method"]:
            case models.JobFrameSelectionMethod.RANDOM_UNIFORM:
                if frame_count := validation_params.get("frame_count"):
                    if len(images) <= frame_count:
                        raise ValidationError(
                            f"The number of validation frames requested ({frame_count}) "
                            f"must be less than the number of task frames ({len(images)})"
                        )
                elif frame_share := validation_params.get("frame_share"):
                    frame_count = max(1, int(len(images) * frame_share))
                else:
                    raise ValidationError("The number of validation frames is not specified")

                pool_frames = rng.choice(
                    all_frames, size=frame_count, shuffle=False, replace=False
                ).tolist()
            case models.JobFrameSelectionMethod.MANUAL:
                known_frame_names = {frame.path: frame.frame for frame in images}
                unknown_requested_frames = []
                for frame_filename in validation_params["frames"]:
                    frame_id = known_frame_names.get(frame_filename)
                    if frame_id is None:
                        unknown_requested_frames.append(frame_filename)
                        continue

                    pool_frames.append(frame_id)

                if unknown_requested_frames:
                    raise ValidationError("Unknown validation frames requested: {}".format(
                        format_list(sorted(unknown_requested_frames)))
                    )
            case _:
                assert False

        if len(all_frames) - len(pool_frames) < 1:
            raise ValidationError(
                "Cannot create task: "
                "too few non-honeypot frames left after selecting validation frames"
            )

        # Even though the sorting is random overall,
        # it's convenient to be able to reasonably navigate in the GT job
        pool_frames = sort(
            pool_frames,
            sorting_method=models.SortingMethod.NATURAL,
            func=lambda frame: images[frame].path,
        )

        # 2. distribute pool frames
        if frames_per_job_count := validation_params.get("frames_per_job_count"):
            if len(pool_frames) < frames_per_job_count and validation_params.get("frame_count"):
                raise ValidationError(
                    f"The requested number of validation frames per job ({frames_per_job_count}) "
                    f"is greater than the validation pool size ({len(pool_frames)})"
                )
        elif frames_per_job_share := validation_params.get("frames_per_job_share"):
            frames_per_job_count = max(1, int(frames_per_job_share * db_task.segment_size))
        else:
            raise ValidationError("The number of validation frames is not specified")

        frames_per_job_count = min(len(pool_frames), frames_per_job_count)

        non_pool_frames = sorted(
            # set() doesn't guarantee ordering,
            # so sort additionally before shuffling to make results reproducible
            set(all_frames).difference(pool_frames)
        )
        rng.shuffle(non_pool_frames)

        validation_frame_counts = {f: 0 for f in pool_frames}
        frame_selector = HoneypotFrameSelector(validation_frame_counts, rng=rng)

        # Don't use the same rng as for frame ordering to simplify random_seed maintenance in future
        # We still use the same seed, but in this case the frame selection rng is separate
        # from job frame ordering rng
        job_frame_ordering_rng = random.Generator(random.MT19937(seed=seed))

        # Allocate frames for jobs
        job_file_mapping: JobFileMapping = []
        new_db_images: list[models.Image] = []
        validation_frames: list[int] = []
        frame_idx_map: dict[int, int] = {} # new to original id
        for job_frames in take_by(non_pool_frames, chunk_size=db_task.segment_size or db_data.size):
            job_validation_frames = list(frame_selector.select_next_frames(frames_per_job_count))
            job_frames += job_validation_frames

            job_frame_ordering_rng.shuffle(job_frames)

            job_images = []
            for job_frame in job_frames:
                # Insert placeholder frames into the frame sequence and shift frame ids
                image = images[job_frame]
                image = models.Image(
                    data=db_data, **deepcopy(model_to_dict(image, exclude=["data"]))
                )
                image.frame = len(new_db_images)

                if job_frame in job_validation_frames:
                    image.is_placeholder = True
                    image.real_frame = job_frame
                    validation_frames.append(image.frame)

                job_images.append(image.path)
                new_db_images.append(image)
                frame_idx_map[image.frame] = job_frame

            job_file_mapping.append(job_images)

        # Append pool frames in the end, shift their ids, establish placeholder pointers
        frame_id_map: dict[int, int] = {} # original to new id
        for pool_frame in pool_frames:
            # Insert placeholder frames into the frame sequence and shift frame ids
            image = images[pool_frame]
            image = models.Image(
                data=db_data, **deepcopy(model_to_dict(image, exclude=["data"]))
            )
            new_frame_id = len(new_db_images)
            image.frame = new_frame_id

            frame_id_map[pool_frame] = new_frame_id

            new_db_images.append(image)
            frame_idx_map[image.frame] = pool_frame

        pool_frames = [frame_id_map[i] for i in pool_frames if i in frame_id_map]

        # Store information about the real frame placement in the validation frames
        for validation_frame in validation_frames:
            image = new_db_images[validation_frame]
            assert image.is_placeholder
            image.real_frame = frame_id_map[image.real_frame]

        # Update manifest
        manifest.reorder([images[frame_idx_map[image.frame]].path for image in new_db_images])

        images = new_db_images
        db_data.size = len(images)
        db_data.start_frame = 0
        db_data.stop_frame = 0
        db_data.frame_filter = ''


        db_data.update_validation_layout(models.ValidationLayout(
            mode=models.ValidationMode.GT_POOL,
            frames=pool_frames,
            frames_per_job_count=frames_per_job_count,
        ))

    if db_task.mode == 'annotation':
        images = bulk_create(models.Image, images)

        db_related_files = [
            models.RelatedFile(
                data=db_data,
                path=os.path.join(upload_dir, related_file_path),
            )
            for related_file_path in set(itertools.chain.from_iterable(related_images.values()))
        ]
        db_related_files = bulk_create(models.RelatedFile, db_related_files)
        db_related_files_by_path = {
            os.path.relpath(rf.path.path, upload_dir): rf for rf in db_related_files
        }

        ThroughModel = models.RelatedFile.images.through
        bulk_create(
            ThroughModel,
            (
                ThroughModel(
                    relatedfile_id=db_related_files_by_path[related_file_path].id,
                    image_id=image.id
                )
                for image in images
                for related_file_path in related_images.get(image.path, [])
            )
        )
    else:
        models.Video.objects.create(
            data=db_data,
            path=os.path.relpath(video_path, upload_dir),
            width=video_frame_size[0], height=video_frame_size[1]
        )

    # validate stop_frame
    if db_data.stop_frame == 0:
        db_data.stop_frame = db_data.start_frame + (db_data.size - 1) * db_data.get_frame_step()
    else:
        db_data.stop_frame = min(db_data.stop_frame, \
            db_data.start_frame + (db_data.size - 1) * db_data.get_frame_step())

    slogger.glob.info("Found frames {} for Data #{}".format(db_data.size, db_data.id))

    _create_segments_and_jobs(db_task, job_file_mapping=job_file_mapping, update_status_callback=update_status)

    if validation_params and validation_params['mode'] == models.ValidationMode.GT:
        # The RNG backend must not change to yield reproducible frame picks,
        # so here we specify it explicitly
        from numpy import random
        seed = validation_params.get("random_seed")
        rng = random.Generator(random.MT19937(seed=seed))

        def _to_rel_frame(abs_frame: int) -> int:
            return (abs_frame - db_data.start_frame) // db_data.get_frame_step()

        match validation_params["frame_selection_method"]:
            case models.JobFrameSelectionMethod.RANDOM_UNIFORM:
                all_frames = range(db_data.size)

                if frame_count := validation_params.get("frame_count"):
                    if db_data.size < frame_count:
                        raise ValidationError(
                            f"The number of validation frames requested ({frame_count}) "
                            f"is greater that the number of task frames ({db_data.size})"
                        )
                elif frame_share := validation_params.get("frame_share"):
                    frame_count = max(1, int(frame_share * len(all_frames)))
                else:
                    raise ValidationError("The number of validation frames is not specified")

                validation_frames = rng.choice(
                    all_frames, size=frame_count, shuffle=False, replace=False
                ).tolist()
            case models.JobFrameSelectionMethod.RANDOM_PER_JOB:
                if frame_count := validation_params.get("frames_per_job_count"):
                    if db_task.segment_size < frame_count:
                        raise ValidationError(
                            "The requested number of GT frames per job must be less "
                            f"than task segment size ({db_task.segment_size})"
                        )
                elif frame_share := validation_params.get("frames_per_job_share"):
                    frame_count = min(max(1, int(frame_share * db_task.segment_size)), db_data.size)
                else:
                    raise ValidationError("The number of validation frames is not specified")

                validation_frames: list[int] = []
                overlap = db_task.overlap
                for segment in db_task.segment_set.all():
                    segment_frames = set(map(_to_rel_frame, segment.frame_set))
                    selected_frames = segment_frames.intersection(validation_frames)
                    selected_count = len(selected_frames)

                    missing_count = min(len(segment_frames), frame_count) - selected_count
                    if missing_count <= 0:
                        continue

                    selectable_segment_frames = set(
                        sorted(segment_frames)[overlap * (segment.start_frame != 0) : ]
                    ).difference(selected_frames)

                    validation_frames.extend(rng.choice(
                        tuple(selectable_segment_frames), size=missing_count, replace=False
                    ).tolist())
            case models.JobFrameSelectionMethod.MANUAL:
                if not images:
                    raise ValidationError(
                        "{} validation frame selection method at task creation "
                        "is only available for image-based tasks. "
                        "Please create the GT job after the task is created.".format(
                            models.JobFrameSelectionMethod.MANUAL
                        )
                    )

                validation_frames: list[int] = []
                known_frame_names = {frame.path: _to_rel_frame(frame.frame) for frame in images}
                unknown_requested_frames = []
                for frame_filename in validation_params['frames']:
                    frame_id = known_frame_names.get(frame_filename)
                    if frame_id is None:
                        unknown_requested_frames.append(frame_filename)
                        continue

                    validation_frames.append(frame_id)

                if unknown_requested_frames:
                    raise ValidationError("Unknown validation frames requested: {}".format(
                        format_list(sorted(unknown_requested_frames)))
                    )
            case _:
                assert False, (
                    f'Unknown frame selection method {validation_params["frame_selection_method"]}'
                )

        db_data.update_validation_layout(models.ValidationLayout(
            mode=models.ValidationMode.GT,
            frames=sorted(validation_frames),
        ))

    # TODO: refactor
    if hasattr(db_data, 'validation_layout'):
        if db_data.validation_layout.mode == models.ValidationMode.GT:
            def _to_abs_frame(rel_frame: int) -> int:
                return rel_frame * db_data.get_frame_step() + db_data.start_frame

            db_gt_segment = models.Segment(
                task=db_task,
                start_frame=0,
                stop_frame=db_data.size - 1,
                frames=list(map(_to_abs_frame, db_data.validation_layout.frames)),
                type=models.SegmentType.SPECIFIC_FRAMES,
            )
        elif db_data.validation_layout.mode == models.ValidationMode.GT_POOL:
            db_gt_segment = models.Segment(
                task=db_task,
                start_frame=min(db_data.validation_layout.frames),
                stop_frame=max(db_data.validation_layout.frames),
                type=models.SegmentType.RANGE,
            )
        else:
            assert False

        db_gt_segment.save()

        db_gt_job = models.Job(segment=db_gt_segment, type=models.JobType.GROUND_TRUTH)
        db_gt_job.save()
        db_gt_job.make_dirs()

    db_task.save()

    if (
        settings.MEDIA_CACHE_ALLOW_STATIC_CACHE and
        db_data.storage_method == models.StorageMethodChoice.FILE_SYSTEM
    ):
        _create_static_chunks(db_task, media_extractor=extractor, upload_dir=upload_dir)

    # Prepare the preview image and save it in the cache
    TaskFrameProvider(db_task=db_task).get_preview()

def _create_static_chunks(db_task: models.Task, *, media_extractor: IMediaReader, upload_dir: str):
    @attrs.define
    class _ChunkProgressUpdater:
        _call_counter: int = attrs.field(default=0, init=False)
        _rq_job: rq.job.Job = attrs.field(factory=rq.get_current_job)

        def update_progress(self, progress: float):
            progress_animation = '|/-\\'

            status_message = 'CVAT is preparing data chunks'
            if not progress:
                status_message = '{} {}'.format(
                    status_message, progress_animation[self._call_counter]
                )

            rq_job_meta = ImportRQMeta.for_job(self._rq_job)
            rq_job_meta.status = status_message
            rq_job_meta.task_progress = progress or 0.
            rq_job_meta.save()

            self._call_counter = (self._call_counter + 1) % len(progress_animation)

    def save_chunks(
        executor: concurrent.futures.ThreadPoolExecutor,
        db_segment: models.Segment,
        chunk_idx: int,
        chunk_frame_ids: Sequence[int]
    ):
        chunk_data = [media_iterator[frame_idx] for frame_idx in chunk_frame_ids]

        if (
            db_task.dimension == models.DimensionType.DIM_2D and
            isinstance(media_extractor, (
                MEDIA_TYPES['image']['extractor'],
                MEDIA_TYPES['zip']['extractor'],
                MEDIA_TYPES['pdf']['extractor'],
                MEDIA_TYPES['archive']['extractor'],
            ))
        ):
            chunk_data = list(map(load_image, chunk_data))

        # TODO: extract into a class

        fs_original = executor.submit(
            original_chunk_writer.save_as_chunk,
            images=chunk_data,
            chunk_path=db_data.get_original_segment_chunk_path(
                chunk_idx, segment_id=db_segment.id
            ),
        )
        compressed_chunk_writer.save_as_chunk(
            images=chunk_data,
            chunk_path=db_data.get_compressed_segment_chunk_path(
                chunk_idx, segment_id=db_segment.id
            ),
        )

        fs_original.result()

    db_data = db_task.data

    if db_data.compressed_chunk_type == models.DataChoice.VIDEO:
        compressed_chunk_writer_class = Mpeg4CompressedChunkWriter
    else:
        compressed_chunk_writer_class = ZipCompressedChunkWriter

    if db_data.original_chunk_type == models.DataChoice.VIDEO:
        original_chunk_writer_class = Mpeg4ChunkWriter

        # Let's use QP=17 (that is 67 for 0-100 range) for the original chunks,
        # which should be visually lossless or nearly so.
        # A lower value will significantly increase the chunk size with a slight increase of quality.
        original_quality = 67 # TODO: fix discrepancy in values in different parts of code
    else:
        original_chunk_writer_class = ZipChunkWriter
        original_quality = 100

    chunk_writer_kwargs = {}
    if db_task.dimension == models.DimensionType.DIM_3D:
        chunk_writer_kwargs["dimension"] = db_task.dimension
    compressed_chunk_writer = compressed_chunk_writer_class(
        db_data.image_quality, **chunk_writer_kwargs
    )
    original_chunk_writer = original_chunk_writer_class(original_quality, **chunk_writer_kwargs)

    db_segments = db_task.segment_set.order_by('start_frame').all()

    frame_map = {} # frame number -> extractor frame number

    if isinstance(media_extractor, MEDIA_TYPES['video']['extractor']):
        def _get_frame_size(frame_tuple: tuple[av.VideoFrame, Any, Any]) -> int:
            # There is no need to be absolutely precise here,
            # just need to provide the reasonable upper boundary.
            # Return bytes needed for 1 frame
            frame = frame_tuple[0]
            return frame.width * frame.height * (frame.format.padded_bits_per_pixel // 8)

        # Currently, we only optimize video creation for sequential
        # chunks with potential overlap, so parallel processing is likely to
        # help only for image datasets
        media_iterator = CachingMediaIterator(
            media_extractor,
            max_cache_memory=2 ** 30, max_cache_entries=db_task.overlap,
            object_size_callback=_get_frame_size
        )
    else:
        extractor_frame_ids = {
            media_extractor.get_path(abs_frame_number): abs_frame_number
            for abs_frame_number in media_extractor.frame_range
        }

        frame_map = {
            frame.frame: extractor_frame_ids[os.path.join(upload_dir, frame.path)]
            for frame in db_data.images.all()
        }

        media_iterator = RandomAccessIterator(media_extractor)

    with closing(media_iterator):
        progress_updater = _ChunkProgressUpdater()

        # TODO: remove 2 * or the configuration option
        # TODO: maybe make real multithreading support, currently the code is limited by 1
        # video segment chunk, even if more threads are available
        max_concurrency = 2 * settings.CVAT_CONCURRENT_CHUNK_PROCESSING if not isinstance(
            media_extractor, MEDIA_TYPES['video']['extractor']
        ) else 2
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrency) as executor:
            for segment_idx, db_segment in enumerate(db_segments):
                frame_counter = itertools.count()
                for chunk_idx, chunk_frame_ids in (
                    (chunk_idx, tuple(chunk_frame_ids))
                    for chunk_idx, chunk_frame_ids in itertools.groupby(
                        (
                            # Convert absolute to relative ids (extractor output positions)
                            # Extractor will skip frames outside requested
                            (abs_frame_id - media_extractor.start) // media_extractor.step
                            for abs_frame_id in (
                                frame_map.get(frame, frame)
                                for frame in sorted(db_segment.frame_set)
                            )
                        ),
                        lambda _: next(frame_counter) // db_data.chunk_size
                    )
                ):
                    save_chunks(executor, db_segment, chunk_idx, chunk_frame_ids)

                progress_updater.update_progress(segment_idx / len(db_segments))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\task_validation.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from typing import Callable, Generic, Iterable, Mapping, Sequence, TypeVar

import attrs
import numpy as np

_K = TypeVar("_K")


@attrs.define
class _BaggedCounter(Generic[_K]):
    # Stores items with count = k in a single "bag". Bags are stored in the ascending order
    bags: dict[
        int,
        dict[_K, None],
        # dict is used instead of a set to preserve item order. It's also more performant
    ]

    @staticmethod
    def from_dict(item_counts: Mapping[_K, int]) -> _BaggedCounter:
        return _BaggedCounter.from_counts(item_counts, item_count=item_counts.__getitem__)

    @staticmethod
    def from_counts(items: Sequence[_K], item_count: Callable[[_K], int]) -> _BaggedCounter:
        bags = {}
        for item in items:
            count = item_count(item)
            bags.setdefault(count, dict())[item] = None

        return _BaggedCounter(bags=bags)

    def __attrs_post_init__(self):
        self._sort_bags()

    def _sort_bags(self):
        self.bags = dict(sorted(self.bags.items(), key=lambda e: e[0]))

    def shuffle(self, *, rng: np.random.Generator | None):
        if not rng:
            rng = np.random.default_rng()

        for count, bag in self.bags.items():
            items = list(bag.items())
            rng.shuffle(items)
            self.bags[count] = dict(items)

    def use_item(self, item: _K, *, count: int | None = None, bag: dict | None = None):
        if count is not None:
            if bag is None:
                bag = self.bags[count]
        elif count is None and bag is None:
            count, bag = next((c, b) for c, b in self.bags.items() if item in b)
        else:
            raise AssertionError("'bag' can only be used together with 'count'")

        bag.pop(item)

        if not bag:
            self.bags.pop(count)

        next_bag = self.bags.get(count + 1)
        if next_bag is None:
            next_bag = {}
            self.bags[count + 1] = next_bag
            self._sort_bags()  # the new bag can be added in the wrong position if there were gaps

        next_bag[item] = None

    def __iter__(self) -> Iterable[tuple[int, _K, dict]]:
        for count, bag in self.bags.items():  # bags must be ordered
            for item in bag:
                yield (count, item, bag)

    def select_next_least_used(self, count: int) -> Sequence[_K]:
        pick = [None] * count
        pick_original_use_counts = [(None, None)] * count
        for i, (use_count, item, bag) in zip(range(count), self):
            pick[i] = item
            pick_original_use_counts[i] = (use_count, bag)

        for item, (use_count, bag) in zip(pick, pick_original_use_counts):
            self.use_item(item, count=use_count, bag=bag)

        return pick


class HoneypotFrameSelector(Generic[_K]):
    def __init__(
        self,
        validation_frame_counts: Mapping[_K, int],
        *,
        rng: np.random.Generator | None = None,
    ):
        if not rng:
            rng = np.random.default_rng()

        self.rng = rng

        self._counter = _BaggedCounter.from_dict(validation_frame_counts)
        self._counter.shuffle(rng=rng)

    def select_next_frames(self, count: int) -> Sequence[_K]:
        # This approach guarantees that:
        # - every GT frame is used
        # - GT frames are used uniformly (at most min count + 1)
        # - GT frames are not repeated in jobs
        # - honeypot sets are different in jobs
        # - honeypot sets are random
        # if possible (if the job and GT counts allow this).
        # Picks must be reproducible for a given rng state.
        """
        Selects 'count' least used items randomly, without repetition
        """
        return self._counter.select_next_least_used(count)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\types.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.http import HttpRequest
from rest_framework.request import Request

from cvat.apps.engine.middleware import WithUUID
from cvat.apps.iam.middleware import WithIAMContext


class ExtendedRequest(HttpRequest, Request, WithUUID, WithIAMContext):
    pass


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\urls.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings
from django.urls import include, path
from django.views.generic import RedirectView
from drf_spectacular.views import SpectacularAPIView, SpectacularRedocView, SpectacularSwaggerView
from rest_framework import routers

from . import views

router = routers.DefaultRouter(trailing_slash=False)
router.register("projects", views.ProjectViewSet)
router.register("tasks", views.TaskViewSet)
router.register("jobs", views.JobViewSet)
router.register("users", views.UserViewSet)
router.register("server", views.ServerViewSet, basename="server")
router.register("issues", views.IssueViewSet)
router.register("comments", views.CommentViewSet)
router.register("labels", views.LabelViewSet)
router.register("cloudstorages", views.CloudStorageViewSet)
router.register("assets", views.AssetsViewSet)
router.register("guides", views.AnnotationGuidesViewSet)
router.register("requests", views.RequestViewSet, basename="request")

urlpatterns = [
    # Entry point for a client
    path("", RedirectView.as_view(url=settings.UI_URL, permanent=True, query_string=True)),
    # documentation for API
    path(
        "api/schema/",
        SpectacularAPIView.as_view(
            permission_classes=[],  # This endpoint is available for everyone
        ),
        name="schema",
    ),
    path(
        "api/swagger/",
        SpectacularSwaggerView.as_view(
            url_name="schema",
            permission_classes=[],  # This endpoint is available for everyone
        ),
        name="swagger",
    ),
    path(
        "api/docs/",
        SpectacularRedocView.as_view(
            url_name="schema",
            permission_classes=[],  # This endpoint is available for everyone
        ),
        name="redoc",
    ),
    # entry point for API
    path("api/", include("cvat.apps.iam.urls")),
    path("api/", include("cvat.apps.organizations.urls")),
    path("api/", include(router.urls)),
]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\utils.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import ast
import hashlib
import importlib
import logging
import os
import platform
import re
import subprocess
import sys
import traceback
import urllib.parse
from collections import defaultdict, namedtuple
from collections.abc import Generator, Iterable, Mapping, Sequence
from contextlib import nullcontext, suppress
from itertools import islice
from multiprocessing import cpu_count
from pathlib import Path
from typing import Any, Callable, Optional, TypeVar, Union

import cv2 as cv
from attr.converters import to_bool
from av import VideoFrame
from datumaro.util.os_util import walk
from django.conf import settings
from django.core.exceptions import ValidationError
from django.utils.http import urlencode
from django_rq.queues import DjangoRQ
from django_sendfile import sendfile as _sendfile
from PIL import Image
from redis.lock import Lock
from rest_framework.reverse import reverse as _reverse
from rq.job import Job as RQJob

from cvat.apps.engine.types import ExtendedRequest

Import = namedtuple("Import", ["module", "name", "alias"])

def parse_imports(source_code: str):
    root = ast.parse(source_code)

    for node in ast.iter_child_nodes(root):
        if isinstance(node, ast.Import):
            module = []
        elif isinstance(node, ast.ImportFrom):
            module = node.module
        else:
            continue

        for n in node.names:
            yield Import(module, n.name, n.asname)

def import_modules(source_code: str):
    results = {}
    imports = parse_imports(source_code)
    for import_ in imports:
        module = import_.module if import_.module else import_.name
        loaded_module = importlib.import_module(module)

        if not import_.name == module:
            loaded_module = getattr(loaded_module, import_.name)

        if import_.alias:
            results[import_.alias] = loaded_module
        else:
            results[import_.name] = loaded_module

    return results

class InterpreterError(Exception):
    pass

def execute_python_code(source_code, global_vars=None, local_vars=None):
    try:
        # pylint: disable=exec-used
        exec(source_code, global_vars, local_vars)
    except SyntaxError as err:
        error_class = err.__class__.__name__
        details = err.args[0]
        line_number = err.lineno
        raise InterpreterError("{} at line {}: {}".format(error_class, line_number, details))
    except AssertionError as err:
        # AssertionError doesn't contain any args and line number
        error_class = err.__class__.__name__
        raise InterpreterError("{}".format(error_class))
    except Exception as err:
        error_class = err.__class__.__name__
        details = err.args[0]
        _, _, tb = sys.exc_info()
        line_number = traceback.extract_tb(tb)[-1][1]
        raise InterpreterError("{} at line {}: {}".format(error_class, line_number, details))

class CvatChunkTimestampMismatchError(Exception):
    pass

def av_scan_paths(*paths):
    if 'yes' == os.environ.get('CLAM_AV'):
        command = ['clamscan', '--no-summary', '-i', '-o']
        command.extend(paths)
        res = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # nosec
        if res.returncode:
            raise ValidationError(res.stdout)

def rotate_image(image, angle):
    height, width = image.shape[:2]
    image_center = (width/2, height/2)
    matrix = cv.getRotationMatrix2D(image_center, angle, 1.)
    abs_cos = abs(matrix[0,0])
    abs_sin = abs(matrix[0,1])
    bound_w = int(height * abs_sin + width * abs_cos)
    bound_h = int(height * abs_cos + width * abs_sin)
    matrix[0, 2] += bound_w/2 - image_center[0]
    matrix[1, 2] += bound_h/2 - image_center[1]
    matrix = cv.warpAffine(image, matrix, (bound_w, bound_h))
    return matrix

def md5_hash(frame):
    if isinstance(frame, VideoFrame):
        frame = frame.to_image()
    elif isinstance(frame, str):
        frame = Image.open(frame, 'r')
    return hashlib.md5(frame.tobytes()).hexdigest() # nosec

def parse_specific_attributes(specific_attributes):
    assert isinstance(specific_attributes, str), 'Specific attributes must be a string'
    parsed_specific_attributes = urllib.parse.parse_qsl(specific_attributes)
    return {
        key: value for (key, value) in parsed_specific_attributes
    } if parsed_specific_attributes else dict()


def parse_exception_message(msg: str) -> str:
    parsed_msg = msg
    try:
        if 'ErrorDetail' in msg:
            # msg like: 'rest_framework.exceptions.ValidationError:
            # [ErrorDetail(string="...", code=\'invalid\')]\n'
            parsed_msg = msg.split('string=')[1].split(', code=')[0].strip("\"")
        elif msg.startswith('rest_framework.exceptions.'):
            parsed_msg = msg.split(':')[1].strip()
    except Exception: # nosec
        pass
    return parsed_msg

def process_failed_job(rq_job: RQJob) -> str:
    exc_info = str(rq_job.exc_info or '')
    rq_job.delete()

    msg = parse_exception_message(exc_info)
    log = logging.getLogger('cvat.server.engine')
    log.error(msg)
    return msg


def get_rq_lock_by_user(queue: DjangoRQ, user_id: int, *, timeout: Optional[int] = 30, blocking_timeout: Optional[int] = None) -> Union[Lock, nullcontext]:
    if settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER:
        return queue.connection.lock(
            name=f'{queue.name}-lock-{user_id}',
            timeout=timeout,
            blocking_timeout=blocking_timeout,
        )
    return nullcontext()

def get_rq_lock_for_job(queue: DjangoRQ, rq_id: str, *, timeout: int = 60, blocking_timeout: int = 50) -> Lock:
    # lock timeout corresponds to the nginx request timeout (proxy_read_timeout)

    assert timeout is not None
    assert blocking_timeout is not None
    return queue.connection.lock(
        name=f'lock-for-job-{rq_id}'.lower(),
        timeout=timeout,
        blocking_timeout=blocking_timeout,
    )

def reverse(viewname, *, args=None, kwargs=None,
    query_params: Optional[dict[str, str]] = None,
    request: ExtendedRequest | None = None,
) -> str:
    """
    The same as rest_framework's reverse(), but adds custom query params support.
    The original request can be passed in the 'request' parameter to
    return absolute URLs.
    """

    url = _reverse(viewname, args, kwargs, request)

    if query_params:
        return f'{url}?{urlencode(query_params)}'

    return url

def get_server_url(request: ExtendedRequest) -> str:
    return request.build_absolute_uri('/')

def build_field_filter_params(field: str, value: Any) -> dict[str, str]:
    """
    Builds a collection filter query params for a single field and value.
    """
    return { field: value }

def get_list_view_name(model):
    # Implemented after
    # rest_framework/utils/field_mapping.py.get_detail_view_name()
    """
    Given a model class, return the view name to use for URL relationships
    that refer to instances of the model.
    """
    return '%(model_name)s-list' % {
        'model_name': model._meta.object_name.lower()
    }

def import_resource_with_clean_up_after(
    func: Union[Callable[[str, int, int], int], Callable[[str, int, str, bool], None]],
    filename: str,
    *args,
    **kwargs,
) -> Any:
    try:
        result = func(filename, *args, **kwargs)
    finally:
        with suppress(FileNotFoundError):
            os.remove(filename)
    return result

def get_cpu_number() -> int:
    cpu_number = None
    try:
        if platform.system() == 'Linux':
            # we cannot use just multiprocessing.cpu_count because when it runs
            # inside a docker container, it will just return the number of CPU cores
            # for the physical machine the container runs on
            cfs_quota_us_path = Path("/sys/fs/cgroup/cpu/cpu.cfs_quota_us")
            cfs_period_us_path = Path("/sys/fs/cgroup/cpu/cpu.cfs_period_us")

            if cfs_quota_us_path.exists() and cfs_period_us_path.exists():
                with open(cfs_quota_us_path) as fp:
                    cfs_quota_us = int(fp.read())
                with open(cfs_period_us_path) as fp:
                    cfs_period_us = int(fp.read())
                container_cpu_number = cfs_quota_us // cfs_period_us
                # For physical machine, the `cfs_quota_us` could be '-1'
                cpu_number = cpu_count() if container_cpu_number < 1 else container_cpu_number
        cpu_number = cpu_number or cpu_count()
    except NotImplementedError:
        # the number of cpu cannot be determined
        cpu_number = 1
    return cpu_number

def make_attachment_file_name(filename: str) -> str:
    # Borrowed from sendfile() to minimize changes for users.
    # Added whitespace conversion and squashing into a single space
    # Added removal of control characters

    filename = str(filename).replace("\\", "\\\\").replace('"', r"\"")
    filename = re.sub(r"\s+", " ", filename)

    # From https://github.com/encode/uvicorn/blob/cd18c3b14aa810a4a6ebb264b9a297d6f8afb9ac/uvicorn/protocols/http/httptools_impl.py#L51
    filename = re.sub(r"[\x00-\x1F\x7F]", "", filename)

    return filename

def sendfile(
    request: ExtendedRequest, filename,
    attachment=False, attachment_filename=None, mimetype=None, encoding=None
):
    """
    Create a response to send file using backend configured in ``SENDFILE_BACKEND``

    ``filename`` is the absolute path to the file to send.

    If ``attachment`` is ``True`` the ``Content-Disposition`` header will be set accordingly.
    This will typically prompt the user to download the file, rather
    than view it. But even if ``False``, the user may still be prompted, depending
    on the browser capabilities and configuration.

    The ``Content-Disposition`` filename depends on the value of ``attachment_filename``:

        ``None`` (default): Same as ``filename``
        ``False``: No ``Content-Disposition`` filename
        ``String``: Value used as filename

    If neither ``mimetype`` or ``encoding`` are specified, then they will be guessed via the
    filename (using the standard Python mimetypes module)
    """
    # A drop-in replacement for sendfile with extra filename cleaning

    if attachment_filename:
        attachment_filename = make_attachment_file_name(attachment_filename)

    return _sendfile(request, filename, attachment, attachment_filename, mimetype, encoding)


def build_backup_file_name(
    *,
    class_name: str,
    identifier: str | int,
    timestamp: str,
) -> str:
    # "<project|task>_<name>_backup_<timestamp>.zip"
    return "{}_{}_backup_{}.zip".format(
        class_name, identifier, timestamp,
    ).lower()

def build_annotations_file_name(
    *,
    class_name: str,
    identifier: str | int,
    timestamp: str,
    format_name: str,
    extension: str,
    is_annotation_file: bool = True,
) -> str:
    # "<project|task|job>_<name|id>_<annotations|dataset>_<timestamp>_<format>.<ext>"
    return "{}_{}_{}_{}_{}.{}".format(
        class_name, identifier, 'annotations' if is_annotation_file else 'dataset',
        timestamp, format_name, extension
    ).lower()


def directory_tree(path, max_depth=None) -> str:
    if not os.path.exists(path):
        raise Exception(f"No such file or directory: {path}")

    tree = ""

    baselevel = path.count(os.sep)
    for root, _, files in walk(path, max_depth=max_depth):
        curlevel = root.count(os.sep)
        indent = "|  " * (curlevel - baselevel) + "|-"
        tree += f"{indent}{os.path.basename(root)}/\n"
        for file in files:
            tree += f"{indent}-{file}\n"
    return tree

def is_dataset_export(request: ExtendedRequest) -> bool:
    return to_bool(request.query_params.get('save_images', False))

_T = TypeVar('_T')

def take_by(iterable: Iterable[_T], chunk_size: int) -> Generator[list[_T], None, None]:
    """
    Returns elements from the input iterable by batches of N items.
    ('abcdefg', 3) -> ['a', 'b', 'c'], ['d', 'e', 'f'], ['g']
    """
    # can be changed to itertools.batched after migration to python3.12

    it = iter(iterable)
    while True:
        batch = list(islice(it, chunk_size))
        if len(batch) == 0:
            break

        yield batch


FORMATTED_LIST_DISPLAY_THRESHOLD = 10
"""
Controls maximum rendered list items. The remainder is appended as ' (and X more)'.
"""

def format_list(
    items: Sequence[str], *, max_items: Optional[int] = None, separator: str = ", "
) -> str:
    if max_items is None:
        max_items = FORMATTED_LIST_DISPLAY_THRESHOLD

    remainder_count = len(items) - max_items
    return "{}{}".format(
        separator.join(items[:max_items]),
        f" (and {remainder_count} more)" if 0 < remainder_count else "",
    )


_K = TypeVar("_K")
_V = TypeVar("_V")


def grouped(items: Iterable[_V], *, key: Callable[[_V], _K]) -> Mapping[_K, Sequence[_V]]:
    """
    Returns a mapping with input iterable elements grouped by key, for example:

    grouped(
        [("apple1", "red"), ("apple2", "green"), ("apple3", "red")],
        key=lambda v: v[1]
    )
    ->
    {
        "red": [("apple1", "red"), ("apple3", "red")],
        "green": [("apple2", "green")]
    }

    Similar to itertools.groupby, but allows reiteration on resulting groups.
    """

    # Can be implemented with itertools.groupby, but it requires extra sorting for input elements
    grouped_items = {}
    for item in items:
        grouped_items.setdefault(key(item), []).append(item)

    return grouped_items


def defaultdict_to_regular(d):
    if isinstance(d, defaultdict):
        d = {k: defaultdict_to_regular(v) for k, v in d.items()}
    return d


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\views.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import functools
import itertools
import os
import os.path as osp
import re
import shutil
import textwrap
import traceback
import zlib
from abc import ABCMeta, abstractmethod
from collections import namedtuple
from collections.abc import Iterable
from contextlib import suppress
from copy import copy
from datetime import datetime
from pathlib import Path
from tempfile import NamedTemporaryFile
from types import SimpleNamespace
from typing import Any, Callable, Optional, Union, cast

import django_rq
from attr.converters import to_bool
from django.conf import settings
from django.contrib.auth.models import User
from django.core.files.storage import storages
from django.db import IntegrityError, transaction
from django.db.models.query import Prefetch
from django.http import HttpResponse, HttpResponseBadRequest, HttpResponseGone, HttpResponseNotFound
from django.utils import timezone
from django.utils.decorators import method_decorator
from django.views.decorators.cache import never_cache
from django_rq.queues import DjangoRQ
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import (
    OpenApiExample,
    OpenApiParameter,
    OpenApiResponse,
    PolymorphicProxySerializer,
    extend_schema,
    extend_schema_view,
)
from PIL import Image
from redis.exceptions import ConnectionError as RedisConnectionError
from rest_framework import mixins, serializers, status, viewsets
from rest_framework.decorators import action
from rest_framework.exceptions import APIException, NotFound, PermissionDenied, ValidationError
from rest_framework.parsers import MultiPartParser
from rest_framework.permissions import SAFE_METHODS
from rest_framework.response import Response
from rest_framework.settings import api_settings
from rq.job import Job as RQJob
from rq.job import JobStatus as RQJobStatus

import cvat.apps.dataset_manager as dm
import cvat.apps.dataset_manager.views  # pylint: disable=unused-import
from cvat.apps.dataset_manager.bindings import CvatImportError
from cvat.apps.dataset_manager.serializers import DatasetFormatsSerializer
from cvat.apps.engine import backup
from cvat.apps.engine.cache import CvatChunkTimestampMismatchError, LockError, MediaCache
from cvat.apps.engine.cloud_provider import (
    db_storage_to_storage_instance,
    import_resource_from_cloud_storage,
)
from cvat.apps.engine.filters import (
    NonModelJsonLogicFilter,
    NonModelOrderingFilter,
    NonModelSimpleFilter,
)
from cvat.apps.engine.frame_provider import (
    DataWithMeta,
    FrameQuality,
    IFrameProvider,
    JobFrameProvider,
    TaskFrameProvider,
)
from cvat.apps.engine.location import StorageType, get_location_configuration
from cvat.apps.engine.media_extractors import get_mime
from cvat.apps.engine.mixins import BackupMixin, DatasetMixin, PartialUpdateModelMixin, UploadMixin
from cvat.apps.engine.model_utils import bulk_create
from cvat.apps.engine.models import AnnotationGuide, Asset, ClientFile, CloudProviderChoice
from cvat.apps.engine.models import CloudStorage as CloudStorageModel
from cvat.apps.engine.models import (
    Comment,
    Data,
    Issue,
    Job,
    JobType,
    Label,
    Location,
    Project,
    RequestAction,
    RequestStatus,
    RequestSubresource,
    RequestTarget,
    StorageChoice,
    StorageMethodChoice,
    Task,
)
from cvat.apps.engine.permissions import (
    AnnotationGuidePermission,
    CloudStoragePermission,
    CommentPermission,
    IssuePermission,
    JobPermission,
    LabelPermission,
    ProjectPermission,
    TaskPermission,
    UserPermission,
    get_cloud_storage_for_import_or_export,
    get_iam_context,
)
from cvat.apps.engine.rq import (
    ImportRQMeta,
    RQId,
    RQMetaWithFailureInfo,
    define_dependent_job,
    is_rq_job_owner,
)
from cvat.apps.engine.serializers import (
    AboutSerializer,
    AnnotationFileSerializer,
    AnnotationGuideReadSerializer,
    AnnotationGuideWriteSerializer,
    AssetReadSerializer,
    AssetWriteSerializer,
    BasicUserSerializer,
    CloudStorageContentSerializer,
    CloudStorageReadSerializer,
    CloudStorageWriteSerializer,
    CommentReadSerializer,
    CommentWriteSerializer,
    DataMetaReadSerializer,
    DataMetaWriteSerializer,
    DataSerializer,
    DatasetFileSerializer,
    FileInfoSerializer,
    IssueReadSerializer,
    IssueWriteSerializer,
    JobDataMetaWriteSerializer,
    JobReadSerializer,
    JobValidationLayoutReadSerializer,
    JobValidationLayoutWriteSerializer,
    JobWriteSerializer,
    LabeledDataSerializer,
    LabelSerializer,
    PluginsSerializer,
    ProjectFileSerializer,
    ProjectReadSerializer,
    ProjectWriteSerializer,
    RequestSerializer,
    RqIdSerializer,
    RqStatusSerializer,
    TaskFileSerializer,
    TaskReadSerializer,
    TaskValidationLayoutReadSerializer,
    TaskValidationLayoutWriteSerializer,
    TaskWriteSerializer,
    UserSerializer,
)
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import (
    av_scan_paths,
    get_rq_lock_by_user,
    get_rq_lock_for_job,
    import_resource_with_clean_up_after,
    parse_exception_message,
    process_failed_job,
    sendfile,
)
from cvat.apps.engine.view_utils import tus_chunk_action
from cvat.apps.events.handlers import handle_dataset_import
from cvat.apps.iam.filters import ORGANIZATION_OPEN_API_PARAMETERS
from cvat.apps.iam.permissions import IsAuthenticatedOrReadPublicResource, PolicyEnforcer
from utils.dataset_manifest import ImageManifestManager

from . import models, task
from .log import ServerLogManager

slogger = ServerLogManager(__name__)

_UPLOAD_PARSER_CLASSES = api_settings.DEFAULT_PARSER_CLASSES + [MultiPartParser]

_DATA_CHECKSUM_HEADER_NAME = 'X-Checksum'
_DATA_UPDATED_DATE_HEADER_NAME = 'X-Updated-Date'
_RETRY_AFTER_TIMEOUT = 10

def get_410_response_for_export_api(path: str) -> HttpResponseGone:
    return HttpResponseGone(textwrap.dedent(f"""\
        This endpoint is no longer supported.
        To initiate the export process, use POST {path}.
        To check the process status, use GET /api/requests/rq_id,
        where rq_id is obtained from the response of the previous request.
        To download the prepared file, use the result_url obtained from the response of the previous request.
    """))

@extend_schema(tags=['server'])
class ServerViewSet(viewsets.ViewSet):
    serializer_class = None
    iam_organization_field = None

    # To get nice documentation about ServerViewSet actions it is necessary
    # to implement the method. By default, ViewSet doesn't provide it.
    def get_serializer(self, *args, **kwargs):
        pass

    @staticmethod
    @extend_schema(summary='Get basic CVAT information',
        responses={
            '200': AboutSerializer,
        })
    @action(detail=False, methods=['GET'], serializer_class=AboutSerializer,
        permission_classes=[] # This endpoint is available for everyone
    )
    def about(request: ExtendedRequest):
        from cvat import __version__ as cvat_version
        about = {
            "name": "Computer Vision Annotation Tool",
            "subtitle": settings.ABOUT_INFO["subtitle"],
            "description": "CVAT is completely re-designed and re-implemented " +
                "version of Video Annotation Tool from Irvine, California " +
                "tool. It is free, online, interactive video and image annotation " +
                "tool for computer vision. It is being used by our team to " +
                "annotate million of objects with different properties. Many UI " +
                "and UX decisions are based on feedbacks from professional data " +
                "annotation team.",
            "version": cvat_version,
            "logo_url": request.build_absolute_uri(storages["staticfiles"].url(settings.LOGO_FILENAME)),
        }
        serializer = AboutSerializer(data=about)
        if serializer.is_valid(raise_exception=True):
            return Response(data=serializer.data)

    @staticmethod
    @extend_schema(
        summary='List files/directories in the mounted share',
        parameters=[
            OpenApiParameter('directory', description='Directory to browse',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR),
            OpenApiParameter('search', description='Search for specific files',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR)
        ],
        responses={
            '200' : FileInfoSerializer(many=True)
        })
    @action(detail=False, methods=['GET'], serializer_class=FileInfoSerializer)
    def share(request: ExtendedRequest):
        directory_param = request.query_params.get('directory', '/')
        search_param = request.query_params.get('search', '')

        if directory_param.startswith("/"):
            directory_param = directory_param[1:]

        directory = (Path(settings.SHARE_ROOT) / directory_param).absolute()

        if str(directory).startswith(settings.SHARE_ROOT) and directory.is_dir():
            data = []
            generator = directory.iterdir() if not search_param else (f for f in directory.iterdir() if f.name.startswith(search_param))

            for entry in generator:
                entry_type, entry_mime_type = None, None
                if entry.is_file():
                    entry_type = "REG"
                    entry_mime_type = get_mime(entry)
                    if entry_mime_type == 'zip':
                        entry_mime_type = 'archive'
                elif entry.is_dir():
                    entry_type = entry_mime_type = "DIR"

                if entry_type:
                    data.append({
                        "name": entry.name,
                        "type": entry_type,
                        "mime_type": entry_mime_type,
                    })

            # return directories at the top of the list
            serializer = FileInfoSerializer(many=True, data=sorted(data, key=lambda x: (x['type'], x['name'])))
            if serializer.is_valid(raise_exception=True):
                return Response(serializer.data)
        else:
            return Response("{} is an invalid directory".format(directory_param),
                status=status.HTTP_400_BAD_REQUEST)

    @staticmethod
    @extend_schema(
        summary='Get supported annotation formats',
        responses={
            '200': DatasetFormatsSerializer,
        })
    @action(detail=False, methods=['GET'], url_path='annotation/formats')
    def annotation_formats(request: ExtendedRequest):
        data = dm.views.get_all_formats()
        return Response(DatasetFormatsSerializer(data).data)

    @staticmethod
    @extend_schema(
        summary='Get enabled plugins',
        responses={
            '200': PluginsSerializer,
        })
    @action(detail=False, methods=['GET'], url_path='plugins', serializer_class=PluginsSerializer)
    def plugins(request: ExtendedRequest):
        data = {
            'GIT_INTEGRATION': False, # kept for backwards compatibility
            'ANALYTICS': settings.ANALYTICS_ENABLED,
            'MODELS': to_bool(os.environ.get("CVAT_SERVERLESS", False)),
            'PREDICT': False, # FIXME: it is unused anymore (for UI only)
        }
        return Response(PluginsSerializer(data).data)

@extend_schema(tags=['projects'])
@extend_schema_view(
    list=extend_schema(
        summary='List projects',
        responses={
            '200': ProjectReadSerializer(many=True),
        }),
    create=extend_schema(
        summary='Create a project',
        request=ProjectWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            '201': ProjectReadSerializer, # check ProjectWriteSerializer.to_representation
        }),
    retrieve=extend_schema(
        summary='Get project details',
        responses={
            '200': ProjectReadSerializer,
        }),
    destroy=extend_schema(
        summary='Delete a project',
        responses={
            '204': OpenApiResponse(description='The project has been deleted'),
        }),
    partial_update=extend_schema(
        summary='Update a project',
        request=ProjectWriteSerializer(partial=True),
        responses={
            '200': ProjectReadSerializer, # check ProjectWriteSerializer.to_representation
        })
)
class ProjectViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,
    PartialUpdateModelMixin, UploadMixin, DatasetMixin, BackupMixin
):
    # NOTE: The search_fields attribute should be a list of names of text
    # type fields on the model,such as CharField or TextField
    queryset = models.Project.objects.select_related(
        'owner', 'assignee', 'organization',
        'annotation_guide', 'source_storage', 'target_storage',
    )

    search_fields = ('name', 'owner', 'assignee', 'status')
    filter_fields = list(search_fields) + ['id', 'updated_date']
    simple_filters = list(search_fields)
    ordering_fields = list(filter_fields)
    ordering = "-id"
    lookup_fields = {'owner': 'owner__username', 'assignee': 'assignee__username'}
    iam_organization_field = 'organization'
    IMPORT_RQ_ID_FACTORY = functools.partial(RQId,
        RequestAction.IMPORT, RequestTarget.PROJECT, subresource=RequestSubresource.DATASET
    )

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return ProjectReadSerializer
        else:
            return ProjectWriteSerializer

    def get_queryset(self):
        queryset = super().get_queryset()
        if self.action in ('list', 'retrieve', 'partial_update', 'update') :
            queryset = queryset.prefetch_related('tasks')

            if self.action == 'list':
                perm = ProjectPermission.create_scope_list(self.request)
                return perm.filter(queryset)

        return queryset

    @transaction.atomic
    def perform_create(self, serializer, **kwargs):
        serializer.save(
            owner=self.request.user,
            organization=self.request.iam_context['organization']
        )

        # Required for the extra summary information added in the queryset
        serializer.instance = self.get_queryset().get(pk=serializer.instance.pk)

    @extend_schema(methods=['GET'], summary='Check dataset import status',
        description=textwrap.dedent("""
            Utilizing this endpoint to check the status of the process
            of importing a project dataset from a file is deprecated.
            In addition, this endpoint no longer handles the project dataset export process.

            Consider using new API:
            - `POST /api/projects/<project_id>/dataset/export/?save_images=True` to initiate export process
            - `GET /api/requests/<rq_id>` to check process status
            - `GET result_url` to download a prepared file

            Where:
            - `rq_id` can be found in the response on initializing request
            - `result_url` can be found in the response on checking status request
        """),
        parameters=[
            OpenApiParameter('format', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                deprecated=True
            ),
            OpenApiParameter('filename', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                deprecated=True
            ),
            OpenApiParameter('action', description='Used to check the import status',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False, enum=['import_status'],
                deprecated=True
            ),
            OpenApiParameter('location', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(),
                deprecated=True
            ),
            OpenApiParameter('cloud_storage_id', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False,
                deprecated=True
            ),
            OpenApiParameter('rq_id', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=True),
        ],
        deprecated=True,
        responses={
            '410': OpenApiResponse(description='API endpoint no longer supports exporting datasets'),
        })
    @extend_schema(methods=['POST'],
        summary='Import a dataset into a project',
        description=textwrap.dedent("""
            The request POST /api/projects/id/dataset initiates a background process to import dataset into a project.
            Please, use the GET /api/requests/<rq_id> endpoint for checking status of the process.
            The `rq_id` parameter can be found in the response on initiating request.
        """),
        parameters=[
            OpenApiParameter('format', description='Desired dataset format name\n'
                'You can get the list of supported formats at:\n/server/annotation/formats',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
            OpenApiParameter('location', description='Where to import the dataset from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list()),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('use_default_location', description='Use the location that was configured in the project to import annotations',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.BOOL, required=False,
                default=True, deprecated=True),
            OpenApiParameter('filename', description='Dataset file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
        ],
        request=PolymorphicProxySerializer('DatasetWrite',
            # TODO: refactor to use required=False when possible
            serializers=[DatasetFileSerializer, OpenApiTypes.NONE],
            resource_type_field_name=None
        ),
        responses={
            '202': OpenApiResponse(RqIdSerializer, description='Importing has been started'),
            '400': OpenApiResponse(description='Failed to import dataset'),
            '405': OpenApiResponse(description='Format is not available'),
        })
    @action(detail=True, methods=['GET', 'POST', 'OPTIONS'], serializer_class=None,
        url_path=r'dataset/?$', parser_classes=_UPLOAD_PARSER_CLASSES,
    )
    def dataset(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # force call of check_object_permissions()

        if request.method == "GET":
            if request.query_params.get("action") == "import_status":
                # We cannot redirect to the requests API here because
                # it wouldn't be compatible with the outdated API:
                # the current API endpoint returns a different status codes
                # depends on rq job status (like 201 - finished),
                # while GET /api/requests/rq_id returns a 200 status code
                # if such a request exists regardless of job status.

                deprecation_timestamp = int(datetime(2025, 2, 27, tzinfo=timezone.utc).timestamp())
                response_headers  = {
                    "Deprecation": f"@{deprecation_timestamp}"
                }

                queue = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)
                rq_id = request.query_params.get('rq_id')
                if not rq_id:
                    return Response(
                        'The rq_id param should be specified in the query parameters',
                        status=status.HTTP_400_BAD_REQUEST,
                        headers=response_headers,
                    )

                rq_job = queue.fetch_job(rq_id)

                if rq_job is None:
                    return Response(status=status.HTTP_404_NOT_FOUND, headers=response_headers)
                # check that the user has access to the current rq_job
                elif not is_rq_job_owner(rq_job, request.user.id):
                    return Response(status=status.HTTP_403_FORBIDDEN, headers=response_headers)

                if rq_job.is_finished:
                    rq_job.delete()
                    return Response(status=status.HTTP_201_CREATED, headers=response_headers)
                elif rq_job.is_failed:
                    exc_info = process_failed_job(rq_job)

                    return Response(
                        data=str(exc_info),
                        status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                        headers=response_headers
                    )
                else:
                    return Response(
                        data=self._get_rq_response(
                            settings.CVAT_QUEUES.IMPORT_DATA.value,
                            rq_id,
                        ),
                        status=status.HTTP_202_ACCEPTED,
                        headers=response_headers
                    )

            # we cannot redirect to the new API here since this endpoint used not only to check the status
            # of exporting process|download a result file, but also to initiate export process
            return get_410_response_for_export_api("/api/projects/id/dataset/export?save_images=True")

        return self.import_annotations(
            request=request,
            db_obj=self._object,
            import_func=_import_project_dataset,
            rq_func=dm.project.import_dataset_as_project,
            rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
        )


    @tus_chunk_action(detail=True, suffix_base="dataset")
    def append_dataset_chunk(self, request: ExtendedRequest, pk: int, file_id: str):
        self._object = self.get_object()
        return self.append_tus_chunk(request, file_id)

    def get_upload_dir(self):
        if 'dataset' in self.action:
            return self._object.get_tmp_dirname()
        elif 'backup' in self.action:
            return backup.get_backup_dirname()
        assert False

    def upload_finished(self, request: ExtendedRequest):
        if self.action == 'dataset':
            format_name = request.query_params.get("format", "")
            filename = request.query_params.get("filename", "")
            conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
            tmp_dir = self._object.get_tmp_dirname()
            uploaded_file = os.path.join(tmp_dir, filename)
            if not os.path.isfile(uploaded_file):
                uploaded_file = None

            return _import_project_dataset(
                request=request,
                filename=uploaded_file,
                rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
                rq_func=dm.project.import_dataset_as_project,
                db_obj=self._object,
                format_name=format_name,
                conv_mask_to_poly=conv_mask_to_poly
            )
        elif self.action == 'import_backup':
            filename = request.query_params.get("filename", "")
            if filename:
                tmp_dir = backup.get_backup_dirname()
                backup_file = os.path.join(tmp_dir, filename)
                if os.path.isfile(backup_file):
                    return backup.import_project(
                        request,
                        settings.CVAT_QUEUES.IMPORT_DATA.value,
                        filename=backup_file,
                    )
                return Response(data='No such file were uploaded',
                        status=status.HTTP_400_BAD_REQUEST)
            return backup.import_project(request, settings.CVAT_QUEUES.IMPORT_DATA.value)
        return Response(data='Unknown upload was finished',
                        status=status.HTTP_400_BAD_REQUEST)

    @extend_schema(exclude=True)
    @action(detail=True, methods=['GET'])
    def annotations(self, request: ExtendedRequest, pk: int):
        return get_410_response_for_export_api("/api/projects/id/dataset/export?save_images=False")

    @extend_schema(exclude=True)
    @action(methods=['GET'], detail=True, url_path='backup')
    def export_backup(self, request: ExtendedRequest, pk: int):
        return get_410_response_for_export_api("/api/projects/id/backup/export")

    @extend_schema(methods=['POST'], summary='Recreate a project from a backup',
        description=textwrap.dedent("""
            The backup import process is as follows:

            The first request POST /api/projects/backup will initiate file upload and will create
            the rq job on the server in which the process of a project creating from an uploaded backup
            will be carried out.

            After initiating the backup upload, you will receive an rq_id parameter.
            Make sure to include this parameter as a query parameter in your subsequent requests
            to track the status of the project creation.
            Once the project has been successfully created, the server will return the id of the newly created project.
        """),
        parameters=[
            *ORGANIZATION_OPEN_API_PARAMETERS,
            OpenApiParameter('location', description='Where to import the backup file from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(), default=Location.LOCAL),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('filename', description='Backup file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
            OpenApiParameter('rq_id', description='rq id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
        ],
        request=PolymorphicProxySerializer('BackupWrite',
            # TODO: refactor to use required=False when possible
            serializers=[ProjectFileSerializer, OpenApiTypes.NONE],
            resource_type_field_name=None
        ),
        # TODO: for some reason the code generated by the openapi generator from schema with different serializers
        # contains only one serializer, need to fix that.
        # https://github.com/OpenAPITools/openapi-generator/issues/6126
        responses={
            # 201: OpenApiResponse(inline_serializer("ImportedProjectIdSerializer", fields={"id": serializers.IntegerField(required=True)})
            '201': OpenApiResponse(description='The project has been imported'),
            '202': OpenApiResponse(RqIdSerializer, description='Importing a backup file has been started'),
        })
    @action(detail=False, methods=['OPTIONS', 'POST'], url_path=r'backup/?$',
        serializer_class=None,
        parser_classes=_UPLOAD_PARSER_CLASSES)
    def import_backup(self, request: ExtendedRequest):
        return self.import_backup_v1(request, backup.import_project)

    @tus_chunk_action(detail=False, suffix_base="backup")
    def append_backup_chunk(self, request: ExtendedRequest, file_id: str):
        return self.append_tus_chunk(request, file_id)

    @extend_schema(summary='Get a preview image for a project',
        responses={
            '200': OpenApiResponse(description='Project image preview'),
            '404': OpenApiResponse(description='Project image preview not found'),

        })
    @action(detail=True, methods=['GET'], url_path='preview')
    def preview(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # call check_object_permissions as well

        first_task: Optional[models.Task] = self._object.tasks.order_by('-id').first()
        if not first_task:
            return HttpResponseNotFound('Project image preview not found')

        data_getter = _TaskDataGetter(
            db_task=first_task,
            data_type='preview',
            data_quality='compressed',
        )

        return data_getter()

    @staticmethod
    def _get_rq_response(queue, job_id):
        queue = django_rq.get_queue(queue)
        job = queue.fetch_job(job_id)
        rq_job_meta = ImportRQMeta.for_job(job)
        response = {}
        if job is None or job.is_finished:
            response = { "state": "Finished" }
        elif job.is_queued or job.is_deferred:
            response = { "state": "Queued" }
        elif job.is_failed:
            response = { "state": "Failed", "message": job.exc_info }
        else:
            response = { "state": "Started" }
            response['message'] = rq_job_meta.status or ""
            response['progress'] = rq_job_meta.progress or 0.

        return response

class _DataGetter(metaclass=ABCMeta):
    def __init__(
        self, data_type: str, data_num: Optional[Union[str, int]], data_quality: str
    ) -> None:
        possible_data_type_values = ('chunk', 'frame', 'preview', 'context_image')
        possible_quality_values = ('compressed', 'original')

        if not data_type or data_type not in possible_data_type_values:
            raise ValidationError('Data type not specified or has wrong value')
        elif data_type == 'chunk' or data_type == 'frame' or data_type == 'preview':
            if data_num is None and data_type != 'preview':
                raise ValidationError('Number is not specified')
            elif data_quality not in possible_quality_values:
                raise ValidationError('Wrong quality value')

        self.type = data_type
        self.number = int(data_num) if data_num is not None else None
        self.quality = FrameQuality.COMPRESSED \
            if data_quality == 'compressed' else FrameQuality.ORIGINAL

    @abstractmethod
    def _get_frame_provider(self) -> IFrameProvider: ...

    def __call__(self):
        frame_provider = self._get_frame_provider()

        try:
            if self.type == 'chunk':
                data = frame_provider.get_chunk(self.number, quality=self.quality)
                return HttpResponse(
                    data.data.getvalue(),
                    content_type=data.mime,
                    headers=self._get_chunk_response_headers(data),
                )
            elif self.type == 'frame' or self.type == 'preview':
                if self.type == 'preview':
                    data = frame_provider.get_preview()
                else:
                    data = frame_provider.get_frame(self.number, quality=self.quality)

                return HttpResponse(data.data.getvalue(), content_type=data.mime)

            elif self.type == 'context_image':
                data = frame_provider.get_frame_context_images_chunk(self.number)
                if not data:
                    return HttpResponseNotFound()

                return HttpResponse(data.data, content_type=data.mime)
            else:
                return Response(data='unknown data type {}.'.format(self.type),
                    status=status.HTTP_400_BAD_REQUEST)
        except (ValidationError, PermissionDenied, NotFound) as ex:
            msg = str(ex) if not isinstance(ex, ValidationError) else \
                '\n'.join([str(d) for d in ex.detail])
            return Response(data=msg, status=ex.status_code)
        except (TimeoutError, CvatChunkTimestampMismatchError, LockError):
            return Response(
                status=status.HTTP_429_TOO_MANY_REQUESTS,
                headers={'Retry-After': _RETRY_AFTER_TIMEOUT},
            )

    @abstractmethod
    def _get_chunk_response_headers(self, chunk_data: DataWithMeta) -> dict[str, str]: ...

    _CHUNK_HEADER_BYTES_LENGTH = 1000
    "The number of significant bytes from the chunk header, used for checksum computation"

    def _get_chunk_checksum(self, chunk_data: DataWithMeta) -> str:
        data = chunk_data.data.getbuffer()
        size_checksum = zlib.crc32(str(len(data)).encode())
        return str(zlib.crc32(data[:self._CHUNK_HEADER_BYTES_LENGTH], size_checksum))

    def _make_chunk_response_headers(self, checksum: str, updated_date: datetime) -> dict[str, str]:
        return {
            _DATA_CHECKSUM_HEADER_NAME: str(checksum or ''),
            _DATA_UPDATED_DATE_HEADER_NAME: serializers.DateTimeField().to_representation(updated_date),
        }

class _TaskDataGetter(_DataGetter):
    def __init__(
        self,
        db_task: models.Task,
        *,
        data_type: str,
        data_quality: str,
        data_num: Optional[Union[str, int]] = None,
    ) -> None:
        super().__init__(data_type=data_type, data_num=data_num, data_quality=data_quality)
        self._db_task = db_task

    def _get_frame_provider(self) -> TaskFrameProvider:
        return TaskFrameProvider(self._db_task)

    def _get_chunk_response_headers(self, chunk_data: DataWithMeta) -> dict[str, str]:
        return self._make_chunk_response_headers(
            self._get_chunk_checksum(chunk_data), self._db_task.get_chunks_updated_date(),
        )


class _JobDataGetter(_DataGetter):
    def __init__(
        self,
        db_job: models.Job,
        *,
        data_type: str,
        data_quality: str,
        data_num: Optional[Union[str, int]] = None,
        data_index: Optional[Union[str, int]] = None,
    ) -> None:
        possible_data_type_values = ('chunk', 'frame', 'preview', 'context_image')
        possible_quality_values = ('compressed', 'original')

        if not data_type or data_type not in possible_data_type_values:
            raise ValidationError('Data type not specified or has wrong value')
        elif data_type == 'chunk' or data_type == 'frame' or data_type == 'preview':
            if data_type == 'chunk':
                if data_num is None and data_index is None:
                    raise ValidationError('Number or Index is not specified')
                if data_num is not None and data_index is not None:
                    raise ValidationError('Number and Index cannot be used together')
            elif data_num is None and data_type != 'preview':
                raise ValidationError('Number is not specified')
            elif data_quality not in possible_quality_values:
                raise ValidationError('Wrong quality value')

        self.type = data_type

        self.index = int(data_index) if data_index is not None else None
        self.number = int(data_num) if data_num is not None else None

        self.quality = FrameQuality.COMPRESSED \
            if data_quality == 'compressed' else FrameQuality.ORIGINAL

        self._db_job = db_job

    def _get_frame_provider(self) -> JobFrameProvider:
        return JobFrameProvider(self._db_job)

    def __call__(self):
        if self.type == 'chunk':
            # Reproduce the task chunk indexing
            frame_provider = self._get_frame_provider()

            try:
                if self.index is not None:
                    data = frame_provider.get_chunk(
                        self.index, quality=self.quality, is_task_chunk=False
                    )
                else:
                    data = frame_provider.get_chunk(
                        self.number, quality=self.quality, is_task_chunk=True
                    )

                return HttpResponse(
                    data.data.getvalue(),
                    content_type=data.mime,
                    headers=self._get_chunk_response_headers(data),
                )
            except (TimeoutError, CvatChunkTimestampMismatchError, LockError):
                return Response(
                    status=status.HTTP_429_TOO_MANY_REQUESTS,
                    headers={'Retry-After': _RETRY_AFTER_TIMEOUT},
                )
        else:
            return super().__call__()

    def _get_chunk_response_headers(self, chunk_data: DataWithMeta) -> dict[str, str]:
        return self._make_chunk_response_headers(
            self._get_chunk_checksum(chunk_data),
            self._db_job.segment.chunks_updated_date
        )


@extend_schema(tags=['tasks'])
@extend_schema_view(
    list=extend_schema(
        summary='List tasks',
        responses={
            '200': TaskReadSerializer(many=True),
        }),
    create=extend_schema(
        summary='Create a task',
        description=textwrap.dedent("""\
            The new task will not have any attached images or videos.
            To attach them, use the /api/tasks/<id>/data endpoint.
        """),
        request=TaskWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            '201': TaskReadSerializer, # check TaskWriteSerializer.to_representation
        }),
    retrieve=extend_schema(
        summary='Get task details',
        responses={
            '200': TaskReadSerializer
        }),
    destroy=extend_schema(
        summary='Delete a task',
        description='All attached jobs, annotations and data will be deleted as well.',
        responses={
            '204': OpenApiResponse(description='The task has been deleted'),
        }),
    partial_update=extend_schema(
        summary='Update a task',
        request=TaskWriteSerializer(partial=True),
        responses={
            '200': TaskReadSerializer, # check TaskWriteSerializer.to_representation
        })
)

class TaskViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,
    PartialUpdateModelMixin, UploadMixin, DatasetMixin, BackupMixin
):
    queryset = Task.objects.select_related(
        'data',
        'data__validation_layout',
        'assignee',
        'owner',
        'target_storage',
        'source_storage',
        'annotation_guide',
    ).prefetch_related(
        'segment_set__job_set',
        'segment_set__job_set__assignee',
    ).with_job_summary()

    lookup_fields = {
        'project_name': 'project__name',
        'owner': 'owner__username',
        'assignee': 'assignee__username',
        'tracker_link': 'bug_tracker',
        'validation_mode': 'data__validation_layout__mode',
    }
    search_fields = (
        'project_name', 'name', 'owner', 'status', 'assignee',
        'subset', 'mode', 'dimension', 'tracker_link', 'validation_mode'
    )
    filter_fields = list(search_fields) + ['id', 'project_id', 'updated_date']
    filter_description = textwrap.dedent("""

        There are few examples for complex filtering tasks:\n
            - Get all tasks from 1,2,3 projects - { "and" : [{ "in" : [{ "var" : "project_id" }, [1, 2, 3]]}]}\n
            - Get all completed tasks from 1 project - { "and": [{ "==": [{ "var" : "status" }, "completed"]}, { "==" : [{ "var" : "project_id"}, 1]}]}\n
    """)
    simple_filters = list(search_fields) + ['project_id']
    ordering_fields = list(filter_fields)
    ordering = "-id"
    iam_organization_field = 'organization'
    IMPORT_RQ_ID_FACTORY = functools.partial(RQId,
        RequestAction.IMPORT, RequestTarget.TASK, subresource=RequestSubresource.ANNOTATIONS,
    )

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return TaskReadSerializer
        else:
            return TaskWriteSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = TaskPermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)
        elif self.action == 'preview':
            queryset = Task.objects.select_related('data')

        return queryset

    @extend_schema(summary='Recreate a task from a backup',
        description=textwrap.dedent("""
            The backup import process is as follows:

            The first request POST /api/tasks/backup will initiate file upload and will create
            the rq job on the server in which the process of a task creating from an uploaded backup
            will be carried out.

            After initiating the backup upload, you will receive an rq_id parameter.
            Make sure to include this parameter as a query parameter in your subsequent requests
            to track the status of the task creation.
            Once the task has been successfully created, the server will return the id of the newly created task.
        """),
        parameters=[
            *ORGANIZATION_OPEN_API_PARAMETERS,
            OpenApiParameter('location', description='Where to import the backup file from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(), default=Location.LOCAL),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('filename', description='Backup file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
            OpenApiParameter('rq_id', description='rq id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
        ],
        request=TaskFileSerializer(required=False),
        # TODO: for some reason the code generated by the openapi generator from schema with different serializers
        # contains only one serializer, need to fix that.
        # https://github.com/OpenAPITools/openapi-generator/issues/6126
        responses={
            # 201: OpenApiResponse(inline_serializer("ImportedTaskIdSerializer", fields={"id": serializers.IntegerField(required=True)})
            '201': OpenApiResponse(description='The task has been imported'),
            '202': OpenApiResponse(RqIdSerializer, description='Importing a backup file has been started'),
        })

    @action(detail=False, methods=['OPTIONS', 'POST'], url_path=r'backup/?$',
        serializer_class=None,
        parser_classes=_UPLOAD_PARSER_CLASSES)
    def import_backup(self, request: ExtendedRequest):
        return self.import_backup_v1(request, backup.import_task)

    @tus_chunk_action(detail=False, suffix_base="backup")
    def append_backup_chunk(self, request: ExtendedRequest, file_id: str):
        return self.append_tus_chunk(request, file_id)

    @extend_schema(exclude=True)
    @action(methods=['GET'], detail=True, url_path='backup')
    def export_backup(self, request: ExtendedRequest, pk: int):
        return get_410_response_for_export_api("/api/tasks/id/backup/export")

    @transaction.atomic
    def perform_update(self, serializer):
        instance = serializer.instance

        super().perform_update(serializer)

        updated_instance = serializer.instance

        if instance.project:
            instance.project.touch()
        if updated_instance.project and updated_instance.project != instance.project:
            updated_instance.project.touch()

    @transaction.atomic
    def perform_create(self, serializer, **kwargs):
        serializer.save(
            owner=self.request.user,
            organization=self.request.iam_context['organization']
        )

        if db_project := serializer.instance.project:
            db_project.touch()
            assert serializer.instance.organization == db_project.organization

        # Required for the extra summary information added in the queryset
        serializer.instance = self.get_queryset().get(pk=serializer.instance.pk)

    def _is_data_uploading(self) -> bool:
        return 'data' in self.action

    # UploadMixin method
    def get_upload_dir(self):
        if 'annotations' in self.action:
            return self._object.get_tmp_dirname()
        elif self._is_data_uploading():
            return self._object.data.get_upload_dirname()
        elif 'backup' in self.action:
            return backup.get_backup_dirname()

        assert False

    def _prepare_upload_info_entry(self, filename: str) -> str:
        filename = osp.normpath(filename)
        upload_dir = self.get_upload_dir()
        return osp.join(upload_dir, filename)

    def _maybe_append_upload_info_entry(self, filename: str):
        task_data = cast(Data, self._object.data)

        filename = self._prepare_upload_info_entry(filename)
        task_data.client_files.get_or_create(file=filename)

    def _append_upload_info_entries(self, client_files: list[dict[str, Any]]):
        # batch version of _maybe_append_upload_info_entry() without optional insertion
        task_data = cast(Data, self._object.data)
        bulk_create(
            ClientFile,
            [
                ClientFile(file=self._prepare_upload_info_entry(cf['file'].name), data=task_data)
                for cf in client_files
            ]
        )

    def _sort_uploaded_files(self, uploaded_files: list[str], ordering: list[str]) -> list[str]:
        """
        Applies file ordering for the "predefined" file sorting method of the task creation.

        Read more: https://github.com/cvat-ai/cvat/issues/5061
        """

        expected_files = ordering

        uploaded_file_names = set(uploaded_files)
        mismatching_files = list(uploaded_file_names.symmetric_difference(expected_files))
        if mismatching_files:
            DISPLAY_ENTRIES_COUNT = 5
            mismatching_display = [
                fn + (" (extra)" if fn in uploaded_file_names else " (missing)")
                for fn in mismatching_files[:DISPLAY_ENTRIES_COUNT]
            ]
            remaining_count = len(mismatching_files) - DISPLAY_ENTRIES_COUNT
            raise ValidationError(
                "Uploaded files do not match the '{}' field contents. "
                "Please check the uploaded data and the list of uploaded files. "
                "Mismatching files: {}{}"
                .format(
                    self._UPLOAD_FILE_ORDER_FIELD,
                    ", ".join(mismatching_display),
                    f" (and {remaining_count} more). " if 0 < remaining_count else ""
                )
            )

        return list(expected_files)

    # UploadMixin method
    def init_tus_upload(self, request):
        response = super().init_tus_upload(request)

        if self._is_data_uploading() and response.status_code == status.HTTP_201_CREATED:
            self._maybe_append_upload_info_entry(self._get_metadata(request)['filename'])

        return response

    # UploadMixin method
    @transaction.atomic
    def append_files(self, request):
        client_files = self._get_request_client_files(request)
        if self._is_data_uploading() and client_files:
            self._append_upload_info_entries(client_files)

        return super().append_files(request)

    # UploadMixin method
    def upload_finished(self, request: ExtendedRequest):
        @transaction.atomic
        def _handle_upload_annotations(request: ExtendedRequest):
            format_name = request.query_params.get("format", "")
            filename = request.query_params.get("filename", "")
            conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
            tmp_dir = self._object.get_tmp_dirname()
            annotation_file = os.path.join(tmp_dir, filename)
            if os.path.isfile(annotation_file):
                return _import_annotations(
                        request=request,
                        filename=annotation_file,
                        rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
                        rq_func=dm.task.import_task_annotations,
                        db_obj=self._object,
                        format_name=format_name,
                        conv_mask_to_poly=conv_mask_to_poly,
                    )
            return Response(data='No such file were uploaded',
                    status=status.HTTP_400_BAD_REQUEST)

        def _handle_upload_data(request: ExtendedRequest):
            with transaction.atomic():
                task_data = self._object.data
                serializer = DataSerializer(task_data, data=request.data)
                serializer.is_valid(raise_exception=True)

                # Append new files to the previous ones
                if uploaded_files := serializer.validated_data.get('client_files', None):
                    self.append_files(request)
                    serializer.validated_data['client_files'] = [] # avoid file info duplication

                # Refresh the db value with the updated file list and other request parameters
                db_data = serializer.save()
                self._object.data = db_data
                self._object.save()

                # Create a temporary copy of the parameters we will try to create the task with
                data = copy(serializer.data)

                for optional_field in ['job_file_mapping', 'server_files_exclude', 'validation_params']:
                    if optional_field in serializer.validated_data:
                        data[optional_field] = serializer.validated_data[optional_field]

                if validation_params := getattr(db_data, 'validation_params', None):
                    data['validation_params']['frames'] = set(itertools.chain(
                        data['validation_params'].get('frames', []),
                        validation_params.frames.values_list('path', flat=True).all()
                    ))

                if (
                    data['sorting_method'] == models.SortingMethod.PREDEFINED
                    and (uploaded_files := data['client_files'])
                    and (
                        uploaded_file_order := serializer.validated_data[self._UPLOAD_FILE_ORDER_FIELD]
                    )
                ):
                    # In the case of predefined sorting and custom file ordering,
                    # the requested order must be applied
                    data['client_files'] = self._sort_uploaded_files(
                        uploaded_files, uploaded_file_order
                    )

                data['use_zip_chunks'] = serializer.validated_data['use_zip_chunks']
                data['use_cache'] = serializer.validated_data['use_cache']
                data['copy_data'] = serializer.validated_data['copy_data']

                if data['use_cache']:
                    self._object.data.storage_method = StorageMethodChoice.CACHE
                    self._object.data.save(update_fields=['storage_method'])
                if data['server_files'] and not data.get('copy_data'):
                    self._object.data.storage = StorageChoice.SHARE
                    self._object.data.save(update_fields=['storage'])
                if db_data.cloud_storage:
                    self._object.data.storage = StorageChoice.CLOUD_STORAGE
                    self._object.data.save(update_fields=['storage'])
                if 'stop_frame' not in serializer.validated_data:
                    # if the value of stop_frame is 0, then inside the function we cannot know
                    # the value specified by the user or it's default value from the database
                    data['stop_frame'] = None

            # Need to process task data when the transaction is committed
            rq_id = task.create(self._object, data, request)
            rq_id_serializer = RqIdSerializer(data={'rq_id': rq_id})
            rq_id_serializer.is_valid(raise_exception=True)

            return Response(rq_id_serializer.data, status=status.HTTP_202_ACCEPTED)

        @transaction.atomic
        def _handle_upload_backup(request: ExtendedRequest):
            filename = request.query_params.get("filename", "")
            if filename:
                tmp_dir = backup.get_backup_dirname()
                backup_file = os.path.join(tmp_dir, filename)
                if os.path.isfile(backup_file):
                    return backup.import_task(
                        request,
                        settings.CVAT_QUEUES.IMPORT_DATA.value,
                        filename=backup_file,
                    )
                return Response(data='No such file were uploaded',
                        status=status.HTTP_400_BAD_REQUEST)
            return backup.import_task(request, settings.CVAT_QUEUES.IMPORT_DATA.value)

        if self.action == 'annotations':
            return _handle_upload_annotations(request)
        elif self.action == 'data':
            return _handle_upload_data(request)
        elif self.action == 'import_backup':
            return _handle_upload_backup(request)

        return Response(data='Unknown upload was finished',
                        status=status.HTTP_400_BAD_REQUEST)

    _UPLOAD_FILE_ORDER_FIELD = 'upload_file_order'
    assert _UPLOAD_FILE_ORDER_FIELD in DataSerializer().fields

    @extend_schema(methods=['POST'],
        summary="Attach data to a task",
        description=textwrap.dedent("""\
            Allows to upload data (images, video, etc.) to a task.
            Supports the TUS open file uploading protocol (https://tus.io/).

            Supports the following protocols:

            1. A single Data request

            and

            2.1. An Upload-Start request
            2.2.a. Regular TUS protocol requests (Upload-Length + Chunks)
            2.2.b. Upload-Multiple requests
            2.3. An Upload-Finish request

            Requests:
            - Data - POST, no extra headers or 'Upload-Start' + 'Upload-Finish' headers.
              Contains data in the body.
            - Upload-Start - POST, has an 'Upload-Start' header. No body is expected.
            - Upload-Length - POST, has an 'Upload-Length' header (see the TUS specification)
            - Chunk - HEAD/PATCH (see the TUS specification). Sent to /data/<file id> endpoints.
            - Upload-Finish - POST, has an 'Upload-Finish' header. Can contain data in the body.
            - Upload-Multiple - POST, has an 'Upload-Multiple' header. Contains data in the body.

            The 'Upload-Finish' request allows to specify the uploaded files should be ordered.
            This may be needed if the files can be sent unordered. To state that the input files
            are sent ordered, pass an empty list of files in the '{upload_file_order_field}' field.
            If the files are sent unordered, the ordered file list is expected
            in the '{upload_file_order_field}' field. It must be a list of string file paths,
            relative to the dataset root.

            Example:
            files = [
                "cats/cat_1.jpg",
                "dogs/dog2.jpg",
                "image_3.png",
                ...
            ]

            Independently of the file declaration field used
            ('client_files', 'server_files', etc.), when the 'predefined'
            sorting method is selected, the uploaded files will be ordered according
            to the '.jsonl' manifest file, if it is found in the list of files.
            For archives (e.g. '.zip'), a manifest file ('*.jsonl') is required when using
            the 'predefined' file ordering. Such file must be provided next to the archive
            in the list of files. Read more about manifest files here:
            https://docs.cvat.ai/docs/manual/advanced/dataset_manifest/

            After all data is sent, the operation status can be retrieved via
            the `GET /api/requests/<rq_id>`, where **rq_id** is request ID returned for this request.

            Once data is attached to a task, it cannot be detached or replaced.
        """.format_map(
            {'upload_file_order_field': _UPLOAD_FILE_ORDER_FIELD}
        )),
        # TODO: add a tutorial on this endpoint in the REST API docs
        request=DataSerializer(required=False),
        parameters=[
            OpenApiParameter('Upload-Start', location=OpenApiParameter.HEADER, type=OpenApiTypes.BOOL,
                description='Initializes data upload. Optionally, can include upload metadata in the request body.'),
            OpenApiParameter('Upload-Multiple', location=OpenApiParameter.HEADER, type=OpenApiTypes.BOOL,
                description='Indicates that data with this request are single or multiple files that should be attached to a task'),
            OpenApiParameter('Upload-Finish', location=OpenApiParameter.HEADER, type=OpenApiTypes.BOOL,
                description='Finishes data upload. Can be combined with Upload-Start header to create task data with one request'),
        ],
        responses={
            '202': OpenApiResponse(
                response=PolymorphicProxySerializer(
                    component_name='DataResponse',
                    # FUTURE-FIXME: endpoint should return RqIdSerializer or OpenApiTypes.NONE
                    # but SDK generated from a schema with nullable RqIdSerializer
                    # throws an error when tried to convert empty response to a specific type
                    serializers=[RqIdSerializer, OpenApiTypes.BINARY],
                    resource_type_field_name=None
                ),

                description='Request to attach a data to a task has been accepted'
            ),
        })
    @extend_schema(methods=['GET'],
        summary='Get data of a task',
        parameters=[
            OpenApiParameter('type', location=OpenApiParameter.QUERY, required=False,
                type=OpenApiTypes.STR, enum=['chunk', 'frame', 'context_image'],
                description='Specifies the type of the requested data'),
            OpenApiParameter('quality', location=OpenApiParameter.QUERY, required=False,
                type=OpenApiTypes.STR, enum=['compressed', 'original'],
                description="Specifies the quality level of the requested data"),
            OpenApiParameter('number', location=OpenApiParameter.QUERY, required=False, type=OpenApiTypes.INT,
                description="A unique number value identifying chunk or frame"),
            OpenApiParameter(
                _DATA_CHECKSUM_HEADER_NAME,
                location=OpenApiParameter.HEADER, type=OpenApiTypes.STR, required=False,
                response=[200],
                description="Data checksum, applicable for chunks only",
            ),
            OpenApiParameter(
                _DATA_UPDATED_DATE_HEADER_NAME,
                location=OpenApiParameter.HEADER, type=OpenApiTypes.DATETIME, required=False,
                response=[200],
                description="Data update date, applicable for chunks only",
            )
        ],
        responses={
            '200': OpenApiResponse(description='Data of a specific type'),
        })
    @action(detail=True, methods=['OPTIONS', 'POST', 'GET'], url_path=r'data/?$',
        parser_classes=_UPLOAD_PARSER_CLASSES)
    def data(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # call check_object_permissions as well
        if request.method == 'POST' or request.method == 'OPTIONS':
            with transaction.atomic():
                # Need to make sure that only one Data object can be attached to the task,
                # otherwise this can lead to many problems such as Data objects without a task,
                # multiple RQ data processing jobs at least.
                # It is not possible to use select_for_update with GROUP BY statement and
                # other aggregations that are defined by the viewset queryset,
                # we just need to lock 1 row with the target Task entity.
                locked_instance = Task.objects.select_for_update().get(pk=pk)
                task_data = locked_instance.data
                if not task_data:
                    task_data = Data.objects.create()
                    task_data.make_dirs()
                    locked_instance.data = task_data
                    self._object.data = task_data
                    locked_instance.save()
                elif task_data.size != 0:
                    return Response(data='Adding more data is not supported',
                        status=status.HTTP_400_BAD_REQUEST)
                return self.upload_data(request)
        else:
            data_type = request.query_params.get('type', None)
            data_num = request.query_params.get('number', None)
            data_quality = request.query_params.get('quality', 'compressed')

            data_getter = _TaskDataGetter(
                self._object, data_type=data_type, data_num=data_num, data_quality=data_quality
            )
            return data_getter()

    @tus_chunk_action(detail=True, suffix_base="data")
    def append_data_chunk(self, request: ExtendedRequest, pk: int, file_id: str):
        self._object = self.get_object()
        return self.append_tus_chunk(request, file_id)

    @extend_schema(methods=['GET'], summary='Get task annotations',
        description=textwrap.dedent("""\
            Deprecation warning:

            Utilizing this endpoint to export annotations as a dataset in
            a specific format is no longer possible.

            Consider using new API:
            - `POST /api/tasks/<task_id>/dataset/export?save_images=False` to initiate export process
            - `GET /api/requests/<rq_id>` to check process status,
                where `rq_id` is request id returned on initializing request
            - `GET result_url` to download a prepared file,
                where `result_url` can be found in the response on checking status request
        """),
        parameters=[
            # FUTURE-TODO: the following parameters should be removed after a few releases
            OpenApiParameter('format', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description="This parameter is no longer supported",
                deprecated=True
            ),
            OpenApiParameter('filename', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                deprecated=True
            ),
            OpenApiParameter('action', location=OpenApiParameter.QUERY,
                description='This parameter is no longer supported',
                type=OpenApiTypes.STR, required=False, enum=['download'],
                deprecated=True
            ),
            OpenApiParameter('location', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(),
                deprecated=True
            ),
            OpenApiParameter('cloud_storage_id', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False,
                deprecated=True
            ),
        ],
        responses={
            '200': OpenApiResponse(LabeledDataSerializer),
            '400': OpenApiResponse(description="Exporting without data is not allowed"),
            '410': OpenApiResponse(description="API endpoint no longer handles exporting process"),
        })
    @extend_schema(methods=['PUT'], summary='Replace task annotations / Get annotation import status',
        description=textwrap.dedent("""
            Utilizing this endpoint to check status of the import process is deprecated
            in favor of the new requests API:

            GET /api/requests/<rq_id>, where `rq_id` parameter is returned in the response
            on initializing request.
        """),
        parameters=[
            # deprecated parameters
            OpenApiParameter(
                'format', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='Input format name\nYou can get the list of supported formats at:\n/server/annotation/formats',
                deprecated=True,
            ),
            OpenApiParameter(
                'rq_id', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='rq id',
                deprecated=True,
            ),
        ],
        request=PolymorphicProxySerializer('TaskAnnotationsUpdate',
            # TODO: refactor to use required=False when possible
            serializers=[LabeledDataSerializer, AnnotationFileSerializer, OpenApiTypes.NONE],
            resource_type_field_name=None
        ),
        responses={
            '201': OpenApiResponse(description='Import has finished'),
            '202': OpenApiResponse(description='Import is in progress'),
            '405': OpenApiResponse(description='Format is not available'),
        })
    @extend_schema(methods=['POST'],
        summary="Import annotations into a task",
        description=textwrap.dedent("""
            The request POST /api/tasks/id/annotations initiates a background process to import annotations into a task.
            Please, use the GET /api/requests/<rq_id> endpoint for checking status of the process.
            The `rq_id` parameter can be found in the response on initiating request.
        """),
        parameters=[
            OpenApiParameter('format', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='Input format name\nYou can get the list of supported formats at:\n/server/annotation/formats'),
            OpenApiParameter('location', description='where to import the annotation from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list()),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('use_default_location', description='Use the location that was configured in task to import annotations',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.BOOL, required=False,
                default=True, deprecated=True),
            OpenApiParameter('filename', description='Annotation file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
        ],
        request=PolymorphicProxySerializer('TaskAnnotationsWrite',
            # TODO: refactor to use required=False when possible
            serializers=[AnnotationFileSerializer, OpenApiTypes.NONE],
            resource_type_field_name=None
        ),
        responses={
            '201': OpenApiResponse(description='Uploading has finished'),
            '202': OpenApiResponse(RqIdSerializer, description='Uploading has been started'),
            '405': OpenApiResponse(description='Format is not available'),
        })
    @extend_schema(methods=['PATCH'], summary='Update task annotations',
        parameters=[
            OpenApiParameter('action', location=OpenApiParameter.QUERY, required=True,
                type=OpenApiTypes.STR, enum=['create', 'update', 'delete']),
        ],
        request=LabeledDataSerializer,
        responses={
            '200': LabeledDataSerializer,
        })
    @extend_schema(methods=['DELETE'], summary='Delete task annotations',
        responses={
            '204': OpenApiResponse(description='The annotation has been deleted'),
        })
    @action(detail=True, methods=['GET', 'DELETE', 'PUT', 'PATCH', 'POST', 'OPTIONS'], url_path=r'annotations/?$',
        serializer_class=None, parser_classes=_UPLOAD_PARSER_CLASSES)
    def annotations(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # force call of check_object_permissions()
        if request.method == 'GET':
            if not self._object.data:
                return HttpResponseBadRequest("Exporting annotations from a task without data is not allowed")

            if (
                {"format", "filename", "action", "location", "cloud_storage_id"}
                & request.query_params.keys()
            ):
                return get_410_response_for_export_api("/api/tasks/id/dataset/export?save_images=False")

            data = dm.task.get_task_data(self._object.pk)
            return Response(data)

        elif request.method == 'POST' or request.method == 'OPTIONS':
            # NOTE: initialization process of annotations import
            format_name = request.query_params.get('format', '')
            return self.import_annotations(
                request=request,
                db_obj=self._object,
                import_func=_import_annotations,
                rq_func=dm.task.import_task_annotations,
                rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
            )
        elif request.method == 'PUT':
            format_name = request.query_params.get('format', '')
            # deprecated logic, will be removed in one of the next releases
            if format_name:
                # NOTE: continue process of import annotations
                conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
                location_conf = get_location_configuration(
                    db_instance=self._object, query_params=request.query_params, field_name=StorageType.SOURCE
                )
                return _import_annotations(
                    request=request,
                    rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
                    rq_func=dm.task.import_task_annotations,
                    db_obj=self._object,
                    format_name=format_name,
                    location_conf=location_conf,
                    conv_mask_to_poly=conv_mask_to_poly
                )
            else:
                serializer = LabeledDataSerializer(data=request.data)
                if serializer.is_valid(raise_exception=True):
                    data = dm.task.put_task_data(pk, serializer.validated_data)
                    return Response(data)
        elif request.method == 'DELETE':
            dm.task.delete_task_data(pk)
            return Response(status=status.HTTP_204_NO_CONTENT)
        elif request.method == 'PATCH':
            action = self.request.query_params.get("action", None)
            if action not in dm.task.PatchAction.values():
                raise serializers.ValidationError(
                    "Please specify a correct 'action' for the request")
            serializer = LabeledDataSerializer(data=request.data)
            if serializer.is_valid(raise_exception=True):
                try:
                    data = dm.task.patch_task_data(pk, serializer.validated_data, action)
                except (AttributeError, IntegrityError) as e:
                    return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)
                return Response(data)

    @tus_chunk_action(detail=True, suffix_base="annotations")
    def append_annotations_chunk(self, request: ExtendedRequest, pk: int, file_id: str):
        self._object = self.get_object()
        return self.append_tus_chunk(request, file_id)

    ### --- DEPRECATED METHOD --- ###
    @extend_schema(
        summary='Get the creation status of a task',
        responses={
            '200': RqStatusSerializer,
        },
        deprecated=True,
        description="This method is deprecated and will be removed in one of the next releases. "
                    "To check status of task creation, use new common API "
                    "for managing background operations: GET /api/requests/?action=create&task_id=<task_id>",
    )
    @action(detail=True, methods=['GET'], serializer_class=RqStatusSerializer)
    def status(self, request, pk):
        task = self.get_object() # force call of check_object_permissions()
        response = self._get_rq_response(
            queue=settings.CVAT_QUEUES.IMPORT_DATA.value,
            job_id=RQId(RequestAction.CREATE, RequestTarget.TASK, task.id).render()
        )
        serializer = RqStatusSerializer(data=response)

        serializer.is_valid(raise_exception=True)
        return Response(serializer.data,  headers={'Deprecation': 'true'})

    ### --- DEPRECATED METHOD--- ###
    @staticmethod
    def _get_rq_response(queue, job_id):
        queue = django_rq.get_queue(queue)
        job = queue.fetch_job(job_id)
        rq_job_meta = ImportRQMeta.for_job(job)
        response = {}
        if job is None or job.is_finished:
            response = { "state": "Finished" }
        elif job.is_queued or job.is_deferred:
            response = { "state": "Queued" }
        elif job.is_failed:
            # FIXME: It seems that in some cases exc_info can be None.
            # It's not really clear how it is possible, but it can
            # lead to an error in serializing the response
            # https://github.com/cvat-ai/cvat/issues/5215
            response = { "state": "Failed", "message": parse_exception_message(job.exc_info or "Unknown error") }
        else:
            response = { "state": "Started" }
            if rq_job_meta.status:
                response['message'] = rq_job_meta.status
            response['progress'] = rq_job_meta.progress or 0.

        return response

    @extend_schema(methods=['GET'], summary='Get metainformation for media files in a task',
        responses={
            '200': DataMetaReadSerializer,
        })
    @extend_schema(methods=['PATCH'], summary='Update metainformation for media files in a task',
        request=DataMetaWriteSerializer,
        responses={
            '200': DataMetaReadSerializer,
        })
    @action(detail=True, methods=['GET', 'PATCH'], serializer_class=DataMetaReadSerializer,
        url_path='data/meta')
    def metadata(self, request: ExtendedRequest, pk: int):
        self.get_object() #force to call check_object_permissions
        db_task = models.Task.objects.prefetch_related(
            'segment_set',
            Prefetch('data', queryset=models.Data.objects.select_related('video').prefetch_related(
                Prefetch('images', queryset=models.Image.objects.prefetch_related('related_files').order_by('frame'))
            ))
        ).get(pk=pk)

        if request.method == 'PATCH':
            serializer = DataMetaWriteSerializer(instance=db_task.data, data=request.data)
            serializer.is_valid(raise_exception=True)
            db_task.data = serializer.save()

        if hasattr(db_task.data, 'video'):
            media = [db_task.data.video]
        else:
            media = list(db_task.data.images.all())

        frame_meta = [{
            'width': item.width,
            'height': item.height,
            'name': item.path,
            'related_files': item.related_files.count() if hasattr(item, 'related_files') else 0
        } for item in media]

        db_data = db_task.data
        db_data.frames = frame_meta
        db_data.chunks_updated_date = db_task.get_chunks_updated_date()

        serializer = DataMetaReadSerializer(db_data)
        return Response(serializer.data)

    @extend_schema(exclude=True)
    @action(detail=True, methods=['GET'], serializer_class=None, url_path='dataset')
    def dataset_export(self, request: ExtendedRequest, pk: int):
        return get_410_response_for_export_api("/api/tasks/id/dataset/export?save_images=True")

    @extend_schema(summary='Get a preview image for a task',
        responses={
            '200': OpenApiResponse(description='Task image preview'),
            '404': OpenApiResponse(description='Task image preview not found'),
        })
    @action(detail=True, methods=['GET'], url_path='preview')
    def preview(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # call check_object_permissions as well

        if not self._object.data:
            return HttpResponseNotFound('Task image preview not found')

        data_getter = _TaskDataGetter(
            db_task=self._object,
            data_type='preview',
            data_quality='compressed',
        )
        return data_getter()

    @extend_schema(
        methods=["GET"],
        summary="Allows getting current validation configuration",
        responses={
            '200': OpenApiResponse(TaskValidationLayoutReadSerializer),
        })
    @extend_schema(
        methods=["PATCH"],
        summary="Allows updating current validation configuration",
        description=textwrap.dedent("""
            WARNING: this operation is not protected from race conditions.
            It's up to the user to ensure no parallel calls to this operation happen.
            It affects image access, including exports with images, backups, chunk downloading etc.
        """),
        request=TaskValidationLayoutWriteSerializer,
        responses={
            '200': OpenApiResponse(TaskValidationLayoutReadSerializer),
        },
        examples=[
            OpenApiExample("set honeypots to random validation frames", {
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_UNIFORM
            }),
            OpenApiExample("set honeypots manually", {
                "frame_selection_method": models.JobFrameSelectionMethod.MANUAL,
                "honeypot_real_frames": [10, 20, 22]
            }),
            OpenApiExample("disable validation frames", {
                "disabled_frames": [4, 5, 8]
            }),
            OpenApiExample("restore all validation frames", {
                "disabled_frames": []
            }),
        ])
    @action(detail=True, methods=["GET", "PATCH"], url_path='validation_layout')
    @transaction.atomic
    def validation_layout(self, request: ExtendedRequest, pk: int):
        db_task = cast(models.Task, self.get_object()) # call check_object_permissions as well

        validation_layout = getattr(db_task.data, 'validation_layout', None)

        if request.method == "PATCH":
            if not validation_layout:
                return ValidationError(
                    "Task has no validation setup configured. "
                    "Validation must be initialized during task creation"
                )

            request_serializer = TaskValidationLayoutWriteSerializer(db_task, data=request.data)
            request_serializer.is_valid(raise_exception=True)
            validation_layout = request_serializer.save().data.validation_layout

        if not validation_layout:
            response_serializer = TaskValidationLayoutReadSerializer(SimpleNamespace(mode=None))
            return Response(response_serializer.data, status=status.HTTP_200_OK)

        response_serializer = TaskValidationLayoutReadSerializer(validation_layout)
        return Response(response_serializer.data, status=status.HTTP_200_OK)


@extend_schema(tags=['jobs'])
@extend_schema_view(
    create=extend_schema(
        summary='Create a job',
        request=JobWriteSerializer,
        responses={
            '201': JobReadSerializer, # check JobWriteSerializer.to_representation
        },
        examples=[
            OpenApiExample("create gt job with random 10 frames", {
                "type": models.JobType.GROUND_TRUTH,
                "task_id": 42,
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_UNIFORM,
                "frame_count": 10,
                "random_seed": 1,
            }),
            OpenApiExample("create gt job with random 15% frames", {
                "type": models.JobType.GROUND_TRUTH,
                "task_id": 42,
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_UNIFORM,
                "frame_share": 0.15,
                "random_seed": 1,
            }),
            OpenApiExample("create gt job with 3 random frames in each job", {
                "type": models.JobType.GROUND_TRUTH,
                "task_id": 42,
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_PER_JOB,
                "frames_per_job_count": 3,
                "random_seed": 1,
            }),
            OpenApiExample("create gt job with 20% random frames in each job", {
                "type": models.JobType.GROUND_TRUTH,
                "task_id": 42,
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_PER_JOB,
                "frames_per_job_share": 0.2,
                "random_seed": 1,
            }),
            OpenApiExample("create gt job with manual frame selection", {
                "type": models.JobType.GROUND_TRUTH,
                "task_id": 42,
                "frame_selection_method": models.JobFrameSelectionMethod.MANUAL,
                "frames": [1, 5, 10, 18],
            }),
        ]),
    retrieve=extend_schema(
        summary='Get job details',
        responses={
            '200': JobReadSerializer,
        }),
    list=extend_schema(
        summary='List jobs',
        responses={
            '200': JobReadSerializer(many=True),
        }),
    partial_update=extend_schema(
        summary='Update a job',
        request=JobWriteSerializer(partial=True),
        responses={
            '200': JobReadSerializer, # check JobWriteSerializer.to_representation
        }),
    destroy=extend_schema(
        summary='Delete a job',
        description=textwrap.dedent("""\
            Related annotations will be deleted as well.

            Please note, that not every job can be removed. Currently,
            it is only available for Ground Truth jobs.
            """),
        responses={
            '204': OpenApiResponse(description='The job has been deleted'),
        }),
)
class JobViewSet(viewsets.GenericViewSet, mixins.ListModelMixin, mixins.CreateModelMixin,
    mixins.RetrieveModelMixin, PartialUpdateModelMixin, mixins.DestroyModelMixin,
    UploadMixin, DatasetMixin
):
    queryset = Job.objects.select_related('assignee', 'segment__task__data',
        'segment__task__project', 'segment__task__annotation_guide', 'segment__task__project__annotation_guide',
    )

    iam_organization_field = 'segment__task__organization'
    search_fields = ('task_name', 'project_name', 'assignee', 'state', 'stage')
    filter_fields = list(search_fields) + [
        'id', 'task_id', 'project_id', 'updated_date', 'dimension', 'type', 'parent_job_id',
    ]
    simple_filters = list(set(filter_fields) - {'id', 'updated_date'})
    ordering_fields = list(filter_fields)
    ordering = "-id"
    lookup_fields = {
        'dimension': 'segment__task__dimension',
        'task_id': 'segment__task_id',
        'project_id': 'segment__task__project_id',
        'task_name': 'segment__task__name',
        'project_name': 'segment__task__project__name',
        'assignee': 'assignee__username'
    }
    IMPORT_RQ_ID_FACTORY = functools.partial(RQId,
        RequestAction.IMPORT, RequestTarget.JOB, subresource=RequestSubresource.ANNOTATIONS
    )

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = JobPermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

            queryset = queryset.prefetch_related(
                "segment__task__source_storage", "segment__task__target_storage"
            )
        else:
            queryset = queryset.with_issue_counts() # optimized in JobReadSerializer

        return queryset

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return JobReadSerializer
        else:
            return JobWriteSerializer

    @transaction.atomic
    def perform_create(self, serializer):
        super().perform_create(serializer)

        # Required for the extra summary information added in the queryset
        serializer.instance = self.get_queryset().get(pk=serializer.instance.pk)

    @transaction.atomic
    def perform_destroy(self, instance):
        if instance.type != JobType.GROUND_TRUTH:
            raise ValidationError("Only ground truth jobs can be removed")

        validation_layout: Optional[models.ValidationLayout] = getattr(
            instance.segment.task.data, 'validation_layout', None
        )
        if (validation_layout and validation_layout.mode == models.ValidationMode.GT_POOL):
            raise ValidationError(
                'GT jobs cannot be removed when task validation mode is "{}"'.format(
                    models.ValidationMode.GT_POOL
                )
            )

        super().perform_destroy(instance)

        if validation_layout:
            validation_layout.delete()

    # UploadMixin method
    def get_upload_dir(self):
        return self._object.get_tmp_dirname()

    # UploadMixin method
    def upload_finished(self, request: ExtendedRequest):
        if self.action == 'annotations':
            format_name = request.query_params.get("format", "")
            filename = request.query_params.get("filename", "")
            conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
            tmp_dir = self.get_upload_dir()
            annotation_file = os.path.join(tmp_dir, filename)
            if os.path.isfile(annotation_file):
                return _import_annotations(
                        request=request,
                        filename=annotation_file,
                        rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
                        rq_func=dm.task.import_job_annotations,
                        db_obj=self._object,
                        format_name=format_name,
                        conv_mask_to_poly=conv_mask_to_poly,
                    )
            else:
                return Response(data='No such file were uploaded',
                        status=status.HTTP_400_BAD_REQUEST)
        return Response(data='Unknown upload was finished',
                        status=status.HTTP_400_BAD_REQUEST)

    @extend_schema(methods=['GET'],
        summary="Get job annotations",
        description=textwrap.dedent("""\
            Deprecation warning:

            Utilizing this endpoint to export job dataset in a specific format
            is no longer possible.

            Consider using new API:
            - `POST /api/jobs/<job_id>/dataset/export?save_images=True` to initiate export process
            - `GET /api/requests/<rq_id>` to check process status,
                where `rq_id` is request id returned on initializing request
            - `GET result_url` to download a prepared file,
                where `result_url` can be found in the response on checking status request
        """),
        parameters=[
            OpenApiParameter('format', location=OpenApiParameter.QUERY,
                description='This parameter is no longer supported',
                type=OpenApiTypes.STR, required=False,
                deprecated=True,
            ),
            OpenApiParameter('filename', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                deprecated=True,
            ),
            OpenApiParameter('action', location=OpenApiParameter.QUERY,
                description='This parameter is no longer supported',
                type=OpenApiTypes.STR, required=False,
                deprecated=True,
            ),
            OpenApiParameter('location', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(),
                deprecated=True,
            ),
            OpenApiParameter('cloud_storage_id', description='This parameter is no longer supported',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False,
                deprecated=True,
            ),
        ],
        responses={
            '200': OpenApiResponse(LabeledDataSerializer),
            '410': OpenApiResponse(description="API endpoint no longer handles dataset exporting process"),
        })
    @extend_schema(methods=['POST'],
        summary='Import annotations into a job',
        description=textwrap.dedent("""
            The request POST /api/jobs/id/annotations initiates a background process to import annotations into a job.
            Please, use the GET /api/requests/<rq_id> endpoint for checking status of the process.
            The `rq_id` parameter can be found in the response on initiating request.
        """),
        parameters=[
            OpenApiParameter('format', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='Input format name\nYou can get the list of supported formats at:\n/server/annotation/formats'),
            OpenApiParameter('location', description='where to import the annotation from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list()),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False),
            OpenApiParameter('use_default_location', description='Use the location that was configured in the task to import annotation',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.BOOL, required=False,
                default=True, deprecated=True),
            OpenApiParameter('filename', description='Annotation file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False),
        ],
        request=AnnotationFileSerializer(required=False),
        responses={
            '201': OpenApiResponse(description='Uploading has finished'),
            '202': OpenApiResponse(RqIdSerializer, description='Uploading has been started'),
            '405': OpenApiResponse(description='Format is not available'),
        })
    @extend_schema(methods=['PUT'],
                   summary='Replace job annotations / Get annotation import status',
        description=textwrap.dedent("""
            Utilizing this endpoint to check status of the import process is deprecated
            in favor of the new requests API:
            GET /api/requests/<rq_id>, where `rq_id` parameter is returned in the response
            on initializing request.
        """),
        parameters=[

            OpenApiParameter('format', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='Input format name\nYou can get the list of supported formats at:\n/server/annotation/formats',
                deprecated=True,
            ),
            OpenApiParameter('location', description='where to import the annotation from',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                enum=Location.list(),
                deprecated=True,
            ),
            OpenApiParameter('cloud_storage_id', description='Storage id',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.INT, required=False,
                deprecated=True,
            ),
            OpenApiParameter('filename', description='Annotation file name',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                deprecated=True,
            ),
            OpenApiParameter('rq_id', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR, required=False,
                description='rq id',
                deprecated=True,
            ),
        ],
        request=PolymorphicProxySerializer(
            component_name='JobAnnotationsUpdate',
            serializers=[LabeledDataSerializer, AnnotationFileSerializer(required=False)],
            resource_type_field_name=None
        ),
        responses={
            '201': OpenApiResponse(description='Import has finished'),
            '202': OpenApiResponse(description='Import is in progress'),
            '405': OpenApiResponse(description='Format is not available'),
        })
    @extend_schema(methods=['PATCH'], summary='Update job annotations',
        parameters=[
            OpenApiParameter('action', location=OpenApiParameter.QUERY, type=OpenApiTypes.STR,
                required=True, enum=['create', 'update', 'delete'])
        ],
        request=LabeledDataSerializer,
        responses={
            '200': OpenApiResponse(description='Annotations successfully uploaded'),
        })
    @extend_schema(methods=['DELETE'], summary='Delete job annotations',
        responses={
            '204': OpenApiResponse(description='The annotation has been deleted'),
        })
    @action(detail=True, methods=['GET', 'DELETE', 'PUT', 'PATCH', 'POST', 'OPTIONS'], url_path=r'annotations/?$',
        serializer_class=LabeledDataSerializer, parser_classes=_UPLOAD_PARSER_CLASSES)
    def annotations(self, request: ExtendedRequest, pk: int):
        self._object: models.Job = self.get_object() # force call of check_object_permissions()
        if request.method == 'GET':

            if (
                {"format", "filename", "location", "action", "cloud_storage_id"}
                & request.query_params.keys()
            ):
                return get_410_response_for_export_api("/api/jobs/id/dataset/export?save_images=False")

            annotations = dm.task.get_job_data(self._object.pk)
            return Response(annotations)

        elif request.method == 'POST' or request.method == 'OPTIONS':
            format_name = request.query_params.get('format', '')
            return self.import_annotations(
                request=request,
                db_obj=self._object,
                import_func=_import_annotations,
                rq_func=dm.task.import_job_annotations,
                rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
            )

        elif request.method == 'PUT':
            format_name = request.query_params.get('format', '')
            if format_name:
                # deprecated logic, will be removed in one of the next releases
                conv_mask_to_poly = to_bool(request.query_params.get('conv_mask_to_poly', True))
                location_conf = get_location_configuration(
                    db_instance=self._object, query_params=request.query_params, field_name=StorageType.SOURCE
                )
                return _import_annotations(
                    request=request,
                    rq_id_factory=self.IMPORT_RQ_ID_FACTORY,
                    rq_func=dm.task.import_job_annotations,
                    db_obj=self._object,
                    format_name=format_name,
                    location_conf=location_conf,
                    conv_mask_to_poly=conv_mask_to_poly
                )
            else:
                serializer = LabeledDataSerializer(data=request.data)
                if serializer.is_valid(raise_exception=True):
                    try:
                        data = dm.task.put_job_data(pk, serializer.validated_data)
                    except (AttributeError, IntegrityError) as e:
                        return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)
                    return Response(data)
        elif request.method == 'DELETE':
            dm.task.delete_job_data(pk)
            return Response(status=status.HTTP_204_NO_CONTENT)
        elif request.method == 'PATCH':
            action = self.request.query_params.get("action", None)
            if action not in dm.task.PatchAction.values():
                raise serializers.ValidationError(
                    "Please specify a correct 'action' for the request")
            serializer = LabeledDataSerializer(data=request.data)
            if serializer.is_valid(raise_exception=True):
                try:
                    data = dm.task.patch_job_data(pk, serializer.validated_data, action)
                except (AttributeError, IntegrityError) as e:
                    return Response(data=str(e), status=status.HTTP_400_BAD_REQUEST)
                return Response(data)


    @tus_chunk_action(detail=True, suffix_base="annotations")
    def append_annotations_chunk(self, request: ExtendedRequest, pk: int, file_id: str):
        self._object = self.get_object()
        return self.append_tus_chunk(request, file_id)

    @extend_schema(exclude=True)
    @action(detail=True, methods=['GET'], serializer_class=None, url_path='dataset')
    def dataset_export(self, request: ExtendedRequest, pk: int):
        return get_410_response_for_export_api("/api/jobs/id/dataset/export?save_images=True")

    @extend_schema(summary='Get data of a job',
        parameters=[
            OpenApiParameter('type', description='Specifies the type of the requested data',
                location=OpenApiParameter.QUERY, required=False, type=OpenApiTypes.STR,
                enum=['chunk', 'frame', 'context_image']),
            OpenApiParameter('quality', location=OpenApiParameter.QUERY, required=False,
                type=OpenApiTypes.STR, enum=['compressed', 'original'],
                description="Specifies the quality level of the requested data"),
            OpenApiParameter('number',
                location=OpenApiParameter.QUERY, required=False, type=OpenApiTypes.INT,
                description="A unique number value identifying chunk or frame. "
                    "The numbers are the same as for the task. "
                    "Deprecated for chunks in favor of 'index'"),
            OpenApiParameter('index',
                location=OpenApiParameter.QUERY, required=False, type=OpenApiTypes.INT,
                description="A unique number value identifying chunk, starts from 0 for each job"),
            ],
        responses={
            '200': OpenApiResponse(OpenApiTypes.BINARY, description='Data of a specific type'),
        })
    @action(detail=True, methods=['GET'],
        simple_filters=[] # type query parameter conflicts with the filter
    )
    def data(self, request: ExtendedRequest, pk: int):
        db_job = self.get_object() # call check_object_permissions as well
        data_type = request.query_params.get('type', None)
        data_num = request.query_params.get('number', None)
        data_index = request.query_params.get('index', None)
        data_quality = request.query_params.get('quality', 'compressed')

        data_getter = _JobDataGetter(
            db_job,
            data_type=data_type, data_quality=data_quality,
            data_index=data_index, data_num=data_num
        )
        return data_getter()


    @extend_schema(methods=['GET'], summary='Get metainformation for media files in a job',
        responses={
            '200': DataMetaReadSerializer,
        })
    @extend_schema(methods=['PATCH'], summary='Update metainformation for media files in a job',
        request=JobDataMetaWriteSerializer,
        responses={
            '200': DataMetaReadSerializer,
        }, versions=['2.0'])
    @action(detail=True, methods=['GET', 'PATCH'], serializer_class=DataMetaReadSerializer,
        url_path='data/meta')
    def metadata(self, request: ExtendedRequest, pk: int):
        self.get_object() # force call of check_object_permissions()

        db_job = models.Job.objects.select_related(
            'segment',
            'segment__task',
        ).prefetch_related(
            Prefetch(
                'segment__task__data',
                queryset=models.Data.objects.select_related(
                    'video',
                    'validation_layout',
                ).prefetch_related(
                    Prefetch(
                        'images',
                        queryset=(
                            models.Image.objects
                            .prefetch_related('related_files')
                            .order_by('frame')
                        )
                    )
                )
            )
        ).get(pk=pk)

        if request.method == 'PATCH':
            serializer = JobDataMetaWriteSerializer(instance=db_job, data=request.data)
            serializer.is_valid(raise_exception=True)
            db_job = serializer.save()

        db_segment = db_job.segment
        db_task = db_segment.task
        db_data = db_task.data
        start_frame = db_segment.start_frame
        stop_frame = db_segment.stop_frame
        frame_step = db_data.get_frame_step()
        data_start_frame = db_data.start_frame + start_frame * frame_step
        data_stop_frame = min(db_data.stop_frame, db_data.start_frame + stop_frame * frame_step)
        segment_frame_set = db_segment.frame_set

        if hasattr(db_data, 'video'):
            media = [db_data.video]
        else:
            media = [
                # Insert placeholders if frames are skipped
                # TODO: remove placeholders, UI supports chunks without placeholders already
                # after https://github.com/cvat-ai/cvat/pull/8272
                f if f.frame in segment_frame_set else SimpleNamespace(
                    path=f'placeholder.jpg', width=f.width, height=f.height
                )
                for f in db_data.images.all()
                if f.frame in range(data_start_frame, data_stop_frame + frame_step, frame_step)
            ]

        deleted_frames = set(db_data.deleted_frames)
        if db_job.type == models.JobType.GROUND_TRUTH:
            deleted_frames.update(db_data.validation_layout.disabled_frames)

        # Filter data with segment size
        db_data.deleted_frames = sorted(filter(
            lambda frame: frame >= start_frame and frame <= stop_frame,
            deleted_frames,
        ))

        db_data.start_frame = data_start_frame
        db_data.stop_frame = data_stop_frame
        db_data.size = len(segment_frame_set)
        db_data.included_frames = db_segment.frames or None
        db_data.chunks_updated_date = db_segment.chunks_updated_date

        frame_meta = [{
            'width': item.width,
            'height': item.height,
            'name': item.path,
            'related_files': item.related_files.count() if hasattr(item, 'related_files') else 0
        } for item in media]

        db_data.frames = frame_meta

        serializer = DataMetaReadSerializer(db_data)
        return Response(serializer.data)

    @extend_schema(summary='Get a preview image for a job',
        responses={
            '200': OpenApiResponse(description='Job image preview'),
        })
    @action(detail=True, methods=['GET'], url_path='preview')
    def preview(self, request: ExtendedRequest, pk: int):
        self._object = self.get_object() # call check_object_permissions as well

        data_getter = _JobDataGetter(
            db_job=self._object,
            data_type='preview',
            data_quality='compressed',
        )
        return data_getter()

    @extend_schema(
        methods=["GET"],
        summary="Allows getting current validation configuration",
        responses={
            '200': OpenApiResponse(JobValidationLayoutReadSerializer),
        })
    @extend_schema(
        methods=["PATCH"],
        summary="Allows updating current validation configuration",
        description=textwrap.dedent("""
            WARNING: this operation is not protected from race conditions.
            It's up to the user to ensure no parallel calls to this operation happen.
            It affects image access, including exports with images, backups, chunk downloading etc.
        """),
        request=JobValidationLayoutWriteSerializer,
        responses={
            '200': OpenApiResponse(JobValidationLayoutReadSerializer),
        },
        examples=[
            OpenApiExample("set honeypots to random validation frames", {
                "frame_selection_method": models.JobFrameSelectionMethod.RANDOM_UNIFORM
            }),
            OpenApiExample("set honeypots manually", {
                "frame_selection_method": models.JobFrameSelectionMethod.MANUAL,
                "honeypot_real_frames": [10, 20, 22]
            }),
        ])
    @action(detail=True, methods=["GET", "PATCH"], url_path='validation_layout')
    @transaction.atomic
    def validation_layout(self, request: ExtendedRequest, pk: int):
        self.get_object() # call check_object_permissions as well

        db_job = models.Job.objects.prefetch_related(
            'segment',
            'segment__task',
            Prefetch('segment__task__data',
                queryset=(
                    models.Data.objects
                    .select_related('video', 'validation_layout')
                    .prefetch_related(
                        Prefetch('images', queryset=models.Image.objects.order_by('frame'))
                    )
                )
            )
        ).get(pk=pk)

        if request.method == "PATCH":
            request_serializer = JobValidationLayoutWriteSerializer(db_job, data=request.data)
            request_serializer.is_valid(raise_exception=True)
            db_job = request_serializer.save()

        response_serializer = JobValidationLayoutReadSerializer(db_job)
        return Response(response_serializer.data, status=status.HTTP_200_OK)

@extend_schema(tags=['issues'])
@extend_schema_view(
    retrieve=extend_schema(
        summary='Get issue details',
        responses={
            '200': IssueReadSerializer,
        }),
    list=extend_schema(
        summary='List issues',
        responses={
            '200': IssueReadSerializer(many=True),
        }),
    partial_update=extend_schema(
        summary='Update an issue',
        request=IssueWriteSerializer(partial=True),
        responses={
            '200': IssueReadSerializer, # check IssueWriteSerializer.to_representation
        }),
    create=extend_schema(
        summary='Create an issue',
        request=IssueWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            '201': IssueReadSerializer, # check IssueWriteSerializer.to_representation
        }),
    destroy=extend_schema(
        summary='Delete an issue',
        responses={
            '204': OpenApiResponse(description='The issue has been deleted'),
        })
)
class IssueViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,
    PartialUpdateModelMixin
):
    queryset = Issue.objects.prefetch_related(
        'job__segment__task', 'owner', 'assignee', 'job'
    ).all()

    iam_organization_field = 'job__segment__task__organization'
    search_fields = ('owner', 'assignee')
    filter_fields = list(search_fields) + ['id', 'job_id', 'task_id', 'resolved', 'frame_id']
    simple_filters = list(search_fields) + ['job_id', 'task_id', 'resolved', 'frame_id']
    ordering_fields = list(filter_fields)
    lookup_fields = {
        'owner': 'owner__username',
        'assignee': 'assignee__username',
        'job_id': 'job',
        'task_id': 'job__segment__task__id',
        'frame_id': 'frame',
    }
    ordering = '-id'

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = IssuePermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

        return queryset

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return IssueReadSerializer
        else:
            return IssueWriteSerializer

    def perform_create(self, serializer, **kwargs):
        serializer.save(owner=self.request.user)

@extend_schema(tags=['comments'])
@extend_schema_view(
    retrieve=extend_schema(
        summary='Get comment details',
        responses={
            '200': CommentReadSerializer,
        }),
    list=extend_schema(
        summary='List comments',
        responses={
            '200': CommentReadSerializer(many=True),
        }),
    partial_update=extend_schema(
        summary='Update a comment',
        request=CommentWriteSerializer(partial=True),
        responses={
            '200': CommentReadSerializer, # check CommentWriteSerializer.to_representation
        }),
    create=extend_schema(
        summary='Create a comment',
        request=CommentWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            '201': CommentReadSerializer, # check CommentWriteSerializer.to_representation
        }),
    destroy=extend_schema(
        summary='Delete a comment',
        responses={
            '204': OpenApiResponse(description='The comment has been deleted'),
        })
)
class CommentViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,
    PartialUpdateModelMixin
):
    queryset = Comment.objects.prefetch_related(
        'issue', 'issue__job', 'owner'
    ).all()

    iam_organization_field = 'issue__job__segment__task__organization'
    search_fields = ('owner',)
    filter_fields = list(search_fields) + ['id', 'issue_id', 'frame_id', 'job_id']
    simple_filters = list(search_fields) + ['issue_id', 'frame_id', 'job_id']
    ordering_fields = list(filter_fields)
    ordering = '-id'
    lookup_fields = {
        'owner': 'owner__username',
        'issue_id': 'issue__id',
        'job_id': 'issue__job__id',
        'frame_id': 'issue__frame',
    }

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = CommentPermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

        return queryset

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return CommentReadSerializer
        else:
            return CommentWriteSerializer

    def perform_create(self, serializer, **kwargs):
        serializer.save(owner=self.request.user)


@extend_schema(tags=['labels'])
@extend_schema_view(
    retrieve=extend_schema(
        summary='Get label details',
        responses={
            '200': LabelSerializer,
        }),
    list=extend_schema(
        summary='List labels',
        parameters=[
            # These filters are implemented differently from others
            OpenApiParameter('job_id', type=OpenApiTypes.INT,
                description='A simple equality filter for job id'),
            OpenApiParameter('task_id', type=OpenApiTypes.INT,
                description='A simple equality filter for task id'),
            OpenApiParameter('project_id', type=OpenApiTypes.INT,
                description='A simple equality filter for project id'),
            *ORGANIZATION_OPEN_API_PARAMETERS
        ],
        responses={
            '200': LabelSerializer(many=True),
        }),
    partial_update=extend_schema(
        summary='Update a label',
        description='To modify a sublabel, please use the PATCH method of the parent label.',
        request=LabelSerializer(partial=True),
        responses={
            '200': LabelSerializer,
        }),
    destroy=extend_schema(
        summary='Delete a label',
        description='To delete a sublabel, please use the PATCH method of the parent label.',
        responses={
            '204': OpenApiResponse(description='The label has been deleted'),
        })
)
class LabelViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.DestroyModelMixin, PartialUpdateModelMixin
):
    queryset = Label.objects.prefetch_related(
        'attributespec_set',
        'sublabels__attributespec_set',
        'task',
        'task__owner',
        'task__assignee',
        'task__organization',
        'project',
        'project__owner',
        'project__assignee',
        'project__organization'
    ).all()

    iam_organization_field = ('task__organization', 'project__organization')

    search_fields = ('name', 'parent')
    filter_fields = list(search_fields) + ['id', 'type', 'color', 'parent_id']
    simple_filters = list(set(filter_fields) - {'id'})
    ordering_fields = list(filter_fields)
    lookup_fields = {
        'parent': 'parent__name',
    }
    ordering = 'id'
    serializer_class = LabelSerializer

    def get_queryset(self):
        if self.action == 'list':
            job_id = self.request.GET.get('job_id', None)
            task_id = self.request.GET.get('task_id', None)
            project_id = self.request.GET.get('project_id', None)
            if sum(v is not None for v in [job_id, task_id, project_id]) > 1:
                raise ValidationError(
                    "job_id, task_id and project_id parameters cannot be used together",
                    code=status.HTTP_400_BAD_REQUEST
                )

            if job_id or task_id or project_id:
                if job_id:
                    instance = Job.objects.select_related(
                        'assignee', 'segment__task__organization',
                        'segment__task__owner', 'segment__task__assignee',
                        'segment__task__project__organization',
                        'segment__task__project__owner',
                        'segment__task__project__assignee',
                    ).get(id=job_id)
                elif task_id:
                    instance = Task.objects.select_related(
                        'owner', 'assignee', 'organization',
                        'project__owner', 'project__assignee', 'project__organization',
                    ).get(id=task_id)
                elif project_id:
                    instance = Project.objects.select_related(
                        'owner', 'assignee', 'organization',
                    ).get(id=project_id)

                # NOTE: This filter is too complex to be implemented by other means
                # It requires the following filter query:
                # (
                #  project__task__segment__job__id = job_id
                #  OR
                #  task__segment__job__id = job_id
                #  OR
                #  project__task__id = task_id
                # )
                self.check_object_permissions(self.request, instance)
                queryset = instance.get_labels(prefetch=True)
            else:
                # In other cases permissions are checked already
                queryset = super().get_queryset()
                perm = LabelPermission.create_scope_list(self.request)
                # Include only 1st level labels in list responses
                queryset = perm.filter(queryset).filter(parent__isnull=True)
        else:
            queryset = super().get_queryset()

        return queryset

    def get_serializer(self, *args, **kwargs):
        kwargs['local'] = True
        return super().get_serializer(*args, **kwargs)

    def perform_update(self, serializer):
        if serializer.instance.parent is not None:
            # NOTE: this can be relaxed when skeleton updates are implemented properly
            raise ValidationError(
                "Sublabels cannot be modified this way. "
                "Please send a PATCH request with updated parent label data instead.",
                code=status.HTTP_400_BAD_REQUEST)

        return super().perform_update(serializer)

    def perform_destroy(self, instance: models.Label):
        if instance.parent is not None:
            # NOTE: this can be relaxed when skeleton updates are implemented properly
            raise ValidationError(
                "Sublabels cannot be deleted this way. "
                "Please send a PATCH request with updated parent label data instead.",
                code=status.HTTP_400_BAD_REQUEST)

        if project := instance.project:
            project.touch()
            ProjectWriteSerializer(project).update_child_objects_on_labels_update(project)
        elif task := instance.task:
            task.touch()
            TaskWriteSerializer(task).update_child_objects_on_labels_update(task)

        return super().perform_destroy(instance)


@extend_schema(tags=['users'])
@extend_schema_view(
    list=extend_schema(
        summary='List users',
        responses={
            '200': PolymorphicProxySerializer(
                component_name='MetaUser',
                serializers=[
                    UserSerializer,
                    BasicUserSerializer,
                ],
                resource_type_field_name=None,
                many=True, # https://github.com/tfranzel/drf-spectacular/issues/910
            ),
        }),
    retrieve=extend_schema(
        summary='Get user details',
        responses={
            '200': PolymorphicProxySerializer(
                component_name='MetaUser',
                serializers=[
                    UserSerializer,
                    BasicUserSerializer,
                ],
                resource_type_field_name=None,
            ),
        }),
    partial_update=extend_schema(
        summary='Update a user',
        responses={
            '200': PolymorphicProxySerializer(
                component_name='MetaUser',
                serializers=[
                    UserSerializer(partial=True),
                    BasicUserSerializer(partial=True),
                ],
                resource_type_field_name=None,
            ),
        }),
    destroy=extend_schema(
        summary='Delete a user',
        responses={
            '204': OpenApiResponse(description='The user has been deleted'),
        })
)
class UserViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, PartialUpdateModelMixin, mixins.DestroyModelMixin):
    queryset = User.objects.prefetch_related('groups').all()
    iam_organization_field = 'memberships__organization'

    search_fields = ('username', 'first_name', 'last_name')
    filter_fields = list(search_fields) + ['id', 'is_active']
    simple_filters = list(search_fields) + ['is_active']
    ordering_fields = list(filter_fields)
    ordering = "-id"

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = UserPermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

        return queryset

    def get_serializer_class(self):
        # Early exit for drf-spectacular compatibility
        if getattr(self, 'swagger_fake_view', False):
            return UserSerializer

        user = self.request.user
        is_self = int(self.kwargs.get("pk", 0)) == user.id or \
            self.action == "self"
        if user.is_staff:
            return UserSerializer if not is_self else UserSerializer
        else:
            if is_self and self.request.method in SAFE_METHODS:
                return UserSerializer
            else:
                return BasicUserSerializer

    @extend_schema(summary='Get details of the current user',
        responses={
            '200': PolymorphicProxySerializer(component_name='MetaUser',
                serializers=[
                    UserSerializer, BasicUserSerializer,
                ], resource_type_field_name=None),
        })
    @action(detail=False, methods=['GET'])
    def self(self, request: ExtendedRequest):
        """
        Method returns an instance of a user who is currently authenticated
        """
        serializer_class = self.get_serializer_class()
        serializer = serializer_class(request.user, context={ "request": request })
        return Response(serializer.data)

@extend_schema(tags=['cloudstorages'])
@extend_schema_view(
    retrieve=extend_schema(
        summary='Get cloud storage details',
        responses={
            '200': CloudStorageReadSerializer,
        }),
    list=extend_schema(
        summary='List cloud storages',
        responses={
            '200': CloudStorageReadSerializer(many=True),
        }),
    destroy=extend_schema(
        summary='Delete a cloud storage',
        responses={
            '204': OpenApiResponse(description='The cloud storage has been removed'),
        }),
    partial_update=extend_schema(
        summary='Update a cloud storage',
        request=CloudStorageWriteSerializer(partial=True),
        responses={
            '200': CloudStorageReadSerializer, # check CloudStorageWriteSerializer.to_representation
        }),
    create=extend_schema(
        summary='Create a cloud storage',
        request=CloudStorageWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            '201': CloudStorageReadSerializer, # check CloudStorageWriteSerializer.to_representation
        })
)
class CloudStorageViewSet(viewsets.GenericViewSet, mixins.ListModelMixin,
    mixins.RetrieveModelMixin, mixins.CreateModelMixin, mixins.DestroyModelMixin,
    PartialUpdateModelMixin
):
    queryset = CloudStorageModel.objects.prefetch_related('data').all()

    search_fields = ('provider_type', 'name', 'resource',
                    'credentials_type', 'owner', 'description')
    filter_fields = list(search_fields) + ['id']
    simple_filters = list(set(search_fields) - {'description'})
    ordering_fields = list(filter_fields)
    ordering = "-id"
    lookup_fields = {'owner': 'owner__username', 'name': 'display_name'}
    iam_organization_field = 'organization'

    # Multipart support is necessary here, as CloudStorageWriteSerializer
    # contains a file field (key_file).
    parser_classes = _UPLOAD_PARSER_CLASSES

    def get_serializer_class(self):
        if self.request.method in ('POST', 'PATCH'):
            return CloudStorageWriteSerializer
        else:
            return CloudStorageReadSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == 'list':
            perm = CloudStoragePermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

        provider_type = self.request.query_params.get('provider_type', None)
        if provider_type:
            if provider_type in CloudProviderChoice.list():
                return queryset.filter(provider_type=provider_type)
            raise ValidationError('Unsupported type of cloud provider')
        return queryset

    def perform_create(self, serializer):
        serializer.save(
            owner=self.request.user,
            organization=self.request.iam_context['organization'])

    def create(self, request: ExtendedRequest, *args, **kwargs):
        try:
            response = super().create(request, *args, **kwargs)
        except ValidationError as exceptions:
            msg_body = ""
            for ex in exceptions.args:
                for field, ex_msg in ex.items():
                    msg_body += ': '.join([field, ex_msg if isinstance(ex_msg, str) else str(ex_msg[0])])
                    msg_body += '\n'
            return HttpResponseBadRequest(msg_body)
        except APIException as ex:
            return Response(data=ex.get_full_details(), status=ex.status_code)
        except Exception as ex:
            response = HttpResponseBadRequest(str(ex))
        return response

    @extend_schema(summary='Get cloud storage content',
        parameters=[
            OpenApiParameter('manifest_path', description='Path to the manifest file in a cloud storage',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR),
            OpenApiParameter('prefix', description='Prefix to filter data',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR),
            OpenApiParameter('next_token', description='Used to continue listing files in the bucket',
                location=OpenApiParameter.QUERY, type=OpenApiTypes.STR),
            OpenApiParameter('page_size', location=OpenApiParameter.QUERY, type=OpenApiTypes.INT),
        ],
        responses={
            '200': OpenApiResponse(response=CloudStorageContentSerializer, description='A manifest content'),
        },
    )
    @action(detail=True, methods=['GET'], url_path='content-v2')
    def content_v2(self, request: ExtendedRequest, pk: int):
        storage = None
        try:
            db_storage = self.get_object()
            storage = db_storage_to_storage_instance(db_storage)
            prefix = request.query_params.get('prefix', "")
            page_size = request.query_params.get('page_size', str(settings.BUCKET_CONTENT_MAX_PAGE_SIZE))
            if not page_size.isnumeric():
                return HttpResponseBadRequest('Wrong value for page_size was found')
            page_size = min(int(page_size), settings.BUCKET_CONTENT_MAX_PAGE_SIZE)

            # make api identical to share api
            if prefix and prefix.startswith('/'):
                prefix = prefix[1:]


            next_token = request.query_params.get('next_token')

            if (manifest_path := request.query_params.get('manifest_path')):
                manifest_prefix = os.path.dirname(manifest_path)

                full_manifest_path = os.path.join(db_storage.get_storage_dirname(), manifest_path)
                if not os.path.exists(full_manifest_path) or \
                        datetime.fromtimestamp(os.path.getmtime(full_manifest_path), tz=timezone.utc) < storage.get_file_last_modified(manifest_path):
                    storage.download_file(manifest_path, full_manifest_path)
                manifest = ImageManifestManager(full_manifest_path, db_storage.get_storage_dirname())
                # need to update index
                manifest.set_index()
                try:
                    start_index = int(next_token or '0')
                except ValueError:
                    return HttpResponseBadRequest('Wrong value for the next_token parameter was found.')
                content = manifest.emulate_hierarchical_structure(
                    page_size, manifest_prefix=manifest_prefix, prefix=prefix, default_prefix=storage.prefix, start_index=start_index)
            else:
                content = storage.list_files_on_one_page(prefix, next_token=next_token, page_size=page_size, _use_sort=True)
            for i in content['content']:
                mime_type = get_mime(i['name']) if i['type'] != 'DIR' else 'DIR' # identical to share point
                if mime_type == 'zip':
                    mime_type = 'archive'
                i['mime_type'] = mime_type
            serializer = CloudStorageContentSerializer(data=content)
            serializer.is_valid(raise_exception=True)
            content = serializer.data
            return Response(data=content)

        except CloudStorageModel.DoesNotExist:
            message = f"Storage {pk} does not exist"
            slogger.glob.error(message)
            return HttpResponseNotFound(message)
        except (ValidationError, PermissionDenied, NotFound) as ex:
            msg = str(ex) if not isinstance(ex, ValidationError) else \
                '\n'.join([str(d) for d in ex.detail])
            slogger.cloud_storage[pk].info(msg)
            return Response(data=msg, status=ex.status_code)
        except Exception as ex:
            slogger.glob.error(str(ex))
            return Response("An internal error has occurred",
                status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    @extend_schema(summary='Get a preview image for a cloud storage',
        responses={
            '200': OpenApiResponse(description='Cloud Storage preview'),
            '400': OpenApiResponse(description='Failed to get cloud storage preview'),
            '404': OpenApiResponse(description='Cloud Storage preview not found'),
        })
    @action(detail=True, methods=['GET'], url_path='preview')
    def preview(self, request: ExtendedRequest, pk: int):
        try:
            db_storage = self.get_object()
            cache = MediaCache()

            # The idea is try to define real manifest preview only for the storages that have related manifests
            # because otherwise it can lead to extra calls to a bucket, that are usually not free.
            if not db_storage.has_at_least_one_manifest:
                result = cache.get_cloud_preview(db_storage)
                if not result:
                    return HttpResponseNotFound('Cloud storage preview not found')
                return HttpResponse(result[0].getvalue(), result[1])

            preview, mime = cache.get_or_set_cloud_preview(db_storage)
            return HttpResponse(preview.getvalue(), mime)
        except CloudStorageModel.DoesNotExist:
            message = f"Storage {pk} does not exist"
            slogger.glob.error(message)
            return HttpResponseNotFound(message)
        except (ValidationError, PermissionDenied, NotFound) as ex:
            msg = str(ex) if not isinstance(ex, ValidationError) else \
                '\n'.join([str(d) for d in ex.detail])
            slogger.cloud_storage[pk].info(msg)
            return Response(data=msg, status=ex.status_code)
        except (TimeoutError, CvatChunkTimestampMismatchError, LockError):
            return Response(
                status=status.HTTP_429_TOO_MANY_REQUESTS,
                headers={'Retry-After': _RETRY_AFTER_TIMEOUT},
            )
        except Exception as ex:
            slogger.glob.error(str(ex))
            return Response("An internal error has occurred",
                status=status.HTTP_500_INTERNAL_SERVER_ERROR)

    @extend_schema(summary='Get the status of a cloud storage',
        responses={
            '200': OpenApiResponse(response=OpenApiTypes.STR, description='Cloud Storage status (AVAILABLE | NOT_FOUND | FORBIDDEN)'),
        })
    @action(detail=True, methods=['GET'], url_path='status')
    def status(self, request: ExtendedRequest, pk: int):
        try:
            db_storage = self.get_object()
            storage = db_storage_to_storage_instance(db_storage)
            storage_status = storage.get_status()
            return Response(storage_status)
        except CloudStorageModel.DoesNotExist:
            message = f"Storage {pk} does not exist"
            slogger.glob.error(message)
            return HttpResponseNotFound(message)
        except Exception as ex:
            msg = str(ex)
            return HttpResponseBadRequest(msg)

    @extend_schema(summary='Get allowed actions for a cloud storage',
        responses={
            '200': OpenApiResponse(response=OpenApiTypes.STR, description='Cloud Storage actions (GET | PUT | DELETE)'),
        })
    @action(detail=True, methods=['GET'], url_path='actions')
    def actions(self, request: ExtendedRequest, pk: int):
        '''
        Method return allowed actions for cloud storage. It's required for reading/writing
        '''
        try:
            db_storage = self.get_object()
            storage = db_storage_to_storage_instance(db_storage)
            actions = storage.supported_actions
            return Response(actions, content_type="text/plain")
        except CloudStorageModel.DoesNotExist:
            message = f"Storage {pk} does not exist"
            slogger.glob.error(message)
            return HttpResponseNotFound(message)
        except Exception as ex:
            msg = str(ex)
            return HttpResponseBadRequest(msg)

@extend_schema(tags=['assets'])
@extend_schema_view(
    create=extend_schema(
        summary='Create an asset',
        request={
            'multipart/form-data': {
                'type': 'object',
                'properties': {
                    'file': {
                        'type': 'string',
                        'format': 'binary'
                    }
                }
            }
        },
        responses={
            '201': AssetReadSerializer,
        }),
    retrieve=extend_schema(
        summary='Get an asset',
        responses={
            '200': OpenApiResponse(description='Asset file')
        }),
    destroy=extend_schema(
        summary='Delete an asset',
        responses={
            '204': OpenApiResponse(description='The asset has been deleted'),
        }),
)
class AssetsViewSet(
    viewsets.GenericViewSet, mixins.RetrieveModelMixin,
    mixins.CreateModelMixin, mixins.DestroyModelMixin
):
    queryset = Asset.objects.select_related(
        'owner', 'guide', 'guide__project', 'guide__task', 'guide__project__organization', 'guide__task__organization',
    ).all()
    parser_classes=_UPLOAD_PARSER_CLASSES
    search_fields = ()
    ordering = "uuid"

    def check_object_permissions(self, request: ExtendedRequest, obj):
        super().check_object_permissions(request, obj.guide)

    def get_permissions(self):
        if self.action == 'retrieve':
            return [IsAuthenticatedOrReadPublicResource(), PolicyEnforcer()]
        return super().get_permissions()

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return AssetReadSerializer
        else:
            return AssetWriteSerializer

    def create(self, request: ExtendedRequest, *args, **kwargs):
        file = request.data.get('file', None)
        if not file:
            raise ValidationError('Asset file was not provided')

        if file.size / (1024 * 1024) > settings.ASSET_MAX_SIZE_MB:
            raise ValidationError(f'Maximum size of asset is {settings.ASSET_MAX_SIZE_MB} MB')

        if file.content_type not in settings.ASSET_SUPPORTED_TYPES:
            raise ValidationError(f'File is not supported as an asset. Supported are {settings.ASSET_SUPPORTED_TYPES}')

        guide_id = request.data.get('guide_id')
        db_guide = AnnotationGuide.objects.prefetch_related('assets').get(pk=guide_id)
        if db_guide.assets.count() >= settings.ASSET_MAX_COUNT_PER_GUIDE:
            raise ValidationError(f'Maximum number of assets per guide reached')

        serializer = self.get_serializer(data={
            'filename': file.name,
            'guide_id': guide_id,
        })

        serializer.is_valid(raise_exception=True)
        self.perform_create(serializer)
        path = os.path.join(settings.ASSETS_ROOT, str(serializer.instance.uuid))
        os.makedirs(path)
        if file.content_type in ('image/jpeg', 'image/png'):
            image = Image.open(file)
            if any(map(lambda x: x > settings.ASSET_MAX_IMAGE_SIZE, image.size)):
                scale_factor = settings.ASSET_MAX_IMAGE_SIZE / max(image.size)
                image = image.resize((map(lambda x: int(x * scale_factor), image.size)))
            image.save(os.path.join(path, file.name))
        else:
            with open(os.path.join(path, file.name), 'wb+') as destination:
                for chunk in file.chunks():
                    destination.write(chunk)

        headers = self.get_success_headers(serializer.data)
        return Response(serializer.data, status=status.HTTP_201_CREATED, headers=headers)

    def perform_create(self, serializer):
        serializer.save(owner=self.request.user)

    def retrieve(self, request: ExtendedRequest, *args, **kwargs):
        instance = self.get_object()
        return sendfile(request, os.path.join(settings.ASSETS_ROOT, str(instance.uuid), instance.filename))

    def perform_destroy(self, instance):
        full_path = os.path.join(instance.get_asset_dir(), instance.filename)
        if os.path.exists(full_path):
            os.remove(full_path)
        instance.delete()


@extend_schema(tags=['guides'])
@extend_schema_view(
    create=extend_schema(
        summary='Create an annotation guide',
        description='The new guide will be bound either to a project or a task, depending on parameters.',
        request=AnnotationGuideWriteSerializer,
        responses={
            '201': AnnotationGuideReadSerializer,
        }),
    retrieve=extend_schema(
        summary='Get annotation guide details',
        responses={
            '200': AnnotationGuideReadSerializer,
        }),
    destroy=extend_schema(
        summary='Delete an annotation guide',
        description='This also deletes all assets attached to the guide.',
        responses={
            '204': OpenApiResponse(description='The annotation guide has been deleted'),
        }),
    partial_update=extend_schema(
        summary='Update an annotation guide',
        request=AnnotationGuideWriteSerializer(partial=True),
        responses={
            '200': AnnotationGuideReadSerializer, # check TaskWriteSerializer.to_representation
        })
)
class AnnotationGuidesViewSet(
    viewsets.GenericViewSet, mixins.RetrieveModelMixin,
    mixins.CreateModelMixin, mixins.DestroyModelMixin, PartialUpdateModelMixin
):
    queryset = AnnotationGuide.objects.order_by('-id').select_related(
        'project', 'project__owner', 'project__organization', 'task', 'task__owner', 'task__organization'
    ).prefetch_related('assets').all()
    search_fields = ()
    ordering = "-id"
    iam_organization_field = None

    def _update_related_assets(self, request: ExtendedRequest, guide: AnnotationGuide):
        existing_assets = list(guide.assets.all())
        new_assets = []

        # pylint: disable=anomalous-backslash-in-string
        pattern = re.compile(r'\(/api/assets/([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\)')
        results = set(re.findall(pattern, guide.markdown))

        db_assets_to_copy = {}

        # first check if we need to copy some assets and if user has permissions to access them
        for asset_id in results:
            with suppress(models.Asset.DoesNotExist):
                db_asset = models.Asset.objects.select_related('guide').get(pk=asset_id)
                if db_asset.guide.id != guide.id:
                    perm = AnnotationGuidePermission.create_base_perm(
                        request,
                        self,
                        AnnotationGuidePermission.Scopes.VIEW,
                        get_iam_context(request, db_asset.guide),
                        db_asset.guide
                    )

                    if perm.check_access().allow:
                        db_assets_to_copy[asset_id] = db_asset
                else:
                    new_assets.append(db_asset)

        # then copy those assets, where user has permissions
        assets_mapping = {}
        with transaction.atomic():
            for asset_id in results:
                db_asset = db_assets_to_copy.get(asset_id)
                if db_asset is not None:
                    copied_asset = Asset(
                        filename=db_asset.filename,
                        owner=request.user,
                        guide=guide,
                    )
                    copied_asset.save()
                    assets_mapping[asset_id] = copied_asset

        # finally apply changes on filesystem out of transaction
        try:
            for asset_id in results:
                copied_asset = assets_mapping.get(asset_id)
                if copied_asset is not None:
                    db_asset = db_assets_to_copy.get(asset_id)
                    os.makedirs(copied_asset.get_asset_dir())
                    shutil.copyfile(
                        os.path.join(db_asset.get_asset_dir(), db_asset.filename),
                        os.path.join(copied_asset.get_asset_dir(), db_asset.filename),
                    )

                    guide.markdown = guide.markdown.replace(
                        f'(/api/assets/{asset_id})',
                        f'(/api/assets/{assets_mapping[asset_id].uuid})',
                    )

                    new_assets.append(copied_asset)
        except Exception as ex:
            # in case of any errors, remove copied assets
            for asset_id in assets_mapping:
                assets_mapping[asset_id].delete()
            raise ex

        guide.save()
        for existing_asset in existing_assets:
            if existing_asset not in new_assets:
                existing_asset.delete()

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return AnnotationGuideReadSerializer
        else:
            return AnnotationGuideWriteSerializer

    def perform_create(self, serializer):
        super().perform_create(serializer)
        self._update_related_assets(self.request, serializer.instance)
        serializer.instance.target.touch()

    def perform_update(self, serializer):
        super().perform_update(serializer)
        self._update_related_assets(self.request, serializer.instance)
        serializer.instance.target.touch()

    def perform_destroy(self, instance):
        target = instance.target
        super().perform_destroy(instance)
        target.touch()

def rq_exception_handler(rq_job: RQJob, exc_type: type[Exception], exc_value: Exception, tb):
    rq_job_meta = RQMetaWithFailureInfo.for_job(rq_job)
    rq_job_meta.formatted_exception = "".join(
        traceback.format_exception_only(exc_type, exc_value))
    if rq_job.origin == settings.CVAT_QUEUES.CHUNKS.value:
        rq_job_meta.exc_type = exc_type
        rq_job_meta.exc_args = exc_value.args
    rq_job_meta.save()

    return True

def _import_annotations(
    request: ExtendedRequest,
    rq_id_factory: Callable[..., RQId],
    rq_func: Callable[..., None],
    db_obj: Task | Job,
    format_name: str,
    filename: str = None,
    location_conf: dict[str, Any] | None = None,
    conv_mask_to_poly: bool = True,
):

    format_desc = {f.DISPLAY_NAME: f
        for f in dm.views.get_import_formats()}.get(format_name)
    if format_desc is None:
        raise serializers.ValidationError(
            "Unknown input format '{}'".format(format_name))
    elif not format_desc.ENABLED:
        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)

    rq_id = request.query_params.get('rq_id')
    rq_id_should_be_checked = bool(rq_id)
    if not rq_id:
        rq_id = rq_id_factory(db_obj.pk).render()

    queue = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)

    # ensure that there is no race condition when processing parallel requests
    with get_rq_lock_for_job(queue, rq_id):
        rq_job = queue.fetch_job(rq_id)

        if rq_job:
            if rq_id_should_be_checked and not is_rq_job_owner(rq_job, request.user.id):
                return Response(status=status.HTTP_403_FORBIDDEN)

            if request.method == 'POST':
                if rq_job.get_status(refresh=False) not in (RQJobStatus.FINISHED, RQJobStatus.FAILED):
                    return Response(status=status.HTTP_409_CONFLICT, data='Import job already exists')

                rq_job.delete()
                rq_job = None

        if not rq_job:
            # If filename is specified we consider that file was uploaded via TUS, so it exists in filesystem
            # Then we dont need to create temporary file
            # Or filename specify key in cloud storage so we need to download file
            location = location_conf.get('location') if location_conf else Location.LOCAL
            db_storage = None

            if not filename or location == Location.CLOUD_STORAGE:
                if location != Location.CLOUD_STORAGE:
                    serializer = AnnotationFileSerializer(data=request.data)
                    if serializer.is_valid(raise_exception=True):
                        anno_file = serializer.validated_data['annotation_file']
                        with NamedTemporaryFile(
                            prefix='cvat_{}'.format(db_obj.pk),
                            dir=settings.TMP_FILES_ROOT,
                            delete=False) as tf:
                            filename = tf.name
                            for chunk in anno_file.chunks():
                                tf.write(chunk)
                else:
                    assert filename, 'The filename was not specified'

                    try:
                        storage_id = location_conf['storage_id']
                    except KeyError:
                        raise serializers.ValidationError(
                            'Cloud storage location was selected as the source,'
                            ' but cloud storage id was not specified')
                    db_storage = get_cloud_storage_for_import_or_export(
                        storage_id=storage_id, request=request,
                        is_default=location_conf['is_default'])

                    key = filename
                    with NamedTemporaryFile(
                        prefix='cvat_{}'.format(db_obj.pk),
                        dir=settings.TMP_FILES_ROOT,
                        delete=False) as tf:
                        filename = tf.name

            func = import_resource_with_clean_up_after
            func_args = (rq_func, filename, db_obj.pk, format_name, conv_mask_to_poly)

            if location == Location.CLOUD_STORAGE:
                func_args = (db_storage, key, func) + func_args
                func = import_resource_from_cloud_storage

            av_scan_paths(filename)
            user_id = request.user.id

            with get_rq_lock_by_user(queue, user_id):
                meta = ImportRQMeta.build_for(request=request, db_obj=db_obj, tmp_file=filename)
                queue.enqueue_call(
                    func=func,
                    args=func_args,
                    job_id=rq_id,
                    depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),
                    meta=meta,
                    result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),
                    failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()
                )

    # log events after releasing Redis lock
    if not rq_job:
        handle_dataset_import(db_obj, format_name=format_name, cloud_storage_id=db_storage.id if db_storage else None)

        serializer = RqIdSerializer(data={'rq_id': rq_id})
        serializer.is_valid(raise_exception=True)

        return Response(serializer.data, status=status.HTTP_202_ACCEPTED)

    # Deprecated logic, /api/requests API should be used instead
    # https://greenbytes.de/tech/webdav/draft-ietf-httpapi-deprecation-header-latest.html#the-deprecation-http-response-header-field
    deprecation_timestamp = int(datetime(2025, 2, 14, tzinfo=timezone.utc).timestamp())
    response_headers  = {
        "Deprecation": f"@{deprecation_timestamp}"
    }

    rq_job_status = rq_job.get_status(refresh=False)
    if RQJobStatus.FINISHED == rq_job_status:
        rq_job.delete()
        return Response(status=status.HTTP_201_CREATED, headers=response_headers)
    elif RQJobStatus.FAILED == rq_job_status:
        exc_info = process_failed_job(rq_job)

        import_error_prefix = f'{CvatImportError.__module__}.{CvatImportError.__name__}:'
        if exc_info.startswith("Traceback") and import_error_prefix in exc_info:
            exc_message = exc_info.split(import_error_prefix)[-1].strip()
            return Response(data=exc_message, status=status.HTTP_400_BAD_REQUEST, headers=response_headers)
        else:
            return Response(data=exc_info,
                status=status.HTTP_500_INTERNAL_SERVER_ERROR, headers=response_headers)

    return Response(status=status.HTTP_202_ACCEPTED, headers=response_headers)

def _import_project_dataset(
    request: ExtendedRequest,
    rq_id_factory: Callable[..., RQId],
    rq_func: Callable[..., None],
    db_obj: Project,
    format_name: str,
    filename: str | None = None,
    conv_mask_to_poly: bool = True,
    location_conf: dict[str, Any] | None = None
):
    format_desc = {f.DISPLAY_NAME: f
        for f in dm.views.get_import_formats()}.get(format_name)
    if format_desc is None:
        raise serializers.ValidationError(
            "Unknown input format '{}'".format(format_name))
    elif not format_desc.ENABLED:
        return Response(status=status.HTTP_405_METHOD_NOT_ALLOWED)

    rq_id = rq_id_factory(db_obj.pk).render()

    queue: DjangoRQ = django_rq.get_queue(settings.CVAT_QUEUES.IMPORT_DATA.value)

    # ensure that there is no race condition when processing parallel requests
    with get_rq_lock_for_job(queue, rq_id):
        rq_job = queue.fetch_job(rq_id)

        if rq_job:
            rq_job_status = rq_job.get_status(refresh=False)
            if rq_job_status not in (RQJobStatus.FINISHED, RQJobStatus.FAILED):
                return Response(status=status.HTTP_409_CONFLICT, data='Import job already exists')

            # for some reason the previous job has not been deleted
            # (e.g the user closed the browser tab when job has been created
            # but no one requests for checking status were not made)
            rq_job.delete()
            rq_job = None

        location = location_conf.get('location') if location_conf else None
        db_storage = None

        if not filename and location != Location.CLOUD_STORAGE:
            serializer = DatasetFileSerializer(data=request.data)
            if serializer.is_valid(raise_exception=True):
                dataset_file = serializer.validated_data['dataset_file']
                with NamedTemporaryFile(
                    prefix='cvat_{}'.format(db_obj.pk),
                    dir=settings.TMP_FILES_ROOT,
                    delete=False) as tf:
                    filename = tf.name
                    for chunk in dataset_file.chunks():
                        tf.write(chunk)

        elif location == Location.CLOUD_STORAGE:
            assert filename, 'The filename was not specified'
            try:
                storage_id = location_conf['storage_id']
            except KeyError:
                raise serializers.ValidationError(
                    'Cloud storage location was selected as the source,'
                    ' but cloud storage id was not specified')
            db_storage = get_cloud_storage_for_import_or_export(
                storage_id=storage_id, request=request,
                is_default=location_conf['is_default'])

            key = filename
            with NamedTemporaryFile(
                prefix='cvat_{}'.format(db_obj.pk),
                dir=settings.TMP_FILES_ROOT,
                delete=False) as tf:
                filename = tf.name

        func = import_resource_with_clean_up_after
        func_args = (rq_func, filename, db_obj.pk, format_name, conv_mask_to_poly)

        if location == Location.CLOUD_STORAGE:
            func_args = (db_storage, key, func) + func_args
            func = import_resource_from_cloud_storage

        user_id = request.user.id

        with get_rq_lock_by_user(queue, user_id):
            meta = ImportRQMeta.build_for(request=request, db_obj=db_obj, tmp_file=filename)
            queue.enqueue_call(
                func=func,
                args=func_args,
                job_id=rq_id,
                meta=meta,
                depends_on=define_dependent_job(queue, user_id, rq_id=rq_id),
                result_ttl=settings.IMPORT_CACHE_SUCCESS_TTL.total_seconds(),
                failure_ttl=settings.IMPORT_CACHE_FAILED_TTL.total_seconds()
            )


    handle_dataset_import(db_obj, format_name=format_name, cloud_storage_id=db_storage.id if db_storage else None)

    serializer = RqIdSerializer(data={'rq_id': rq_id})
    serializer.is_valid(raise_exception=True)

    return Response(serializer.data, status=status.HTTP_202_ACCEPTED)

@extend_schema(tags=['requests'])
@extend_schema_view(
    list=extend_schema(
        summary='List requests',
        responses={
            '200': RequestSerializer(many=True),
        }
    ),
    retrieve=extend_schema(
        summary='Get request details',
        responses={
            '200': RequestSerializer,
        }
    ),
)
class RequestViewSet(viewsets.GenericViewSet):
    # FUTURE-TODO: support re-enqueue action
    SUPPORTED_QUEUES = (
        settings.CVAT_QUEUES.IMPORT_DATA.value,
        settings.CVAT_QUEUES.EXPORT_DATA.value,
    )

    serializer_class = RequestSerializer
    iam_organization_field = None
    filter_backends = [
        NonModelSimpleFilter,
        NonModelJsonLogicFilter,
        NonModelOrderingFilter,
    ]

    ordering_fields = ['created_date', 'status', 'action']
    ordering = '-created_date'

    filter_fields = [
        # RQ job fields
        'status',
        # derivatives fields (from meta)
        'project_id',
        'task_id',
        'job_id',
        # derivatives fields (from parsed rq_id)
        'action',
        'target',
        'subresource',
        'format',
    ]

    simple_filters = filter_fields + ['org']

    lookup_fields = {
        'created_date': 'created_at',
        'action': 'parsed_rq_id.action',
        'target': 'parsed_rq_id.target',
        'subresource': 'parsed_rq_id.subresource',
        'format': 'parsed_rq_id.format',
        'status': 'get_status',
        'project_id': 'meta.project_id',
        'task_id': 'meta.task_id',
        'job_id': 'meta.job_id',
        'org': 'meta.org_slug',
    }

    SchemaField = namedtuple('SchemaField', ['type', 'choices'], defaults=(None,))

    simple_filters_schema = {
        'status': SchemaField('string', RequestStatus.choices),
        'project_id': SchemaField('integer'),
        'task_id': SchemaField('integer'),
        'job_id': SchemaField('integer'),
        'action': SchemaField('string', RequestAction.choices),
        'target': SchemaField('string', RequestTarget.choices),
        'subresource': SchemaField('string', RequestSubresource.choices),
        'format': SchemaField('string'),
        'org': SchemaField('string'),
    }

    def get_queryset(self):
        return None

    @property
    def queues(self) -> Iterable[DjangoRQ]:
        return (django_rq.get_queue(queue_name) for queue_name in self.SUPPORTED_QUEUES)

    def _get_rq_jobs_from_queue(self, queue: DjangoRQ, user_id: int) -> list[RQJob]:
        job_ids = set(queue.get_job_ids() +
            queue.started_job_registry.get_job_ids() +
            queue.finished_job_registry.get_job_ids() +
            queue.failed_job_registry.get_job_ids() +
            queue.deferred_job_registry.get_job_ids()
        )
        jobs = []
        for job in queue.job_class.fetch_many(job_ids, queue.connection):
            if job and is_rq_job_owner(job, user_id):
                try:
                    parsed_rq_id = RQId.parse(job.id)
                except Exception: # nosec B112
                    continue
                job.parsed_rq_id = parsed_rq_id
                jobs.append(job)

        return jobs


    def _get_rq_jobs(self, user_id: int) -> list[RQJob]:
        """
        Get all RQ jobs for a specific user and return them as a list of RQJob objects.

        Parameters:
            user_id (int): The ID of the user for whom to retrieve jobs.

        Returns:
            List[RQJob]: A list of RQJob objects representing all jobs for the specified user.
        """
        all_jobs = []
        for queue in self.queues:
            jobs = self._get_rq_jobs_from_queue(queue, user_id)
            all_jobs.extend(jobs)

        return all_jobs

    def _get_rq_job_by_id(self, rq_id: str) -> Optional[RQJob]:
        """
        Get a RQJob by its ID from the queues.

        Args:
            rq_id (str): The ID of the RQJob to retrieve.

        Returns:
            Optional[RQJob]: The retrieved RQJob, or None if not found.
        """
        try:
            parsed_rq_id = RQId.parse(rq_id)
        except Exception:
            return None

        job: Optional[RQJob] = None

        for queue in self.queues:
            job = queue.fetch_job(rq_id)
            if job:
                job.parsed_rq_id = parsed_rq_id
                break

        return job

    def _handle_redis_exceptions(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            try:
                return func(*args, **kwargs)
            except RedisConnectionError as ex:
                msg = 'Redis service is not available'
                slogger.glob.exception(f'{msg}: {str(ex)}')
                return Response(msg, status=status.HTTP_503_SERVICE_UNAVAILABLE)
        return wrapper

    @method_decorator(never_cache)
    @_handle_redis_exceptions
    def retrieve(self, request: ExtendedRequest, pk: str):
        job = self._get_rq_job_by_id(pk)

        if not job:
            return HttpResponseNotFound("There is no request with specified id")

        self.check_object_permissions(request, job)

        serializer = self.get_serializer(job, context={'request': request})
        return Response(data=serializer.data, status=status.HTTP_200_OK)

    @method_decorator(never_cache)
    @_handle_redis_exceptions
    def list(self, request: ExtendedRequest):
        user_id = request.user.id
        user_jobs = self._get_rq_jobs(user_id)

        filtered_jobs = self.filter_queryset(user_jobs)

        page = self.paginate_queryset(filtered_jobs)
        if page is not None:
            serializer = self.get_serializer(page, many=True, context={'request': request})
            return self.get_paginated_response(serializer.data)

        serializer = self.get_serializer(filtered_jobs, many=True, context={'request': request})
        return Response(data=serializer.data, status=status.HTTP_200_OK)

    @extend_schema(
        summary='Cancel request',
        request=None,
        responses={
            '200': OpenApiResponse(description='The request has been cancelled'),
        },
    )
    @method_decorator(never_cache)
    @action(detail=True, methods=['POST'], url_path='cancel')
    @_handle_redis_exceptions
    def cancel(self, request: ExtendedRequest, pk: str):
        rq_job = self._get_rq_job_by_id(pk)

        if not rq_job:
            return HttpResponseNotFound("There is no request with specified id")

        self.check_object_permissions(request, rq_job)

        if rq_job.get_status(refresh=False) not in {RQJobStatus.QUEUED, RQJobStatus.DEFERRED}:
            return HttpResponseBadRequest("Only requests that have not yet been started can be cancelled")

        # FUTURE-TODO: race condition is possible here
        rq_job.cancel(enqueue_dependents=settings.ONE_RUNNING_JOB_IN_QUEUE_PER_USER)
        rq_job.delete()

        return Response(status=status.HTTP_200_OK)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\view_utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# NOTE: importing in the utils.py header leads to circular importing

from typing import Optional

from django.db.models.query import QuerySet
from django.http.response import HttpResponse
from drf_spectacular.utils import extend_schema
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework.serializers import Serializer
from rest_framework.viewsets import GenericViewSet

from cvat.apps.engine.mixins import UploadMixin
from cvat.apps.engine.parsers import TusUploadParser
from cvat.apps.engine.types import ExtendedRequest


def make_paginated_response(
    queryset: QuerySet,
    *,
    viewset: GenericViewSet,
    response_type: Optional[type[HttpResponse]] = None,
    serializer_type: Optional[type[Serializer]] = None,
    request: Optional[type[ExtendedRequest]] = None,
    **serializer_params
):
    # Adapted from the mixins.ListModelMixin.list()

    serializer_params.setdefault('many', True)

    if response_type is None:
        response_type = Response

    if request is None:
        request = getattr(viewset, 'request', None)

    if request is not None:
        context = serializer_params.setdefault('context', {})
        context.setdefault('request', request)

    if serializer_type is None:
        serializer_type = viewset.get_serializer

    page = viewset.paginate_queryset(queryset)
    if page is not None:
        serializer = serializer_type(page, **serializer_params)
        return viewset.get_paginated_response(serializer.data)

    serializer = serializer_type(queryset, **serializer_params)

    return response_type(serializer.data)

def list_action(serializer_class: type[Serializer], **kwargs):
    params = dict(
        detail=True,
        methods=["GET"],
        serializer_class=serializer_class,

        # Restore the default pagination
        pagination_class=GenericViewSet.pagination_class,

        # Remove the regular list() parameters from the swagger schema.
        # Unset, they would be taken from the enclosing class, which is wrong.
        # https://drf-spectacular.readthedocs.io/en/latest/faq.html#my-action-is-erroneously-paginated-or-has-filter-parameters-that-i-do-not-want
        filter_fields=None, search_fields=None, ordering_fields=None, simple_filters=None
    )
    params.update(kwargs)

    return action(**params)


def tus_chunk_action(*, detail: bool, suffix_base: str):
    def decorator(f):
        f = action(detail=detail, methods=['HEAD', 'PATCH'],
            url_path=f'{suffix_base}/{UploadMixin.file_id_regex}',
            parser_classes=[TusUploadParser],
            serializer_class=None,
        )(f)

        # tus chunk endpoints are never accessed directly (the client must
        # access them by following the Location header from the response to
        # the creation endpoint). Moreover, the details of how these endpoints
        # work are already described by the tus specification. Since we don't
        # need to document either where these points are or how they work,
        # they don't need to be in the schema.
        f = extend_schema(exclude=True)(f)

        return f

    return decorator


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Force execution of declared symbols
from .schema import *  # pylint: disable=wildcard-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\management\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\management\commands\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0001_release_v0_1_0.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-05-23 11:51

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):
    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name="AttributeSpec",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("text", models.CharField(max_length=1024)),
            ],
        ),
        migrations.CreateModel(
            name="Job",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="Label",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("name", models.CharField(max_length=64)),
            ],
        ),
        migrations.CreateModel(
            name="LabeledBox",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                ("xtl", models.FloatField()),
                ("ytl", models.FloatField()),
                ("xbr", models.FloatField()),
                ("ybr", models.FloatField()),
                ("occluded", models.BooleanField(default=False)),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
                ),
                (
                    "label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Label"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledBoxAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "box",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledBox"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="ObjectPath",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
                ),
                (
                    "label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Label"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="ObjectPathAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
                (
                    "track",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.ObjectPath"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="Segment",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("start_frame", models.IntegerField()),
                ("stop_frame", models.IntegerField()),
            ],
        ),
        migrations.CreateModel(
            name="Task",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("name", models.CharField(max_length=256)),
                ("size", models.PositiveIntegerField()),
                ("path", models.CharField(max_length=256)),
                ("mode", models.CharField(max_length=32)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now_add=True)),
                ("status", models.CharField(default="annotate", max_length=32)),
                ("bug_tracker", models.CharField(default="", max_length=2000)),
                (
                    "owner",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                ("overlap", models.PositiveIntegerField(default=0)),
            ],
            options={
                "permissions": (
                    ("view_task", "Can see available tasks"),
                    ("view_annotation", "Can see annotation for the task"),
                    ("change_annotation", "Can modify annotation for the task"),
                ),
            },
        ),
        migrations.CreateModel(
            name="TrackedBox",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("xtl", models.FloatField()),
                ("ytl", models.FloatField()),
                ("xbr", models.FloatField()),
                ("ybr", models.FloatField()),
                ("occluded", models.BooleanField(default=False)),
                ("frame", models.PositiveIntegerField()),
                ("outside", models.BooleanField(default=False)),
                (
                    "track",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.ObjectPath"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedBoxAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "box",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.TrackedBox"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.AddField(
            model_name="segment",
            name="task",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Task"),
        ),
        migrations.AddField(
            model_name="label",
            name="task",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Task"),
        ),
        migrations.AddField(
            model_name="job",
            name="segment",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.Segment"
            ),
        ),
        migrations.AddField(
            model_name="attributespec",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Label"),
        ),
        migrations.AlterField(
            model_name="labeledbox",
            name="id",
            field=models.PositiveIntegerField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="objectpath",
            name="id",
            field=models.PositiveIntegerField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedbox",
            name="id",
            field=models.PositiveIntegerField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledbox",
            name="id",
            field=models.AutoField(
                auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
            ),
        ),
        migrations.AlterField(
            model_name="objectpath",
            name="id",
            field=models.AutoField(
                auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
            ),
        ),
        migrations.AlterField(
            model_name="trackedbox",
            name="id",
            field=models.AutoField(
                auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
            ),
        ),
        migrations.AddField(
            model_name="job",
            name="annotator",
            field=models.ForeignKey(
                null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0002_labeledpoints_labeledpointsattributeval_labeledpolygon_labeledpolygonattributeval_labeledpolyline_la.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-05-30 09:53

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0001_release_v0_1_0"),
    ]

    operations = [
        migrations.CreateModel(
            name="LabeledPoints",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
                ),
                (
                    "label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Label"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledPointsAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "points",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledPoints"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledPolygon",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
                ),
                (
                    "label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Label"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledPolygonAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "polygon",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledPolygon"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledPolyline",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
                ),
                (
                    "label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Label"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="LabeledPolylineAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "polyline",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledPolyline"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPoints",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                ("frame", models.PositiveIntegerField()),
                ("outside", models.BooleanField(default=False)),
                (
                    "track",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.ObjectPath"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPointsAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "points",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.TrackedPoints"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPolygon",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                ("frame", models.PositiveIntegerField()),
                ("outside", models.BooleanField(default=False)),
                (
                    "track",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.ObjectPath"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPolygonAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "polygon",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.TrackedPolygon"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPolyline",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("points", models.TextField()),
                ("frame", models.PositiveIntegerField()),
                ("outside", models.BooleanField(default=False)),
                (
                    "track",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.ObjectPath"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
        migrations.CreateModel(
            name="TrackedPolylineAttributeVal",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("value", models.CharField(max_length=64)),
                (
                    "polyline",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.TrackedPolyline"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0003_objectpath_shapes.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-06-04 11:21

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        (
            "engine",
            "0002_labeledpoints_labeledpointsattributeval_labeledpolygon_labeledpolygonattributeval_labeledpolyline_la",
        ),
    ]

    operations = [
        migrations.AddField(
            model_name="objectpath",
            name="shapes",
            field=models.CharField(default="boxes", max_length=10),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0004_task_z_order.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-06-09 10:09

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0003_objectpath_shapes"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="z_order",
            field=models.BooleanField(default=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0005_auto_20180609_1512.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-06-09 12:12

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0004_task_z_order"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledbox",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpoints",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpolygon",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpolyline",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="trackedbox",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="trackedpoints",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="trackedpolygon",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
        migrations.AddField(
            model_name="trackedpolyline",
            name="z_order",
            field=models.IntegerField(default=0),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0006_auto_20180629_1501.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-06-29 12:01

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0005_auto_20180609_1512"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledbox",
            name="group_id",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpoints",
            name="group_id",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpolygon",
            name="group_id",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="labeledpolyline",
            name="group_id",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="objectpath",
            name="group_id",
            field=models.PositiveIntegerField(default=0),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0007_task_flipped.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Generated by Django 2.0.3 on 2018-07-16 08:19

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0006_auto_20180629_1501"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="flipped",
            field=models.BooleanField(default=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0008_auto_20180917_1424.py =====
# Generated by Django 2.0.3 on 2018-09-17 11:24

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0007_task_flipped"),
    ]

    operations = [
        migrations.AlterField(
            model_name="task",
            name="owner",
            field=models.ForeignKey(
                null=True, on_delete=django.db.models.deletion.SET_NULL, to=settings.AUTH_USER_MODEL
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0009_auto_20180917_1424.py =====
# Generated by Django 2.0.3 on 2018-09-17 11:24

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0008_auto_20180917_1424"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledbox",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledboxattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpoints",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpointsattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpolygon",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpolygonattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpolyline",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="labeledpolylineattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="objectpath",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="objectpathattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedbox",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedboxattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpoints",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpointsattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpolygon",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpolygonattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpolyline",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
        migrations.AlterField(
            model_name="trackedpolylineattributeval",
            name="id",
            field=models.BigAutoField(primary_key=True, serialize=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0010_auto_20181011_1517.py =====
# Generated by Django 2.0.9 on 2018-10-11 12:17

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0009_auto_20180917_1424"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledbox",
            name="client_id",
            field=models.BigIntegerField(default=-1),
        ),
        migrations.AddField(
            model_name="labeledpoints",
            name="client_id",
            field=models.BigIntegerField(default=-1),
        ),
        migrations.AddField(
            model_name="labeledpolygon",
            name="client_id",
            field=models.BigIntegerField(default=-1),
        ),
        migrations.AddField(
            model_name="labeledpolyline",
            name="client_id",
            field=models.BigIntegerField(default=-1),
        ),
        migrations.AddField(
            model_name="objectpath",
            name="client_id",
            field=models.BigIntegerField(default=-1),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0011_add_task_source_and_safecharfield.py =====
# Generated by Django 2.0.9 on 2018-10-24 10:50

from django.db import migrations

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0010_auto_20181011_1517"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="source",
            field=cvat.apps.engine.models.SafeCharField(default="unknown", max_length=256),
        ),
        migrations.AlterField(
            model_name="label",
            name="name",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="labeledboxattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="labeledpointsattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="labeledpolygonattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="labeledpolylineattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="objectpathattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="task",
            name="name",
            field=cvat.apps.engine.models.SafeCharField(max_length=256),
        ),
        migrations.AlterField(
            model_name="trackedboxattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="trackedpointsattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="trackedpolygonattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
        migrations.AlterField(
            model_name="trackedpolylineattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=64),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0012_auto_20181025_1618.py =====
# Generated by Django 2.0.9 on 2018-10-25 13:18
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0011_add_task_source_and_safecharfield"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="status",
            field=models.CharField(default="annotation", max_length=32),
        ),
        migrations.AlterField(
            model_name="task",
            name="status",
            field=models.CharField(default="annotation", max_length=32),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0013_auth_no_default_permissions.py =====
# Generated by Django 2.0.9 on 2018-11-07 12:25

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0012_auto_20181025_1618"),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.AlterModelOptions(
            name="attributespec",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="job",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="label",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="labeledboxattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="labeledpointsattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="labeledpolygonattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="labeledpolylineattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="objectpathattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="segment",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="task",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedbox",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedboxattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpoints",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpointsattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpolygon",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpolygonattributeval",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpolyline",
            options={"default_permissions": ()},
        ),
        migrations.AlterModelOptions(
            name="trackedpolylineattributeval",
            options={"default_permissions": ()},
        ),
        migrations.RenameField(
            model_name="job",
            old_name="annotator",
            new_name="assignee",
        ),
        migrations.AddField(
            model_name="task",
            name="assignee",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="assignees",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
        migrations.AlterField(
            model_name="task",
            name="owner",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="owners",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
        migrations.AlterField(
            model_name="job",
            name="assignee",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                to=settings.AUTH_USER_MODEL,
            ),
        ),
        migrations.AlterField(
            model_name="task",
            name="bug_tracker",
            field=models.CharField(blank=True, default="", max_length=2000),
        ),
        migrations.AlterField(
            model_name="task",
            name="owner",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="owners",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0014_job_max_shape_id.py =====
# Generated by Django 2.1.3 on 2018-11-23 10:07

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0013_auth_no_default_permissions"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="max_shape_id",
            field=models.BigIntegerField(default=-1),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0015_db_redesign_20190217.py =====
# Generated by Django 2.1.5 on 2019-02-17 19:32

import django.db.migrations.operations.special
import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.engine.models


def set_segment_size(apps, schema_editor):
    Task = apps.get_model("engine", "Task")
    for task in Task.objects.all():
        segment = task.segment_set.first()
        if segment:
            task.segment_size = segment.stop_frame - segment.start_frame + 1
            task.save()


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0014_job_max_shape_id"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="segment_size",
            field=models.PositiveIntegerField(null=True),
        ),
        migrations.RunPython(
            code=set_segment_size,
            reverse_code=django.db.migrations.operations.special.RunPython.noop,
        ),
        migrations.AlterField(
            model_name="task",
            name="segment_size",
            field=models.PositiveIntegerField(),
        ),
        migrations.CreateModel(
            name="ClientFile",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "file",
                    models.FileField(
                        max_length=1024,
                        storage=cvat.apps.engine.models.MyFileSystemStorage(),
                        upload_to=cvat.apps.engine.models.upload_path_handler,
                    ),
                ),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Task"
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="RemoteFile",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("file", models.CharField(max_length=1024)),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Task"
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="ServerFile",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("file", models.CharField(max_length=1024)),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Task"
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.AlterField(
            model_name="task",
            name="status",
            field=models.CharField(
                choices=[
                    ("ANNOTATION", "annotation"),
                    ("VALIDATION", "validation"),
                    ("COMPLETED", "completed"),
                ],
                default="annotation",
                max_length=32,
            ),
        ),
        migrations.AlterField(
            model_name="task",
            name="overlap",
            field=models.PositiveIntegerField(null=True),
        ),
        migrations.RemoveField(
            model_name="task",
            name="path",
        ),
        migrations.AddField(
            model_name="task",
            name="image_quality",
            field=models.PositiveSmallIntegerField(default=50),
        ),
        migrations.CreateModel(
            name="Plugin",
            fields=[
                ("name", models.SlugField(max_length=32, primary_key=True, serialize=False)),
                ("description", cvat.apps.engine.models.SafeCharField(max_length=8192)),
                ("created_at", models.DateTimeField(auto_now_add=True)),
                ("updated_at", models.DateTimeField(auto_now_add=True)),
                (
                    "maintainer",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="maintainers",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="PluginOption",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("name", cvat.apps.engine.models.SafeCharField(max_length=32)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=1024)),
                (
                    "plugin",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Plugin"
                    ),
                ),
            ],
        ),
        migrations.AlterUniqueTogether(
            name="label",
            unique_together={("task", "name")},
        ),
        migrations.AlterUniqueTogether(
            name="clientfile",
            unique_together={("task", "file")},
        ),
        migrations.AddField(
            model_name="attributespec",
            name="default_value",
            field=models.CharField(default="", max_length=128),
            preserve_default=False,
        ),
        migrations.AddField(
            model_name="attributespec",
            name="input_type",
            field=models.CharField(
                choices=[
                    ("CHECKBOX", "checkbox"),
                    ("RADIO", "radio"),
                    ("NUMBER", "number"),
                    ("TEXT", "text"),
                    ("SELECT", "select"),
                ],
                default="select",
                max_length=16,
            ),
            preserve_default=False,
        ),
        migrations.AddField(
            model_name="attributespec",
            name="mutable",
            field=models.BooleanField(default=True),
            preserve_default=False,
        ),
        migrations.AddField(
            model_name="attributespec",
            name="name",
            field=models.CharField(default="test", max_length=64),
            preserve_default=False,
        ),
        migrations.AddField(
            model_name="attributespec",
            name="values",
            field=models.CharField(default="", max_length=4096),
            preserve_default=False,
        ),
        migrations.AlterField(
            model_name="job",
            name="status",
            field=models.CharField(
                choices=[
                    ("ANNOTATION", "annotation"),
                    ("VALIDATION", "validation"),
                    ("COMPLETED", "completed"),
                ],
                default="annotation",
                max_length=32,
            ),
        ),
        migrations.AlterField(
            model_name="attributespec",
            name="text",
            field=models.CharField(default="", max_length=1024),
        ),
        migrations.AlterField(
            model_name="attributespec",
            name="input_type",
            field=models.CharField(
                choices=[
                    ("checkbox", "CHECKBOX"),
                    ("radio", "RADIO"),
                    ("number", "NUMBER"),
                    ("text", "TEXT"),
                    ("select", "SELECT"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterField(
            model_name="task",
            name="segment_size",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AlterField(
            model_name="job",
            name="status",
            field=models.CharField(
                choices=[
                    ("annotation", "ANNOTATION"),
                    ("validation", "VALIDATION"),
                    ("completed", "COMPLETED"),
                ],
                default="annotation",
                max_length=32,
            ),
        ),
        migrations.AlterField(
            model_name="task",
            name="status",
            field=models.CharField(
                choices=[
                    ("annotation", "ANNOTATION"),
                    ("validation", "VALIDATION"),
                    ("completed", "COMPLETED"),
                ],
                default="annotation",
                max_length=32,
            ),
        ),
        migrations.CreateModel(
            name="Image",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("path", models.CharField(max_length=1024)),
                ("frame", models.PositiveIntegerField()),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Task"
                    ),
                ),
                ("height", models.PositiveIntegerField()),
                ("width", models.PositiveIntegerField()),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="Video",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("path", models.CharField(max_length=1024)),
                ("start_frame", models.PositiveIntegerField()),
                ("stop_frame", models.PositiveIntegerField()),
                ("step", models.PositiveIntegerField(default=1)),
                (
                    "task",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.Task"
                    ),
                ),
                ("height", models.PositiveIntegerField()),
                ("width", models.PositiveIntegerField()),
            ],
            options={
                "default_permissions": (),
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0016_attribute_spec_20190217.py =====
import csv
import os
import re
from io import StringIO

from django.conf import settings
from django.db import migrations
from PIL import Image

from cvat.apps.engine.media_extractors import get_mime


def parse_attribute(value):
    match = re.match(r"^([~@])(\w+)=(\w+):(.+)?$", value)
    if match:
        prefix = match.group(1)
        input_type = match.group(2)
        name = match.group(3)
        if match.group(4):
            values = list(csv.reader(StringIO(match.group(4)), quotechar="'"))[0]
        else:
            values = []

        return {"prefix": prefix, "type": input_type, "name": name, "values": values}
    else:
        return None


def split_text_attribute(apps, schema_editor):
    AttributeSpec = apps.get_model("engine", "AttributeSpec")
    for attribute in AttributeSpec.objects.all():
        spec = parse_attribute(attribute.text)
        if spec:
            attribute.mutable = spec["prefix"] == "~"
            attribute.input_type = spec["type"]
            attribute.name = spec["name"]
            attribute.default_value = spec["values"][0] if spec["values"] else ""
            attribute.values = "\n".join(spec["values"])
            attribute.save()


def join_text_attribute(apps, schema_editor):
    AttributeSpec = apps.get_model("engine", "AttributeSpec")
    for attribute in AttributeSpec.objects.all():
        attribute.text = ""
        if attribute.mutable:
            attribute.text += "~"
        else:
            attribute.text += "@"

        attribute.text += attribute.input_type
        attribute.text += "=" + attribute.name + ":"
        attribute.text += ",".join(attribute.values.split("\n"))
        attribute.save()


def _get_task_dirname(task_obj):
    return os.path.join(settings.DATA_ROOT, str(task_obj.id))


def _get_upload_dirname(task_obj):
    return os.path.join(_get_task_dirname(task_obj), ".upload")


def _get_frame_path(task_obj, frame):
    return os.path.join(
        _get_task_dirname(task_obj),
        "data",
        str(int(frame) // 10000),
        str(int(frame) // 100),
        str(frame) + ".jpg",
    )


def fill_task_meta_data_forward(apps, schema_editor):
    db_alias = schema_editor.connection.alias
    task_model = apps.get_model("engine", "Task")
    video_model = apps.get_model("engine", "Video")
    image_model = apps.get_model("engine", "Image")

    for db_task in task_model.objects.all():
        if db_task.mode == "interpolation":
            db_video = video_model()
            db_video.task_id = db_task.id
            db_video.start_frame = 0
            db_video.stop_frame = db_task.size
            db_video.step = 1

            video = ""
            for root, _, files in os.walk(_get_upload_dirname(db_task)):
                fullnames = map(lambda f: os.path.join(root, f), files)
                videos = list(filter(lambda x: get_mime(x) == "video", fullnames))
                if len(videos):
                    video = videos[0]
                    break
            db_video.path = video
            try:
                image = Image.open(_get_frame_path(db_task, 0))
                db_video.width = image.width
                db_video.height = image.height
                image.close()
            except FileNotFoundError:
                db_video.width = 0
                db_video.height = 0

            db_video.save()
        else:
            filenames = []
            for root, _, files in os.walk(_get_upload_dirname(db_task)):
                fullnames = map(lambda f: os.path.join(root, f), files)
                images = filter(lambda x: get_mime(x) == "image", fullnames)
                filenames.extend(images)
            filenames.sort()

            db_images = []
            for i, image_path in enumerate(filenames):
                db_image = image_model()
                db_image.task_id = db_task.id
                db_image.path = image_path
                db_image.frame = i
                try:
                    image = Image.open(image_path)
                    db_image.width = image.width
                    db_image.height = image.height
                    image.close()
                except FileNotFoundError:
                    db_image.width = 0
                    db_image.height = 0

                db_images.append(db_image)
            image_model.objects.using(db_alias).bulk_create(db_images)


def fill_task_meta_data_backward(apps, schema_editor):
    task_model = apps.get_model("engine", "Task")
    video_model = apps.get_model("engine", "Video")
    image_model = apps.get_model("engine", "Image")

    for db_task in task_model.objects.all():
        upload_dir = _get_upload_dirname(db_task)
        if db_task.mode == "interpolation":
            video = video_model.objects.get(task__id=db_task.id)
            db_task.source = os.path.relpath(video.path, upload_dir)
            video.delete()
        else:
            images = image_model.objects.filter(task__id=db_task.id)
            db_task.source = "{} images: {}, ...".format(
                len(images), ", ".join([os.path.relpath(x.path, upload_dir) for x in images[0:2]])
            )
            images.delete()
        db_task.save()


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0015_db_redesign_20190217"),
    ]

    operations = [
        migrations.RunPython(
            code=split_text_attribute,
            reverse_code=join_text_attribute,
        ),
        migrations.RemoveField(
            model_name="attributespec",
            name="text",
        ),
        migrations.AlterUniqueTogether(
            name="attributespec",
            unique_together={("label", "name")},
        ),
        migrations.RunPython(
            code=fill_task_meta_data_forward,
            reverse_code=fill_task_meta_data_backward,
        ),
        migrations.RemoveField(
            model_name="task",
            name="source",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0017_db_redesign_20190221.py =====
# Generated by Django 2.1.5 on 2019-02-21 12:25

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.engine.models
from cvat.apps.dataset_manager.task import merge_table_rows as _merge_table_rows


# some modified functions to transfer annotation
def _bulk_create(db_model, db_alias, objects, flt_param):
    if objects:
        if flt_param:
            if "postgresql" in settings.DATABASES["default"]["ENGINE"]:
                return db_model.objects.using(db_alias).bulk_create(objects)
            else:
                ids = list(
                    db_model.objects.using(db_alias)
                    .filter(**flt_param)
                    .values_list("id", flat=True)
                )
                db_model.objects.using(db_alias).bulk_create(objects)

                return list(
                    db_model.objects.using(db_alias).exclude(id__in=ids).filter(**flt_param)
                )
        else:
            return db_model.objects.using(db_alias).bulk_create(objects)


def get_old_db_shapes(shape_type, db_job):
    def _get_shape_set(db_job, shape_type):
        if shape_type == "polygons":
            return db_job.labeledpolygon_set
        elif shape_type == "polylines":
            return db_job.labeledpolyline_set
        elif shape_type == "boxes":
            return db_job.labeledbox_set
        elif shape_type == "points":
            return db_job.labeledpoints_set

    def get_values(shape_type):
        if shape_type == "polygons":
            return [
                (
                    "id",
                    "frame",
                    "points",
                    "label_id",
                    "group_id",
                    "occluded",
                    "z_order",
                    "client_id",
                    "labeledpolygonattributeval__value",
                    "labeledpolygonattributeval__spec_id",
                    "labeledpolygonattributeval__id",
                ),
                {
                    "attributes": [
                        "labeledpolygonattributeval__value",
                        "labeledpolygonattributeval__spec_id",
                        "labeledpolygonattributeval__id",
                    ]
                },
                "labeledpolygonattributeval_set",
            ]
        elif shape_type == "polylines":
            return [
                (
                    "id",
                    "frame",
                    "points",
                    "label_id",
                    "group_id",
                    "occluded",
                    "z_order",
                    "client_id",
                    "labeledpolylineattributeval__value",
                    "labeledpolylineattributeval__spec_id",
                    "labeledpolylineattributeval__id",
                ),
                {
                    "attributes": [
                        "labeledpolylineattributeval__value",
                        "labeledpolylineattributeval__spec_id",
                        "labeledpolylineattributeval__id",
                    ]
                },
                "labeledpolylineattributeval_set",
            ]
        elif shape_type == "boxes":
            return [
                (
                    "id",
                    "frame",
                    "xtl",
                    "ytl",
                    "xbr",
                    "ybr",
                    "label_id",
                    "group_id",
                    "occluded",
                    "z_order",
                    "client_id",
                    "labeledboxattributeval__value",
                    "labeledboxattributeval__spec_id",
                    "labeledboxattributeval__id",
                ),
                {
                    "attributes": [
                        "labeledboxattributeval__value",
                        "labeledboxattributeval__spec_id",
                        "labeledboxattributeval__id",
                    ]
                },
                "labeledboxattributeval_set",
            ]
        elif shape_type == "points":
            return [
                (
                    "id",
                    "frame",
                    "points",
                    "label_id",
                    "group_id",
                    "occluded",
                    "z_order",
                    "client_id",
                    "labeledpointsattributeval__value",
                    "labeledpointsattributeval__spec_id",
                    "labeledpointsattributeval__id",
                ),
                {
                    "attributes": [
                        "labeledpointsattributeval__value",
                        "labeledpointsattributeval__spec_id",
                        "labeledpointsattributeval__id",
                    ]
                },
                "labeledpointsattributeval_set",
            ]

    (values, merge_keys, prefetch) = get_values(shape_type)
    db_shapes = list(
        _get_shape_set(db_job, shape_type)
        .prefetch_related(prefetch)
        .values(*values)
        .order_by("frame")
    )
    return _merge_table_rows(db_shapes, merge_keys, "id")


def get_old_db_paths(db_job):
    db_paths = db_job.objectpath_set
    for shape in [
        "trackedpoints_set",
        "trackedbox_set",
        "trackedpolyline_set",
        "trackedpolygon_set",
    ]:
        db_paths.prefetch_related(shape)
    for shape_attr in [
        "trackedpoints_set__trackedpointsattributeval_set",
        "trackedbox_set__trackedboxattributeval_set",
        "trackedpolygon_set__trackedpolygonattributeval_set",
        "trackedpolyline_set__trackedpolylineattributeval_set",
    ]:
        db_paths.prefetch_related(shape_attr)
    db_paths.prefetch_related("objectpathattributeval_set")
    db_paths = list(
        db_paths.values(
            "id",
            "frame",
            "group_id",
            "shapes",
            "client_id",
            "objectpathattributeval__spec_id",
            "objectpathattributeval__id",
            "objectpathattributeval__value",
            "trackedbox",
            "trackedpolygon",
            "trackedpolyline",
            "trackedpoints",
            "trackedbox__id",
            "label_id",
            "trackedbox__xtl",
            "trackedbox__ytl",
            "trackedbox__xbr",
            "trackedbox__ybr",
            "trackedbox__frame",
            "trackedbox__occluded",
            "trackedbox__z_order",
            "trackedbox__outside",
            "trackedbox__trackedboxattributeval__spec_id",
            "trackedbox__trackedboxattributeval__value",
            "trackedbox__trackedboxattributeval__id",
            "trackedpolygon__id",
            "trackedpolygon__points",
            "trackedpolygon__frame",
            "trackedpolygon__occluded",
            "trackedpolygon__z_order",
            "trackedpolygon__outside",
            "trackedpolygon__trackedpolygonattributeval__spec_id",
            "trackedpolygon__trackedpolygonattributeval__value",
            "trackedpolygon__trackedpolygonattributeval__id",
            "trackedpolyline__id",
            "trackedpolyline__points",
            "trackedpolyline__frame",
            "trackedpolyline__occluded",
            "trackedpolyline__z_order",
            "trackedpolyline__outside",
            "trackedpolyline__trackedpolylineattributeval__spec_id",
            "trackedpolyline__trackedpolylineattributeval__value",
            "trackedpolyline__trackedpolylineattributeval__id",
            "trackedpoints__id",
            "trackedpoints__points",
            "trackedpoints__frame",
            "trackedpoints__occluded",
            "trackedpoints__z_order",
            "trackedpoints__outside",
            "trackedpoints__trackedpointsattributeval__spec_id",
            "trackedpoints__trackedpointsattributeval__value",
            "trackedpoints__trackedpointsattributeval__id",
        ).order_by(
            "id",
            "trackedbox__frame",
            "trackedpolygon__frame",
            "trackedpolyline__frame",
            "trackedpoints__frame",
        )
    )

    db_box_paths = list(filter(lambda path: path["shapes"] == "boxes", db_paths))
    db_polygon_paths = list(filter(lambda path: path["shapes"] == "polygons", db_paths))
    db_polyline_paths = list(filter(lambda path: path["shapes"] == "polylines", db_paths))
    db_points_paths = list(filter(lambda path: path["shapes"] == "points", db_paths))

    object_path_attr_merge_key = [
        "objectpathattributeval__value",
        "objectpathattributeval__spec_id",
        "objectpathattributeval__id",
    ]

    db_box_paths = _merge_table_rows(
        db_box_paths,
        {
            "attributes": object_path_attr_merge_key,
            "shapes": [
                "trackedbox__id",
                "trackedbox__xtl",
                "trackedbox__ytl",
                "trackedbox__xbr",
                "trackedbox__ybr",
                "trackedbox__frame",
                "trackedbox__occluded",
                "trackedbox__z_order",
                "trackedbox__outside",
                "trackedbox__trackedboxattributeval__value",
                "trackedbox__trackedboxattributeval__spec_id",
                "trackedbox__trackedboxattributeval__id",
            ],
        },
        "id",
    )

    db_polygon_paths = _merge_table_rows(
        db_polygon_paths,
        {
            "attributes": object_path_attr_merge_key,
            "shapes": [
                "trackedpolygon__id",
                "trackedpolygon__points",
                "trackedpolygon__frame",
                "trackedpolygon__occluded",
                "trackedpolygon__z_order",
                "trackedpolygon__outside",
                "trackedpolygon__trackedpolygonattributeval__value",
                "trackedpolygon__trackedpolygonattributeval__spec_id",
                "trackedpolygon__trackedpolygonattributeval__id",
            ],
        },
        "id",
    )

    db_polyline_paths = _merge_table_rows(
        db_polyline_paths,
        {
            "attributes": object_path_attr_merge_key,
            "shapes": [
                "trackedpolyline__id",
                "trackedpolyline__points",
                "trackedpolyline__frame",
                "trackedpolyline__occluded",
                "trackedpolyline__z_order",
                "trackedpolyline__outside",
                "trackedpolyline__trackedpolylineattributeval__value",
                "trackedpolyline__trackedpolylineattributeval__spec_id",
                "trackedpolyline__trackedpolylineattributeval__id",
            ],
        },
        "id",
    )

    db_points_paths = _merge_table_rows(
        db_points_paths,
        {
            "attributes": object_path_attr_merge_key,
            "shapes": [
                "trackedpoints__id",
                "trackedpoints__points",
                "trackedpoints__frame",
                "trackedpoints__occluded",
                "trackedpoints__z_order",
                "trackedpoints__outside",
                "trackedpoints__trackedpointsattributeval__value",
                "trackedpoints__trackedpointsattributeval__spec_id",
                "trackedpoints__trackedpointsattributeval__id",
            ],
        },
        "id",
    )

    for db_box_path in db_box_paths:
        db_box_path.attributes = list(set(db_box_path.attributes))
        db_box_path.type = "box_path"
        db_box_path.shapes = _merge_table_rows(
            db_box_path.shapes,
            {
                "attributes": [
                    "trackedboxattributeval__value",
                    "trackedboxattributeval__spec_id",
                    "trackedboxattributeval__id",
                ]
            },
            "id",
        )

    for db_polygon_path in db_polygon_paths:
        db_polygon_path.attributes = list(set(db_polygon_path.attributes))
        db_polygon_path.type = "poligon_path"
        db_polygon_path.shapes = _merge_table_rows(
            db_polygon_path.shapes,
            {
                "attributes": [
                    "trackedpolygonattributeval__value",
                    "trackedpolygonattributeval__spec_id",
                    "trackedpolygonattributeval__id",
                ]
            },
            "id",
        )

    for db_polyline_path in db_polyline_paths:
        db_polyline_path.attributes = list(set(db_polyline_path.attributes))
        db_polyline_path.type = "polyline_path"
        db_polyline_path.shapes = _merge_table_rows(
            db_polyline_path.shapes,
            {
                "attributes": [
                    "trackedpolylineattributeval__value",
                    "trackedpolylineattributeval__spec_id",
                    "trackedpolylineattributeval__id",
                ]
            },
            "id",
        )

    for db_points_path in db_points_paths:
        db_points_path.attributes = list(set(db_points_path.attributes))
        db_points_path.type = "points_path"
        db_points_path.shapes = _merge_table_rows(
            db_points_path.shapes,
            {
                "attributes": [
                    "trackedpointsattributeval__value",
                    "trackedpointsattributeval__spec_id",
                    "trackedpointsattributeval__id",
                ]
            },
            "id",
        )
    return db_box_paths + db_polygon_paths + db_polyline_paths + db_points_paths


def process_shapes(db_job, apps, db_labels, db_attributes, db_alias):
    LabeledShape = apps.get_model("engine", "LabeledShape")
    LabeledShapeAttributeVal = apps.get_model("engine", "LabeledShapeAttributeVal")
    new_db_shapes = []
    new_db_attrvals = []
    for shape_type in ["boxes", "points", "polygons", "polylines"]:
        for shape in get_old_db_shapes(shape_type, db_job):
            new_db_shape = LabeledShape()
            new_db_shape.job = db_job
            new_db_shape.label = db_labels[shape.label_id]
            new_db_shape.group = shape.group_id

            if shape_type == "boxes":
                new_db_shape.type = cvat.apps.engine.models.ShapeType.RECTANGLE
                new_db_shape.points = [shape.xtl, shape.ytl, shape.xbr, shape.ybr]
            else:
                new_db_shape.points = shape.points.replace(",", " ").split()
                if shape_type == "points":
                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POINTS
                elif shape_type == "polygons":
                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POLYGON
                elif shape_type == "polylines":
                    new_db_shape.type = cvat.apps.engine.models.ShapeType.POLYLINE

            new_db_shape.frame = shape.frame
            new_db_shape.occluded = shape.occluded
            new_db_shape.z_order = shape.z_order

            for attr in shape.attributes:
                db_attrval = LabeledShapeAttributeVal()
                db_attrval.shape_id = len(new_db_shapes)
                db_attrval.spec = db_attributes[attr.spec_id]
                db_attrval.value = attr.value
                new_db_attrvals.append(db_attrval)

            new_db_shapes.append(new_db_shape)

    new_db_shapes = _bulk_create(LabeledShape, db_alias, new_db_shapes, {"job_id": db_job.id})
    for db_attrval in new_db_attrvals:
        db_attrval.shape_id = new_db_shapes[db_attrval.shape_id].id

    _bulk_create(LabeledShapeAttributeVal, db_alias, new_db_attrvals, {})


def process_paths(db_job, apps, db_labels, db_attributes, db_alias):
    TrackedShape = apps.get_model("engine", "TrackedShape")
    LabeledTrack = apps.get_model("engine", "LabeledTrack")
    LabeledTrackAttributeVal = apps.get_model("engine", "LabeledTrackAttributeVal")
    TrackedShapeAttributeVal = apps.get_model("engine", "TrackedShapeAttributeVal")
    tracks = get_old_db_paths(db_job)

    new_db_tracks = []
    new_db_track_attrvals = []
    new_db_shapes = []
    new_db_shape_attrvals = []

    for track in tracks:
        db_track = LabeledTrack()
        db_track.job = db_job
        db_track.label = db_labels[track.label_id]
        db_track.frame = track.frame
        db_track.group = track.group_id

        for attr in track.attributes:
            db_attrspec = db_attributes[attr.spec_id]
            db_attrval = LabeledTrackAttributeVal()
            db_attrval.track_id = len(new_db_tracks)
            db_attrval.spec = db_attrspec
            db_attrval.value = attr.value
            new_db_track_attrvals.append(db_attrval)

        for shape in track.shapes:
            db_shape = TrackedShape()
            db_shape.track_id = len(new_db_tracks)
            db_shape.frame = shape.frame
            db_shape.occluded = shape.occluded
            db_shape.z_order = shape.z_order
            db_shape.outside = shape.outside
            if track.type == "box_path":
                db_shape.type = cvat.apps.engine.models.ShapeType.RECTANGLE
                db_shape.points = [shape.xtl, shape.ytl, shape.xbr, shape.ybr]
            else:
                db_shape.points = shape.points.replace(",", " ").split()
                if track.type == "points_path":
                    db_shape.type = cvat.apps.engine.models.ShapeType.POINTS
                elif track.type == "polygon_path":
                    db_shape.type = cvat.apps.engine.models.ShapeType.POLYGON
                elif track.type == "polyline_path":
                    db_shape.type = cvat.apps.engine.models.ShapeType.POLYLINE

            for attr in shape.attributes:
                db_attrspec = db_attributes[attr.spec_id]
                db_attrval = TrackedShapeAttributeVal()
                db_attrval.shape_id = len(new_db_shapes)
                db_attrval.spec = db_attrspec
                db_attrval.value = attr.value
                new_db_shape_attrvals.append(db_attrval)

            new_db_shapes.append(db_shape)
        new_db_tracks.append(db_track)

    new_db_tracks = _bulk_create(LabeledTrack, db_alias, new_db_tracks, {"job_id": db_job.id})

    for db_attrval in new_db_track_attrvals:
        db_attrval.track_id = new_db_tracks[db_attrval.track_id].id
    _bulk_create(LabeledTrackAttributeVal, db_alias, new_db_track_attrvals, {})

    for db_shape in new_db_shapes:
        db_shape.track_id = new_db_tracks[db_shape.track_id].id

    new_db_shapes = _bulk_create(
        TrackedShape, db_alias, new_db_shapes, {"track__job_id": db_job.id}
    )

    for db_attrval in new_db_shape_attrvals:
        db_attrval.shape_id = new_db_shapes[db_attrval.shape_id].id

    _bulk_create(TrackedShapeAttributeVal, db_alias, new_db_shape_attrvals, {})


def copy_annotations_forward(apps, schema_editor):
    db_alias = schema_editor.connection.alias
    Task = apps.get_model("engine", "Task")
    AttributeSpec = apps.get_model("engine", "AttributeSpec")

    for task in Task.objects.all():
        print("run anno migration for the task {}".format(task.id))
        db_labels = {db_label.id: db_label for db_label in task.label_set.all()}
        db_attributes = {
            db_attr.id: db_attr for db_attr in AttributeSpec.objects.filter(label__task__id=task.id)
        }
        for segment in task.segment_set.prefetch_related("job_set").all():
            db_job = segment.job_set.first()
            print("run anno migration for the job {}".format(db_job.id))
            process_shapes(db_job, apps, db_labels, db_attributes, db_alias)
            process_paths(db_job, apps, db_labels, db_attributes, db_alias)


def _save_old_shapes_to_db(apps, db_shapes, db_attributes, db_alias, db_job):
    def _get_shape_class(shape_type):
        if shape_type == "polygons":
            return apps.get_model("engine", "LabeledPolygon")
        elif shape_type == "polylines":
            return apps.get_model("engine", "LabeledPolyline")
        elif shape_type == "boxes":
            return apps.get_model("engine", "LabeledBox")
        elif shape_type == "points":
            return apps.get_model("engine", "LabeledPoints")

    def _get_shape_attr_class(shape_type):
        if shape_type == "polygons":
            return apps.get_model("engine", "LabeledPolygonAttributeVal")
        elif shape_type == "polylines":
            return apps.get_model("engine", "LabeledPolylineAttributeVal")
        elif shape_type == "boxes":
            return apps.get_model("engine", "LabeledBoxAttributeVal")
        elif shape_type == "points":
            return apps.get_model("engine", "LabeledPointsAttributeVal")

    shapes = [
        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.RECTANGLE, db_shapes)),
        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POLYLINE, db_shapes)),
        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POLYGON, db_shapes)),
        list(filter(lambda s: s.type == cvat.apps.engine.models.ShapeType.POINTS, db_shapes)),
    ]
    for i, shape_type in enumerate(["boxes", "polylines", "polygons", "points"]):
        new_db_shapes = []
        new_db_attrvals = []
        for shape in shapes[i]:
            db_shape = _get_shape_class(shape_type)()
            db_shape.job = shape.job
            db_shape.label = shape.label
            db_shape.group_id = shape.group
            if shape.type == cvat.apps.engine.models.ShapeType.RECTANGLE:
                db_shape.xtl = shape.points[0]
                db_shape.ytl = shape.points[1]
                db_shape.xbr = shape.points[2]
                db_shape.ybr = shape.points[3]
            else:
                point_iterator = iter(shape.points)
                db_shape.points = " ".join(
                    ["{},{}".format(point, next(point_iterator)) for point in point_iterator]
                )
            db_shape.frame = shape.frame
            db_shape.occluded = shape.occluded
            db_shape.z_order = shape.z_order

            for attr in list(shape.labeledshapeattributeval_set.all()):
                db_attrval = _get_shape_attr_class(shape_type)()
                if shape.type == cvat.apps.engine.models.ShapeType.POLYGON:
                    db_attrval.polygon_id = len(new_db_shapes)
                elif shape.type == cvat.apps.engine.models.ShapeType.POLYLINE:
                    db_attrval.polyline_id = len(new_db_shapes)
                elif shape.type == cvat.apps.engine.models.ShapeType.RECTANGLE:
                    db_attrval.box_id = len(new_db_shapes)
                else:
                    db_attrval.points_id = len(new_db_shapes)

                db_attrval.spec = db_attributes[attr.spec_id]
                db_attrval.value = attr.value
                new_db_attrvals.append(db_attrval)

            new_db_shapes.append(db_shape)

        new_db_shapes = _bulk_create(
            _get_shape_class(shape_type), db_alias, new_db_shapes, {"job_id": db_job.id}
        )

        for db_attrval in new_db_attrvals:
            if shape_type == "polygons":
                db_attrval.polygon_id = new_db_shapes[db_attrval.polygon_id].id
            elif shape_type == "polylines":
                db_attrval.polyline_id = new_db_shapes[db_attrval.polyline_id].id
            elif shape_type == "boxes":
                db_attrval.box_id = new_db_shapes[db_attrval.box_id].id
            else:
                db_attrval.points_id = new_db_shapes[db_attrval.points_id].id

        _bulk_create(_get_shape_attr_class(shape_type), db_alias, new_db_attrvals, {})


def _save_old_tracks_to_db(apps, db_shapes, db_attributes, db_alias, db_job):
    def _get_shape_class(shape_type):
        if shape_type == "polygon_paths":
            return apps.get_model("engine", "TrackedPolygon")
        elif shape_type == "polyline_paths":
            return apps.get_model("engine", "TrackedPolyline")
        elif shape_type == "box_paths":
            return apps.get_model("engine", "TrackedBox")
        elif shape_type == "points_paths":
            return apps.get_model("engine", "TrackedPoints")

    def _get_shape_attr_class(shape_type):
        if shape_type == "polygon_paths":
            return apps.get_model("engine", "TrackedPolygonAttributeVal")
        elif shape_type == "polyline_paths":
            return apps.get_model("engine", "TrackedPolylineAttributeVal")
        elif shape_type == "box_paths":
            return apps.get_model("engine", "TrackedBoxAttributeVal")
        elif shape_type == "points_paths":
            return apps.get_model("engine", "TrackedPointsAttributeVal")

    tracks = [
        list(
            filter(
                lambda t: t.trackedshape_set.first().type
                == cvat.apps.engine.models.ShapeType.RECTANGLE,
                db_shapes,
            )
        ),
        list(
            filter(
                lambda t: t.trackedshape_set.first().type
                == cvat.apps.engine.models.ShapeType.POLYLINE,
                db_shapes,
            )
        ),
        list(
            filter(
                lambda t: t.trackedshape_set.first().type
                == cvat.apps.engine.models.ShapeType.POLYGON,
                db_shapes,
            )
        ),
        list(
            filter(
                lambda t: t.trackedshape_set.first().type
                == cvat.apps.engine.models.ShapeType.POINTS,
                db_shapes,
            )
        ),
    ]

    ObjectPath = apps.get_model("engine", "ObjectPath")
    ObjectPathAttributeVal = apps.get_model("engine", "ObjectPathAttributeVal")

    for i, shape_type in enumerate(
        [
            "box_paths",
            "polyline_paths",
            "polygon_paths",
            "points_paths",
        ]
    ):
        new_db_paths = []
        new_db_path_attrvals = []
        new_db_shapes = []
        new_db_shape_attrvals = []

        for path in tracks[i]:
            db_path = ObjectPath()
            db_path.job = db_job
            db_path.label = path.label
            db_path.frame = path.frame
            db_path.group_id = path.group
            # db_path.client_id = path.client_id
            if shape_type == "polygon_paths":
                db_path.shapes = "polygons"
            elif shape_type == "polyline_paths":
                db_path.shapes = "polylines"
            elif shape_type == "box_paths":
                db_path.shapes = "boxes"
            elif shape_type == "points_paths":
                db_path.shapes = "points"

            for attr in list(path.labeledtrackattributeval_set.all()):
                db_attrspec = db_attributes[attr.spec_id]
                db_attrval = ObjectPathAttributeVal()
                db_attrval.track_id = len(new_db_paths)
                db_attrval.spec = db_attrspec
                db_attrval.value = attr.value
                new_db_path_attrvals.append(db_attrval)

            for shape in list(path.trackedshape_set.all()):
                db_shape = _get_shape_class(shape_type)()
                db_shape.track_id = len(new_db_paths)
                if shape_type == "box_paths":
                    db_shape.xtl = shape.points[0]
                    db_shape.ytl = shape.points[1]
                    db_shape.xbr = shape.points[2]
                    db_shape.ybr = shape.points[3]
                else:
                    point_iterator = iter(shape.points)
                    db_shape.points = " ".join(
                        ["{},{}".format(point, next(point_iterator)) for point in point_iterator]
                    )

                db_shape.frame = shape.frame
                db_shape.occluded = shape.occluded
                db_shape.z_order = shape.z_order
                db_shape.outside = shape.outside

                for attr in list(shape.trackedshapeattributeval_set.all()):
                    db_attrspec = db_attributes[attr.spec_id]
                    db_attrval = _get_shape_attr_class(shape_type)()
                    if shape_type == "polygon_paths":
                        db_attrval.polygon_id = len(new_db_shapes)
                    elif shape_type == "polyline_paths":
                        db_attrval.polyline_id = len(new_db_shapes)
                    elif shape_type == "box_paths":
                        db_attrval.box_id = len(new_db_shapes)
                    elif shape_type == "points_paths":
                        db_attrval.points_id = len(new_db_shapes)
                    db_attrval.spec = db_attrspec
                    db_attrval.value = attr.value
                    new_db_shape_attrvals.append(db_attrval)

                new_db_shapes.append(db_shape)
            new_db_paths.append(db_path)

        new_db_paths = _bulk_create(ObjectPath, db_alias, new_db_paths, {"job_id": db_job.id})

        for db_attrval in new_db_path_attrvals:
            db_attrval.track_id = new_db_paths[db_attrval.track_id].id
        _bulk_create(ObjectPathAttributeVal, db_alias, new_db_path_attrvals, {})

        for db_shape in new_db_shapes:
            db_shape.track_id = new_db_paths[db_shape.track_id].id

        db_shapes = _bulk_create(
            _get_shape_class(shape_type), db_alias, new_db_shapes, {"track__job_id": db_job.id}
        )

        for db_attrval in new_db_shape_attrvals:
            if shape_type == "polygon_paths":
                db_attrval.polygon_id = db_shapes[db_attrval.polygon_id].id
            elif shape_type == "polyline_paths":
                db_attrval.polyline_id = db_shapes[db_attrval.polyline_id].id
            elif shape_type == "box_paths":
                db_attrval.box_id = db_shapes[db_attrval.box_id].id
            elif shape_type == "points_paths":
                db_attrval.points_id = db_shapes[db_attrval.points_id].id

        _bulk_create(_get_shape_attr_class(shape_type), db_alias, new_db_shape_attrvals, {})


def copy_annotations_backward(apps, schema_editor):
    Task = apps.get_model("engine", "Task")
    AttributeSpec = apps.get_model("engine", "AttributeSpec")
    db_alias = schema_editor.connection.alias

    for task in Task.objects.all():
        db_attributes = {
            db_attr.id: db_attr for db_attr in AttributeSpec.objects.filter(label__task__id=task.id)
        }
        for segment in task.segment_set.prefetch_related("job_set").all():
            db_job = segment.job_set.first()

            db_shapes = list(
                db_job.labeledshape_set.prefetch_related("label").prefetch_related(
                    "labeledshapeattributeval_set"
                )
            )
            _save_old_shapes_to_db(apps, db_shapes, db_attributes, db_alias, db_job)

            db_tracks = list(
                db_job.labeledtrack_set.select_related("label")
                .prefetch_related("labeledtrackattributeval_set")
                .prefetch_related("trackedshape_set__trackedshapeattributeval_set")
            )
            _save_old_tracks_to_db(apps, db_tracks, db_attributes, db_alias, db_job)


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0016_attribute_spec_20190217"),
    ]

    operations = [
        migrations.CreateModel(
            name="LabeledImageAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=64)),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="LabeledShapeAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=64)),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="LabeledTrackAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=64)),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="TrackedShape",
            fields=[
                (
                    "type",
                    models.CharField(
                        choices=[
                            ("rectangle", "RECTANGLE"),
                            ("polygon", "POLYGON"),
                            ("polyline", "POLYLINE"),
                            ("points", "POINTS"),
                        ],
                        max_length=16,
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("z_order", models.IntegerField(default=0)),
                ("points", cvat.apps.engine.models.FloatArrayField()),
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("frame", models.PositiveIntegerField()),
                ("outside", models.BooleanField(default=False)),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="TrackedShapeAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=64)),
                (
                    "shape",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.TrackedShape"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.AttributeSpec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="LabeledImage",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("frame", models.PositiveIntegerField()),
                ("group", models.PositiveIntegerField(null=True)),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="LabeledShape",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("frame", models.PositiveIntegerField()),
                ("group", models.PositiveIntegerField(null=True)),
                (
                    "type",
                    models.CharField(
                        choices=[
                            ("rectangle", "RECTANGLE"),
                            ("polygon", "POLYGON"),
                            ("polyline", "POLYLINE"),
                            ("points", "POINTS"),
                        ],
                        max_length=16,
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("z_order", models.IntegerField(default=0)),
                ("points", cvat.apps.engine.models.FloatArrayField()),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="LabeledTrack",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("frame", models.PositiveIntegerField()),
                ("group", models.PositiveIntegerField(null=True)),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="labeledimage",
            name="job",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
        ),
        migrations.AddField(
            model_name="labeledtrack",
            name="job",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
        ),
        migrations.AddField(
            model_name="labeledshape",
            name="job",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Job"),
        ),
        migrations.AddField(
            model_name="labeledimage",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Label"),
        ),
        migrations.AddField(
            model_name="labeledshape",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Label"),
        ),
        migrations.AddField(
            model_name="labeledtrack",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.Label"),
        ),
        migrations.AddField(
            model_name="trackedshape",
            name="track",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledTrack"
            ),
        ),
        migrations.AddField(
            model_name="labeledtrackattributeval",
            name="track",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledTrack"
            ),
        ),
        migrations.AddField(
            model_name="labeledshapeattributeval",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledShape"
            ),
        ),
        migrations.AddField(
            model_name="labeledimageattributeval",
            name="image",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.LabeledImage"
            ),
        ),
        migrations.RunPython(
            code=copy_annotations_forward,
            reverse_code=copy_annotations_backward,
        ),
        migrations.RemoveField(
            model_name="labeledbox",
            name="job",
        ),
        migrations.RemoveField(
            model_name="labeledbox",
            name="label",
        ),
        migrations.RemoveField(
            model_name="labeledboxattributeval",
            name="box",
        ),
        migrations.RemoveField(
            model_name="labeledboxattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="labeledpoints",
            name="job",
        ),
        migrations.RemoveField(
            model_name="labeledpoints",
            name="label",
        ),
        migrations.RemoveField(
            model_name="labeledpointsattributeval",
            name="points",
        ),
        migrations.RemoveField(
            model_name="labeledpointsattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="labeledpolygon",
            name="job",
        ),
        migrations.RemoveField(
            model_name="job",
            name="max_shape_id",
        ),
        migrations.RemoveField(
            model_name="labeledpolygon",
            name="label",
        ),
        migrations.RemoveField(
            model_name="labeledpolygonattributeval",
            name="polygon",
        ),
        migrations.RemoveField(
            model_name="labeledpolygonattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="labeledpolyline",
            name="job",
        ),
        migrations.RemoveField(
            model_name="labeledpolyline",
            name="label",
        ),
        migrations.RemoveField(
            model_name="labeledpolylineattributeval",
            name="polyline",
        ),
        migrations.RemoveField(
            model_name="labeledpolylineattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="objectpath",
            name="job",
        ),
        migrations.RemoveField(
            model_name="objectpath",
            name="label",
        ),
        migrations.RemoveField(
            model_name="objectpathattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="objectpathattributeval",
            name="track",
        ),
        migrations.RemoveField(
            model_name="trackedbox",
            name="track",
        ),
        migrations.RemoveField(
            model_name="trackedboxattributeval",
            name="box",
        ),
        migrations.RemoveField(
            model_name="trackedboxattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="trackedpoints",
            name="track",
        ),
        migrations.RemoveField(
            model_name="trackedpointsattributeval",
            name="points",
        ),
        migrations.RemoveField(
            model_name="trackedpointsattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="trackedpolygon",
            name="track",
        ),
        migrations.RemoveField(
            model_name="trackedpolygonattributeval",
            name="polygon",
        ),
        migrations.RemoveField(
            model_name="trackedpolygonattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="trackedpolyline",
            name="track",
        ),
        migrations.RemoveField(
            model_name="trackedpolylineattributeval",
            name="polyline",
        ),
        migrations.RemoveField(
            model_name="trackedpolylineattributeval",
            name="spec",
        ),
        migrations.DeleteModel(
            name="LabeledBox",
        ),
        migrations.DeleteModel(
            name="LabeledBoxAttributeVal",
        ),
        migrations.DeleteModel(
            name="LabeledPoints",
        ),
        migrations.DeleteModel(
            name="LabeledPointsAttributeVal",
        ),
        migrations.DeleteModel(
            name="LabeledPolygon",
        ),
        migrations.DeleteModel(
            name="LabeledPolygonAttributeVal",
        ),
        migrations.DeleteModel(
            name="LabeledPolyline",
        ),
        migrations.DeleteModel(
            name="LabeledPolylineAttributeVal",
        ),
        migrations.DeleteModel(
            name="ObjectPath",
        ),
        migrations.DeleteModel(
            name="ObjectPathAttributeVal",
        ),
        migrations.DeleteModel(
            name="TrackedBox",
        ),
        migrations.DeleteModel(
            name="TrackedBoxAttributeVal",
        ),
        migrations.DeleteModel(
            name="TrackedPoints",
        ),
        migrations.DeleteModel(
            name="TrackedPointsAttributeVal",
        ),
        migrations.DeleteModel(
            name="TrackedPolygon",
        ),
        migrations.DeleteModel(
            name="TrackedPolygonAttributeVal",
        ),
        migrations.DeleteModel(
            name="TrackedPolyline",
        ),
        migrations.DeleteModel(
            name="TrackedPolylineAttributeVal",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0018_jobcommit.py =====
# Generated by Django 2.1.7 on 2019-04-17 09:25

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0017_db_redesign_20190221"),
    ]

    operations = [
        migrations.CreateModel(
            name="JobCommit",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("version", models.PositiveIntegerField(default=0)),
                ("timestamp", models.DateTimeField(auto_now=True)),
                ("message", models.CharField(default="", max_length=4096)),
                (
                    "author",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "job",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="commits",
                        to="engine.Job",
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0019_frame_selection.py =====
# Generated by Django 2.1.7 on 2019-05-10 08:23

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0018_jobcommit"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="video",
            name="start_frame",
        ),
        migrations.RemoveField(
            model_name="video",
            name="step",
        ),
        migrations.RemoveField(
            model_name="video",
            name="stop_frame",
        ),
        migrations.AddField(
            model_name="task",
            name="frame_filter",
            field=models.CharField(default="", max_length=256),
        ),
        migrations.AddField(
            model_name="task",
            name="start_frame",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="task",
            name="stop_frame",
            field=models.PositiveIntegerField(default=0),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0020_remove_task_flipped.py =====
# Generated by Django 2.1.7 on 2019-06-18 11:08

import os
from ast import literal_eval

from django.conf import settings
from django.db import migrations
from PIL import Image

from cvat.apps.engine.media_extractors import get_mime
from cvat.apps.engine.models import Job, ShapeType


def make_image_meta_cache(db_task):
    with open(db_task.get_image_meta_cache_path(), "w") as meta_file:
        cache = {"original_size": []}

        if db_task.mode == "interpolation":
            image = Image.open(db_task.get_frame_path(0))
            cache["original_size"].append({"width": image.size[0], "height": image.size[1]})
            image.close()
        else:
            filenames = []
            for root, _, files in os.walk(db_task.get_upload_dirname()):
                fullnames = map(lambda f: os.path.join(root, f), files)
                images = filter(lambda x: get_mime(x) == "image", fullnames)
                filenames.extend(images)
            filenames.sort()

            for image_path in filenames:
                image = Image.open(image_path)
                cache["original_size"].append({"width": image.size[0], "height": image.size[1]})
                image.close()

        meta_file.write(str(cache))


def get_image_meta_cache(db_task):
    try:
        with open(db_task.get_image_meta_cache_path()) as meta_cache_file:
            return literal_eval(meta_cache_file.read())
    except Exception:
        make_image_meta_cache(db_task)
        with open(db_task.get_image_meta_cache_path()) as meta_cache_file:
            return literal_eval(meta_cache_file.read())


def _flip_shape(shape, size):
    if shape.type == ShapeType.RECTANGLE:
        shape.points = [
            shape.points[2],  # xbr -> xtl
            shape.points[3],  # ybr -> ytl
            shape.points[0],  # xtl -> xbr
            shape.points[1],  # ytl -> ybr
        ]

    for x in range(0, len(shape.points), 2):
        y = x + 1
        shape.points[x] = size["width"] - shape.points[x]
        shape.points[y] = size["height"] - shape.points[y]


def frame_path(db_task, frame):
    task_dirname = os.path.join(settings.DATA_ROOT, str(db_task.id))
    d1 = str(int(frame) // 10000)
    d2 = str(int(frame) // 100)
    path = os.path.join(task_dirname, "data", d1, d2, str(frame) + ".jpg")
    return path


def _get_image_meta_cache_path(self):
    task_dirname = os.path.join(settings.DATA_ROOT, str(self.id))
    return os.path.join(task_dirname, "image_meta.cache")


def forwards_func(apps, schema_editor):
    Task = apps.get_model("engine", "Task")

    # class methods unavailable in the class which got via get_model()
    # nevertheless it is needed for us to use the function get_image_meta_cache()
    setattr(Task, "get_image_meta_cache_path", _get_image_meta_cache_path)

    print("Getting flipped tasks...")
    db_flipped_tasks = (
        Task.objects.prefetch_related(
            "image_set",
        )
        .filter(flipped=True)
        .all()
    )

    print("Conversion started...")
    for db_task in db_flipped_tasks:
        print("Processing task {}...".format(db_task.id))
        db_image_by_frame = {}
        if db_task.mode == "annotation":
            db_image_by_frame = {
                db_image.frame: {"width": db_image.width, "height": db_image.height}
                for db_image in db_task.image_set.all()
            }
        else:
            im_meta_data = get_image_meta_cache(db_task)["original_size"]
            db_image_by_frame = {
                0: {"width": im_meta_data[0]["width"], "height": im_meta_data[0]["height"]}
            }

        def get_size(frame):
            if frame in db_image_by_frame:
                return db_image_by_frame[frame]
            else:
                return db_image_by_frame[0]

        db_jobs = (
            Job.objects.select_related("segment")
            .prefetch_related(
                "labeledshape_set", "labeledtrack_set", "labeledtrack_set__trackedshape_set"
            )
            .filter(segment__task_id=db_task.id)
            .all()
        )

        for db_job in db_jobs:
            db_shapes = db_job.labeledshape_set.all()
            db_tracks = db_job.labeledtrack_set.all()
            for db_shape in db_shapes:
                _flip_shape(db_shape, get_size(db_shape.frame))
                db_shape.save()

            for db_track in db_tracks:
                db_shapes = db_track.trackedshape_set.all()
                for db_shape in db_shapes:
                    _flip_shape(db_shape, get_size(db_shape.frame))
                    db_shape.save()

    for db_task in db_flipped_tasks:
        for frame in range(db_task.size):
            path = frame_path(db_task, frame)
            if os.path.islink(path):
                path = os.path.realpath(path)

            try:
                image = Image.open(path)
                image = image.transpose(Image.ROTATE_180)
                image.save(path)
            except IOError as ex:
                print("Error of handling the frame {}".format(frame))
                print(ex)


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0019_frame_selection"),
    ]

    operations = [
        migrations.RunPython(forwards_func),
        migrations.RemoveField(
            model_name="task",
            name="flipped",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0021_auto_20190826_1827.py =====
# Generated by Django 2.2.4 on 2019-08-26 15:27

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0020_remove_task_flipped"),
    ]

    operations = [
        migrations.AlterField(
            model_name="task",
            name="frame_filter",
            field=models.CharField(blank=True, default="", max_length=256),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0022_auto_20191004_0817.py =====
# Generated by Django 2.2.3 on 2019-10-04 08:17

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0021_auto_20190826_1827"),
    ]

    operations = [
        migrations.CreateModel(
            name="Project",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("name", cvat.apps.engine.models.SafeCharField(max_length=256)),
                ("bug_tracker", models.CharField(blank=True, default="", max_length=2000)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now_add=True)),
                (
                    "status",
                    models.CharField(
                        choices=[
                            ("annotation", "ANNOTATION"),
                            ("validation", "VALIDATION"),
                            ("completed", "COMPLETED"),
                        ],
                        default="annotation",
                        max_length=32,
                    ),
                ),
                (
                    "assignee",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="+",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="+",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="task",
            name="project",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="tasks",
                related_query_name="task",
                to="engine.Project",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0023_auto_20200113_1323.py =====
# Generated by Django 2.2.8 on 2020-01-13 13:23

from django.db import migrations

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0022_auto_20191004_0817"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledimageattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=4096),
        ),
        migrations.AlterField(
            model_name="labeledshapeattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=4096),
        ),
        migrations.AlterField(
            model_name="labeledtrackattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=4096),
        ),
        migrations.AlterField(
            model_name="trackedshapeattributeval",
            name="value",
            field=cvat.apps.engine.models.SafeCharField(max_length=4096),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0024_auto_20191023_1025.py =====
# Generated by Django 2.2.4 on 2019-10-23 10:25

import glob
import itertools
import multiprocessing
import os
import re
import shutil
import sys
import time
import traceback

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

from cvat.apps.engine.log import get_migration_logger
from cvat.apps.engine.media_extractors import (
    ArchiveReader,
    ImageListReader,
    Mpeg4ChunkWriter,
    PdfReader,
    VideoReader,
    ZipChunkWriter,
    ZipCompressedChunkWriter,
    ZipReader,
    get_mime,
)
from cvat.apps.engine.models import DataChoice

MIGRATION_THREAD_COUNT = 2


def fix_path(path):
    ind = path.find(".upload")
    if ind != -1:
        path = path[ind + len(".upload") + 1 :]
    return path


def get_frame_step(frame_filter):
    match = re.search(r"step\s*=\s*([1-9]\d*)", frame_filter)
    return int(match.group(1)) if match else 1


def get_task_on_disk():
    folders = [
        os.path.relpath(f, settings.DATA_ROOT)
        for f in glob.glob(os.path.join(settings.DATA_ROOT, "*"), recursive=False)
    ]

    return set(int(f) for f in folders if f.isdigit())


def get_frame_path(task_data_dir, frame):
    d1 = str(int(frame) // 10000)
    d2 = str(int(frame) // 100)
    path = os.path.join(task_data_dir, d1, d2, str(frame) + ".jpg")

    return path


def slice_by_size(frames, size):
    it = itertools.islice(frames, 0, None)
    frames = list(itertools.islice(it, 0, size, 1))
    while frames:
        yield frames
        frames = list(itertools.islice(it, 0, size, 1))


def migrate_task_data(
    db_task_id,
    db_data_id,
    original_video,
    original_images,
    size,
    start_frame,
    stop_frame,
    frame_filter,
    image_quality,
    chunk_size,
    return_dict,
):
    try:
        db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data_id))
        compressed_cache_dir = os.path.join(db_data_dir, "compressed")
        original_cache_dir = os.path.join(db_data_dir, "original")
        old_db_task_dir = os.path.join(settings.DATA_ROOT, str(db_task_id))
        old_task_data_dir = os.path.join(old_db_task_dir, "data")
        if os.path.exists(old_task_data_dir) and size != 0:
            if original_video:
                if os.path.exists(original_video):
                    _stop_frame = stop_frame if stop_frame else None
                    reader = VideoReader(
                        [original_video], get_frame_step(frame_filter), start_frame, _stop_frame
                    )
                    original_chunk_writer = Mpeg4ChunkWriter(100)
                    compressed_chunk_writer = ZipCompressedChunkWriter(image_quality)

                    counter = itertools.count()
                    generator = itertools.groupby(reader, lambda x: next(counter) // chunk_size)
                    for chunk_idx, chunk_images in generator:
                        chunk_images = list(chunk_images)
                        original_chunk_path = os.path.join(
                            original_cache_dir, "{}.mp4".format(chunk_idx)
                        )
                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)

                        compressed_chunk_path = os.path.join(
                            compressed_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        compressed_chunk_writer.save_as_chunk(chunk_images, compressed_chunk_path)

                    preview = reader.get_preview(0)
                    preview.save(os.path.join(db_data_dir, "preview.jpeg"))
                else:
                    original_chunk_writer = ZipChunkWriter(100)
                    for chunk_idx, chunk_image_ids in enumerate(
                        slice_by_size(range(size), chunk_size)
                    ):
                        chunk_images = []
                        for image_id in chunk_image_ids:
                            image_path = get_frame_path(old_task_data_dir, image_id)
                            chunk_images.append((image_path, image_path))

                        original_chunk_path = os.path.join(
                            original_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)

                        compressed_chunk_path = os.path.join(
                            compressed_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        os.symlink(original_chunk_path, compressed_chunk_path)
                        shutil.copyfile(
                            get_frame_path(old_task_data_dir, image_id),
                            os.path.join(db_data_dir, "preview.jpeg"),
                        )
            else:
                reader = None
                if os.path.exists(original_images[0]):  # task created from images
                    reader = ImageListReader(original_images)
                else:  # task created from archive or pdf
                    archives = []
                    pdfs = []
                    zips = []
                    for p in glob.iglob(
                        os.path.join(db_data_dir, "raw", "**", "*"), recursive=True
                    ):
                        mime_type = get_mime(p)
                        if mime_type == "archive":
                            archives.append(p)
                        elif mime_type == "pdf":
                            pdfs.append(p)
                        elif mime_type == "zip":
                            zips.append(p)
                    if archives:
                        reader = ArchiveReader(archives)
                    elif zips:
                        reader = ZipReader(archives)
                    elif pdfs:
                        reader = PdfReader(pdfs)

                if not reader:
                    original_chunk_writer = ZipChunkWriter(100)
                    for chunk_idx, chunk_image_ids in enumerate(
                        slice_by_size(range(size), chunk_size)
                    ):
                        chunk_images = []
                        for image_id in chunk_image_ids:
                            image_path = get_frame_path(old_task_data_dir, image_id)
                            chunk_images.append((image_path, image_path))

                        original_chunk_path = os.path.join(
                            original_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)

                        compressed_chunk_path = os.path.join(
                            compressed_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        os.symlink(original_chunk_path, compressed_chunk_path)
                        shutil.copyfile(
                            get_frame_path(old_task_data_dir, image_id),
                            os.path.join(db_data_dir, "preview.jpeg"),
                        )
                else:
                    original_chunk_writer = ZipChunkWriter(100)
                    compressed_chunk_writer = ZipCompressedChunkWriter(image_quality)

                    counter = itertools.count()
                    generator = itertools.groupby(reader, lambda x: next(counter) // chunk_size)
                    for chunk_idx, chunk_images in generator:
                        chunk_images = list(chunk_images)
                        compressed_chunk_path = os.path.join(
                            compressed_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        compressed_chunk_writer.save_as_chunk(chunk_images, compressed_chunk_path)

                        original_chunk_path = os.path.join(
                            original_cache_dir, "{}.zip".format(chunk_idx)
                        )
                        original_chunk_writer.save_as_chunk(chunk_images, original_chunk_path)

                    preview = reader.get_preview(0)
                    preview.save(os.path.join(db_data_dir, "preview.jpeg"))
            shutil.rmtree(old_db_task_dir)
        return_dict[db_task_id] = (True, "")
    except Exception as e:
        traceback.print_exc(file=sys.stderr)
        return_dict[db_task_id] = (False, str(e))
    return 0


def migrate_task_schema(db_task, Data, log):
    log.info("Start schema migration of task ID {}.".format(db_task.id))
    try:
        # create folders
        new_task_dir = os.path.join(settings.TASKS_ROOT, str(db_task.id))
        os.makedirs(new_task_dir, exist_ok=True)
        os.makedirs(os.path.join(new_task_dir, "artifacts"), exist_ok=True)
        new_task_logs_dir = os.path.join(new_task_dir, "logs")
        os.makedirs(new_task_logs_dir, exist_ok=True)

        # create Data object
        db_data = Data.objects.create(
            size=db_task.size,
            image_quality=db_task.image_quality,
            start_frame=db_task.start_frame,
            stop_frame=db_task.stop_frame,
            frame_filter=db_task.frame_filter,
            compressed_chunk_type=DataChoice.IMAGESET,
            original_chunk_type=(
                DataChoice.VIDEO if db_task.mode == "interpolation" else DataChoice.IMAGESET
            ),
        )
        db_data.save()

        db_task.data = db_data

        db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data.id))
        os.makedirs(db_data_dir, exist_ok=True)
        compressed_cache_dir = os.path.join(db_data_dir, "compressed")
        os.makedirs(compressed_cache_dir, exist_ok=True)

        original_cache_dir = os.path.join(db_data_dir, "original")
        os.makedirs(original_cache_dir, exist_ok=True)

        old_db_task_dir = os.path.join(settings.DATA_ROOT, str(db_task.id))

        # move logs
        for log_file in ("task.log", "client.log"):
            task_log_file = os.path.join(old_db_task_dir, log_file)
            if os.path.isfile(task_log_file):
                shutil.move(task_log_file, new_task_logs_dir)

        if hasattr(db_task, "video"):
            db_task.video.data = db_data
            db_task.video.path = fix_path(db_task.video.path)
            db_task.video.save()

        for db_image in db_task.image_set.all():
            db_image.data = db_data
            db_image.path = fix_path(db_image.path)
            db_image.save()

        old_raw_dir = os.path.join(old_db_task_dir, ".upload")
        new_raw_dir = os.path.join(db_data_dir, "raw")

        for client_file in db_task.clientfile_set.all():
            client_file.file = client_file.file.path.replace(old_raw_dir, new_raw_dir)
            client_file.save()

        for server_file in db_task.serverfile_set.all():
            server_file.file = server_file.file.replace(old_raw_dir, new_raw_dir)
            server_file.save()

        for remote_file in db_task.remotefile_set.all():
            remote_file.file = remote_file.file.replace(old_raw_dir, new_raw_dir)
            remote_file.save()

        db_task.save()

        # move old raw data
        if os.path.exists(old_raw_dir):
            shutil.move(old_raw_dir, new_raw_dir)

        return (db_task.id, db_data.id)

    except Exception as e:
        log.error("Cannot migrate schema for the task: {}".format(db_task.id))
        log.error(str(e))
        traceback.print_exc(file=sys.stderr)


def create_data_objects(apps, schema_editor):
    migration_name = os.path.splitext(os.path.basename(__file__))[0]
    with get_migration_logger(migration_name) as log:
        disk_tasks = get_task_on_disk()

        Task = apps.get_model("engine", "Task")
        Data = apps.get_model("engine", "Data")

        db_tasks = Task.objects
        task_count = db_tasks.count()
        log.info("\nStart schema migration...")
        migrated_db_tasks = []
        for counter, db_task in enumerate(db_tasks.all().iterator()):
            res = migrate_task_schema(db_task, Data, log)
            log.info(
                "Schema migration for the task {} completed. Progress {}/{}".format(
                    db_task.id, counter + 1, task_count
                )
            )
            if res:
                migrated_db_tasks.append(res)

        log.info("\nSchema migration is finished...")
        log.info("\nStart data migration...")

        manager = multiprocessing.Manager()
        return_dict = manager.dict()

        def create_process(db_task_id, db_data_id):
            db_data = Data.objects.get(pk=db_data_id)
            db_data_dir = os.path.join(settings.MEDIA_DATA_ROOT, str(db_data_id))
            new_raw_dir = os.path.join(db_data_dir, "raw")

            original_video = None
            original_images = None
            if hasattr(db_data, "video"):
                original_video = os.path.join(new_raw_dir, db_data.video.path)
            else:
                original_images = [
                    os.path.realpath(os.path.join(new_raw_dir, db_image.path))
                    for db_image in db_data.images.all()
                ]

            args = (
                db_task_id,
                db_data_id,
                original_video,
                original_images,
                db_data.size,
                db_data.start_frame,
                db_data.stop_frame,
                db_data.frame_filter,
                db_data.image_quality,
                db_data.chunk_size,
                return_dict,
            )

            return multiprocessing.Process(target=migrate_task_data, args=args)

        results = {}
        task_idx = 0
        while True:
            for res_idx in list(results.keys()):
                res = results[res_idx]
                if not res.is_alive():
                    del results[res_idx]
                    if res.exitcode == 0:
                        ret_code, message = return_dict[res_idx]
                        if ret_code:
                            counter = task_idx - len(results)
                            progress = (100 * counter) / task_count
                            log.info(
                                "Data migration for the task {} completed. Progress: {:.02f}% | {}/{}.".format(
                                    res_idx, progress, counter, task_count
                                )
                            )
                        else:
                            log.error("Cannot migrate data for the task: {}".format(res_idx))
                            log.error(str(message))
                        if res_idx in disk_tasks:
                            disk_tasks.remove(res_idx)
                    else:
                        log.error("#Cannot migrate data for the task: {}".format(res_idx))

            while task_idx < len(migrated_db_tasks) and len(results) < MIGRATION_THREAD_COUNT:
                log.info(
                    "Start data migration for the task {}, data ID {}".format(
                        migrated_db_tasks[task_idx][0], migrated_db_tasks[task_idx][1]
                    )
                )
                results[migrated_db_tasks[task_idx][0]] = create_process(
                    *migrated_db_tasks[task_idx]
                )
                results[migrated_db_tasks[task_idx][0]].start()
                task_idx += 1

            if len(results) == 0:
                break

            time.sleep(5)

        if disk_tasks:
            suspicious_tasks_dir = os.path.join(settings.DATA_ROOT, "suspicious_tasks")
            os.makedirs(suspicious_tasks_dir, exist_ok=True)
            for tid in disk_tasks:
                suspicious_task_path = os.path.join(settings.DATA_ROOT, str(tid))
                try:
                    shutil.move(suspicious_task_path, suspicious_tasks_dir)
                except Exception as e:
                    log.error(
                        "Cannot move data for the suspicious task {}, \
                        that is not represented in the database.".format(
                            suspicious_task_path
                        )
                    )
                    log.error(str(e))

        # DL models migration
        if apps.is_installed("auto_annotation"):
            DLModel = apps.get_model("auto_annotation", "AnnotationModel")

            for db_model in DLModel.objects.all():
                try:
                    old_location = os.path.join(settings.BASE_DIR, "models", str(db_model.id))
                    new_location = os.path.join(
                        settings.BASE_DIR, "data", "models", str(db_model.id)
                    )

                    if os.path.isdir(old_location):
                        shutil.move(old_location, new_location)

                        db_model.model_file.name = db_model.model_file.name.replace(
                            old_location, new_location
                        )
                        db_model.weights_file.name = db_model.weights_file.name.replace(
                            old_location, new_location
                        )
                        db_model.labelmap_file.name = db_model.labelmap_file.name.replace(
                            old_location, new_location
                        )
                        db_model.interpretation_file.name = (
                            db_model.interpretation_file.name.replace(old_location, new_location)
                        )

                        db_model.save()
                except Exception as e:
                    log.error("Cannot migrate data for the DL model: {}".format(db_model.id))
                    log.error(str(e))


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0023_auto_20200113_1323"),
    ]

    operations = [
        migrations.CreateModel(
            name="Data",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("chunk_size", models.PositiveIntegerField(default=36)),
                ("size", models.PositiveIntegerField(default=0)),
                ("image_quality", models.PositiveSmallIntegerField(default=50)),
                ("start_frame", models.PositiveIntegerField(default=0)),
                ("stop_frame", models.PositiveIntegerField(default=0)),
                ("frame_filter", models.CharField(blank=True, default="", max_length=256)),
                (
                    "compressed_chunk_type",
                    models.CharField(
                        choices=[("video", "VIDEO"), ("imageset", "IMAGESET"), ("list", "LIST")],
                        default=DataChoice("imageset"),
                        max_length=32,
                    ),
                ),
                (
                    "original_chunk_type",
                    models.CharField(
                        choices=[("video", "VIDEO"), ("imageset", "IMAGESET"), ("list", "LIST")],
                        default=DataChoice("imageset"),
                        max_length=32,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="task",
            name="data",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="tasks",
                to="engine.Data",
            ),
        ),
        migrations.AddField(
            model_name="image",
            name="data",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="images",
                to="engine.Data",
            ),
        ),
        migrations.AddField(
            model_name="video",
            name="data",
            field=models.OneToOneField(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="video",
                to="engine.Data",
            ),
        ),
        migrations.AddField(
            model_name="clientfile",
            name="data",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="client_files",
                to="engine.Data",
            ),
        ),
        migrations.AddField(
            model_name="remotefile",
            name="data",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="remote_files",
                to="engine.Data",
            ),
        ),
        migrations.AddField(
            model_name="serverfile",
            name="data",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="server_files",
                to="engine.Data",
            ),
        ),
        migrations.RunPython(code=create_data_objects),
        migrations.RemoveField(
            model_name="image",
            name="task",
        ),
        migrations.RemoveField(
            model_name="remotefile",
            name="task",
        ),
        migrations.RemoveField(
            model_name="serverfile",
            name="task",
        ),
        migrations.RemoveField(
            model_name="task",
            name="frame_filter",
        ),
        migrations.RemoveField(
            model_name="task",
            name="image_quality",
        ),
        migrations.RemoveField(
            model_name="task",
            name="size",
        ),
        migrations.RemoveField(
            model_name="task",
            name="start_frame",
        ),
        migrations.RemoveField(
            model_name="task",
            name="stop_frame",
        ),
        migrations.RemoveField(
            model_name="video",
            name="task",
        ),
        migrations.AlterField(
            model_name="image",
            name="path",
            field=models.CharField(default="", max_length=1024),
        ),
        migrations.AlterField(
            model_name="video",
            name="path",
            field=models.CharField(default="", max_length=1024),
        ),
        migrations.AlterUniqueTogether(
            name="clientfile",
            unique_together={("data", "file")},
        ),
        migrations.RemoveField(
            model_name="clientfile",
            name="task",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0025_auto_20200324_1222.py =====
# Generated by Django 2.2.10 on 2020-03-24 12:22

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0024_auto_20191023_1025"),
    ]

    operations = [
        migrations.AlterField(
            model_name="data",
            name="chunk_size",
            field=models.PositiveIntegerField(null=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0026_auto_20200719_1511.py =====
# Generated by Django 2.2.13 on 2020-07-19 15:11

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0025_auto_20200324_1222"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("cuboid", "CUBOID"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterField(
            model_name="trackedshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("cuboid", "CUBOID"),
                ],
                max_length=16,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0027_auto_20200719_1552.py =====
# Generated by Django 2.2.10 on 2020-07-19 15:52

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0026_auto_20200719_1511"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledimage",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AddField(
            model_name="labeledshape",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AddField(
            model_name="labeledtrack",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0028_labelcolor.py =====
# Generated by Django 2.2.13 on 2020-08-11 11:26
from django.db import migrations, models

from cvat.apps.dataset_manager.formats.utils import get_label_color


def alter_label_colors(apps, schema_editor):
    Label = apps.get_model("engine", "Label")
    Task = apps.get_model("engine", "Task")

    for task in Task.objects.all():
        labels = Label.objects.filter(task_id=task.id).order_by("id")
        label_names = list()
        for label in labels:
            label.color = get_label_color(label.name, label_names)
            label_names.append(label.color)
            label.save()


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0027_auto_20200719_1552"),
    ]

    operations = [
        migrations.AddField(
            model_name="label",
            name="color",
            field=models.CharField(default="", max_length=8),
        ),
        migrations.RunPython(
            code=alter_label_colors,
            reverse_code=migrations.RunPython.noop,
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0029_data_storage_method.py =====
# Generated by Django 2.2.13 on 2020-08-13 05:49

import os

from django.conf import settings
from django.db import migrations, models
from pyunpack import Archive

import cvat.apps.engine.models
from cvat.apps.engine.media_extractors import _is_archive, _is_zip


def unzip(apps, schema_editor):
    Data = apps.get_model("engine", "Data")
    data_q_set = Data.objects.all()
    archive_paths = []

    for data_instance in data_q_set:
        for root, _, files in os.walk(
            os.path.join(settings.MEDIA_DATA_ROOT, "{}/raw/".format(data_instance.id))
        ):
            archive_paths.extend(
                [os.path.join(root, file) for file in files if _is_archive(file) or _is_zip(file)]
            )

    for path in archive_paths:
        Archive(path).extractall(os.path.dirname(path))
        os.remove(path)


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0028_labelcolor"),
    ]

    operations = [
        migrations.AddField(
            model_name="data",
            name="storage_method",
            field=models.CharField(
                choices=[("cache", "CACHE"), ("file_system", "FILE_SYSTEM")],
                default=cvat.apps.engine.models.StorageMethodChoice("file_system"),
                max_length=15,
            ),
        ),
        migrations.RunPython(unzip),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0030_auto_20200914_1331.py =====
# Generated by Django 3.1.1 on 2020-09-14 13:31

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0029_data_storage_method"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="pluginoption",
            name="plugin",
        ),
        migrations.DeleteModel(
            name="Plugin",
        ),
        migrations.DeleteModel(
            name="PluginOption",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0031_auto_20201011_0220.py =====
# Generated by Django 3.1.1 on 2020-10-11 02:20

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0030_auto_20200914_1331"),
    ]

    operations = [
        migrations.AlterField(
            model_name="task",
            name="updated_date",
            field=models.DateTimeField(auto_now=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0032_remove_task_z_order.py =====
# Generated by Django 3.1.1 on 2020-10-12 17:16

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0031_auto_20201011_0220"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="task",
            name="z_order",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0033_projects_adjastment.py =====
# Generated by Django 3.1.1 on 2020-09-24 12:44

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0032_remove_task_z_order"),
    ]

    operations = [
        migrations.AddField(
            model_name="label",
            name="project",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                to="engine.project",
            ),
        ),
        migrations.AlterField(
            model_name="label",
            name="task",
            field=models.ForeignKey(
                blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to="engine.task"
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0034_auto_20201125_1426.py =====
# Generated by Django 3.1.1 on 2020-11-25 14:26

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.engine.models


def create_profile(apps, schema_editor):
    User = apps.get_model("auth", "User")
    Profile = apps.get_model("engine", "Profile")
    for user in User.objects.all():
        profile = Profile()
        profile.user = user
        profile.save()


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0033_projects_adjastment"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="reviewer",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="review_job_set",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
        migrations.CreateModel(
            name="Review",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("estimated_quality", models.FloatField()),
                (
                    "status",
                    models.CharField(
                        choices=[
                            ("accepted", "ACCEPTED"),
                            ("rejected", "REJECTED"),
                            ("review_further", "REVIEW_FURTHER"),
                        ],
                        max_length=16,
                    ),
                ),
                (
                    "assignee",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="reviewed",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.job"),
                ),
                (
                    "reviewer",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="reviews",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="Profile",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("rating", models.FloatField(default=0.0)),
                (
                    "user",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="Issue",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                ("position", cvat.apps.engine.models.FloatArrayField()),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("resolved_date", models.DateTimeField(blank=True, null=True)),
                (
                    "job",
                    models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.job"),
                ),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="issues",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "resolver",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="resolved_issues",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "review",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        to="engine.review",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="Comment",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("message", models.TextField(default="")),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                (
                    "author",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "issue",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.issue"
                    ),
                ),
            ],
        ),
        migrations.RunPython(create_profile),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0035_data_storage.py =====
# Generated by Django 3.1.1 on 2020-12-02 06:47

from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0034_auto_20201125_1426"),
    ]

    operations = [
        migrations.AddField(
            model_name="data",
            name="storage",
            field=models.CharField(
                choices=[("local", "LOCAL"), ("share", "SHARE")],
                default=cvat.apps.engine.models.StorageChoice["LOCAL"],
                max_length=15,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0036_auto_20201216_0943.py =====
# Generated by Django 3.1.1 on 2020-12-16 09:43

import django.db.models.deletion
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0035_data_storage"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="dimension",
            field=models.CharField(
                choices=[("3d", "DIM_3D"), ("2d", "DIM_2D")],
                default=cvat.apps.engine.models.DimensionType["DIM_2D"],
                max_length=2,
            ),
        ),
        migrations.CreateModel(
            name="RelatedFile",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "path",
                    models.FileField(
                        max_length=1024,
                        storage=cvat.apps.engine.models.MyFileSystemStorage(),
                        upload_to=cvat.apps.engine.models.upload_path_handler,
                    ),
                ),
                (
                    "data",
                    models.ForeignKey(
                        default=1,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="related_files",
                        to="engine.data",
                    ),
                ),
                (
                    "primary_image",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="related_files",
                        to="engine.image",
                    ),
                ),
            ],
            options={
                "default_permissions": (),
                "unique_together": {("data", "path")},
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0037_task_subset.py =====
# Generated by Django 3.1.1 on 2021-01-29 11:21

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0036_auto_20201216_0943"),
    ]

    operations = [
        migrations.AddField(
            model_name="task",
            name="subset",
            field=models.CharField(blank=True, default="", max_length=64),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0038_manifest.py =====
# Generated by Django 3.1.1 on 2021-02-20 08:36

import glob
import itertools
import os
from re import search

from django.conf import settings
from django.db import migrations

from cvat.apps.engine.log import get_logger
from cvat.apps.engine.media_extractors import get_mime
from cvat.apps.engine.models import DimensionType, StorageChoice, StorageMethodChoice
from utils.dataset_manifest import ImageManifestManager, VideoManifestManager

MIGRATION_NAME = os.path.splitext(os.path.basename(__file__))[0]
MIGRATION_LOG = os.path.join(settings.MIGRATIONS_LOGS_ROOT, f"{MIGRATION_NAME}.log")


def _get_query_set(apps):
    Data = apps.get_model("engine", "Data")
    query_set = Data.objects.filter(storage_method=StorageMethodChoice.CACHE)
    return query_set


def migrate2meta(apps, shema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    query_set = _get_query_set(apps)
    for db_data in query_set:
        try:
            upload_dir = "{}/{}/raw".format(settings.MEDIA_DATA_ROOT, db_data.id)
            logger.info("Migrate data({}), folder - {}".format(db_data.id, upload_dir))
            meta_path = os.path.join(upload_dir, "meta_info.txt")
            if os.path.exists(os.path.join(upload_dir, "manifest.jsonl")):
                os.remove(os.path.join(upload_dir, "manifest.jsonl"))
                logger.info("A manifest file has been deleted")
            if os.path.exists(os.path.join(upload_dir, "index.json")):
                os.remove(os.path.join(upload_dir, "index.json"))
                logger.info("A manifest index file has been deleted")
            data_dir = upload_dir if db_data.storage == StorageChoice.LOCAL else settings.SHARE_ROOT
            if hasattr(db_data, "video"):
                if os.path.exists(meta_path):
                    logger.info("A meta_info.txt already exists")
                    continue
                media_file = os.path.join(data_dir, db_data.video.path)
                logger.info("Preparing of the video meta has begun")
                manifest = VideoManifestManager(manifest_path=upload_dir)
                manifest.link(media_file=media_file, force=True)
                manifest.init_index()
                with open(meta_path, "w") as meta_file:
                    for idx, pts, _ in manifest.reader:
                        meta_file.write(f"{idx} {pts}\n")
            else:
                name_format = "dummy_{}.txt"
                sources = [db_image.path for db_image in db_data.images.all().order_by("frame")]
                counter = itertools.count()
                logger.info("Preparing of the dummy chunks has begun")
                for idx, img_paths in itertools.groupby(
                    sources, lambda x: next(counter) // db_data.chunk_size
                ):
                    if os.path.exists(os.path.join(upload_dir, name_format.format(idx))):
                        logger.info(name_format.format(idx) + " already exists")
                        continue
                    with open(
                        os.path.join(upload_dir, name_format.format(idx)), "w"
                    ) as dummy_chunk:
                        dummy_chunk.writelines([f"{img_path}\n" for img_path in img_paths])
            logger.info("Succesfull migration for the data({})".format(db_data.id))
        except Exception as ex:
            logger.error(str(ex))


def migrate2manifest(apps, shema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    logger.info("The data migration has been started for creating manifest`s files")
    query_set = _get_query_set(apps)
    logger.info("Need to update {} data objects".format(len(query_set)))
    for db_data in query_set:
        try:
            upload_dir = "{}/{}/raw".format(settings.MEDIA_DATA_ROOT, db_data.id)
            logger.info("Migrate data({}), folder - {}".format(db_data.id, upload_dir))
            if os.path.exists(os.path.join(upload_dir, "meta_info.txt")):
                os.remove(os.path.join(upload_dir, "meta_info.txt"))
                logger.info("{}/meta_info.txt has been deleted".format(upload_dir))
            else:
                for path in glob.glob(f"{upload_dir}/dummy_*.txt"):
                    os.remove(path)
                    logger.info(f"{path} has been deleted")
            # it's necessary for case with long data migration
            if os.path.exists(os.path.join(upload_dir, "manifest.jsonl")):
                logger.info("Manifest file already exists")
                continue
            data_dir = upload_dir if db_data.storage == StorageChoice.LOCAL else settings.SHARE_ROOT
            if hasattr(db_data, "video"):
                media_file = os.path.join(data_dir, db_data.video.path)
                manifest = VideoManifestManager(manifest_path=upload_dir)
                manifest.link(media_file=media_file, force=True)
                logger.info("Manifest creating has begun")
                manifest.create()
            else:
                manifest = ImageManifestManager(manifest_path=upload_dir)
                sources = []
                if db_data.storage == StorageChoice.LOCAL:
                    for root, _, files in os.walk(data_dir):
                        sources.extend(
                            [os.path.join(root, f) for f in files if get_mime(f) == "image"]
                        )
                    sources.sort()
                # using share, this means that we can not explicitly restore the entire data structure
                else:
                    sources = [
                        os.path.join(data_dir, db_image.path)
                        for db_image in db_data.images.all().order_by("frame")
                    ]
                if any(
                    list(filter(lambda x: x.dimension == DimensionType.DIM_3D, db_data.tasks.all()))
                ):
                    logger.info("Preparing of images 3d meta information has begun")
                    manifest.link(sources=sources, data_dir=data_dir, DIM_3D=True)
                else:
                    logger.info("Preparing of 2d images meta information has begun")
                    manifest.link(sources=sources, data_dir=data_dir)

                if db_data.storage == StorageChoice.SHARE:

                    def _get_frame_step(str_):
                        match = search(r"step\s*=\s*([1-9]\d*)", str_)
                        return int(match.group(1)) if match else 1

                    logger.info("Data is located on the share, metadata update has been started")
                    manifest.step = _get_frame_step(db_data.frame_filter)
                    manifest.start = db_data.start_frame
                    manifest.stop = db_data.stop_frame + 1
                logger.info("Manifest creating has begun")
                manifest.create()
            logger.info("Succesfull migration for the data({})".format(db_data.id))
        except Exception as ex:
            logger.error(str(ex))


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0037_task_subset"),
    ]

    operations = [migrations.RunPython(code=migrate2manifest, reverse_code=migrate2meta)]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0039_auto_training.py =====
# Generated by Django 3.1.7 on 2021-04-02 13:17

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0038_manifest"),
    ]

    operations = [
        migrations.CreateModel(
            name="TrainingProject",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("host", models.CharField(max_length=256)),
                ("username", models.CharField(max_length=256)),
                ("password", models.CharField(max_length=256)),
                ("training_id", models.CharField(max_length=64)),
                ("enabled", models.BooleanField(null=True)),
                (
                    "project_class",
                    models.CharField(
                        blank=True, choices=[("OD", "Object Detection")], max_length=2, null=True
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="TrainingProjectLabel",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("training_label_id", models.CharField(max_length=64)),
                (
                    "cvat_label",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="training_project_label",
                        to="engine.label",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="TrainingProjectImage",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("idx", models.PositiveIntegerField()),
                ("training_image_id", models.CharField(max_length=64)),
                (
                    "task",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.task"
                    ),
                ),
            ],
        ),
        migrations.AddField(
            model_name="project",
            name="training_project",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                to="engine.trainingproject",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0040_cloud_storage.py =====
# Generated by Django 3.1.8 on 2021-05-07 06:42

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0039_auto_training"),
    ]

    operations = [
        migrations.AlterField(
            model_name="data",
            name="storage",
            field=models.CharField(
                choices=[
                    ("cloud_storage", "CLOUD_STORAGE"),
                    ("local", "LOCAL"),
                    ("share", "SHARE"),
                ],
                default=cvat.apps.engine.models.StorageChoice["LOCAL"],
                max_length=15,
            ),
        ),
        migrations.CreateModel(
            name="CloudStorage",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "provider_type",
                    models.CharField(
                        choices=[
                            ("AWS_S3_BUCKET", "AWS_S3"),
                            ("AZURE_CONTAINER", "AZURE_CONTAINER"),
                            ("GOOGLE_DRIVE", "GOOGLE_DRIVE"),
                        ],
                        max_length=20,
                    ),
                ),
                ("resource", models.CharField(max_length=63)),
                ("display_name", models.CharField(max_length=63)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                ("credentials", models.CharField(max_length=500)),
                (
                    "credentials_type",
                    models.CharField(
                        choices=[
                            ("TEMP_KEY_SECRET_KEY_TOKEN_SET", "TEMP_KEY_SECRET_KEY_TOKEN_SET"),
                            ("ACCOUNT_NAME_TOKEN_PAIR", "ACCOUNT_NAME_TOKEN_PAIR"),
                            ("ANONYMOUS_ACCESS", "ANONYMOUS_ACCESS"),
                        ],
                        max_length=29,
                    ),
                ),
                ("specific_attributes", models.CharField(blank=True, max_length=50)),
                ("description", models.TextField(blank=True)),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="cloud_storages",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
                "unique_together": {("provider_type", "resource", "credentials")},
            },
        ),
        migrations.AddField(
            model_name="data",
            name="cloud_storage",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="data",
                to="engine.cloudstorage",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0041_auto_20210827_0258.py =====
# Generated by Django 3.1.13 on 2021-08-27 02:58

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0040_cloud_storage"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="credentials_type",
            field=models.CharField(
                choices=[
                    ("TEMP_KEY_SECRET_KEY_TOKEN_SET", "TEMP_KEY_SECRET_KEY_TOKEN_SET"),
                    ("ACCOUNT_NAME_TOKEN_PAIR", "ACCOUNT_NAME_TOKEN_PAIR"),
                    ("KEY_FILE_PATH", "KEY_FILE_PATH"),
                    ("ANONYMOUS_ACCESS", "ANONYMOUS_ACCESS"),
                ],
                max_length=29,
            ),
        ),
        migrations.AlterField(
            model_name="cloudstorage",
            name="provider_type",
            field=models.CharField(
                choices=[
                    ("AWS_S3_BUCKET", "AWS_S3"),
                    ("AZURE_CONTAINER", "AZURE_CONTAINER"),
                    ("GOOGLE_DRIVE", "GOOGLE_DRIVE"),
                    ("GOOGLE_CLOUD_STORAGE", "GOOGLE_CLOUD_STORAGE"),
                ],
                max_length=20,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0042_auto_20210830_1056.py =====
# Generated by Django 3.1.13 on 2021-08-30 10:56

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0041_auto_20210827_0258"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="credentials_type",
            field=models.CharField(
                choices=[
                    ("KEY_SECRET_KEY_PAIR", "KEY_SECRET_KEY_PAIR"),
                    ("ACCOUNT_NAME_TOKEN_PAIR", "ACCOUNT_NAME_TOKEN_PAIR"),
                    ("KEY_FILE_PATH", "KEY_FILE_PATH"),
                    ("ANONYMOUS_ACCESS", "ANONYMOUS_ACCESS"),
                ],
                max_length=29,
            ),
        ),
        migrations.CreateModel(
            name="Manifest",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("filename", models.CharField(default="manifest.jsonl", max_length=1024)),
                (
                    "cloud_storage",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="manifests",
                        to="engine.cloudstorage",
                    ),
                ),
            ],
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0043_auto_20211027_0718.py =====
# Generated by Django 3.1.13 on 2021-10-27 07:18

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0042_auto_20210830_1056"),
    ]

    operations = [
        migrations.AlterField(
            model_name="project",
            name="updated_date",
            field=models.DateTimeField(auto_now=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0044_auto_20211115_0858.py =====
# Generated by Django 3.1.13 on 2021-11-15 08:58

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0043_auto_20211027_0718"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledshape",
            name="rotation",
            field=models.FloatField(default=0),
        ),
        migrations.AddField(
            model_name="trackedshape",
            name="rotation",
            field=models.FloatField(default=0),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0045_auto_20211123_0824.py =====
# Generated by Django 3.1.13 on 2021-11-23 08:24

from django.db import migrations, models


class Migration(migrations.Migration):
    replaces = [("engine", "0044_auto_20211123_0824")]

    dependencies = [
        ("engine", "0044_auto_20211115_0858"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="resource",
            field=models.CharField(max_length=222),
        ),
        migrations.AlterField(
            model_name="cloudstorage",
            name="specific_attributes",
            field=models.CharField(blank=True, max_length=128),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0046_data_sorting_method.py =====
# Generated by Django 3.1.13 on 2021-12-03 08:06

from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):
    replaces = [("engine", "0045_data_sorting_method")]

    dependencies = [
        ("engine", "0045_auto_20211123_0824"),
    ]

    operations = [
        migrations.AddField(
            model_name="data",
            name="sorting_method",
            field=models.CharField(
                choices=[
                    ("lexicographical", "LEXICOGRAPHICAL"),
                    ("natural", "NATURAL"),
                    ("predefined", "PREDEFINED"),
                    ("random", "RANDOM"),
                ],
                default=cvat.apps.engine.models.SortingMethod["LEXICOGRAPHICAL"],
                max_length=15,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0047_auto_20211110_1938.py =====
# Generated by Django 3.2.8 on 2021-11-10 19:38

import django.db.models.deletion
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("organizations", "0001_initial"),
        ("engine", "0046_data_sorting_method"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="issue",
            name="review",
        ),
        migrations.RemoveField(
            model_name="job",
            name="reviewer",
        ),
        migrations.AddField(
            model_name="cloudstorage",
            name="organization",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="cloudstorages",
                to="organizations.organization",
            ),
        ),
        migrations.AddField(
            model_name="job",
            name="stage",
            field=models.CharField(
                choices=[
                    ("annotation", "ANNOTATION"),
                    ("validation", "VALIDATION"),
                    ("acceptance", "ACCEPTANCE"),
                ],
                default=cvat.apps.engine.models.StageChoice["ANNOTATION"],
                max_length=32,
            ),
        ),
        migrations.AddField(
            model_name="job",
            name="state",
            field=models.CharField(
                choices=[
                    ("new", "NEW"),
                    ("in progress", "IN_PROGRESS"),
                    ("completed", "COMPLETED"),
                    ("rejected", "REJECTED"),
                ],
                default=cvat.apps.engine.models.StateChoice["NEW"],
                max_length=32,
            ),
        ),
        migrations.AddField(
            model_name="project",
            name="organization",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="projects",
                to="organizations.organization",
            ),
        ),
        migrations.AddField(
            model_name="task",
            name="organization",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="tasks",
                to="organizations.organization",
            ),
        ),
        migrations.DeleteModel(
            name="Review",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0048_auto_20211112_1918.py =====
# Generated by Django 3.2.8 on 2021-11-12 19:18

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0047_auto_20211110_1938"),
    ]

    operations = [
        migrations.RenameField(
            model_name="comment",
            old_name="author",
            new_name="owner",
        ),
        migrations.RenameField(
            model_name="issue",
            old_name="resolved_date",
            new_name="updated_date",
        ),
        migrations.RenameField(
            model_name="jobcommit",
            old_name="author",
            new_name="owner",
        ),
        migrations.RemoveField(
            model_name="issue",
            name="resolver",
        ),
        migrations.AddField(
            model_name="issue",
            name="assignee",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
        migrations.AddField(
            model_name="issue",
            name="resolved",
            field=models.BooleanField(default=False),
        ),
        migrations.AlterField(
            model_name="comment",
            name="issue",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="comments",
                to="engine.issue",
            ),
        ),
        migrations.AlterField(
            model_name="issue",
            name="job",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, related_name="issues", to="engine.job"
            ),
        ),
        migrations.AlterField(
            model_name="issue",
            name="owner",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0049_auto_20220202_0710.py =====
# Generated by Django 3.2.11 on 2022-02-02 07:10

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0048_auto_20211112_1918"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterField(
            model_name="trackedshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                ],
                max_length=16,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0050_auto_20220211_1425.py =====
# Generated by Django 3.2.12 on 2022-02-11 14:25

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0049_auto_20220202_0710"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="trainingprojectimage",
            name="task",
        ),
        migrations.RemoveField(
            model_name="trainingprojectlabel",
            name="cvat_label",
        ),
        migrations.RemoveField(
            model_name="project",
            name="training_project",
        ),
        migrations.DeleteModel(
            name="TrainingProject",
        ),
        migrations.DeleteModel(
            name="TrainingProjectImage",
        ),
        migrations.DeleteModel(
            name="TrainingProjectLabel",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0051_auto_20220220_1824.py =====
# Generated by Django 3.2.12 on 2022-02-20 18:24

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0050_auto_20220211_1425"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="jobcommit",
            name="message",
        ),
        migrations.RemoveField(
            model_name="jobcommit",
            name="version",
        ),
        migrations.AddField(
            model_name="jobcommit",
            name="data",
            field=models.JSONField(default=dict),
        ),
        migrations.AddField(
            model_name="jobcommit",
            name="scope",
            field=models.CharField(default="", max_length=32),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0052_alter_cloudstorage_specific_attributes.py =====
# Generated by Django 3.2.12 on 2022-03-14 10:51

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0051_auto_20220220_1824"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="specific_attributes",
            field=models.CharField(blank=True, max_length=1024),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0053_data_deleted_frames.py =====
# Generated by Django 3.2.12 on 2022-05-20 09:21

from django.db import migrations

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0052_alter_cloudstorage_specific_attributes"),
    ]

    operations = [
        migrations.AddField(
            model_name="data",
            name="deleted_frames",
            field=cvat.apps.engine.models.IntArrayField(default=""),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0054_auto_20220610_1829.py =====
# Generated by Django 3.2.12 on 2022-06-10 18:29

import django.db.models.deletion
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0053_data_deleted_frames"),
    ]

    operations = [
        migrations.CreateModel(
            name="Storage",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "location",
                    models.CharField(
                        choices=[("cloud_storage", "CLOUD_STORAGE"), ("local", "LOCAL")],
                        default=cvat.apps.engine.models.Location["LOCAL"],
                        max_length=16,
                    ),
                ),
                ("cloud_storage_id", models.IntegerField(blank=True, default=None, null=True)),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="job",
            name="updated_date",
            field=models.DateTimeField(auto_now=True),
        ),
        migrations.AddField(
            model_name="project",
            name="source_storage",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to="engine.storage",
            ),
        ),
        migrations.AddField(
            model_name="project",
            name="target_storage",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to="engine.storage",
            ),
        ),
        migrations.AddField(
            model_name="task",
            name="source_storage",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to="engine.storage",
            ),
        ),
        migrations.AddField(
            model_name="task",
            name="target_storage",
            field=models.ForeignKey(
                blank=True,
                default=None,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="+",
                to="engine.storage",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0055_jobs_directories.py =====
# Generated by Django 3.2.12 on 2022-06-10 18:29

import os
import shutil

from django.conf import settings
from django.db import migrations

from cvat.apps.engine.log import get_logger

MIGRATION_NAME = os.path.splitext(os.path.basename(__file__))[0]
MIGRATION_LOG = os.path.join(settings.MIGRATIONS_LOGS_ROOT, f"{MIGRATION_NAME}.log")


def _get_query_set(apps):
    Job = apps.get_model("engine", "Job")
    query_set = Job.objects.all()
    return query_set


def _get_job_dir_path(jid):
    return os.path.join(settings.JOBS_ROOT, str(jid))


def create_directories(apps, schema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    query_set = _get_query_set(apps)
    logger.info(f"Migration has been started. Need to create {query_set.count()} directories.")

    for db_job in query_set:
        jid = db_job.id
        os.makedirs(_get_job_dir_path(jid), exist_ok=True)
    logger.info(f"Migration has been finished successfully.")


def delete_directories(apps, schema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    query_set = _get_query_set(apps)
    logger.info(
        f"Reverse migration has been started. Need to delete {query_set.count()} directories."
    )
    for db_job in query_set:
        jid = db_job.id
        job_dir = _get_job_dir_path(jid)
        if os.path.isdir(job_dir):
            shutil.rmtree(job_dir)
    logger.info(f"Migration has been reversed successfully.")


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0054_auto_20220610_1829"),
    ]

    operations = [migrations.RunPython(code=create_directories, reverse_code=delete_directories)]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0056_jobs_previews.py =====
# Generated by Django 3.2.14 on 2022-07-18 06:37

import os
import shutil

from django.conf import settings
from django.db import migrations

from cvat.apps.engine.log import get_logger

MIGRATION_NAME = os.path.splitext(os.path.basename(__file__))[0]
MIGRATION_LOG = os.path.join(settings.MIGRATIONS_LOGS_ROOT, f"{MIGRATION_NAME}.log")


def _get_query_set(apps):
    Job = apps.get_model("engine", "Job")
    query_set = Job.objects.all()
    return query_set


def _get_job_preview_path(jid):
    return os.path.join(settings.JOBS_ROOT, str(jid), "preview.jpeg")


def _get_data_preview_path(did):
    return os.path.join(settings.MEDIA_DATA_ROOT, str(did), "preview.jpeg")


def create_previews(apps, schema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    query_set = _get_query_set(apps)
    logger.info(f"Migration has been started. Need to create {query_set.count()} previews.")
    for db_job in query_set:
        try:
            jid = db_job.id
            did = db_job.segment.task.data.id
            task_preview = _get_data_preview_path(did)
            job_preview = _get_job_preview_path(jid)
            if os.path.isfile(task_preview) and not os.path.isfile(job_preview):
                shutil.copy(task_preview, job_preview)
        except Exception as e:
            logger.error(f"Cannot create preview for job {db_job.id}")
            logger.error(str(e))


def delete_previews(apps, schema_editor):
    logger = get_logger(MIGRATION_NAME, MIGRATION_LOG)
    query_set = _get_query_set(apps)
    logger.info(f"Reverse migration has been started. Need to delete {query_set.count()} previews.")
    for db_job in query_set:
        try:
            jid = db_job.id
            job_preview = _get_job_preview_path(jid)
            if os.path.isfile(job_preview):
                os.remove(job_preview)
        except Exception as e:
            logger.error(f"Cannot delete preview for job {db_job.id}")
            logger.error(str(e))


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0055_jobs_directories"),
    ]

    operations = [migrations.RunPython(code=create_previews, reverse_code=delete_previews)]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0057_auto_20220726_0926.py =====
# Generated by Django 3.2.14 on 2022-07-26 09:26

import django.db.models.deletion
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0056_jobs_previews"),
    ]

    operations = [
        migrations.CreateModel(
            name="LabeledSkeleton",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("frame", models.PositiveIntegerField()),
                (
                    "type",
                    models.CharField(
                        choices=[
                            ("rectangle", "RECTANGLE"),
                            ("polygon", "POLYGON"),
                            ("polyline", "POLYLINE"),
                            ("points", "POINTS"),
                            ("ellipse", "ELLIPSE"),
                            ("cuboid", "CUBOID"),
                            ("skeleton", "SKELETON"),
                        ],
                        max_length=16,
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("outside", models.BooleanField(default=False)),
                ("points", cvat.apps.engine.models.FloatArrayField(default="")),
            ],
        ),
        migrations.CreateModel(
            name="TrackedSkeleton",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                (
                    "type",
                    models.CharField(
                        choices=[
                            ("rectangle", "RECTANGLE"),
                            ("polygon", "POLYGON"),
                            ("polyline", "POLYLINE"),
                            ("points", "POINTS"),
                            ("ellipse", "ELLIPSE"),
                            ("cuboid", "CUBOID"),
                            ("skeleton", "SKELETON"),
                        ],
                        max_length=16,
                    ),
                ),
                ("occluded", models.BooleanField(default=False)),
                ("outside", models.BooleanField(default=False)),
                ("points", cvat.apps.engine.models.FloatArrayField(default="")),
            ],
        ),
        migrations.AddField(
            model_name="label",
            name="parent",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="sublabels",
                to="engine.label",
            ),
        ),
        migrations.AddField(
            model_name="label",
            name="type",
            field=models.CharField(
                choices=[
                    ("bbox", "BBOX"),
                    ("ellipse", "ELLIPSE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("cuboid", "CUBOID"),
                    ("cuboid_3d", "CUBOID_3D"),
                    ("skeleton", "SKELETON"),
                    ("tag", "TAG"),
                    ("any", "ANY"),
                ],
                default=cvat.apps.engine.models.LabelType["ANY"],
                max_length=32,
                null=True,
            ),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="points",
            field=cvat.apps.engine.models.FloatArrayField(default=[]),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                    ("skeleton", "SKELETON"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterField(
            model_name="trackedshape",
            name="points",
            field=cvat.apps.engine.models.FloatArrayField(default=[]),
        ),
        migrations.AlterField(
            model_name="trackedshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                    ("skeleton", "SKELETON"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterUniqueTogether(
            name="label",
            unique_together={("task", "name", "parent")},
        ),
        migrations.CreateModel(
            name="TrackedSkeletonAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=4096)),
                (
                    "skeleton",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.trackedskeleton"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.attributespec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="trackedskeleton",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.label"),
        ),
        migrations.AddField(
            model_name="trackedskeleton",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.trackedshape"
            ),
        ),
        migrations.CreateModel(
            name="LabeledSkeletonAttributeVal",
            fields=[
                ("id", models.BigAutoField(primary_key=True, serialize=False)),
                ("value", cvat.apps.engine.models.SafeCharField(max_length=4096)),
                (
                    "skeleton",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.labeledskeleton"
                    ),
                ),
                (
                    "spec",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.attributespec"
                    ),
                ),
            ],
            options={
                "abstract": False,
                "default_permissions": (),
            },
        ),
        migrations.AddField(
            model_name="labeledskeleton",
            name="label",
            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to="engine.label"),
        ),
        migrations.AddField(
            model_name="labeledskeleton",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE, to="engine.labeledshape"
            ),
        ),
        migrations.CreateModel(
            name="Skeleton",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("svg", models.TextField(default=None, null=True)),
                (
                    "root",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE, to="engine.label"
                    ),
                ),
            ],
            options={
                "default_permissions": (),
                "unique_together": {("root",)},
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0058_auto_20220809_1236.py =====
# Generated by Django 3.2.15 on 2022-08-09 12:36

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0057_auto_20220726_0926"),
    ]

    operations = [
        migrations.RemoveField(
            model_name="labeledskeletonattributeval",
            name="skeleton",
        ),
        migrations.RemoveField(
            model_name="labeledskeletonattributeval",
            name="spec",
        ),
        migrations.RemoveField(
            model_name="trackedskeleton",
            name="label",
        ),
        migrations.RemoveField(
            model_name="trackedskeleton",
            name="shape",
        ),
        migrations.RemoveField(
            model_name="trackedskeletonattributeval",
            name="skeleton",
        ),
        migrations.RemoveField(
            model_name="trackedskeletonattributeval",
            name="spec",
        ),
        migrations.AddField(
            model_name="labeledshape",
            name="parent",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="elements",
                to="engine.labeledshape",
            ),
        ),
        migrations.AddField(
            model_name="labeledtrack",
            name="parent",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="elements",
                to="engine.labeledtrack",
            ),
        ),
        migrations.DeleteModel(
            name="LabeledSkeleton",
        ),
        migrations.DeleteModel(
            name="LabeledSkeletonAttributeVal",
        ),
        migrations.DeleteModel(
            name="TrackedSkeleton",
        ),
        migrations.DeleteModel(
            name="TrackedSkeletonAttributeVal",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0059_labeledshape_outside.py =====
# Generated by Django 3.2.15 on 2022-08-10 08:45

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0058_auto_20220809_1236"),
    ]

    operations = [
        migrations.AddField(
            model_name="labeledshape",
            name="outside",
            field=models.BooleanField(default=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0060_alter_label_parent.py =====
# Generated by Django 3.2.15 on 2022-09-09 09:00

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0059_labeledshape_outside"),
    ]

    operations = [
        migrations.AlterField(
            model_name="label",
            name="parent",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="sublabels",
                to="engine.label",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0061_auto_20221130_0844.py =====
# Generated by Django 3.2.16 on 2022-11-30 08:44

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0060_alter_label_parent"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                    ("mask", "MASK"),
                    ("skeleton", "SKELETON"),
                ],
                max_length=16,
            ),
        ),
        migrations.AlterField(
            model_name="trackedshape",
            name="type",
            field=models.CharField(
                choices=[
                    ("rectangle", "RECTANGLE"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("points", "POINTS"),
                    ("ellipse", "ELLIPSE"),
                    ("cuboid", "CUBOID"),
                    ("mask", "MASK"),
                    ("skeleton", "SKELETON"),
                ],
                max_length=16,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0062_delete_previews.py =====
import os
import sys
import traceback

from django.conf import settings
from django.db import migrations

from cvat.apps.engine.log import get_migration_logger


def delete_previews(apps, schema_editor):
    migration_name = os.path.splitext(os.path.basename(__file__))[0]
    with get_migration_logger(migration_name) as log:

        def delete_object_previews(db_objects, root_path):
            for db_obj in db_objects:
                preview_path = os.path.join(root_path, str(db_obj.id), "preview.jpeg")
                try:
                    os.remove(preview_path)
                except Exception as e:
                    log.error(f"Cannot delete path {preview_path}")
                    log.error(str(e))
                    traceback.print_exc(file=sys.stderr)

        log.info("\nDeleting Data previews...")
        Data = apps.get_model("engine", "Data")
        delete_object_previews(Data.objects.all(), settings.MEDIA_DATA_ROOT)

        log.info("\nDeleting Job previews...")
        Job = apps.get_model("engine", "Job")
        delete_object_previews(Job.objects.all(), settings.JOBS_ROOT)

        log.info("\nDeleting CloudStorage previews...")
        CloudStorage = apps.get_model("engine", "CloudStorage")
        delete_object_previews(CloudStorage.objects.all(), settings.CLOUD_STORAGE_ROOT)


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0061_auto_20221130_0844"),
    ]

    operations = [
        migrations.RunPython(code=delete_previews),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0063_delete_jobcommit.py =====
# Generated by Django 3.2.16 on 2023-02-02 18:50

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0062_delete_previews"),
    ]

    operations = [
        migrations.DeleteModel(
            name="JobCommit",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0064_delete_or_rename_wrong_labels.py =====
import os

from django.db import migrations

from cvat.apps.engine.log import get_migration_logger


def delete_or_rename_wrong_labels(apps, schema_editor):
    migration_name = os.path.splitext(os.path.basename(__file__))[0]
    with get_migration_logger(migration_name) as log:
        log.info("\nDeleting skeleton Labels without skeletons...")

        Label = apps.get_model("engine", "Label")
        for label in Label.objects.all():
            if label.type == "skeleton" and not hasattr(label, "skeleton"):
                label.delete()

        log.info("\nDeleting duplicate skeleton sublabels and renaming duplicate Labels...")
        for name, parent, project, task in Label.objects.values_list(
            "name", "parent", "project", "task"
        ).distinct():
            duplicate_labels = Label.objects.filter(name=name, parent=parent, project=project)
            if task is not None:
                duplicate_labels = Label.objects.filter(name=name, parent=parent, task=task)

            if len(duplicate_labels) > 1:
                label = duplicate_labels[0]
                if label.parent is not None:
                    label.delete()
                else:
                    for i, label in enumerate(duplicate_labels[1:]):
                        label.name = f"{label.name}_duplicate_{i + 1}"
                        label.save()


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0063_delete_jobcommit"),
    ]

    operations = [
        migrations.RunPython(code=delete_or_rename_wrong_labels),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0065_auto_20230221_0931.py =====
# Generated by Django 3.2.18 on 2023-02-21 09:31

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0064_delete_or_rename_wrong_labels"),
    ]

    operations = [
        migrations.AlterUniqueTogether(
            name="label",
            unique_together=set(),
        ),
        migrations.AddConstraint(
            model_name="label",
            constraint=models.UniqueConstraint(
                condition=models.Q(("parent__isnull", True), ("task__isnull", True)),
                fields=("project", "name"),
                name="project_name_unique",
            ),
        ),
        migrations.AddConstraint(
            model_name="label",
            constraint=models.UniqueConstraint(
                condition=models.Q(("parent__isnull", True), ("project__isnull", True)),
                fields=("task", "name"),
                name="task_name_unique",
            ),
        ),
        migrations.AddConstraint(
            model_name="label",
            constraint=models.UniqueConstraint(
                condition=models.Q(("task__isnull", True)),
                fields=("project", "name", "parent"),
                name="project_name_parent_unique",
            ),
        ),
        migrations.AddConstraint(
            model_name="label",
            constraint=models.UniqueConstraint(
                condition=models.Q(("project__isnull", True)),
                fields=("task", "name", "parent"),
                name="task_name_parent_unique",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0066_auto_20230319_1252.py =====
# Generated by Django 3.2.18 on 2023-03-19 12:52

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0065_auto_20230221_0931"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="credentials",
            field=models.CharField(blank=True, max_length=500, null=True),
        ),
        migrations.AlterUniqueTogether(
            name="cloudstorage",
            unique_together=set(),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0067_alter_cloudstorage_credentials_type.py =====
# Generated by Django 3.2.18 on 2023-03-13 21:34

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0066_auto_20230319_1252"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="credentials_type",
            field=models.CharField(
                choices=[
                    ("KEY_SECRET_KEY_PAIR", "KEY_SECRET_KEY_PAIR"),
                    ("ACCOUNT_NAME_TOKEN_PAIR", "ACCOUNT_NAME_TOKEN_PAIR"),
                    ("KEY_FILE_PATH", "KEY_FILE_PATH"),
                    ("ANONYMOUS_ACCESS", "ANONYMOUS_ACCESS"),
                    ("CONNECTION_STRING", "CONNECTION_STRING"),
                ],
                max_length=29,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0068_auto_20230418_0901.py =====
# Generated by Django 3.2.18 on 2023-04-18 09:01

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0067_alter_cloudstorage_credentials_type"),
    ]

    operations = [
        migrations.AlterModelOptions(
            name="clientfile",
            options={"default_permissions": (), "ordering": ("id",)},
        ),
        migrations.AlterModelOptions(
            name="relatedfile",
            options={"default_permissions": (), "ordering": ("id",)},
        ),
        migrations.AlterModelOptions(
            name="remotefile",
            options={"default_permissions": (), "ordering": ("id",)},
        ),
        migrations.AlterModelOptions(
            name="serverfile",
            options={"default_permissions": (), "ordering": ("id",)},
        ),
        migrations.AlterUniqueTogether(
            name="remotefile",
            unique_together={("data", "file")},
        ),
        migrations.AlterUniqueTogether(
            name="serverfile",
            unique_together={("data", "file")},
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0069_auto_20230608_1915.py =====
# Generated by Django 3.2.18 on 2023-06-08 19:15

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0068_auto_20230418_0901"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledimage",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("semi-auto", "SEMI_AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("semi-auto", "SEMI_AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrack",
            name="source",
            field=models.CharField(
                choices=[("auto", "AUTO"), ("semi-auto", "SEMI_AUTO"), ("manual", "MANUAL")],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0070_add_job_type_created_date.py =====
import django.utils.timezone
from django.db import migrations, models

import cvat.apps.engine.models


def add_created_date_to_existing_jobs(apps, schema_editor):
    Job = apps.get_model("engine", "Job")

    jobs = Job.objects.prefetch_related("segment__task").all()
    for job in jobs:
        task = job.segment.task
        job.created_date = task.created_date

    Job.objects.bulk_update(jobs, fields=["created_date"], batch_size=500)


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0069_auto_20230608_1915"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="type",
            field=models.CharField(
                choices=[("annotation", "ANNOTATION"), ("ground_truth", "GROUND_TRUTH")],
                default="annotation",
                max_length=32,
            ),
        ),
        migrations.AddField(
            model_name="segment",
            name="frames",
            field=cvat.apps.engine.models.IntArrayField(blank=True, default=""),
        ),
        migrations.AddField(
            model_name="segment",
            name="type",
            field=models.CharField(
                choices=[("range", "RANGE"), ("specific_frames", "SPECIFIC_FRAMES")],
                default="range",
                max_length=32,
            ),
        ),
        migrations.AddField(
            model_name="job",
            name="created_date",
            field=models.DateTimeField(
                auto_now_add=True, default=django.utils.timezone.now, null=True
            ),
            preserve_default=False,
        ),
        migrations.RunPython(
            code=add_created_date_to_existing_jobs,
        ),
        migrations.AlterField(
            model_name="job",
            name="created_date",
            field=models.DateTimeField(auto_now_add=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0071_annotationguide_asset.py =====
# Generated by Django 3.2.18 on 2023-06-13 13:14

import uuid

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("engine", "0070_add_job_type_created_date"),
    ]

    operations = [
        migrations.CreateModel(
            name="AnnotationGuide",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("markdown", models.TextField(blank=True, default="")),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                (
                    "project",
                    models.OneToOneField(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="annotation_guide",
                        to="engine.project",
                    ),
                ),
                (
                    "task",
                    models.OneToOneField(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="annotation_guide",
                        to="engine.task",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="Asset",
            fields=[
                (
                    "uuid",
                    models.UUIDField(
                        default=uuid.uuid4, editable=False, primary_key=True, serialize=False
                    ),
                ),
                ("filename", models.CharField(max_length=1024)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                (
                    "guide",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="assets",
                        to="engine.annotationguide",
                    ),
                ),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="assets",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0072_alter_issue_updated_date.py =====
# Generated by Django 4.2.1 on 2023-06-23 19:26

from django.db import migrations, models


def forwards_func(apps, schema_editor):
    Issue = apps.get_model("engine", "Issue")

    issues = Issue.objects.all()
    for issue in issues:
        issue.updated_date = issue.created_date

    Issue.objects.bulk_update(issues, fields=["updated_date"], batch_size=500)


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0071_annotationguide_asset"),
    ]

    operations = [
        migrations.RunPython(
            code=forwards_func,
        ),
        migrations.AlterField(
            model_name="issue",
            name="updated_date",
            field=models.DateTimeField(auto_now=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0073_alter_attributespec_default_value_and_more.py =====
# Generated by Django 4.2.1 on 2023-07-10 15:57

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0072_alter_issue_updated_date"),
    ]

    operations = [
        migrations.AlterField(
            model_name="attributespec",
            name="default_value",
            field=models.CharField(blank=True, max_length=128),
        ),
        migrations.AlterField(
            model_name="attributespec",
            name="values",
            field=models.CharField(blank=True, max_length=4096),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0074_alter_labeledimage_source_alter_labeledshape_source_and_more.py =====
# Generated by Django 4.2.1 on 2023-07-18 07:09

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0073_alter_attributespec_default_value_and_more"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledimage",
            name="source",
            field=models.CharField(
                choices=[
                    ("auto", "AUTO"),
                    ("semi-auto", "SEMI_AUTO"),
                    ("manual", "MANUAL"),
                    ("file", "FILE"),
                ],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="source",
            field=models.CharField(
                choices=[
                    ("auto", "AUTO"),
                    ("semi-auto", "SEMI_AUTO"),
                    ("manual", "MANUAL"),
                    ("file", "FILE"),
                ],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrack",
            name="source",
            field=models.CharField(
                choices=[
                    ("auto", "AUTO"),
                    ("semi-auto", "SEMI_AUTO"),
                    ("manual", "MANUAL"),
                    ("file", "FILE"),
                ],
                default="manual",
                max_length=16,
                null=True,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0075_annotationguide_is_public.py =====
# Generated by Django 4.2.1 on 2023-07-26 11:22

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0074_alter_labeledimage_source_alter_labeledshape_source_and_more"),
    ]

    operations = [
        migrations.AddField(
            model_name="annotationguide",
            name="is_public",
            field=models.BooleanField(default=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0076_remove_storages_that_refer_to_deleted_cloud_storages.py =====
# Generated by Django 4.2.6 on 2023-11-17 10:10

from django.db import migrations, models

from cvat.apps.engine.models import Location


def manually_remove_outdated_relations(apps, schema_editor):
    Storage = apps.get_model("engine", "Storage")
    CloudStorage = apps.get_model("engine", "CloudStorage")
    Storage.objects.filter(location=Location.LOCAL, cloud_storage_id__isnull=False).update(
        cloud_storage_id=None
    )
    Storage.objects.filter(
        ~models.Exists(CloudStorage.objects.filter(pk=models.OuterRef("cloud_storage_id"))),
        location=Location.CLOUD_STORAGE,
    ).delete()


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0075_annotationguide_is_public"),
    ]

    operations = [
        migrations.RunPython(manually_remove_outdated_relations),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0077_auto_20231121_1952.py =====
# Generated by Django 4.2.6 on 2023-11-21 19:52

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0076_remove_storages_that_refer_to_deleted_cloud_storages"),
    ]

    operations = [
        migrations.AlterField(
            model_name="storage",
            name="cloud_storage_id",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="+",
                to="engine.cloudstorage",
            ),
        ),
        migrations.RenameField(
            model_name="storage",
            old_name="cloud_storage_id",
            new_name="cloud_storage",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0078_alter_cloudstorage_credentials.py =====
# Generated by Django 4.2.6 on 2024-01-09 09:55

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("engine", "0077_auto_20231121_1952"),
    ]

    operations = [
        migrations.AlterField(
            model_name="cloudstorage",
            name="credentials",
            field=models.CharField(blank=True, max_length=1024, null=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0079_alter_labeledimageattributeval_image_and_more.py =====
# Generated by Django 4.2.13 on 2024-07-09 11:08

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0078_alter_cloudstorage_credentials"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledimageattributeval",
            name="image",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledimage",
            ),
        ),
        migrations.AlterField(
            model_name="labeledshapeattributeval",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledshape",
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrackattributeval",
            name="track",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledtrack",
            ),
        ),
        migrations.AlterField(
            model_name="trackedshapeattributeval",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.trackedshape",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0080_alter_trackedshape_track.py =====
# Generated by Django 4.2.13 on 2024-07-12 19:01

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0079_alter_labeledimageattributeval_image_and_more"),
    ]

    operations = [
        migrations.AlterField(
            model_name="trackedshape",
            name="track",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="shapes",
                related_query_name="shape",
                to="engine.labeledtrack",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0081_job_assignee_updated_date_and_more.py =====
# Generated by Django 4.2.13 on 2024-07-12 19:06

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0080_alter_trackedshape_track"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="assignee_updated_date",
            field=models.DateTimeField(blank=True, default=None, null=True),
        ),
        migrations.AddField(
            model_name="project",
            name="assignee_updated_date",
            field=models.DateTimeField(blank=True, default=None, null=True),
        ),
        migrations.AddField(
            model_name="task",
            name="assignee_updated_date",
            field=models.DateTimeField(blank=True, default=None, null=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0082_alter_labeledimage_job_and_more.py =====
# Generated by Django 4.2.14 on 2024-07-22 07:27

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0081_job_assignee_updated_date_and_more"),
    ]

    operations = [
        migrations.AlterField(
            model_name="labeledimage",
            name="job",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING, to="engine.job"
            ),
        ),
        migrations.AlterField(
            model_name="labeledimageattributeval",
            name="image",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledimage",
            ),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="job",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING, to="engine.job"
            ),
        ),
        migrations.AlterField(
            model_name="labeledshape",
            name="parent",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="elements",
                to="engine.labeledshape",
            ),
        ),
        migrations.AlterField(
            model_name="labeledshapeattributeval",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledshape",
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrack",
            name="job",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING, to="engine.job"
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrack",
            name="parent",
            field=models.ForeignKey(
                null=True,
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="elements",
                to="engine.labeledtrack",
            ),
        ),
        migrations.AlterField(
            model_name="labeledtrackattributeval",
            name="track",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.labeledtrack",
            ),
        ),
        migrations.AlterField(
            model_name="trackedshapeattributeval",
            name="shape",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.DO_NOTHING,
                related_name="attributes",
                related_query_name="attribute",
                to="engine.trackedshape",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0083_move_to_segment_chunks.py =====
# Generated by Django 4.2.13 on 2024-08-12 09:49

import os
from collections.abc import Iterable
from itertools import islice
from typing import TypeVar

from django.db import migrations

from cvat.apps.engine.log import get_migration_log_dir, get_migration_logger

T = TypeVar("T")


def take_by(iterable: Iterable[T], count: int) -> Iterable[T]:
    """
    Returns elements from the input iterable by batches of N items.
    ('abcdefg', 3) -> ['a', 'b', 'c'], ['d', 'e', 'f'], ['g']
    """

    it = iter(iterable)
    while True:
        batch = list(islice(it, count))
        if len(batch) == 0:
            break

        yield batch


def get_migration_name() -> str:
    return os.path.splitext(os.path.basename(__file__))[0]


def get_updated_ids_filename(log_dir: str, migration_name: str) -> str:
    return os.path.join(log_dir, migration_name + "-data_ids.log")


MIGRATION_LOG_HEADER = (
    'The following Data ids have been switched from using "filesystem" chunk storage ' 'to "cache":'
)


def switch_tasks_with_static_chunks_to_dynamic_chunks(apps, schema_editor):
    migration_name = get_migration_name()
    migration_log_dir = get_migration_log_dir()
    with get_migration_logger(migration_name) as common_logger:
        Data = apps.get_model("engine", "Data")

        data_with_static_cache_query = Data.objects.filter(storage_method="file_system")

        data_with_static_cache_ids = list(
            v[0]
            for v in (
                data_with_static_cache_query.order_by("id")
                .values_list("id")
                .iterator(chunk_size=100000)
            )
        )

        data_with_static_cache_query.update(storage_method="cache")

        updated_ids_filename = get_updated_ids_filename(migration_log_dir, migration_name)
        with open(updated_ids_filename, "w") as data_ids_file:
            print(MIGRATION_LOG_HEADER, file=data_ids_file)

            for data_id in data_with_static_cache_ids:
                print(data_id, file=data_ids_file)

        common_logger.info(
            "Information about migrated tasks is available in the migration log file: "
            "{}. You will need to remove data manually for these tasks.".format(
                updated_ids_filename
            )
        )


def revert_switch_tasks_with_static_chunks_to_dynamic_chunks(apps, schema_editor):
    migration_name = get_migration_name()
    migration_log_dir = get_migration_log_dir()

    updated_ids_filename = get_updated_ids_filename(migration_log_dir, migration_name)
    if not os.path.isfile(updated_ids_filename):
        raise FileNotFoundError(
            "Can't revert the migration: can't file forward migration logfile at "
            f"'{updated_ids_filename}'."
        )

    with open(updated_ids_filename, "r") as data_ids_file:
        header = data_ids_file.readline().strip()
        if header != MIGRATION_LOG_HEADER:
            raise ValueError(
                "Can't revert the migration: the migration log file has unexpected header"
            )

        forward_updated_ids = tuple(map(int, data_ids_file))

    if not forward_updated_ids:
        return

    Data = apps.get_model("engine", "Data")

    for id_batch in take_by(forward_updated_ids, 1000):
        Data.objects.filter(storage_method="cache", id__in=id_batch).update(
            storage_method="file_system"
        )


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0082_alter_labeledimage_job_and_more"),
    ]

    operations = [
        migrations.RunPython(
            switch_tasks_with_static_chunks_to_dynamic_chunks,
            reverse_code=revert_switch_tasks_with_static_chunks_to_dynamic_chunks,
        )
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0084_honeypot_support.py =====
# Generated by Django 4.2.15 on 2024-09-23 13:11

from collections import defaultdict
from collections.abc import Collection

import django.db.models.deletion
from django.db import migrations, models
from django.db.models import Count, Q

import cvat.apps.engine.models


def get_frame_step(db_data) -> int:
    v = db_data.frame_filter or "step=1"
    return int(v.split("=")[-1])


def get_rel_frame(abs_frame: int, db_data) -> int:
    data_start_frame = db_data.start_frame
    step = get_frame_step(db_data)
    return (abs_frame - data_start_frame) // step


def get_segment_rel_frame_set(db_segment) -> Collection[int]:
    db_data = db_segment.task.data
    data_start_frame = db_data.start_frame
    data_stop_frame = db_data.stop_frame
    step = get_frame_step(db_data)
    frame_range = range(
        data_start_frame + db_segment.start_frame * step,
        min(data_start_frame + db_segment.stop_frame * step, data_stop_frame) + step,
        step,
    )

    if db_segment.type == "range":
        frame_set = frame_range
    elif db_segment.type == "specific_frames":
        frame_set = set(frame_range).intersection(db_segment.frames or [])
    else:
        raise ValueError(f"Unknown segment type: {db_segment.type}")

    return sorted(get_rel_frame(abs_frame, db_data) for abs_frame in frame_set)


def delete_duplicate_ground_truth_jobs(apps, schema_editor):
    Task = apps.get_model("engine", "Task")
    Job = apps.get_model("engine", "Job")

    broken_tasks = Task.objects.annotate(
        ground_truth_jobs_count=Count("segment__job", filter=Q(segment__job__type="ground_truth"))
    ).filter(ground_truth_jobs_count__gt=1)

    gt_jobs = (
        Job.objects.filter(segment__task__in=broken_tasks)
        .filter(type="ground_truth")
        .order_by("-updated_date")
        .iterator(1000)
    )

    groups = defaultdict(list)
    for gt_job in gt_jobs:
        assert gt_job.type == "ground_truth"
        groups[gt_job.segment.task.id].append(gt_job)

    for gt_jobs in groups.values():
        for gt_job in gt_jobs[1:]:
            gt_job.delete()


def init_validation_layout_in_tasks_with_gt_job(apps, schema_editor):
    Job = apps.get_model("engine", "Job")
    ValidationLayout = apps.get_model("engine", "ValidationLayout")

    gt_jobs = (
        Job.objects.filter(type="ground_truth")
        .select_related("segment", "segment__task", "segment__task__data")
        .iterator(chunk_size=100)
    )

    validation_layouts = []
    for gt_job in gt_jobs:
        validation_layout = ValidationLayout(
            task_data=gt_job.segment.task.data,
            mode="gt",
            frames=get_segment_rel_frame_set(gt_job.segment),
        )
        validation_layouts.append(validation_layout)

    ValidationLayout.objects.bulk_create(validation_layouts, batch_size=100)


def init_m2m_for_related_files(apps, schema_editor):
    RelatedFile = apps.get_model("engine", "RelatedFile")

    ThroughModel = RelatedFile.images.through
    ThroughModel.objects.bulk_create(
        (
            ThroughModel(relatedfile_id=related_file_id, image_id=image_id)
            for related_file_id, image_id in (
                RelatedFile.objects.filter(primary_image__isnull=False)
                .values_list("id", "primary_image_id")
                .iterator(chunk_size=1000)
            )
        ),
        batch_size=1000,
    )


def revert_m2m_for_related_files(apps, schema_editor):
    RelatedFile = apps.get_model("engine", "RelatedFile")

    if top_related_file_uses := (
        RelatedFile.objects.annotate(
            images_count=models.aggregates.Count(
                "images", filter=models.Q(images__is_placeholder=False)
            )
        )
        .order_by("-images_count")
        .filter(images_count__gt=1)
        .values_list("id", "images_count")[:10]
    ):
        raise Exception(
            "Can't run backward migration: "
            "there are RelatedFile objects with more than 1 related Image. "
            "Top RelatedFile uses: {}".format(
                ", ".join(f"\n\tid = {id}: {count}" for id, count in top_related_file_uses)
            )
        )

    ThroughModel = RelatedFile.images.through

    (
        RelatedFile.objects.annotate(
            images_count=models.aggregates.Count(
                "images", filter=models.Q(images__is_placeholder=False)
            )
        )
        .filter(images_count__gt=0)
        .update(
            primary_image_id=models.Subquery(
                ThroughModel.objects.filter(relatedfile_id=models.OuterRef("id")).values_list(
                    "image_id", flat=True
                )[:1]
            )
        )
    )


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0083_move_to_segment_chunks"),
    ]

    operations = [
        migrations.AddField(
            model_name="image",
            name="is_placeholder",
            field=models.BooleanField(default=False),
        ),
        migrations.AddField(
            model_name="image",
            name="real_frame",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.CreateModel(
            name="ValidationParams",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "mode",
                    models.CharField(choices=[("gt", "GT"), ("gt_pool", "GT_POOL")], max_length=32),
                ),
                (
                    "frame_selection_method",
                    models.CharField(
                        choices=[
                            ("random_uniform", "RANDOM_UNIFORM"),
                            ("random_per_job", "RANDOM_PER_JOB"),
                            ("manual", "MANUAL"),
                        ],
                        max_length=32,
                    ),
                ),
                ("random_seed", models.IntegerField(null=True)),
                ("frame_count", models.IntegerField(null=True)),
                ("frame_share", models.FloatField(null=True)),
                ("frames_per_job_count", models.IntegerField(null=True)),
                ("frames_per_job_share", models.FloatField(null=True)),
                (
                    "task_data",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="validation_params",
                        to="engine.data",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="ValidationLayout",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                (
                    "mode",
                    models.CharField(choices=[("gt", "GT"), ("gt_pool", "GT_POOL")], max_length=32),
                ),
                ("frames_per_job_count", models.IntegerField(null=True)),
                ("frames", cvat.apps.engine.models.IntArrayField(default="")),
                ("disabled_frames", cvat.apps.engine.models.IntArrayField(default="")),
                (
                    "task_data",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="validation_layout",
                        to="engine.data",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="ValidationFrame",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("path", models.CharField(default="", max_length=1024)),
                (
                    "validation_params",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="frames",
                        to="engine.validationparams",
                    ),
                ),
            ],
        ),
        migrations.RunPython(
            delete_duplicate_ground_truth_jobs,
            reverse_code=migrations.RunPython.noop,
        ),
        migrations.RunPython(
            init_validation_layout_in_tasks_with_gt_job,
            reverse_code=migrations.RunPython.noop,
        ),
        migrations.AddField(
            model_name="relatedfile",
            name="images",
            field=models.ManyToManyField(to="engine.image"),
        ),
        migrations.RunPython(
            init_m2m_for_related_files,
            reverse_code=revert_m2m_for_related_files,
        ),
        migrations.RemoveField(
            model_name="relatedfile",
            name="primary_image",
            field=models.ForeignKey(
                null=True,
                on_delete=models.deletion.CASCADE,
                related_name="related_files",
                to="engine.image",
            ),
        ),
        migrations.AlterField(
            model_name="relatedfile",
            name="images",
            field=models.ManyToManyField(to="engine.image", related_name="related_files"),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0085_segment_chunks_updated_date.py =====
# Generated by Django 4.2.15 on 2024-09-25 13:52

from datetime import datetime

from django.db import migrations, models


def _get_0083_move_to_segment_chunks_migration_date(apps, schema_editor) -> datetime:
    with schema_editor.connection.cursor() as cursor:
        cursor.execute(
            """\
            SELECT applied
            FROM django_migrations
            WHERE app = %s AND name = %s
            """,
            ["engine", "0083_move_to_segment_chunks"],
        )
        return cursor.fetchone()[0]


def init_chunks_updated_date(apps, schema_editor):
    # The 0083 migration changed data distribution by chunks
    migration_0083_date = _get_0083_move_to_segment_chunks_migration_date(apps, schema_editor)

    Segment = apps.get_model("engine", "Segment")
    task_created_date_subquery = models.Subquery(
        Segment.objects.select_related("task")
        .filter(pk=models.OuterRef("pk"))
        .values("task__created_date")[:1]
    )

    Segment.objects.update(
        chunks_updated_date=models.functions.Greatest(
            task_created_date_subquery,
            migration_0083_date,
        )
    )


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0084_honeypot_support"),
    ]

    operations = [
        migrations.AddField(
            model_name="segment",
            name="chunks_updated_date",
            field=models.DateTimeField(default=None, null=True),
            preserve_default=False,
        ),
        migrations.RunPython(
            init_chunks_updated_date,
            reverse_code=migrations.RunPython.noop,
        ),
        migrations.AlterField(
            model_name="segment",
            name="chunks_updated_date",
            field=models.DateTimeField(null=False, auto_now_add=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0086_profile_has_analytics_access.py =====
# Generated by Django 4.2.16 on 2024-10-22 08:41

from django.conf import settings
from django.db import migrations, models


def set_has_analytics_access(apps, schema_editor):
    User = apps.get_model("auth", "User")
    for user in User.objects.all():
        is_admin = user.groups.filter(name=settings.IAM_ADMIN_ROLE).exists()
        user.profile.has_analytics_access = user.is_superuser or is_admin
        user.profile.save()


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0085_segment_chunks_updated_date"),
    ]

    operations = [
        migrations.AddField(
            model_name="profile",
            name="has_analytics_access",
            field=models.BooleanField(
                default=False,
                help_text="Designates whether the user can access analytics.",
                verbose_name="has access to analytics",
            ),
        ),
        migrations.RunPython(
            set_has_analytics_access,
            reverse_code=migrations.RunPython.noop,
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0087_alter_label_type.py =====
# Generated by Django 4.2.17 on 2025-01-22 13:48

from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0086_profile_has_analytics_access"),
    ]

    operations = [
        migrations.AlterField(
            model_name="label",
            name="type",
            field=models.CharField(
                choices=[
                    ("any", "ANY"),
                    ("cuboid", "CUBOID"),
                    ("ellipse", "ELLIPSE"),
                    ("mask", "MASK"),
                    ("points", "POINTS"),
                    ("polygon", "POLYGON"),
                    ("polyline", "POLYLINE"),
                    ("rectangle", "RECTANGLE"),
                    ("skeleton", "SKELETON"),
                    ("tag", "TAG"),
                ],
                default=cvat.apps.engine.models.LabelType["ANY"],
                max_length=32,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\0088_consensus_jobs.py =====
# Generated by Django 4.2.17 on 2025-01-13 15:51

import django.db.models.deletion
from django.db import migrations, models

import cvat.apps.engine.models


class Migration(migrations.Migration):

    dependencies = [
        ("engine", "0087_alter_label_type"),
    ]

    operations = [
        migrations.AddField(
            model_name="job",
            name="parent_job",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.CASCADE,
                related_name="child_jobs",
                related_query_name="child_job",
                to="engine.job",
            ),
        ),
        migrations.AddField(
            model_name="task",
            name="consensus_replicas",
            field=models.IntegerField(default=0),
        ),
        migrations.AlterField(
            model_name="job",
            name="type",
            field=models.CharField(
                choices=[
                    ("annotation", "ANNOTATION"),
                    ("ground_truth", "GROUND_TRUTH"),
                    ("consensus_replica", "CONSENSUS_REPLICA"),
                ],
                default=cvat.apps.engine.models.JobType["ANNOTATION"],
                max_length=32,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\migrations\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\redis_migrations\001_cleanup_scheduled_jobs.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import django_rq
from django.conf import settings
from rq_scheduler import Scheduler

from cvat.apps.redis_handler.redis_migrations import BaseMigration


class Migration(BaseMigration):
    def run(self):
        scheduler: Scheduler = django_rq.get_scheduler(settings.CVAT_QUEUES.EXPORT_DATA.value)

        for job in scheduler.get_jobs():
            if job.func_name == "cvat.apps.dataset_manager.views.clear_export_cache":
                scheduler.cancel(job)
                job.delete()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\redis_migrations\002_update_meta_in_export_related_jobs.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from logging import Logger
from pathlib import Path
from urllib.parse import parse_qs, urlparse

import django_rq
from django.conf import settings
from rq.job import Job, JobStatus
from rq_scheduler import Scheduler

from cvat.apps.engine.log import get_migration_logger
from cvat.apps.engine.utils import take_by
from cvat.apps.redis_handler.redis_migrations import BaseMigration


def process_job(
    job: Job,
    *,
    logger: Logger,
    scheduler: Scheduler | None = None,
) -> None:
    def update_meta(
        *,
        result_filename: str,
        result_url: str | None = None,
        save_meta: bool = True,
    ) -> None:
        job.meta.update(
            {
                "result_url": result_url,
                "result_filename": result_filename,
            }
        )

        if save_meta:
            job.save_meta()

    func_name = job.func_name.rsplit(".", maxsplit=1)[1]

    if func_name not in (
        "export_job_annotations",
        "export_job_as_dataset",
        "export_task_annotations",
        "export_task_as_dataset",
        "export_project_annotations",
        "export_project_as_dataset",
        "create_backup",
        "export_resource_to_cloud_storage",
    ):
        return

    # job was already processed
    if "result_filename" in job.meta.keys():
        return

    # export was initiated to cloud storage
    if "export_resource_to_cloud_storage" == func_name:
        filename = job.args[1] or job.args[2].format(".zip")
        update_meta(result_filename=filename, save_meta=False)
        # exclude filename and filename_template from args
        job.args = job.args[:1] + job.args[3:]
        job.save(include_meta=True)
        return

    # local downloading was selected
    try:
        result_url = job.meta["result_url"]
    except KeyError:
        logger.warning(
            f"Job({job.id}) is going to be removed since the result_url field is missing in the meta"
        )

        # scheduled jobs should be explicitly removed by calling scheduler.cancel() method
        # since they are not stored in ScheduledJobRegistry
        if scheduler:
            scheduler.cancel(job)

        if job.get_status() not in (
            JobStatus.FINISHED,
            JobStatus.FAILED,
            JobStatus.STOPPED,
            JobStatus.CANCELED,
        ):
            job.cancel()

        job.delete()
        return

    parsed_result_url = urlparse(result_url)
    target, pk, subresource, _ = job.id.removeprefix("export:").split("-", maxsplit=3)
    subpath = subresource if subresource != "annotations" else "dataset"

    actual_result_url = (
        parsed_result_url.scheme
        + "://"
        + parsed_result_url.netloc
        + f"/api/{target}s/{pk}/{subpath}/download?rq_id={job.id}"
    )

    # check whether a filename was provided by a user
    if filename := parse_qs(parsed_result_url.query).get("filename"):
        update_meta(result_filename=filename[0], result_url=actual_result_url)
        return

    # filename was not specified by a user
    update_meta(
        # we cannot provide the same filename structure
        # since there is no instance timestamp in Redis HASH
        result_filename="-".join([target, pk, subresource]) + ".zip",
        result_url=actual_result_url,
    )


class Migration(BaseMigration):
    def run(self):
        queue: django_rq.queues.DjangoRQ = django_rq.get_queue(
            settings.CVAT_QUEUES.EXPORT_DATA.value, connection=self.connection
        )
        scheduler: django_rq.queues.DjangoScheduler = django_rq.get_scheduler(
            settings.CVAT_QUEUES.EXPORT_DATA.value
        )

        with get_migration_logger(Path(__file__).stem) as logger:
            for registry in (
                queue,
                queue.started_job_registry,
                queue.deferred_job_registry,
                queue.finished_job_registry,
                queue.failed_job_registry,
            ):
                job_ids = set(registry.get_job_ids())
                for subset_with_ids in take_by(job_ids, 1000):
                    for idx, job in enumerate(
                        queue.job_class.fetch_many(subset_with_ids, connection=self.connection)
                    ):
                        if job:
                            process_job(job, logger=logger)
                        else:
                            registry.remove(subset_with_ids[idx])

            for job in scheduler.get_jobs():
                process_job(job, logger=logger, scheduler=scheduler)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\redis_migrations\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\annotationguides_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "annotationguides"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = [
    "target:owner",
    "target:assignee",
    "job:assignee",
    "none",
]
GROUPS = ["admin", "user", "worker"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]


def RESOURCES(scope):
    return [
        {
            "id": random.randrange(300, 400),
            "organization": {"id": random.randrange(1200, 1300)},
            "target": {
                "owner": {"id": random.randrange(700, 800)},
                "assignee": {"id": random.randrange(800, 900)},
                "is_job_staff": False,
            },
        }
    ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "job:assignee":
        data["resource"]["target"]["is_job_staff"] = True

    if ownership == "target:owner":
        data["resource"]["target"]["owner"]["id"] = user_id
        data["resource"]["target"]["is_job_staff"] = True

    if ownership == "target:assignee":
        data["resource"]["target"]["assignee"]["id"] = user_id
        data["resource"]["target"]["is_job_staff"] = True

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if context == "sandbox" and same_org:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\cloudstorages_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "cloudstorages"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["owner", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [False, True]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "owner": {"id": random.randrange(400, 500)},
                "organization": {"id": random.randrange(500, 600)},
                "user": {"num_resources": random.randrange(10)},
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" not in v:
                name += _get_name(prefix, **v)
        else:
            name += f'{prefix}_{str(v).upper().replace(":", "_")}'

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\comments_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "comments"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = [
    "project:owner",
    "project:assignee",
    "task:owner",
    "task:assignee",
    "job:assignee",
    "issue:owner",
    "issue:assignee",
    "owner",
    "none",
]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]
HAS_PROJ = [True, False]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "owner": {"id": random.randrange(600, 700)},
                "assignee": {"id": random.randrange(500, 600)},
                "project": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(700, 800)},
                    "assignee": {"id": random.randrange(800, 900)},
                },
                "task": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(900, 1000)},
                    "assignee": {"id": random.randrange(1000, 1100)},
                },
                "job": {
                    "id": random.randrange(300, 400),
                    "assignee": {"id": random.randrange(1100, 1200)},
                },
                "issue": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(1200, 1300)},
                    "assignee": {"id": random.randrange(1300, 1400)},
                },
                "organization": {"id": random.randrange(1400, 1500)},
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    if ownership == "project:owner":
        data["resource"]["project"]["owner"]["id"] = user_id

    if ownership == "project:assignee":
        data["resource"]["project"]["assignee"]["id"] = user_id

    if ownership == "task:owner":
        data["resource"]["task"]["owner"]["id"] = user_id

    if ownership == "task:assignee":
        data["resource"]["task"]["assignee"]["id"] = user_id

    if ownership == "job:assignee":
        data["resource"]["job"]["assignee"]["id"] = user_id

    if ownership == "issue:owner":
        data["resource"]["issue"]["owner"]["id"] = user_id

    if ownership == "issue:assignee":
        data["resource"]["issue"]["assignee"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org, has_proj):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org, has_proj):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False
    if not has_proj and ownership.startswith("project"):
        return False
    if scope == "create@issue" and ownership == "owner":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org, has_proj in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG, HAS_PROJ
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org, has_proj
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org, has_proj
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\issues_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "issues"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = [
    "project:owner",
    "project:assignee",
    "task:owner",
    "task:assignee",
    "job:assignee",
    "owner",
    "assignee",
    "none",
]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]
HAS_PROJ = [True, False]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "owner": {"id": random.randrange(600, 700)},
                "assignee": {"id": random.randrange(500, 600)},
                "project": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(700, 800)},
                    "assignee": {"id": random.randrange(800, 900)},
                },
                "task": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(900, 1000)},
                    "assignee": {"id": random.randrange(1000, 1100)},
                },
                "job": {
                    "id": random.randrange(300, 400),
                    "assignee": {"id": random.randrange(1100, 1200)},
                },
                "organization": {"id": random.randrange(1200, 1300)},
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    if ownership == "assignee":
        data["resource"]["assignee"]["id"] = user_id

    if ownership == "project:owner":
        data["resource"]["project"]["owner"]["id"] = user_id

    if ownership == "project:assignee":
        data["resource"]["project"]["assignee"]["id"] = user_id

    if ownership == "task:owner":
        data["resource"]["task"]["owner"]["id"] = user_id

    if ownership == "task:assignee":
        data["resource"]["task"]["assignee"]["id"] = user_id

    if ownership == "job:assignee":
        data["resource"]["job"]["assignee"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org, has_proj):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org, has_proj):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False
    if not has_proj and ownership.startswith("project"):
        return False
    if scope == "create@job" and ownership in ["owner", "assignee"]:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org, has_proj in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG, HAS_PROJ
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org, has_proj
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org, has_proj
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\jobs_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

random.seed(42)

NAME = "jobs"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource", "strict_privilege"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = [
    "project:owner",
    "project:assignee",
    "task:owner",
    "task:assignee",
    "assignee",
    "none",
]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "assignee": {"id": random.randrange(500, 600)},
                "organization": {"id": random.randrange(600, 700)},
                "project": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(700, 800)},
                    "assignee": {"id": random.randrange(800, 900)},
                },
                "task": {
                    "id": random.randrange(300, 400),
                    "owner": {"id": random.randrange(900, 1000)},
                    "assignee": {"id": random.randrange(1000, 1100)},
                },
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(
        filter(
            lambda r: (
                GROUPS.index(privilege) <= GROUPS.index(r["privilege"])
                if r["strict_privilege"] == "false"
                else privilege == r["privilege"]
            ),
            rules,
        )
    )
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "assignee":
        data["resource"]["assignee"]["id"] = user_id

    if ownership == "project:owner":
        data["resource"]["project"]["owner"]["id"] = user_id

    if ownership == "project:assignee":
        data["resource"]["project"]["assignee"]["id"] = user_id

    if ownership == "task:owner":
        data["resource"]["task"]["owner"]["id"] = user_id

    if ownership == "task:assignee":
        data["resource"]["task"]["assignee"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\projects_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "projects"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["owner", "assignee", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [False, True]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    elif scope in ["create", "import:backup"]:
        return [
            {
                "owner": {"id": random.randrange(400, 500)},
                "assignee": {"id": random.randrange(500, 600)},
                "organization": {"id": random.randrange(600, 700)},
                "user": {"num_resources": count},
            }
            for count in (0, 1, 3, 10)
        ]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "owner": {"id": random.randrange(400, 500)},
                "assignee": {"id": random.randrange(500, 600)},
                "organization": {"id": random.randrange(600, 700)},
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    if ownership == "assignee":
        data["resource"]["assignee"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += f'{prefix}_{str(v).upper().replace(":", "_")}'

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as csv_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in csv_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\server_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "server"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    rules = list(filter(lambda r: not r["limit"] or eval(r["limit"]), rules))

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += f'{prefix}_{str(v).upper().replace(":", "_")}'

    return name


def get_name(scope, context, ownership, privilege, membership):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "organization" and membership is None:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES
        ):
            if not is_valid(scope, context, ownership, privilege, membership):
                continue

            data = get_data(scope, context, ownership, privilege, membership)
            test_name = get_name(scope, context, ownership, privilege, membership)
            result = eval_rule(scope, context, ownership, privilege, membership, data)
            f.write(
                "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                    test_name=test_name,
                    allow="allow" if result else "not allow",
                    data=json.dumps(data),
                )
            )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\tasks_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

random.seed(42)

NAME = "tasks"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["project:owner", "project:assignee", "owner", "assignee", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    elif scope.startswith("create") or scope == "import:backup":
        return [
            {
                "owner": {"id": random.randrange(400, 500)},
                "assignee": {"id": random.randrange(500, 600)},
                "organization": {"id": random.randrange(600, 700)},
                "project": {
                    "owner": {"id": random.randrange(700, 800)},
                    "assignee": {"id": random.randrange(800, 900)},
                    "organization": {"id": random.randrange(900, 1000)},
                },
                "user": {"num_resources": count},
            }
            for count in (0, 3, 10)
        ]
    else:
        return [
            {
                "id": random.randrange(300, 400),
                "owner": {"id": random.randrange(400, 500)},
                "assignee": {"id": random.randrange(500, 600)},
                "organization": {"id": random.randrange(600, 700)},
                "project": {
                    "owner": {"id": random.randrange(700, 800)},
                    "assignee": {"id": random.randrange(800, 900)},
                    "organization": {"id": random.randrange(900, 1000)},
                },
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    if ownership == "assignee":
        data["resource"]["assignee"]["id"] = user_id

    if ownership == "project:owner":
        data["resource"]["project"]["owner"]["id"] = user_id

    if ownership == "project:assignee":
        data["resource"]["project"]["assignee"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False
    if scope.startswith("create") and ownership in ["owner", "assignee"]:
        return False
    if scope in ["create", "import:backup"] and ownership != "None":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\rules\tests\generators\users_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "users"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["self", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {"id": random.randrange(300, 400), "membership": {"role": role}} for role in ORG_ROLES
        ]


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

    if ownership == "self":
        data["resource"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += f'{prefix}_{str(v).upper().replace(":", "_")}'

    return name


def get_name(scope, context, ownership, privilege, membership, resource):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource):
    if context == "sandbox" and membership:
        return False
    if scope == "list":
        return not ownership == "self"
    if context == "sandbox" and resource["membership"]["role"] is not None:
        return False
    if context == "organization" and membership is None:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES
        ):
            for resource in RESOURCES(scope):
                if not is_valid(scope, context, ownership, privilege, membership, resource):
                    continue

                data = get_data(scope, context, ownership, privilege, membership, resource)
                test_name = get_name(scope, context, ownership, privilege, membership, resource)
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\tests\test_lazy_list.py =====
import copy
import pickle
import unittest
from typing import TypeVar

from cvat.apps.engine.lazy_list import LazyList

T = TypeVar('T')


class SliceGetter:
    def __class_getitem__(cls, slice_: slice) -> slice:
        if not isinstance(slice_, slice):
            raise TypeError("Use slice_getter only for slices")
        return slice_


class TestLazyList(unittest.TestCase):

    def setUp(self):
        self.lazy_list = LazyList(string="1,2,,3,4,5,6,7,8,9,10,11,12,,13,14,15", converter=int)
        self.empty_lazy_list = LazyList(string="")

    def test_len(self):
        self.assertEqual(len(self.lazy_list), 15)
        list(self.lazy_list)
        self.assertEqual(len(self.lazy_list), 15)

    def test_repr(self):
        self.assertEqual(repr(self.lazy_list), "LazyList([... + 1,2,,3,4,5,6,7,8,9,10,11,12,,13,14,15', (0.00% parsed))")
        next(iter(self.lazy_list))  # Trigger parsing of the first element
        self.assertIn("1... + 2,,3,4", repr(self.lazy_list))
        list(self.lazy_list)
        self.assertEqual(repr(self.lazy_list), "LazyList([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])")

    def test_deepcopy(self):
        copied_list = copy.deepcopy(self.lazy_list)
        self.assertEqual(copied_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
        self.assertNotEqual(id(copied_list), id(self.lazy_list))
        self.assertEqual(len(copied_list), 15)

    def test_getitem(self):
        self.assertEqual(self.lazy_list[1], 2)
        self.assertEqual(self.lazy_list[:3], [1, 2, 3])

    def test_iter(self):
        iterator = iter(self.lazy_list)
        self.assertEqual(next(iterator), 1)
        self.assertEqual(next(iterator), 2)
        self.assertEqual(list(self.lazy_list), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
        self.assertEqual(next(iterator), 3)
        self.assertEqual(list(self.lazy_list), [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_append(self):
        self.lazy_list.append(16)
        self.assertEqual(self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])

    def test_copy(self):
        copied_list = self.lazy_list.copy()
        self.assertEqual(copied_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_insert(self):
        self.lazy_list.insert(0, 0)
        self.assertEqual(self.lazy_list, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_pop(self):
        value = self.lazy_list.pop()
        self.assertEqual(value, 15)
        self.assertEqual(self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])

    def test_remove(self):
        self.lazy_list.remove(2)
        self.assertEqual(self.lazy_list, [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_reverse(self):
        self.lazy_list.reverse()
        self.assertEqual(self.lazy_list, [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1])

    def test_sort(self):
        unsorted_list = LazyList(string="3,1,2", converter=int)
        unsorted_list.sort()
        self.assertEqual(unsorted_list, [1, 2, 3])

    def test_clear(self):
        self.lazy_list.clear()
        self.assertEqual(len(self.lazy_list), 0)

    def test_index(self):
        self.assertEqual(self.lazy_list.index(3), 2)

    def test_count(self):
        self.assertEqual(self.lazy_list.count(2), 1)

    def test_setitem(self):
        self.lazy_list[0] = 99
        self.assertEqual(self.lazy_list[0], 99)

    def test_delitem(self):
        del self.lazy_list[0]
        self.assertEqual(self.lazy_list, [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_contains(self):
        self.assertIn(2, self.lazy_list)

    def test_reversed(self):
        self.assertEqual(list(reversed(self.lazy_list)), [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1])

    def test_mul(self):
        self.assertEqual(self.lazy_list * 2, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_rmul(self):
        self.assertEqual(2 * self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_imul(self):
        self.lazy_list *= 2
        self.assertEqual(self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_extend(self):
        self.lazy_list.extend([16, 17])
        self.assertEqual(self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])

    def test_add(self):
        new_list = self.lazy_list + [16, 17]
        self.assertEqual(new_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])

    def test_eq(self):
        self.assertTrue(self.lazy_list == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_iadd(self):
        self.lazy_list += [16, 17]
        self.assertEqual(self.lazy_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])

    def test_gt(self):
        self.assertTrue(self.lazy_list > [1, 2, 3])

    def test_ge(self):
        self.assertTrue(self.lazy_list >= [1, 2, 3, 4, 5])

    def test_lt(self):
        self.assertTrue(self.lazy_list < [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])

    def test_le(self):
        self.assertTrue(self.lazy_list <= [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_lazy_list_with_lazy_list(self):
        other_lazy_list = LazyList(string="16,17", converter=int)
        combined_list = self.lazy_list + other_lazy_list
        self.assertEqual(combined_list, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])

    def test_pickle_support(self):
        pickled = pickle.dumps(self.lazy_list)
        unpickled = pickle.loads(pickled)
        self.assertEqual(unpickled, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])
        self.assertEqual(unpickled._string, "")
        self.assertTrue(unpickled._parsed)

    def test_parse_before_accessing_decorator(self):
        lazy_list_copy = LazyList(string="1,2,,3,4,5,6,7,8,9,10,11,12,,13,14,15", converter=int)
        lazy_list_copy.append(16)
        self.assertEqual(lazy_list_copy, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])

    def test_parse_both_before_accessing_decorator(self):
        other_list = LazyList(string="16,17", converter=int)
        result = self.lazy_list + other_list
        self.assertEqual(result, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17])

    def test_length_on_iteration(self):
        elements = []
        for element in self.lazy_list:
            self.assertEqual(len(self.lazy_list), 15)
            elements.append(element)

        self.assertEqual(elements, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15])

    def test_str(self):
        self.assertEqual(str(self.lazy_list), "1,2,,3,4,5,6,7,8,9,10,11,12,,13,14,15")
        self.assertEqual(self.lazy_list, LazyList(str(self.lazy_list), converter=int))

    def test_str_parsed(self):
        list(self.lazy_list)
        self.assertEqual(str(self.lazy_list), "1,2,3,4,5,6,7,8,9,10,11,12,13,14,15")
        self.assertEqual(self.lazy_list, LazyList(str(self.lazy_list), converter=int))

    def test_slice(self):
        for slice_, expected in (
            (SliceGetter[1:], [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]),
            (SliceGetter[:3], [1, 2, 3]),
            (SliceGetter[::2], [1, 3, 5, 7, 9, 11, 13, 15]),
            (SliceGetter[1::2], [2, 4, 6, 8, 10, 12, 14]),
            (SliceGetter[::-1], [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]),
            (SliceGetter[-2:], [14, 15]),
            (SliceGetter[:-2], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]),
            (SliceGetter[:-1], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]),
            (SliceGetter[-2::-1], [14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]),
            (SliceGetter[::2], [1, 3, 5, 7, 9, 11, 13, 15]),
            (SliceGetter[::-2], [15, 13, 11, 9, 7, 5, 3, 1]),
            (SliceGetter[1::-1], [2, 1]),
            (SliceGetter[1:3:2], [2]),
        ):
            with self.subTest(f"self.lazy_list[{slice_}]. Expected: {expected}"):
                self.assertEqual(self.lazy_list[slice_], expected)
            with self.subTest(f"self.empty_lazy_list[{slice_}]. Expected: []"):
                self.assertEqual(self.empty_lazy_list[slice_], [])


if __name__ == "__main__":
    unittest.main()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\tests\test_rest_api.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


import copy
import io
import json
import logging
import os
import random
import shutil
import sysconfig
import tempfile
import xml.etree.ElementTree as ET
import zipfile
from collections import defaultdict
from contextlib import ExitStack
from datetime import timedelta
from enum import Enum
from glob import glob
from io import BytesIO, IOBase
from itertools import product
from time import sleep
from typing import BinaryIO
from unittest import mock

import av
import django_rq
import numpy as np
from django.conf import settings
from django.contrib.auth.models import Group, User
from django.http import HttpResponse
from django.test import override_settings
from pdf2image import convert_from_bytes
from PIL import Image
from pycocotools import coco as coco_loader
from pyunpack import Archive
from rest_framework import status
from rest_framework.test import APIClient
from rq.job import Job as RQJob
from rq.queue import Queue as RQQueue

from cvat.apps.dataset_manager.tests.utils import TestDir
from cvat.apps.dataset_manager.util import current_function_name
from cvat.apps.engine.cloud_provider import AWS_S3, Status
from cvat.apps.engine.media_extractors import ValidateDimension, sort
from cvat.apps.engine.models import (
    AttributeSpec,
    AttributeType,
    Data,
    DimensionType,
    Job,
    Label,
    Project,
    Segment,
    SortingMethod,
    StageChoice,
    StatusChoice,
    StorageChoice,
    StorageMethodChoice,
    Task,
)
from cvat.apps.engine.tests.utils import (
    ApiTestBase,
    ExportApiTestBase,
    ForceLogin,
    generate_image_file,
    generate_video_file,
    get_paginated_collection,
)
from utils.dataset_manifest import ImageManifestManager, VideoManifestManager

#suppress av warnings
logging.getLogger('libav').setLevel(logging.ERROR)

def create_db_users(cls):
    (group_admin, _) = Group.objects.get_or_create(name="admin")
    (group_user, _) = Group.objects.get_or_create(name="user")
    (group_annotator, _) = Group.objects.get_or_create(name="worker")
    (group_somebody, _) = Group.objects.get_or_create(name="somebody")

    user_admin = User.objects.create_superuser(username="admin", email="",
        password="admin")
    user_admin.groups.add(group_admin)
    user_owner = User.objects.create_user(username="user1", password="user1")
    user_owner.groups.add(group_user)
    user_assignee = User.objects.create_user(username="user2", password="user2")
    user_assignee.groups.add(group_annotator)
    user_annotator = User.objects.create_user(username="user3", password="user3")
    user_annotator.groups.add(group_annotator)
    user_somebody = User.objects.create_user(username="user4", password="user4")
    user_somebody.groups.add(group_somebody)
    user_dummy = User.objects.create_user(username="user5", password="user5")
    user_dummy.groups.add(group_user)

    cls.admin = user_admin
    cls.owner = cls.user1 = user_owner
    cls.assignee = cls.user2 = user_assignee
    cls.annotator = cls.user3 = user_annotator
    cls.somebody = cls.user4 = user_somebody
    cls.user = cls.user5 = user_dummy

def create_db_task(data):
    data_settings = {
        "size": data.pop("size"),
        "image_quality": data.pop("image_quality"),
    }

    db_data = Data.objects.create(**data_settings)

    if db_data.stop_frame == 0:
        frame_step = int((db_data.frame_filter or 'step=1').split('=')[-1])
        db_data.stop_frame = db_data.start_frame + (db_data.size - 1) * frame_step
        db_data.save()

    shutil.rmtree(db_data.get_data_dirname(), ignore_errors=True)
    os.makedirs(db_data.get_data_dirname())
    os.makedirs(db_data.get_upload_dirname())

    labels = data.pop('labels', None)
    db_task = Task.objects.create(**data)
    shutil.rmtree(db_task.get_dirname(), ignore_errors=True)
    os.makedirs(db_task.get_dirname())
    db_task.data = db_data
    db_task.save()

    if not labels is None:
        for label_data in labels:
            attributes = label_data.pop('attributes', None)
            db_label = Label(task=db_task, **label_data)
            db_label.save()

            if not attributes is None:
                for attribute_data in attributes:
                    db_attribute = AttributeSpec(label=db_label, **attribute_data)
                    db_attribute.save()

    for x in range(0, db_task.data.size, db_task.segment_size):
        start_frame = x
        stop_frame = min(x + db_task.segment_size - 1, db_task.data.size - 1)

        db_segment = Segment()
        db_segment.task = db_task
        db_segment.start_frame = start_frame
        db_segment.stop_frame = stop_frame
        db_segment.save()

        db_job = Job()
        db_job.segment = db_segment
        db_job.save()

    return db_task

def create_db_project(data):
    labels = data.pop('labels', None)
    db_project = Project.objects.create(**data)
    shutil.rmtree(db_project.get_dirname(), ignore_errors=True)
    os.makedirs(db_project.get_dirname())

    if not labels is None:
        for label_data in labels:
            attributes = label_data.pop('attributes', None)
            db_label = Label(project=db_project, **label_data)
            db_label.save()

            if not attributes is None:
                for attribute_data in attributes:
                    db_attribute = AttributeSpec(label=db_label, **attribute_data)
                    db_attribute.save()

    return db_project

def create_dummy_db_tasks(obj, project=None):
    tasks = []

    data = {
        "name": "my task #1",
        "owner": obj.owner,
        "assignee": obj.assignee,
        "overlap": 0,
        "segment_size": 100,
        "image_quality": 75,
        "size": 100,
        "project": project
    }
    db_task = create_db_task(data)
    tasks.append(db_task)

    data = {
        "name": "my multijob task",
        "owner": obj.user,
        "overlap": 0,
        "segment_size": 100,
        "image_quality": 50,
        "size": 200,
        "project": project
    }
    db_task = create_db_task(data)
    tasks.append(db_task)

    data = {
        "name": "my task #2",
        "owner": obj.owner,
        "assignee": obj.assignee,
        "overlap": 0,
        "segment_size": 100,
        "image_quality": 75,
        "size": 100,
        "project": project
    }
    db_task = create_db_task(data)
    tasks.append(db_task)

    data = {
        "name": "super task",
        "owner": obj.admin,
        "overlap": 0,
        "segment_size": 50,
        "image_quality": 95,
        "size": 50,
        "project": project
    }
    db_task = create_db_task(data)
    tasks.append(db_task)

    return tasks

def create_dummy_db_projects(obj):
    projects = []

    data = {
        "name": "my empty project",
        "owner": obj.owner,
        "assignee": obj.assignee,
    }
    db_project = create_db_project(data)
    projects.append(db_project)

    data = {
        "name": "my project without assignee",
        "owner": obj.user,
    }
    db_project = create_db_project(data)
    create_dummy_db_tasks(obj, db_project)
    projects.append(db_project)

    data = {
        "name": "my big project",
        "owner": obj.owner,
        "assignee": obj.assignee,
    }
    db_project = create_db_project(data)
    create_dummy_db_tasks(obj, db_project)
    projects.append(db_project)

    data = {
        "name": "public project",
    }
    db_project = create_db_project(data)
    create_dummy_db_tasks(obj, db_project)
    projects.append(db_project)

    data = {
        "name": "super project",
        "owner": obj.admin,
        "assignee": obj.assignee,
    }
    db_project = create_db_project(data)
    create_dummy_db_tasks(obj, db_project)
    projects.append(db_project)

    return projects

class JobGetAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.task = create_dummy_db_tasks(cls)[0]
        cls.job = Job.objects.filter(segment__task_id=cls.task.id).first()
        cls.job.assignee = cls.annotator
        cls.job.save()

    def _run_api_v2_jobs_id(self, jid, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/jobs/{}'.format(jid))

        return response

    def _check_request(self, response):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(response.data["id"], self.job.id)
        self.assertEqual(response.data["status"], StatusChoice.ANNOTATION)
        self.assertEqual(response.data["start_frame"], self.job.segment.start_frame)
        self.assertEqual(response.data["stop_frame"], self.job.segment.stop_frame)

    def test_api_v2_jobs_id_admin(self):
        response = self._run_api_v2_jobs_id(self.job.id, self.admin)
        self._check_request(response)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.admin)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_owner(self):
        response = self._run_api_v2_jobs_id(self.job.id, self.owner)
        self._check_request(response)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.owner)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_annotator(self):
        response = self._run_api_v2_jobs_id(self.job.id, self.annotator)
        self._check_request(response)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.annotator)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_somebody(self):
        response = self._run_api_v2_jobs_id(self.job.id, self.somebody)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.somebody)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_user(self):
        response = self._run_api_v2_jobs_id(self.job.id, self.user)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.user)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_no_auth(self):
        response = self._run_api_v2_jobs_id(self.job.id, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
        response = self._run_api_v2_jobs_id(self.job.id + 10, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class JobPartialUpdateAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        self.task = create_dummy_db_tasks(self)[0]
        self.job = Job.objects.filter(segment__task_id=self.task.id).first()
        self.job.assignee = self.annotator
        self.job.save()

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v2_jobs_id(self, jid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/jobs/{}'.format(jid), data=data, format='json')

        return response

    def _check_request(self, response, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(response.data["id"], self.job.id)
        self.assertEqual(response.data["stage"], data.get('stage', self.job.status))
        assignee = self.job.assignee.id if self.job.assignee else None
        self.assertEqual(response.data["assignee"]["id"], data.get('assignee', assignee))
        self.assertEqual(response.data["start_frame"], self.job.segment.start_frame)
        self.assertEqual(response.data["stop_frame"], self.job.segment.stop_frame)

    def test_api_v2_jobs_id_admin(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.owner.id }
        response = self._run_api_v2_jobs_id(self.job.id, self.admin, data)
        self._check_request(response, data)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.admin, data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_owner(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.owner.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.owner, data)
        self._check_request(response, data)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.owner, data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_annotator(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.annotator.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.annotator, data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.annotator, data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_somebody(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.admin.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.somebody, data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.somebody, data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_user(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.user.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.user, data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
        response = self._run_api_v2_jobs_id(self.job.id + 10, self.user, data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_jobs_id_no_auth(self):
        data = {"stage": StageChoice.ANNOTATION, "assignee": self.user.id}
        response = self._run_api_v2_jobs_id(self.job.id, None, data)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
        response = self._run_api_v2_jobs_id(self.job.id + 10, None, data)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_jobs_id_annotator_partial(self):
        data = {"stage": StageChoice.ANNOTATION}
        response = self._run_api_v2_jobs_id(self.job.id, self.annotator, data)
        self.assertEquals(response.status_code, status.HTTP_403_FORBIDDEN, response)

    def test_api_v2_jobs_id_admin_partial(self):
        data = {"assignee": self.user.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.admin, data)
        self._check_request(response, data)

    def test_api_v2_jobs_id_unknown_field(self):
        data = {"foo": "bar"}
        response = self._run_api_v2_jobs_id(self.job.id, self.admin, data)
        self.assertEquals(response.status_code, status.HTTP_403_FORBIDDEN, response)

class JobUpdateAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        self.task = create_dummy_db_tasks(self)[0]
        self.job = Job.objects.filter(segment__task_id=self.task.id).first()
        self.job.assignee = self.annotator
        self.job.save()

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v2_jobs_id(self, jid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.put('/api/jobs/{}'.format(jid), data=data, format='json')

        return response

    def test_api_v2_jobs_id_annotator(self):
        data = {"stage": StageChoice.ANNOTATION}
        response = self._run_api_v2_jobs_id(self.job.id, self.annotator, data)
        self.assertEquals(response.status_code, status.HTTP_405_METHOD_NOT_ALLOWED, response)

    def test_api_v2_jobs_id_admin(self):
        data = {"assignee_id": self.user.id}
        response = self._run_api_v2_jobs_id(self.job.id, self.owner, data)
        self.assertEquals(response.status_code, status.HTTP_405_METHOD_NOT_ALLOWED, response)

class JobDataMetaPartialUpdateAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        self.task = create_dummy_db_tasks(self)[0]
        self.job = Job.objects.filter(segment__task_id=self.task.id).first()
        self.job.assignee = self.annotator
        self.job.save()

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v1_jobs_data_meta_id(self, jid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/jobs/{}/data/meta'.format(jid), data=data, format='json')

        return response

    def _check_response(self, response, db_data, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        deleted_frames = data.get("deleted_frames", db_data.deleted_frames)
        self.assertEqual(response.data["deleted_frames"], deleted_frames)

    def _check_api_v1_jobs_data_meta_id(self, user, data):
        response = self._run_api_v1_jobs_data_meta_id(self.job.id, user, data)

        if user is None:
            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
        elif (
            user == self.job.segment.task.owner or
            user == self.job.segment.task.assignee or
            user == self.job.assignee or
            user.is_superuser
        ):
            self._check_response(response, self.job.segment.task.data, data)
        else:
            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v1_jobs_data_meta(self):
        data = {
            "deleted_frames": list(
                range(self.job.segment.start_frame, self.job.segment.stop_frame + 1)
            )
        }
        self._check_api_v1_jobs_data_meta_id(self.admin, data)

        data = {
            "deleted_frames": []
        }
        self._check_api_v1_jobs_data_meta_id(self.admin, data)

class ServerAboutAPITestCase(ApiTestBase):
    ACCEPT_HEADER_TEMPLATE = 'application/vnd.cvat+json; version={}'

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v2_server_about(self, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/server/about')

        return response

    def _run_api_server_about(self, user, version):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/server/about',
                HTTP_ACCEPT=self.ACCEPT_HEADER_TEMPLATE.format(version))
        return response

    def _check_response_version(self, response, version):
        self.assertEqual(response.accepted_media_type, self.ACCEPT_HEADER_TEMPLATE.format(version))

    def _check_request(self, response):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertIsNotNone(response.data.get("name", None))
        self.assertIsNotNone(response.data.get("description", None))
        self.assertIsNotNone(response.data.get("version", None))

    def test_api_v2_server_about_admin(self):
        response = self._run_api_v2_server_about(self.admin)
        self._check_request(response)

    def test_api_v2_server_about_user(self):
        response = self._run_api_v2_server_about(self.user)
        self._check_request(response)

    def test_api_v2_server_about_no_auth(self):
        response = self._run_api_v2_server_about(None)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_api_server_about_versions_admin(self):
        for version in settings.REST_FRAMEWORK['ALLOWED_VERSIONS']:
            response = self._run_api_server_about(self.admin, version)
            self._check_response_version(response, version)

class ServerExceptionAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.data = {
            "events": [{
            "scope": "send:exception",
            "timestamp": "2019-01-29T12:34:56.000000Z",
            "payload": json.dumps({
                "system": "Linux",
                "client": "rest_framework.APIClient",

                "task_id": 1,
                "job_id": 1,
                "proj_id": 2,
                "client_id": 12321235123,
                "message": "just test message",
                "filename": "http://localhost/my_file.js",
                "line": 1,
                "column": 1,
                "stack": "",
            })}],
            "timestamp": "2019-01-29T12:34:57.000000Z",
        }


    def _run_api_v2_server_exception(self, user):
        with ForceLogin(user, self.client):
            #pylint: disable=unused-variable
            with mock.patch("cvat.apps.events.views.vlogger") as vlogger:
                response = self.client.post('/api/events',
                    self.data, format='json')

        return response

    def test_api_v2_server_exception_admin(self):
        response = self._run_api_v2_server_exception(self.admin)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def test_api_v2_server_exception_user(self):
        response = self._run_api_v2_server_exception(self.user)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def test_api_v2_server_exception_no_auth(self):
        response = self._run_api_v2_server_exception(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class ServerLogsAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.data = {
            "events": [{
                "scope": "debug:info",
                "timestamp": "2024-05-30T17:05:13.776Z",
                "task_id": 1,
                "job_id": 1,
                "project_id": 2,
                "organization": 2,
                "count": 1,
                "payload": json.dumps({
                    "client_id": 123456,
                    "message": "just test message",
                    "name": "add point",
                    "is_active": True,
                }),
            },
            {
                "timestamp": "2024-05-30T17:05:14.776Z",
                "scope": "debug:info",
            }],
            "timestamp": "2024-05-30T17:05:15.776Z",
        }


    def _run_api_v2_server_logs(self, user):
        with ForceLogin(user, self.client):
            #pylint: disable=unused-variable
            with mock.patch("cvat.apps.events.views.vlogger") as vlogger:
                response = self.client.post('/api/events',
                    self.data, format='json')

        return response

    def test_api_v2_server_logs_admin(self):
        response = self._run_api_v2_server_logs(self.admin)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def test_api_v2_server_logs_user(self):
        response = self._run_api_v2_server_logs(self.user)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def test_api_v2_server_logs_no_auth(self):
        response = self._run_api_v2_server_logs(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class UserAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        create_db_users(self)

    def _check_response(self, user, response, is_full=True):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self._check_data(user, response.data, is_full)

    def _check_data(self, user, data, is_full):
        self.assertEqual(data["id"], user.id)
        self.assertEqual(data["username"], user.username)
        self.assertEqual(data["first_name"], user.first_name)
        self.assertEqual(data["last_name"], user.last_name)
        extra_check = self.assertIn if is_full else self.assertNotIn
        extra_check("email", data)
        extra_check("groups", data)
        extra_check("is_staff", data)
        extra_check("is_superuser", data)
        extra_check("is_active", data)
        extra_check("last_login", data)
        extra_check("date_joined", data)
        extra_check("has_analytics_access", data)


class UserListAPITestCase(UserAPITestCase):
    def _run_api_v2_users(self, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/users')

        return response

    def _check_response(self, user, response, is_full=True):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for user_info in response.data['results']:
            db_user = getattr(self, user_info['username'])
            self._check_data(db_user, user_info, is_full)

    def test_api_v2_users_admin(self):
        response = self._run_api_v2_users(self.admin)
        self._check_response(self.admin, response, True)

    def test_api_v2_users_user(self):
        response = self._run_api_v2_users(self.user)
        self._check_response(self.user, response, False)

    def test_api_v2_users_annotator(self):
        response = self._run_api_v2_users(self.annotator)
        self._check_response(self.annotator, response, False)

    def test_api_v2_users_somebody(self):
        response = self._run_api_v2_users(self.somebody)
        self._check_response(self.somebody, response, False)

    def test_api_v2_users_no_auth(self):
        response = self._run_api_v2_users(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class UserSelfAPITestCase(UserAPITestCase):
    def _run_api_v2_users_self(self, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/users/self')

        return response

    def test_api_v2_users_self_admin(self):
        response = self._run_api_v2_users_self(self.admin)
        self._check_response(self.admin, response)

    def test_api_v2_users_self_user(self):
        response = self._run_api_v2_users_self(self.user)
        self._check_response(self.user, response)

    def test_api_v2_users_self_annotator(self):
        response = self._run_api_v2_users_self(self.annotator)
        self._check_response(self.annotator, response)

    def test_api_v2_users_self_somebody(self):
        response = self._run_api_v2_users_self(self.somebody)
        self._check_response(self.somebody, response)

    def test_api_v2_users_self_no_auth(self):
        response = self._run_api_v2_users_self(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class UserGetAPITestCase(UserAPITestCase):
    def _run_api_v2_users_id(self, user, user_id):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/users/{}'.format(user_id))

        return response

    def test_api_v2_users_id_admin(self):
        response = self._run_api_v2_users_id(self.admin, self.user.id)
        self._check_response(self.user, response, True)

        response = self._run_api_v2_users_id(self.admin, self.admin.id)
        self._check_response(self.admin, response, True)

        response = self._run_api_v2_users_id(self.admin, self.owner.id)
        self._check_response(self.owner, response, True)

    def test_api_v2_users_id_user(self):
        response = self._run_api_v2_users_id(self.user, self.user.id)
        self._check_response(self.user, response, True)

        response = self._run_api_v2_users_id(self.user, self.owner.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_users_id_annotator(self):
        response = self._run_api_v2_users_id(self.annotator, self.annotator.id)
        self._check_response(self.annotator, response, True)

        response = self._run_api_v2_users_id(self.annotator, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_users_id_somebody(self):
        response = self._run_api_v2_users_id(self.somebody, self.somebody.id)
        self._check_response(self.somebody, response, True)

        response = self._run_api_v2_users_id(self.somebody, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_users_id_no_auth(self):
        response = self._run_api_v2_users_id(None, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class UserPartialUpdateAPITestCase(UserAPITestCase):
    def _run_api_v2_users_id(self, user, user_id, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/users/{}'.format(user_id), data=data, format='json')

        return response

    def _check_response_with_data(self, user, response, data, is_full):
        # refresh information about the user from DB
        user = User.objects.get(id=user.id)
        for k,v in data.items():
            self.assertEqual(response.data[k], v)
        self._check_response(user, response, is_full)

    def test_api_v2_users_id_admin_partial(self):
        data = {"username": "user09", "last_name": "my last name"}
        response = self._run_api_v2_users_id(self.admin, self.user.id, data)

        self._check_response_with_data(self.user, response, data, True)

    def test_api_v2_users_id_user_partial(self):
        data = {"username": "user10", "first_name": "my name"}
        response = self._run_api_v2_users_id(self.user, self.user.id, data)
        self._check_response_with_data(self.user, response, data, False)

        data = {"is_staff": True}
        response = self._run_api_v2_users_id(self.user, self.user.id, data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

        data = {"username": "admin", "is_superuser": True}
        response = self._run_api_v2_users_id(self.user, self.user.id, data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

        data = {"username": "non_active", "is_active": False}
        response = self._run_api_v2_users_id(self.user, self.user.id, data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

        data = {"username": "annotator01", "first_name": "slave"}
        response = self._run_api_v2_users_id(self.user, self.annotator.id, data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_users_id_no_auth_partial(self):
        data = {"username": "user12"}
        response = self._run_api_v2_users_id(None, self.user.id, data)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class UserDeleteAPITestCase(UserAPITestCase):
    def _run_api_v2_users_id(self, user, user_id):
        with ForceLogin(user, self.client):
            response = self.client.delete('/api/users/{}'.format(user_id))

        return response

    def test_api_v2_users_id_admin(self):
        response = self._run_api_v2_users_id(self.admin, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

        response = self._run_api_v2_users_id(self.admin, self.admin.id)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

    def test_api_v2_users_id_user(self):
        response = self._run_api_v2_users_id(self.user, self.owner.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._run_api_v2_users_id(self.user, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

    def test_api_v2_users_id_annotator(self):
        response = self._run_api_v2_users_id(self.annotator, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._run_api_v2_users_id(self.annotator, self.annotator.id)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

    def test_api_v2_users_id_somebody(self):
        response = self._run_api_v2_users_id(self.somebody, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._run_api_v2_users_id(self.somebody, self.somebody.id)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

    def test_api_v2_users_id_no_auth(self):
        response = self._run_api_v2_users_id(None, self.user.id)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

class ProjectListAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.projects = create_dummy_db_projects(cls)

    def _run_api_v2_projects(self, user, params=""):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/projects{}'.format(params))

        return response

    def test_api_v2_projects_admin(self):
        response = self._run_api_v2_projects(self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([project.name for project in self.projects]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_projects_user(self):
        response = self._run_api_v2_projects(self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([project.name for project in self.projects
                if self.user in [project.owner, project.assignee]]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_projects_somebody(self):
        response = self._run_api_v2_projects(self.somebody)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual([],
            [res["name"] for res in response.data["results"]])

    def test_api_v2_projects_no_auth(self):
        response = self._run_api_v2_projects(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

class ProjectGetAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.projects = create_dummy_db_projects(cls)

    def _run_api_v2_projects_id(self, pid, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/projects/{}'.format(pid))

        return response

    def _check_response(self, response, db_project):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(response.data["name"], db_project.name)
        owner = db_project.owner.id if db_project.owner else None
        response_owner = response.data["owner"]["id"] if response.data["owner"] else None
        self.assertEqual(response_owner, owner)
        assignee = db_project.assignee.id if db_project.assignee else None
        response_assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        self.assertEqual(response_assignee, assignee)
        self.assertEqual(response.data["status"], db_project.status)
        self.assertEqual(response.data["bug_tracker"], db_project.bug_tracker)

    def _check_api_v2_projects_id(self, user):
        for db_project in self.projects:
            response = self._run_api_v2_projects_id(db_project.id, user)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_project.owner or user == db_project.assignee or user.is_superuser:
                self._check_response(response, db_project)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_projects_id_admin(self):
        self._check_api_v2_projects_id(self.admin)

    def test_api_v2_projects_id_user(self):
        self._check_api_v2_projects_id(self.user)

    def test_api_v2_projects_id_somebody(self):
        self._check_api_v2_projects_id(self.somebody)

    def test_api_v2_projects_id_no_auth(self):
        self._check_api_v2_projects_id(None)

class ProjectDeleteAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.projects = create_dummy_db_projects(cls)

    def _run_api_v2_projects_id(self, pid, user):
        with ForceLogin(user, self.client):
            response = self.client.delete('/api/projects/{}'.format(pid), format="json")

        return response

    def _check_api_v2_projects_id(self, user):
        for db_project in self.projects:
            response = self._run_api_v2_projects_id(db_project.id, user)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_project.owner or user.is_superuser:
                self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)


    def test_api_v2_projects_id_admin(self):
        self._check_api_v2_projects_id(self.admin)

    def test_api_v2_projects_id_user(self):
        self._check_api_v2_projects_id(self.user)

    def test_api_v2_projects_id_somebody(self):
        self._check_api_v2_projects_id(self.somebody)

    def test_api_v2_projects_id_no_auth(self):
        self._check_api_v2_projects_id(None)

    def test_api_v2_projects_delete_project_data_after_delete_project(self):
        tasks = {}
        for project in self.projects:
            tasks[project.name] = create_dummy_db_tasks(self.__class__, project)

            project_dir = project.get_dirname()
            self.assertTrue(os.path.exists(project_dir))

            for task in tasks[project.name]:
                task_dir = task.get_dirname()
                self.assertTrue(os.path.exists(task_dir))

        with self.captureOnCommitCallbacks(execute=True):
            self._check_api_v2_projects_id(self.admin)

        for project in self.projects:
            project_dir = project.get_dirname()
            self.assertFalse(os.path.exists(project_dir))

            for task in tasks[project.name]:
                task_dir = task.get_dirname()
                self.assertFalse(os.path.exists(task_dir))

class ProjectCreateAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v2_projects(self, user, data):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/projects', data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get(
                        "/api/labels?project_id=%s&page=%s" % (response.data["id"], page)
                    )
                ))
                response.data["labels"] = labels_response

        return response

    def _check_response(self, response, user, data):
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        self.assertEqual(response.data["name"], data["name"])
        self.assertEqual(response.data["owner"]["id"], data.get("owner_id", user.id))
        response_assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        self.assertEqual(response_assignee, data.get('assignee_id', None))
        self.assertEqual(response.data["bug_tracker"], data.get("bug_tracker", ""))
        self.assertEqual(response.data["status"], StatusChoice.ANNOTATION)
        self.assertListEqual(
            [label["name"] for label in data.get("labels", [])],
            [label["name"] for label in response.data["labels"]]
        )

    def _check_api_v2_projects(self, user, data):
        response = self._run_api_v2_projects(user, data)
        if user:
            self._check_response(response, user, data)
        else:
            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_projects_admin(self):
        data = {
            "name": "new name for the project",
            "bug_tracker": "http://example.com"
        }
        self._check_api_v2_projects(self.admin, data)

        data = {
            "owner_id": self.owner.id,
            "assignee_id": self.assignee.id,
            "name": "new name for the project"
        }
        self._check_api_v2_projects(self.admin, data)

        data = {
            "owner_id": self.admin.id,
            "name": "2"
        }
        self._check_api_v2_projects(self.admin, data)

        data = {
            "name": "Project with labels",
            "labels": [{
                "name": "car",
            }]
        }
        self._check_api_v2_projects(self.admin, data)


    def test_api_v2_projects_user(self):
        data = {
            "name": "Dummy name",
            "bug_tracker": "it is just text"
        }
        self._check_api_v2_projects(self.user, data)

        data = {
            "owner_id": self.user.id,
            "assignee_id": self.user.id,
            "name": "My import project with data"
        }
        self._check_api_v2_projects(self.user, data)


    def test_api_v2_projects_somebody(self):
        data = {
            "name": "My Project #1",
            "owner_id": self.somebody.id,
            "assignee_id": self.somebody.id
        }
        self._check_api_v2_projects(self.somebody, data)

    def test_api_v2_projects_no_auth(self):
        data = {
            "name": "My Project #2",
            "owner_id": self.admin.id,
        }
        self._check_api_v2_projects(None, data)

class ProjectPartialUpdateAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.projects = create_dummy_db_projects(cls)

    def _run_api_v2_projects_id(self, pid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/projects/{}'.format(pid),
                data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?project_id=%s&page=%s" % (pid, page))
                ))
                response.data["labels"] = labels_response

        return response

    def _check_response(self, response, db_project, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        name = data.get("name", db_project.name)
        self.assertEqual(response.data["name"], name)
        response_owner = response.data["owner"]["id"] if response.data["owner"] else None
        db_owner = db_project.owner.id if db_project.owner else None
        self.assertEqual(response_owner, data.get("owner_id", db_owner))
        response_assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        db_assignee = db_project.assignee.id if db_project.assignee else None
        self.assertEqual(response_assignee, data.get("assignee_id", db_assignee))
        self.assertEqual(response.data["status"], data.get("status", db_project.status))
        self.assertEqual(response.data["bug_tracker"], data.get("bug_tracker", db_project.bug_tracker))
        if data.get("labels"):
            self.assertListEqual(
                [label["name"] for label in data.get("labels") if not label.get("deleted", False)],
                [label["name"] for label in response.data["labels"]]
            )
        else:
            self.assertListEqual(
                [label.name for label in db_project.label_set.all()],
                [label["name"] for label in response.data["labels"]]
            )

    def _check_api_v2_projects_id(self, user, data):
        for db_project in self.projects:
            response = self._run_api_v2_projects_id(db_project.id, user, data)
            if user and user.has_perm("engine.project.change", db_project):
                self._check_response(response, db_project, data)
            elif user:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)
            else:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_projects_id_admin(self):
        data = {
            "name": "project with some labels",
            "owner_id": self.owner.id,
            "bug_tracker": "https://new.bug.tracker",
            "labels": [
                {"name": "car"},
                {"name": "person"}
            ],
        }
        self._check_api_v2_projects_id(self.admin, data)

    def test_api_v2_projects_id_user(self):
        data = {
            "name": "new name for the project",
            "owner_id": self.assignee.id,
        }
        self._check_api_v2_projects_id(self.user, data)

    def test_api_v2_projects_id_somebody(self):
        data = {
            "name": "new name for the project",
        }
        self._check_api_v2_projects_id(self.somebody, data)

    def test_api_v2_projects_id_no_auth(self):
        data = {
            "name": "new name for the project",
        }
        self._check_api_v2_projects_id(None, data)

    def test_api_v2_projects_id_unknown_field(self):
        data = {"foo": "bar"}
        response = self._run_api_v2_projects_id(self.projects[0].id, self.admin, data)
        self.assertEquals(response.status_code, status.HTTP_403_FORBIDDEN, response)

class UpdateLabelsAPITestCase(ApiTestBase):
    def assertLabelsEqual(self, label1, label2):
        self.assertEqual(label1.get("name", label2.get("name")), label2.get("name"))
        self.assertEqual(label1.get("color", label2.get("color")), label2.get("color"))

    def _check_response(self, response, db_object, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        db_labels = db_object.label_set.all()
        response_labels = response.data["labels"]
        for label in data["labels"]:
            if label.get("id", None) is None:
                self.assertLabelsEqual(
                    label,
                    [l for l in response_labels if label.get("name") == l.get("name")][0],
                )
                db_labels = [l for l in db_labels if label.get("name") != l.name]
                response_labels = [l for l in response_labels if label.get("name") != l.get("name")]
            else:
                if not label.get("deleted", False):
                    self.assertLabelsEqual(
                        label,
                        [l for l in response_labels if label.get("id") == l.get("id")][0],
                    )
                    response_labels = [l for l in response_labels if label.get("id") != l.get("id")]
                    db_labels = [l for l in db_labels if label.get("id") != l.id]
                else:
                    self.assertEqual(
                        len([l for l in response_labels if label.get("id") == l.get("id")]), 0
                    )
            self.assertEqual(len(response_labels), len(db_labels))

class ProjectUpdateLabelsAPITestCase(UpdateLabelsAPITestCase):
    @classmethod
    def setUpTestData(cls):
        project_data = {
            "name": "Project with labels",
            "bug_tracker": "https://new.bug.tracker",
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
            }, {
                "name": "person",
            }]
        }

        create_db_users(cls)
        db_project = create_db_project(project_data)
        create_dummy_db_tasks(cls, db_project)
        cls.project = db_project

    def _check_api_v2_project(self, data):
        response = self._run_api_v2_project_id(self.project.id, self.admin, data)
        self._check_response(response, self.project, data)

    def _run_api_v2_project_id(self, pid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/projects/{}'.format(pid),
                data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?project_id=%s&page=%s" % (pid, page))
                ))
                response.data["labels"] = labels_response

        return response

    def test_api_v2_projects_create_label(self):
        data = {
            "labels": [{
                "name": "new label",
            }],
        }
        self._check_api_v2_project(data)

    def test_api_v2_projects_edit_label(self):
        data = {
            "labels": [{
                "id": 1,
                "name": "New name for label",
                "color": "#fefefe",
            }],
        }
        self._check_api_v2_project(data)

    def test_api_v2_projects_delete_label(self):
        data = {
            "labels": [{
                "id": 2,
                "name": "Label for deletion",
                "deleted": True
            }]
        }
        self._check_api_v2_project(data)

class ProjectListOfTasksAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.projects = create_dummy_db_projects(cls)

    def _run_api_v2_projects_id_tasks(self, user, pid):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks?project_id={}'.format(pid))

        return response

    def test_api_v2_projects_id_tasks_admin(self):
        project = self.projects[1]
        response = self._run_api_v2_projects_id_tasks(self.admin, project.id)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([task.name for task in project.tasks.all()]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_projects_id_tasks_user(self):
        project = self.projects[1] # the user is owner of the project
        response = self._run_api_v2_projects_id_tasks(self.user, project.id)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([task.name for task in project.tasks.all()]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_projects_id_tasks_somebody(self):
        project = self.projects[1]
        response = self._run_api_v2_projects_id_tasks(self.somebody, project.id)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual([], response.data['results'])

    def test_api_v2_projects_id_tasks_no_auth(self):
        project = self.projects[1]
        response = self._run_api_v2_projects_id_tasks(None, project.id)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class ProjectBackupAPITestCase(ExportApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls._create_media()
        cls.client = APIClient()
        cls._create_projects()

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        for task in cls.tasks:
            shutil.rmtree(os.path.join(settings.TASKS_ROOT, str(task["id"])))
            shutil.rmtree(os.path.join(settings.MEDIA_DATA_ROOT, str(task["data_id"])))

        for f in cls.media['files']:
            os.remove(f)
        for d in cls.media['dirs']:
            shutil.rmtree(d)

    @classmethod
    def _create_media(cls):
        cls.media_data = []
        cls.media = {
            'files': [],
            'dirs': [],
        }
        image_count = 10
        imagename_pattern = "test_{}.jpg"
        for i in range(image_count):
            filename = imagename_pattern.format(i)
            path = os.path.join(settings.SHARE_ROOT, filename)
            cls.media['files'].append(path)
            _, data = generate_random_image_file(filename)
            with open(path, "wb") as image:
                image.write(data.read())

        cls.media_data.append(
            {
                **{"image_quality": 75,
                   "copy_data": True,
                   "start_frame": 2,
                   "stop_frame": 9,
                   "frame_filter": "step=2",
                },
                **{"server_files[{}]".format(i): imagename_pattern.format(i) for i in range(image_count)},
            }
        )

        filename = "test_video_1.mp4"
        path = os.path.join(settings.SHARE_ROOT, filename)
        cls.media['files'].append(path)
        _, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())
        cls.media_data.append(
            {
                "image_quality": 75,
                "copy_data": True,
                "start_frame": 2,
                "stop_frame": 24,
                "frame_filter": "step=2",
                "server_files[0]": filename,
            }
        )

        filename = os.path.join("test_archive_1.zip")
        path = os.path.join(settings.SHARE_ROOT, filename)
        cls.media['files'].append(path)
        _, data = generate_zip_archive_file(filename, count=5)
        with open(path, "wb") as zip_archive:
            zip_archive.write(data.read())
        cls.media_data.append(
            {
                "image_quality": 75,
                "server_files[0]": filename,
            }
        )

        filename = os.path.join("videos", "test_video_1.mp4")
        path = os.path.join(settings.SHARE_ROOT, filename)
        cls.media['dirs'].append(os.path.dirname(path))
        os.makedirs(os.path.dirname(path))
        _, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())

        manifest_path = os.path.join(settings.SHARE_ROOT, 'videos', 'manifest.jsonl')
        generate_manifest_file(data_type='video', manifest_path=manifest_path, sources=[path])

        cls.media_data.append(
            {
                "image_quality": 70,
                "copy_data": True,
                "server_files[0]": filename,
                "server_files[1]": os.path.join("videos", "manifest.jsonl"),
                "use_cache": True,
            }
        )

        manifest_path = manifest_path=os.path.join(settings.SHARE_ROOT, 'manifest.jsonl')
        generate_manifest_file(data_type='images', manifest_path=manifest_path,
            sources=[os.path.join(settings.SHARE_ROOT, imagename_pattern.format(i)) for i in range(1, 8)])
        cls.media['files'].append(manifest_path)
        cls.media_data.append(
            {
                **{"image_quality": 70,
                    "copy_data": True,
                    "use_cache": True,
                    "frame_filter": "step=2",
                    "server_files[0]": "manifest.jsonl",
                },
                **{
                    **{"server_files[{}]".format(i): imagename_pattern.format(i) for i in range(1, 8)},
                }
            }
        )

        cls.media_data.extend([
            # image list local
            {
                "client_files[0]": generate_random_image_file("test_1.jpg")[1],
                "client_files[1]": generate_random_image_file("test_2.jpg")[1],
                "client_files[2]": generate_random_image_file("test_3.jpg")[1],
                "image_quality": 75,
            },
            # video local
            {
                "client_files[0]": generate_video_file("test_video.mp4")[1],
                "image_quality": 75,
            },
            # zip archive local
            {
                "client_files[0]": generate_zip_archive_file("test_archive_1.zip", 10)[1],
                "image_quality": 50,
            },
            # pdf local
            {
                "client_files[0]": generate_pdf_file("test_pdf_1.pdf", 7)[1],
                "image_quality": 54,
            },
        ])

    @classmethod
    def _create_tasks(cls, project):
        def _create_task(task_data, media_data):
            response = cls.client.post('/api/tasks', data=task_data, format="json")
            assert response.status_code == status.HTTP_201_CREATED
            tid = response.data["id"]

            for media in media_data.values():
                if isinstance(media, io.BytesIO):
                    media.seek(0)
            response = cls.client.post("/api/tasks/{}/data".format(tid), data=media_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = cls.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = cls.client.get("/api/tasks/{}".format(tid))
            data_id = response.data["data"]
            cls.tasks.append({
                "id": tid,
                "data_id": data_id,
            })

        task_data = [
            {
                "name": "my task #1",
                "owner_id": project.owner.id,
                "overlap": 0,
                "segment_size": 100,
                "project_id": project.id,
            },
            {
                "name": "my task #2",
                "owner_id": project.owner.id,
                "overlap": 1,
                "segment_size": 3,
                "project_id": project.id,
            },
        ]

        with ForceLogin(project.owner, cls.client):
            for data in task_data:
                for media in cls.media_data:
                    _create_task(data, media)

    @classmethod
    def _create_projects(cls):
        cls.projects = []
        cls.tasks = []
        data = {
            "name": "my empty project",
            "owner": cls.owner,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
                }, {
                    "name": "person",
                },
            ],
        }
        db_project = create_db_project(data)
        cls.projects.append(db_project)

        data = {
            "name": "my project without assignee",
            "owner": cls.owner,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
                }, {
                    "name": "person",
                },
            ],
        }
        db_project = create_db_project(data)
        cls._create_tasks(db_project)
        cls.projects.append(db_project)

        data = {
            "name": "my big project",
            "owner": cls.owner,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
                }, {
                    "name": "person",
                },
            ],
        }
        db_project = create_db_project(data)
        cls._create_tasks(db_project)
        cls.projects.append(db_project)

        data = {
            "name": "public project",
            "owner": cls.owner,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
                }, {
                    "name": "person",
                },
            ],
        }
        db_project = create_db_project(data)
        cls._create_tasks(db_project)
        cls.projects.append(db_project)

        data = {
            "name": "super project",
            "owner": cls.admin,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
                }, {
                    "name": "person",
                },
            ],
        }
        db_project = create_db_project(data)
        cls._create_tasks(db_project)
        cls.projects.append(db_project)

    def _run_api_v2_projects_import(self, user, data):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/projects/backup', data=data, format="multipart")

        return response

    def _run_api_v2_projects_id(self, pid, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/projects/{}'.format(pid), format="json")

        return response.data

    def _get_tasks_for_project(self, user, pid):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks?project_id={}'.format(pid))

        return sorted(response.data["results"], key=lambda task: task["name"])

    def _run_api_v2_projects_id_export_import(self, user):
        for project in self.projects:
            expected_4xx_status_code = None

            if not user:
                expected_4xx_status_code = status.HTTP_401_UNAUTHORIZED
            elif user not in {project.assignee, project.owner, self.admin}:
                expected_4xx_status_code = status.HTTP_403_FORBIDDEN

            pid = project.id
            response = self._export_project_backup(user, pid, expected_4xx_status_code=expected_4xx_status_code)

            if response.status_code == status.HTTP_200_OK:
                self.assertTrue(response.streaming)
                content = io.BytesIO(b"".join(response.streaming_content))
                content.seek(0)

                uploaded_data = {
                    "project_file": content,
                }
                response = self._run_api_v2_projects_import(user, uploaded_data)
                self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_202_ACCEPTED)
                if response.status_code == status.HTTP_202_ACCEPTED:
                    rq_id = response.data["rq_id"]
                    response = self._run_api_v2_projects_import(user, {"rq_id": rq_id})
                    self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_201_CREATED)
                    original_project = self._run_api_v2_projects_id(pid, user)
                    imported_project = self._run_api_v2_projects_id(response.data["id"], user)
                    compare_objects(
                        self=self,
                        obj1=original_project,
                        obj2=imported_project,
                        ignore_keys=(
                            "data",
                            "id",
                            "url",
                            "owner",
                            "assignee",
                            "created_date",
                            "updated_date",
                            "project_id",
                            "tasks",
                        ),
                    )
                    self.assertEqual(original_project["tasks"]["count"], imported_project["tasks"]["count"])
                    original_tasks = self._get_tasks_for_project(user, original_project["id"])
                    imported_tasks = self._get_tasks_for_project(user, imported_project["id"])
                    for original_task, imported_task in zip(original_tasks, imported_tasks):
                        compare_objects(
                            self=self,
                            obj1=original_task,
                            obj2=imported_task,
                            ignore_keys=(
                                "id",
                                "url",
                                "created_date",
                                "updated_date",
                                "username",
                                "project_id",
                                "data",
                                # backup does not store overlap explicitly
                                "overlap",
                            ),
                        )

    def test_api_v2_projects_id_export_admin(self):
        self._run_api_v2_projects_id_export_import(self.admin)

    def test_api_v2_projects_id_export_user(self):
        self._run_api_v2_projects_id_export_import(self.user)

    def test_api_v2_projects_id_export_somebody(self):
        self._run_api_v2_projects_id_export_import(self.somebody)

    def test_api_v2_projects_id_export_no_auth(self):
        self._run_api_v2_projects_id_export_import(None)


@override_settings(MEDIA_CACHE_ALLOW_STATIC_CACHE=False)
class ProjectCloudBackupAPINoStaticChunksTestCase(ProjectBackupAPITestCase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.client = APIClient()
        cls._create_cloud_storage()
        cls._create_media()
        cls._create_projects()

    @classmethod
    def _create_cloud_storage(cls):
        data = {
            "provider_type": "AWS_S3_BUCKET",
            "resource": "test",
            "display_name": "Bucket",
            "credentials_type": "KEY_SECRET_KEY_PAIR",
            "key": "minio_access_key",
            "secret_key": "minio_secret_key",
            "specific_attributes": "endpoint_url=http://minio:9000",
            "description": "Some description",
            "manifests": [],
        }

        class MockAWS(AWS_S3):
            _files = {}

            def get_status(self):
                return Status.AVAILABLE

            @classmethod
            def create_file(cls, key, _bytes):
                cls._files[key] = _bytes

            def get_file_status(self, key: str, /):
                return Status.AVAILABLE if key in self._files else Status.NOT_FOUND

            def _download_range_of_bytes(self, key: str, /, *, stop_byte: int, start_byte: int):
                return self._files[key][start_byte:stop_byte]

            def _download_fileobj_to_stream(self, key: str, stream: BinaryIO, /):
                stream.write(self._files[key])

        cls.mock_aws = MockAWS

        cls.aws_patch = mock.patch("cvat.apps.engine.cloud_provider.AWS_S3", MockAWS)
        cls.aws_patch.start()

        with ForceLogin(cls.owner, cls.client):
            response = cls.client.post('/api/cloudstorages', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, (response.status_code, response.content)
            cls.cloud_storage_id = response.json()["id"]

    @classmethod
    def tearDownClass(cls):
        cls.aws_patch.stop()
        super().tearDownClass()

    @classmethod
    def _create_media(cls):
        cls.media_data = []
        cls.media = {'files': [], 'dirs': []}
        for file in [
            generate_random_image_file("test_1.jpg")[1],
            generate_random_image_file("test_2.jpg")[1],
            generate_pdf_file("test_pdf_1.pdf", 7)[1],
            generate_zip_archive_file("test_archive_1.zip", 10)[1],
            generate_video_file("test_video.mp4")[1],
        ]:
            cls.mock_aws.create_file(file.name, file.getvalue())

        cls.media_data.extend([
            # image list cloud
            {
                "server_files[0]": "test_1.jpg",
                "server_files[1]": "test_2.jpg",
                "image_quality": 75,
                "cloud_storage_id": cls.cloud_storage_id,
                "storage": StorageChoice.CLOUD_STORAGE,
            },
            # video cloud
            {
                "server_files[0]": "test_video.mp4",
                "image_quality": 75,
                "cloud_storage_id": cls.cloud_storage_id,
                "storage": StorageChoice.CLOUD_STORAGE,
            },
            # zip archive cloud
            {
                "server_files[0]": "test_archive_1.zip",
                "image_quality": 50,
                "cloud_storage_id": cls.cloud_storage_id,
                "storage": StorageChoice.CLOUD_STORAGE,
            },
            # pdf cloud
            {
                "server_files[0]": "test_pdf_1.pdf",
                "image_quality": 54,
                "cloud_storage_id": cls.cloud_storage_id,
                "storage": StorageChoice.CLOUD_STORAGE,
            },
        ])


@override_settings(MEDIA_CACHE_ALLOW_STATIC_CACHE=True)
class ProjectCloudBackupAPIStaticChunksTestCase(ProjectCloudBackupAPINoStaticChunksTestCase):
    pass


class ProjectExportAPITestCase(ExportApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        project_data = {
            "name": "Project for check tasks in a xml",
            "owner": cls.admin,
            "labels": [{
                "name": "car"
            }]
        }

        db_project = create_db_project(project_data)
        create_dummy_db_tasks(cls, db_project)
        cls.project = db_project

    def _run_api_v2_tasks_id_delete(self, tid, user):
        with ForceLogin(user, self.client):
            response = self.client.delete('/api/tasks/{}'.format(tid), format="json")
        return response

    def _check_tasks_count(self, project, expected_result):
        tasks_id = [task.id for task in project.tasks.all()]
        self.assertEqual(len(tasks_id), expected_result)

    def _check_xml(self, pid, user, expected_result):
        export_params = {
            "format": "CVAT for images 1.1"
        }
        response = self._export_project_annotations(user, pid, query_params=export_params)
        content = io.BytesIO(b"".join(response.streaming_content))
        content.seek(0)

        with tempfile.TemporaryDirectory() as tmp_dir:
            zipfile.ZipFile(content).extractall(tmp_dir)
            xml = os.path.join(tmp_dir, 'annotations.xml')
            self.assertTrue(xml)
            root = ET.parse(xml).getroot()
            tasks = root.findall('meta/project/tasks/task/name')
            self.assertEqual(len(tasks), expected_result)

    def test_api_v2_projects_remove_task_export(self):
        project = self.project
        pid = project.id
        user = self.admin

        self._check_tasks_count(project, 4)
        self._check_xml(pid, user, 4)

        tasks_id = [task.id for task in project.tasks.all()]
        response = self._run_api_v2_tasks_id_delete(tasks_id[0], self.admin)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)

        self._check_tasks_count(project, 3)
        self._check_xml(pid, user, 3)


class ProjectImportExportAPITestCase(ExportApiTestBase):
    def setUp(self) -> None:
        super().setUp()
        self.tasks = []
        self.projects = []

    @classmethod
    def setUpTestData(cls) -> None:
        create_db_users(cls)

        cls.media_data = [
            {
                **{
                   **{
                        f"client_files[{i}]": generate_random_image_file(f"test_{i}.jpg")[1]
                        for i in range(10)
                    },
                },
                **{
                    "image_quality": 75,
                },
            },
            {
                **{
                   **{
                        f"client_files[{i}]": generate_random_image_file(f"test_{i}.jpg")[1]
                        for i in range(10)
                    },
                },
                "image_quality": 75,
            },
        ]

    def _create_tasks(self):
        self.tasks = []

        def _create_task(task_data, media_data):
            response = self.client.post('/api/tasks', data=task_data, format="json")
            assert response.status_code == status.HTTP_201_CREATED
            tid = response.data["id"]

            for media in media_data.values():
                if isinstance(media, io.BytesIO):
                    media.seek(0)
            response = self.client.post("/api/tasks/{}/data".format(tid), data=media_data)
            assert response.status_code == status.HTTP_202_ACCEPTED
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/{}".format(tid))
            data_id = response.data["data"]
            self.tasks.append({
                "id": tid,
                "data_id": data_id,
            })

        task_data = [
            {
                "name": "my task #1",
                "owner_id": self.owner.id,
                "overlap": 0,
                "segment_size": 100,
                "project_id": self.projects[0]["id"],
            },
            {
                "name": "my task #2",
                "owner_id": self.owner.id,
                "overlap": 1,
                "segment_size": 3,
                "project_id": self.projects[0]["id"],
            },
        ]

        with ForceLogin(self.owner, self.client):
            for data, media in zip(task_data, self.media_data):
                _create_task(data, media)

    def _create_projects(self):
        self.projects = []

        def _create_project(project_data):
            response = self.client.post('/api/projects', data=project_data, format="json")
            assert response.status_code == status.HTTP_201_CREATED
            self.projects.append(response.data)

        project_data = [
            {
                "name": "Project for export",
                "owner_id": self.owner.id,
                "labels": [
                    {
                        "name": "car",
                        "color": "#ff00ff",
                        "attributes": [{
                            "name": "bool_attribute",
                            "mutable": True,
                            "input_type": AttributeType.CHECKBOX,
                            "default_value": "true",
                            "values": [],
                        }],
                    }, {
                        "name": "person",
                    },
                ]
            }, {
                "name": "Project for import",
                "owner_id": self.owner.id,
            },
        ]

        with ForceLogin(self.owner, self.client):
            for data in project_data:
                _create_project(data)

    def _run_api_v2_projects_id_dataset_import(self, pid, user, data, f):
        with ForceLogin(user, self.client):
            response = self.client.post("/api/projects/{}/dataset?format={}".format(pid, f),  data=data, format="multipart")
        return response

    def _run_api_v2_projects_id_dataset_import_status(self, pid, user, rq_id):
        with ForceLogin(user, self.client):
            response = self.client.get("/api/projects/{}/dataset?action=import_status&rq_id={}".format(pid, rq_id), format="json")
        return response

    def test_api_v2_projects_id_export_import(self):
        self._create_projects()
        self._create_tasks()
        pid_export, pid_import = self.projects[0]["id"], self.projects[1]["id"]

        export_params = {
            "format": "CVAT for images 1.1"
        }

        response = self._export_project_dataset(self.owner, pid_export, query_params=export_params)

        self.assertTrue(response.streaming)
        tmp_file = tempfile.NamedTemporaryFile(suffix=".zip")
        tmp_file.write(b"".join(response.streaming_content))
        tmp_file.seek(0)

        import_data = {
            "dataset_file": tmp_file,
        }

        response = self._run_api_v2_projects_id_dataset_import(pid_import, self.owner, import_data, "CVAT 1.1")
        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)

        rq_id = response.data.get('rq_id')
        response = self._run_api_v2_projects_id_dataset_import_status(pid_import, self.owner, rq_id)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def tearDown(self):
        for task in self.tasks:
            shutil.rmtree(os.path.join(settings.TASKS_ROOT, str(task["id"])))
            shutil.rmtree(os.path.join(settings.MEDIA_DATA_ROOT, str(task["data_id"])))
        for project in self.projects:
            shutil.rmtree(os.path.join(settings.PROJECTS_ROOT, str(project["id"])))
        return super().tearDown()

class TaskListAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _run_api_v2_tasks(self, user, params=""):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks{}'.format(params))

        return response

    def test_api_v2_tasks_admin(self):
        response = self._run_api_v2_tasks(self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([task.name for task in self.tasks]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_tasks_user(self):
        response = self._run_api_v2_tasks(self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual(
            sorted([task.name for task in self.tasks
                if self.user in [task.owner, task.assignee]]),
            sorted([res["name"] for res in response.data["results"]]))

    def test_api_v2_tasks_somebody(self):
        response = self._run_api_v2_tasks(self.somebody)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertListEqual([],
            [res["name"] for res in response.data["results"]])

    def test_api_v2_tasks_no_auth(self):
        response = self._run_api_v2_tasks(None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

class TaskGetAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _run_api_v2_tasks_id(self, tid, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks/{}'.format(tid))

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get(
                        "/api/labels?task_id=%s&page=%s" % (tid, page)
                    )
                ))
                response.data["labels"] = labels_response

        return response

    def _check_response(self, response, db_task):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(response.data["name"], db_task.name)
        self.assertEqual(response.data["size"], db_task.data.size)
        self.assertEqual(response.data["mode"], db_task.mode)
        owner = db_task.owner.id if db_task.owner else None
        response_owner = response.data["owner"]["id"] if response.data["owner"] else None
        self.assertEqual(response_owner, owner)
        assignee = db_task.assignee.id if db_task.assignee else None
        response_assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        self.assertEqual(response_assignee, assignee)
        self.assertEqual(response.data["overlap"], db_task.overlap)
        self.assertEqual(response.data["segment_size"], db_task.segment_size)
        self.assertEqual(response.data["image_quality"], db_task.data.image_quality)
        self.assertEqual(response.data["status"], db_task.status)
        self.assertListEqual(
            [label.name for label in db_task.label_set.all()],
            [label["name"] for label in response.data["labels"]]
        )

    def _check_api_v2_tasks_id(self, user):
        for db_task in self.tasks:
            response = self._run_api_v2_tasks_id(db_task.id, user)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_task.owner or user.is_superuser:
                self._check_response(response, db_task)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)


    def test_api_v2_tasks_id_admin(self):
        self._check_api_v2_tasks_id(self.admin)

    def test_api_v2_tasks_id_user(self):
        self._check_api_v2_tasks_id(self.user)

    def test_api_v2_tasks_id_somebody(self):
        self._check_api_v2_tasks_id(self.somebody)

    def test_api_v2_tasks_id_no_auth(self):
        self._check_api_v2_tasks_id(None)

class TaskDeleteAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _run_api_v2_tasks_id(self, tid, user):
        with ForceLogin(user, self.client):
            response = self.client.delete('/api/tasks/{}'.format(tid), format="json")

        return response

    def _check_api_v2_tasks_id(self, user):
        for db_task in self.tasks:
            response = self._run_api_v2_tasks_id(db_task.id, user)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_task.owner or user.is_superuser:
                self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)


    def test_api_v2_tasks_id_admin(self):
        self._check_api_v2_tasks_id(self.admin)

    def test_api_v2_tasks_id_user(self):
        self._check_api_v2_tasks_id(self.user)

    def test_api_v2_tasks_id_somebody(self):
        self._check_api_v2_tasks_id(self.somebody)

    def test_api_v2_tasks_id_no_auth(self):
        self._check_api_v2_tasks_id(None)

    def test_api_v2_tasks_delete_task_data_after_delete_task(self):
        for task in self.tasks:
            task_dir = task.get_dirname()
            self.assertTrue(os.path.exists(task_dir))

        with self.captureOnCommitCallbacks(execute=True):
            self._check_api_v2_tasks_id(self.admin)

        for task in self.tasks:
            task_dir = task.get_dirname()
            self.assertFalse(os.path.exists(task_dir))

class TaskUpdateAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _run_api_v2_tasks_id(self, tid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.put('/api/tasks/{}'.format(tid),
                data=data, format="json")

        return response

    def _check_api_v2_tasks_id(self, user, data):
        for db_task in self.tasks:
            response = self._run_api_v2_tasks_id(db_task.id, user, data)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            else:
                self.assertEqual(response.status_code, status.HTTP_405_METHOD_NOT_ALLOWED)

    def test_api_v2_tasks_id_admin(self):
        data = { "name": "new name for the task" }
        self._check_api_v2_tasks_id(self.admin, data)

    def test_api_v2_tasks_id_user(self):
        data = { "name": "new name for the task" }
        self._check_api_v2_tasks_id(self.user, data)

    def test_api_v2_tasks_id_somebody(self):
        data = { "name": "new name for the task" }
        self._check_api_v2_tasks_id(self.somebody, data)

    def test_api_v2_tasks_id_no_auth(self):
        data = { "name": "new name for the task" }
        self._check_api_v2_tasks_id(None, data)

class TaskPartialUpdateAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _check_response(self, response, db_task, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        name = data.get("name", db_task.name)
        self.assertEqual(response.data["name"], name)
        self.assertEqual(response.data["size"], db_task.data.size)
        mode = data.get("mode", db_task.mode)
        self.assertEqual(response.data["mode"], mode)
        owner = db_task.owner.id if db_task.owner else None
        owner = data.get("owner_id", owner)
        response_owner = response.data["owner"]["id"] if response.data["owner"] else None
        self.assertEqual(response_owner, owner)
        assignee = db_task.assignee.id if db_task.assignee else None
        assignee = data.get("assignee_id", assignee)
        response_assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        self.assertEqual(response_assignee, assignee)
        self.assertEqual(response.data["overlap"], db_task.overlap)
        self.assertEqual(response.data["segment_size"], db_task.segment_size)
        image_quality = data.get("image_quality", db_task.data.image_quality)
        self.assertEqual(response.data["image_quality"], image_quality)
        self.assertEqual(response.data["status"], db_task.status)
        if data.get("labels"):
            self.assertListEqual(
                [label["name"] for label in data.get("labels")],
                [label["name"] for label in response.data["labels"]]
            )
        else:
            self.assertListEqual(
                [label.name for label in db_task.label_set.all()],
                [label["name"] for label in response.data["labels"]]
            )

    def _run_api_v2_tasks_id(self, tid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/tasks/{}'.format(tid),
                data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                ))
                response.data["labels"] = labels_response

        return response

    def _check_api_v2_tasks_id(self, user, data):
        for db_task in self.tasks:
            response = self._run_api_v2_tasks_id(db_task.id, user, data)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_task.owner or user == db_task.assignee or user.is_superuser:
                self._check_response(response, db_task, data)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_tasks_id_admin(self):
        data = {
            "name": "new name for the task",
            "owner_id": self.owner.id,
            "labels": [{
                "name": "non-vehicle",
                "attributes": [{
                    "name": "my_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }]
            }]
        }
        self._check_api_v2_tasks_id(self.admin, data)

    def test_api_v2_tasks_id_user(self):
        data = {
            "name": "new name for the task",
            "owner_id": self.user.id,
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }]
        }
        self._check_api_v2_tasks_id(self.user, data)

    def test_api_v2_tasks_id_admin_partial(self):
        data = {
            "name": "new name for the task #2",
        }
        self._check_api_v2_tasks_id(self.admin, data)

        data = {
            "name": "new name for the task",
            "owner_id": self.owner.id
        }
        self._check_api_v2_tasks_id(self.admin, data)
        # Now owner is updated, but self.db_tasks are obsolete
        # We can't do any tests without owner in data below


    def test_api_v2_tasks_id_user_partial(self):
        data = {
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }]
        }
        self._check_api_v2_tasks_id(self.user, data)

        data = {
            "owner_id": self.user.id,
            "assignee_id": self.user.id
        }
        self._check_api_v2_tasks_id(self.user, data)


    def test_api_v2_tasks_id_somebody(self):
        data = {
            "name": "my task #3"
        }
        self._check_api_v2_tasks_id(self.somebody, data)

    def test_api_v2_tasks_id_no_auth(self):
        data = {
            "name": "new name for the task",
            "labels": [{
                "name": "test",
            }]
        }
        self._check_api_v2_tasks_id(None, data)

    def test_api_v2_tasks_id_unknown_field(self):
        data = {"foo": "bar"}
        response = self._run_api_v2_tasks_id(self.tasks[0].id, self.admin, data)
        self.assertEquals(response.status_code, status.HTTP_403_FORBIDDEN, response)

class TaskDataMetaPartialUpdateAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)
        cls.tasks = create_dummy_db_tasks(cls)

    def _run_api_v1_task_data_meta_id(self, tid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/tasks/{}/data/meta'.format(tid),
                data=data, format="json")

        return response

    def _check_response(self, response, db_data, data):
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        deleted_frames = data.get("deleted_frames", db_data.deleted_frames)
        self.assertEqual(response.data["deleted_frames"], deleted_frames)

    def _check_api_v1_task_data_id(self, user, data):
        for db_task in self.tasks:
            response = self._run_api_v1_task_data_meta_id(db_task.id, user, data)
            if user is None:
                self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
            elif user == db_task.owner or user == db_task.assignee or user.is_superuser:
                self._check_response(response, db_task.data, data)
            else:
                self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v1_tasks_data_meta(self):
        data = {
            "deleted_frames": [1,2,3]
        }
        self._check_api_v1_task_data_id(self.user, data)

        data = {
            "deleted_frames": []
        }
        self._check_api_v1_task_data_id(self.user, data)

class TaskUpdateLabelsAPITestCase(UpdateLabelsAPITestCase):
    @classmethod
    def setUpTestData(cls):
        task_data = {
            "name": "Project with labels",
            "bug_tracker": "https://new.bug.tracker",
            "overlap": 0,
            "segment_size": 100,
            "image_quality": 75,
            "size": 100,
            "labels": [{
                "name": "car",
                "color": "#ff00ff",
                "attributes": [{
                    "name": "bool_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true"
                }],
            }, {
                "name": "person",
            }]
        }

        create_db_users(cls)
        db_task = create_db_task(task_data)
        cls.task = db_task

    def _check_api_v2_task(self, data):
        response = self._run_api_v2_task_id(self.task.id, self.admin, data)
        self._check_response(response, self.task, data)

    def _run_api_v2_task_id(self, tid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.patch('/api/tasks/{}'.format(tid),
                data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                ))
                response.data["labels"] = labels_response

        return response

    def test_api_v2_tasks_create_label(self):
        data = {
            "labels": [{
                "name": "new label",
            }],
        }
        self._check_api_v2_task(data)

    def test_api_v2_tasks_edit_label(self):
        data = {
            "labels": [{
                "id": 1,
                "name": "New name for label",
                "color": "#fefefe",
            }],
        }
        self._check_api_v2_task(data)

    def test_api_v2_tasks_delete_label(self):
        data = {
            "labels": [{
                "id": 2,
                "name": "Label for deletion",
                "deleted": True
            }]
        }
        self._check_api_v2_task(data)

class TaskMoveAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        self._run_api_v2_job_id_annotation(self.task.segment_set.first().job_set.first().id, self.annotation_data)

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

        projects = []

        project_data = {
            "name": "Project for task move 1",
            "owner": cls.admin,
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }, {
                "name": "person"
            }]
        }
        db_project = create_db_project(project_data)
        projects.append(db_project)

        project_data = {
            "name": "Project for task move 2",
            "owner": cls.admin,
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }, {
                "name": "test"
            }, {
                "name": "other.label"
            }]
        }

        db_project = create_db_project(project_data)
        projects.append(db_project)

        cls.projects = projects

        task_data = {
            "name": "Task for moving",
            "owner": cls.admin,
            "overlap": 0,
            "segment_size": 100,
            "image_quality": 75,
            "size": 100,
            "project": None,
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }]
        }
        db_task = create_db_task(task_data)
        cls.task = db_task

        cls.annotation_data = {
            "version": 1,
            "tags": [
                {
                    "frame": 0,
                    "label_id": cls.task.label_set.first().id,
                    "group": None,
                    "source": "manual",
                    "attributes": []
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": cls.task.label_set.first().id,
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": cls.task.label_set.first().attributespec_set.first().id,
                            "value": cls.task.label_set.first().attributespec_set.first().values.split('\'')[1]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False
                }
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": cls.task.label_set.first().id,
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": cls.task.label_set.first().attributespec_set.first().id,
                            "value": cls.task.label_set.first().attributespec_set.first().values.split('\'')[1]
                        }
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False
                        },
                        {
                            "frame": 2,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True
                        },
                    ]
                }
            ]
        }

    def _run_api_v2_tasks_id(self, tid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.patch('/api/tasks/{}'.format(tid),
                data=data, format="json")

        return response

    def _run_api_v2_job_id_annotation(self, jid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.patch('/api/jobs/{}/annotations?action=create'.format(jid),
                data=data, format="json")

        return response

    def _check_response(self, response, data):
        self.assertEqual(response.data["project_id"], data["project_id"])

    def _check_api_v2_tasks(self, tid, data, expected_status=status.HTTP_200_OK):
        response = self._run_api_v2_tasks_id(tid, data)
        self.assertEqual(response.status_code, expected_status)
        if expected_status == status.HTTP_200_OK:
            self._check_response(response, data)

    def test_move_task_bad_request(self):
        # Try to move task without proper label mapping
        data = {
            "project_id": self.projects[0].id,
            "labels": [{
                "id": self.task.label_set.first().id,
                "name": "some.other.label"
            }]
        }
        self._check_api_v2_tasks(self.task.id, data, status.HTTP_400_BAD_REQUEST)

    def test_move_task(self):
        # Try to move single task to the project
        data = {
            "project_id": self.projects[0].id
        }
        self._check_api_v2_tasks(self.task.id, data)

        # Try to move task from project to the other project
        data = {
            "project_id": self.projects[1].id,
            "labels": [{
                "id": self.projects[0].label_set.all()[1].id,
                "name": "test"
            }]
        }
        self._check_api_v2_tasks(self.task.id, data)

class TaskCreateAPITestCase(ApiTestBase):
    def setUp(self):
        super().setUp()
        project = {
            "name": "Project for task creation",
            "owner": self.user,
        }
        self.project = Project.objects.create(**project)
        label = {
            "name": "car",
            "project": self.project
        }
        Label.objects.create(**label)

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _run_api_v2_tasks(self, user, data):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get(
                        "/api/labels?task_id=%s&page=%s" % (response.data["id"], page)
                    )
                ))
                response.data["labels"] = labels_response

        return response

    def _check_response(self, response, user, data):
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        self.assertEqual(response.data["name"], data["name"])
        self.assertEqual(response.data["mode"], "")
        self.assertEqual(response.data["project_id"], data.get("project_id", None))
        self.assertEqual(response.data["owner"]["id"], data.get("owner_id", user.id))
        assignee = response.data["assignee"]["id"] if response.data["assignee"] else None
        self.assertEqual(assignee, data.get("assignee_id", None))
        self.assertEqual(response.data["bug_tracker"], data.get("bug_tracker", ""))
        self.assertEqual(response.data["overlap"], data.get("overlap", None))
        self.assertEqual(response.data["segment_size"], data.get("segment_size", 0))
        self.assertEqual(response.data["status"], StatusChoice.ANNOTATION)
        self.assertListEqual(
            [label["name"] for label in data.get("labels")],
            [label["name"] for label in response.data["labels"]]
        )

    def _check_api_v2_tasks(self, user, data):
        response = self._run_api_v2_tasks(user, data)
        if user is None:
            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)
        else:
            self._check_response(response, user, data)

    def test_api_v2_tasks_admin(self):
        data = {
            "name": "new name for the task",
            "labels": [{
                "name": "non-vehicle",
                "attributes": [{
                    "name": "my_attribute",
                    "mutable": True,
                    "input_type": AttributeType.CHECKBOX,
                    "default_value": "true",
                    "values": [],
                }]
            }]
        }
        self._check_api_v2_tasks(self.admin, data)

    def test_api_v2_tasks_user(self):
        data = {
            "name": "new name for the task",
            "owner_id": self.user.id,
            "labels": [{
                "name": "car",
                "attributes": [{
                    "name": "color",
                    "mutable": False,
                    "input_type": AttributeType.SELECT,
                    "default_value": "white",
                    "values": ["white", "yellow", "green", "red"]
                }]
            }]
        }
        self._check_api_v2_tasks(self.user, data)

    def test_api_vi_tasks_user_project(self):
        data = {
            "name": "new name for the task",
            "project_id": self.project.id,
        }
        response = self._run_api_v2_tasks(self.user, data)
        data["labels"] = [{
            "name": "car"
        }]
        self._check_response(response, self.user, data)

    def test_api_v2_tasks_somebody(self):
        data = {
            "name": "new name for the task",
            "labels": [{
                "name": "test",
            }]
        }
        self._check_api_v2_tasks(self.somebody, data)

    def test_api_v2_tasks_no_auth(self):
        data = {
            "name": "new name for the task",
            "labels": [{
                "name": "test",
            }]
        }
        self._check_api_v2_tasks(None, data)

class TaskImportExportAPITestCase(ExportApiTestBase):
    def setUp(self):
        super().setUp()
        self.tasks = []

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

        cls.media_data = []

        image_count = 10
        imagename_pattern = "test_{}.jpg"
        for i in range(image_count):
            filename = imagename_pattern.format(i)
            path = os.path.join(settings.SHARE_ROOT, filename)
            _, data = generate_random_image_file(filename)
            with open(path, "wb") as image:
                image.write(data.read())

        data = {
            "image_quality": 75,
            "copy_data": True,
            "start_frame": 2,
            "stop_frame": 9,
            "frame_filter": "step=2",
            **{"server_files[{}]".format(i): imagename_pattern.format(i) for i in range(image_count)},
        }
        use_cache_data = {
            **data,
            'use_cache': True,
        }
        cls.media_data.append(data)

        data['sorting_method'] = SortingMethod.NATURAL
        cls.media_data.append(data)
        cls.media_data.append(use_cache_data)

        use_cache_data['sorting_method'] = SortingMethod.NATURAL
        cls.media_data.append(use_cache_data)

        use_cache_data['sorting_method'] = SortingMethod.RANDOM
        cls.media_data.append(use_cache_data)

        filename = "test_video_1.mp4"
        path = os.path.join(settings.SHARE_ROOT, filename)
        _, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())
        cls.media_data.append(
            {
                "image_quality": 75,
                "copy_data": True,
                "start_frame": 2,
                "stop_frame": 24,
                "frame_filter": "step=2",
                "server_files[0]": filename,
            }
        )

        filename = os.path.join("test_archive_1.zip")
        path = os.path.join(settings.SHARE_ROOT, filename)
        _, data = generate_zip_archive_file(filename, count=5)
        with open(path, "wb") as zip_archive:
            zip_archive.write(data.read())
        cls.media_data.append(
            {
                "image_quality": 75,
                "server_files[0]": filename,
            }
        )

        filename = "test_pointcloud_pcd.zip"
        source_path = os.path.join(os.path.dirname(__file__), 'assets', filename)
        path = os.path.join(settings.SHARE_ROOT, filename)
        shutil.copyfile(source_path, path)
        cls.media_data.append(
            {
                "image_quality": 75,
                "server_files[0]": filename,
            }
        )

        filename = "test_velodyne_points.zip"
        source_path = os.path.join(os.path.dirname(__file__), 'assets', filename)
        path = os.path.join(settings.SHARE_ROOT, filename)
        shutil.copyfile(source_path, path)
        cls.media_data.append(
            {
                "image_quality": 75,
                "server_files[0]": filename,
            }
        )

        for sorting, _ in SortingMethod.choices():
            cls.media_data.append(
                {
                    "image_quality": 75,
                    "server_files[0]": filename,
                    'use_cache': True,
                    'sorting_method': sorting,
                }
            )

        filename = os.path.join("videos", "test_video_1.mp4")
        path = os.path.join(settings.SHARE_ROOT, filename)
        os.makedirs(os.path.dirname(path))
        _, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())

        generate_manifest_file(data_type='video', manifest_path=os.path.join(settings.SHARE_ROOT, 'videos', 'manifest.jsonl'),
            sources=[path])

        cls.media_data.append(
            {
                "image_quality": 70,
                "copy_data": True,
                "server_files[0]": filename,
                "server_files[1]": os.path.join("videos", "manifest.jsonl"),
                "use_cache": True,
            }
        )

        generate_manifest_file(data_type='images', manifest_path=os.path.join(settings.SHARE_ROOT, 'manifest.jsonl'),
            sources=[os.path.join(settings.SHARE_ROOT, imagename_pattern.format(i)) for i in range(1, 8)])
        cls.media_data.append(
            {
                **{"image_quality": 70,
                    "copy_data": True,
                    "use_cache": True,
                    "frame_filter": "step=2",
                    "server_files[0]": "manifest.jsonl",
                },
                **{
                    **{"server_files[{}]".format(i): imagename_pattern.format(i) for i in range(1, 8)},
                }
            }
        )

        data = {
            "client_files[0]": generate_random_image_file("test_1.jpg")[1],
            "client_files[1]": generate_random_image_file("test_2.jpg")[1],
            "client_files[2]": generate_random_image_file("test_10.jpg")[1],
            "client_files[3]": generate_random_image_file("test_3.jpg")[1],
            "image_quality": 75,
        }
        use_cache_data = {
            **data,
            'use_cache': True,
        }
        cls.media_data.extend([
            # image list local
            # sorted data
            # natural: test_1.jpg, test_2.jpg, test_3.jpg, test_10.jpg
            {
                **use_cache_data,
                'sorting_method': SortingMethod.NATURAL,
            },
            {
                **data,
                'sorting_method': SortingMethod.NATURAL,
            },
            # random
            {
                **use_cache_data,
                'sorting_method': SortingMethod.RANDOM,
            },
            # predefined: test_1.jpg, test_2.jpg, test_10.jpg, test_3.jpg
            {
                **use_cache_data,
                'sorting_method': SortingMethod.PREDEFINED,
            },
            # lexicographical: test_1.jpg, test_10.jpg, test_2.jpg, test_3.jpg
            {
                **use_cache_data,
                'sorting_method': SortingMethod.LEXICOGRAPHICAL,
            },
            {
                **data,
                'sorting_method': SortingMethod.LEXICOGRAPHICAL,
            },
            # video local
            {
                "client_files[0]": generate_video_file("test_video.mp4")[1],
                "image_quality": 75,
            },
            # zip archive local
            {
                "client_files[0]": generate_zip_archive_file("test_archive_1.zip", 10)[1],
                "image_quality": 50,
            },
            # pdf local
            {
                "client_files[0]": generate_pdf_file("test_pdf_1.pdf", 7)[1],
                "image_quality": 54,
            },
        ])

    def tearDown(self):
        for task in self.tasks:
            shutil.rmtree(os.path.join(settings.TASKS_ROOT, str(task["id"])))
            shutil.rmtree(os.path.join(settings.MEDIA_DATA_ROOT, str(task["data_id"])))

        return super().tearDown()

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        path = os.path.join(settings.SHARE_ROOT, "test_1.jpg")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "test_2.jpg")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "test_3.jpg")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "test_video_1.mp4")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "videos", "test_video_1.mp4")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "videos", "manifest.jsonl")
        os.remove(path)
        os.rmdir(os.path.dirname(path))

        path = os.path.join(settings.SHARE_ROOT, "test_pointcloud_pcd.zip")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "test_velodyne_points.zip")
        os.remove(path)

        path = os.path.join(settings.SHARE_ROOT, "manifest.jsonl")
        os.remove(path)

    def _create_tasks(self):
        self.tasks = []

        def _create_task(task_data, media_data):
            response = self.client.post('/api/tasks', data=task_data, format="json")
            assert response.status_code == status.HTTP_201_CREATED
            tid = response.data["id"]

            for media in media_data.values():
                if isinstance(media, io.BytesIO):
                    media.seek(0)
            response = self.client.post("/api/tasks/{}/data".format(tid), data=media_data)
            assert response.status_code == status.HTTP_202_ACCEPTED
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/{}".format(tid))
            data_id = response.data["data"]
            self.tasks.append({
                "id": tid,
                "data_id": data_id,
            })

        task_data = [
            {
                "name": "my task #1",
                "owner_id": self.owner.id,
                "assignee_id": self.owner.id,
                "overlap": 0,
                "segment_size": 100,
                "labels": [{
                    "name": "car",
                    "color": "#ff00ff",
                    "attributes": [{
                        "name": "bool_attribute",
                        "mutable": True,
                        "input_type": AttributeType.CHECKBOX,
                        "default_value": "true",
                        "values": [],
                    }],
                    }, {
                        "name": "person",
                    },
                ]
            },
            {
                "name": "my task #2",
                "owner_id": self.owner.id,
                "assignee_id": self.owner.id,
                "overlap": 1,
                "segment_size": 3,
                "labels": [{
                    "name": "car",
                    "color": "#ff00ff",
                    "attributes": [{
                        "name": "bool_attribute",
                        "mutable": True,
                        "input_type": AttributeType.CHECKBOX,
                        "default_value": "true",
                        "values": [],
                    }],
                    }, {
                        "name": "person",
                    },
                ]
            },
        ]

        with ForceLogin(self.owner, self.client):
            for data in task_data:
                for media in self.media_data:
                    _create_task(data, media)

    def _run_api_v2_tasks_id_import(self, user, data):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/tasks/backup', data=data, format="multipart")

        return response

    def _run_api_v2_tasks_id(self, tid, user):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks/{}'.format(tid), format="json")

        return response.data

    def _run_api_v2_tasks_id_export_import(self, user):
        if user:
            expected_4xx_status_code = None if (user == self.owner or user.is_superuser) else status.HTTP_403_FORBIDDEN
        else:
            expected_4xx_status_code = status.HTTP_401_UNAUTHORIZED

        self._create_tasks()
        for task in self.tasks:
            tid = task["id"]
            response = self._export_task_backup(
                user, tid,
                expected_4xx_status_code=expected_4xx_status_code
            )

            if user and user is not self.somebody and user is not self.user and user is not self.annotator:
                self.assertTrue(response.streaming)
                content = io.BytesIO(b"".join(response.streaming_content))
                content.seek(0)

                uploaded_data = {
                    "task_file": content,
                }
                response = self._run_api_v2_tasks_id_import(user, uploaded_data)
                self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_202_ACCEPTED)
                if user is not self.somebody and user is not self.user and user is not self.annotator:
                    rq_id = response.data["rq_id"]
                    response = self._run_api_v2_tasks_id_import(user, {"rq_id": rq_id})
                    self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_201_CREATED)
                    original_task = self._run_api_v2_tasks_id(tid, user)
                    imported_task = self._run_api_v2_tasks_id(response.data["id"], user)
                    compare_objects(
                        self=self,
                        obj1=original_task,
                        obj2=imported_task,
                        ignore_keys=(
                            "id",
                            "url",
                            "owner",
                            "project_id",
                            "assignee",
                            "assignee_updated_date",
                            "created_date",
                            "updated_date",
                            "data",
                            "source_storage",
                            "target_storage",
                            "jobs",
                        ),
                    )

    def test_api_v2_tasks_id_export_admin(self):
        self._run_api_v2_tasks_id_export_import(self.admin)

    def test_api_v2_tasks_id_export_user(self):
        self._run_api_v2_tasks_id_export_import(self.user)

    def test_api_v2_tasks_id_export_annotator(self):
        self._run_api_v2_tasks_id_export_import(self.annotator)

    def test_api_v2_tasks_id_export_somebody(self):
        self._run_api_v2_tasks_id_export_import(self.somebody)

    def test_api_v2_tasks_id_export_no_auth(self):
        self._run_api_v2_tasks_id_export_import(None)

    def test_can_remove_export_cache_automatically_after_successful_export(self):
        from cvat.apps.dataset_manager.cron import (
            cleanup_export_cache_directory,
            clear_export_cache,
        )
        self._create_tasks()
        task_id = self.tasks[0]["id"]
        user = self.admin

        TASK_CACHE_TTL = timedelta(hours=1)
        with (
            mock.patch('cvat.apps.dataset_manager.views.TASK_CACHE_TTL', new=TASK_CACHE_TTL),
            mock.patch('cvat.apps.dataset_manager.views.TTL_CONSTS', new={'task': TASK_CACHE_TTL}),
            mock.patch(
                "cvat.apps.dataset_manager.cron.clear_export_cache",
                side_effect=clear_export_cache,
            ) as mock_clear_export_cache,
        ):
            cleanup_export_cache_directory()
            mock_clear_export_cache.assert_not_called()

            self._export_task_backup(user, task_id, download_locally=False)

            queue: RQQueue = django_rq.get_queue(settings.CVAT_QUEUES.EXPORT_DATA.value)
            rq_job_ids = queue.finished_job_registry.get_job_ids()
            self.assertEqual(len(rq_job_ids), 1)
            job: RQJob | None = queue.fetch_job(rq_job_ids[0])
            self.assertFalse(job is None)
            file_path = job.return_value()
            self.assertTrue(os.path.isfile(file_path))

            with (
                mock.patch('cvat.apps.dataset_manager.views.TASK_CACHE_TTL', new=timedelta(seconds=0)),
                mock.patch('cvat.apps.dataset_manager.views.TTL_CONSTS', new={'task': timedelta(seconds=0)}),
            ):
                cleanup_export_cache_directory()
                mock_clear_export_cache.assert_called_once()
            self.assertFalse(os.path.exists(file_path))


def generate_random_image_file(filename):
    gen = random.SystemRandom()
    width = gen.randint(100, 800)
    height = gen.randint(100, 800)
    f = generate_image_file(filename, size=(width, height))
    return (width, height), f

def generate_random_image_files(*filenames):
    images = []
    image_sizes = []
    for image_name in filenames:
        img_size, image = generate_random_image_file(image_name)
        image_sizes.append(img_size)
        images.append(image)

    return image_sizes, images

def generate_zip_archive_file(filename, count):
    image_sizes = []
    zip_buf = BytesIO()
    with zipfile.ZipFile(zip_buf, 'w') as zip_chunk:
        for idx in range(count):
            image_name = "image_{:6d}.jpg".format(idx)
            size, image_buf = generate_random_image_file(image_name)
            image_sizes.append(size)
            zip_chunk.writestr(image_name, image_buf.getvalue())

    zip_buf.name = filename
    zip_buf.seek(0)
    return image_sizes, zip_buf

def generate_pdf_file(filename, page_count=1):
    images = [Image.fromarray(np.ones((50, 100, 3), dtype=np.uint8))
        for _ in range(page_count)]
    image_sizes = [img.size for img in images]

    file_buf = BytesIO()
    images[0].save(file_buf, 'pdf', save_all=True, resolution=200,
        append_images=images[1:])

    file_buf.name = filename
    file_buf.seek(0)
    return image_sizes, file_buf

def generate_manifest_file(data_type, manifest_path, sources, *,
    sorting_method=SortingMethod.LEXICOGRAPHICAL,
    root_dir=None,
):
    if data_type == 'video':
        kwargs = {
            'media_file': sources[0],
            'upload_dir': os.path.dirname(sources[0]),
            'force': True
        }
        manifest = VideoManifestManager(manifest_path, create_index=False)
    else:
        kwargs = {
            'sources': sources,
            'sorting_method': sorting_method,
            'use_image_hash': True,
            'data_dir': root_dir,
        }
        manifest = ImageManifestManager(manifest_path, create_index=False)
    manifest.link(**kwargs)
    manifest.create()


def get_manifest_images_list(manifest_path):
    return list(ImageManifestManager(manifest_path, create_index=False).data)


class TaskDataAPITestCase(ApiTestBase):
    _share_image_sizes = {}
    _client_images = {}
    _client_mp4_video = {}
    _client_archive = {}
    _client_pdf = {}
    _client_mxf_video = {}

    class ChunkType(str, Enum):
        IMAGESET = 'imageset'
        VIDEO = 'video'

        def __str__(self):
            return self.value

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    @classmethod
    def setUpClass(cls):
        super().setUpClass()

        cls._share_image_sizes = {}
        cls._share_files = []

        for filename in [
            "test_1.jpg", "test_2.jpg", "test_3.jpg", "test_10.jpg", "test_qwe.jpg",
            "subdir2/subdir3/test_zxc.jpg", "data/test_3.jpg"
        ]:
            path = os.path.join(settings.SHARE_ROOT, filename)
            img_size, data = generate_random_image_file(filename)
            os.makedirs(os.path.dirname(path), exist_ok=True)
            with open(path, "wb") as image:
                image.write(data.read())
            cls._share_image_sizes[filename] = img_size
            cls._share_files.append(filename)

        filename = "test_video_1.mp4"
        path = os.path.join(settings.SHARE_ROOT, filename)
        img_sizes, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())
        cls._share_image_sizes[filename] = img_sizes
        cls._share_files.append(filename)

        filename = "test_rotated_90_video.mp4"
        path = os.path.join(os.path.dirname(__file__), 'assets', 'test_rotated_90_video.mp4')
        container = av.open(path, 'r')
        for frame in container.decode(video=0):
            # pyav ignores rotation record in metadata when decoding frames
            img_sizes = [(frame.height, frame.width)] * container.streams.video[0].frames
            break
        container.close()
        cls._share_image_sizes[filename] = img_sizes

        filename = os.path.join("videos", "test_video_1.mp4")
        path = os.path.join(settings.SHARE_ROOT, filename)
        os.makedirs(os.path.dirname(path))
        img_sizes, data = generate_video_file(filename, width=1280, height=720)
        with open(path, "wb") as video:
            video.write(data.read())
        cls._share_image_sizes[filename] = img_sizes
        cls._share_files.append(filename)

        filename = os.path.join("test_archive_1.zip")
        path = os.path.join(settings.SHARE_ROOT, filename)
        img_sizes, data = generate_zip_archive_file(filename, count=5)
        with open(path, "wb") as zip_archive:
            zip_archive.write(data.read())
        cls._share_image_sizes[filename] = img_sizes
        cls._share_files.append(filename)

        filename = "test_pointcloud_pcd.zip"
        path = os.path.join(os.path.dirname(__file__), 'assets', filename)
        image_sizes = []
        # container = av.open(path, 'r')
        zip_file = zipfile.ZipFile(path)
        for info in zip_file.namelist():
            if info.rsplit(".", maxsplit=1)[-1] == "pcd":
                with zip_file.open(info, "r") as file:
                    data = ValidateDimension.get_pcd_properties(file)
                    image_sizes.append((int(data["WIDTH"]), int(data["HEIGHT"])))
        cls._share_image_sizes[filename] = image_sizes

        filename = "test_rar.rar"
        source_path = os.path.join(os.path.dirname(__file__), 'assets', filename)
        path = os.path.join(settings.SHARE_ROOT, filename)
        shutil.copyfile(source_path, path)
        image_sizes = []
        images = cls._extract_rar_archive(source_path)
        for [f, image] in images:
            width, height = image.size
            image_sizes.append((width, height))
        cls._share_image_sizes[filename] = image_sizes
        cls._share_files.append(filename)

        filename = "test_velodyne_points.zip"
        path = os.path.join(os.path.dirname(__file__), 'assets', filename)
        image_sizes = []

        # create zip instance
        zip_file = zipfile.ZipFile(path, mode='a')

        source_path = []
        root_path = os.path.abspath(os.path.split(path)[0])

        for info in zip_file.namelist():
            if os.path.splitext(info)[1][1:] == "bin":
                zip_file.extract(info, root_path)
                bin_path = os.path.abspath(os.path.join(root_path, info))
                source_path.append(ValidateDimension.convert_bin_to_pcd(bin_path))

        for path in source_path:
            zip_file.write(path, os.path.abspath(path.replace(root_path, "")))

        for info in zip_file.namelist():
            if os.path.splitext(info)[1][1:] == "pcd":
                with zip_file.open(info, "r") as file:
                    data = ValidateDimension.get_pcd_properties(file)
                    image_sizes.append((int(data["WIDTH"]), int(data["HEIGHT"])))

        root_path = os.path.abspath(os.path.join(root_path, filename.split(".")[0]))
        shutil.rmtree(root_path, ignore_errors=True)

        cls._share_image_sizes[filename] = image_sizes

        filename = 'test_1.pdf'
        path = os.path.join(settings.SHARE_ROOT, filename)
        img_sizes, data = generate_pdf_file(filename, page_count=5)
        with open(path, "wb") as pdf_file:
            pdf_file.write(data.read())
        cls._share_image_sizes[filename] = img_sizes
        cls._share_files.append(filename)

        filename = 'videos/manifest.jsonl'
        generate_manifest_file(data_type='video',
            manifest_path=os.path.join(settings.SHARE_ROOT, filename),
            sources=[os.path.join(settings.SHARE_ROOT, 'videos', 'test_video_1.mp4')])
        cls._share_files.append(filename)

        image_files = [
            'test_1.jpg', "subdir2/subdir3/test_zxc.jpg", 'test_qwe.jpg',
            'test_3.jpg', 'test_10.jpg', 'data/test_3.jpg', 'test_2.jpg',
        ]
        for ordered in [True, False]:
            filename = 'images_manifest{}.jsonl'.format(
                "_sorted" if ordered else ""
            )
            generate_manifest_file(data_type='images',
                manifest_path=os.path.join(settings.SHARE_ROOT, filename),
                sources=[os.path.join(settings.SHARE_ROOT, fn) for fn in image_files],
                sorting_method=SortingMethod.LEXICOGRAPHICAL if ordered else SortingMethod.PREDEFINED,
                root_dir=settings.SHARE_ROOT,
            )
            cls._share_files.append(filename)

        filename = "test_archive_2.zip"
        with zipfile.ZipFile(os.path.join(settings.SHARE_ROOT, filename), 'x') as f:
            for fn in image_files:
                f.write(os.path.join(settings.SHARE_ROOT, fn), fn)
        cls._share_files.append(filename)

        filename = "test_archive_2_sorted.zip"
        with zipfile.ZipFile(os.path.join(settings.SHARE_ROOT, filename), 'x') as f:
            for fn in sorted(image_files):
                f.write(os.path.join(settings.SHARE_ROOT, fn), fn)
        cls._share_files.append(filename)

        image_sizes, images = generate_random_image_files("test_1.jpg", "test_2.jpg", "test_3.jpg")
        cls._client_images = {
            'images': images,
            'image_sizes': image_sizes,
        }

        image_sizes, video = generate_video_file(filename="test_video_1.mp4", width=1280, height=720)
        cls._client_mp4_video = {
            'video': video,
            'image_sizes': image_sizes,
        }

        image_sizes, archive = generate_zip_archive_file("test_archive_2.zip", 7)
        cls._client_archive = {
            'archive': archive,
            'image_sizes': image_sizes
        }

        image_sizes, document = generate_pdf_file("test_pdf_1.pdf", 5)
        cls._client_pdf = {
            'pdf': document,
            'image_sizes': image_sizes
        }

        image_sizes, video = generate_video_file(filename="test_video_1.mxf",
            width=1280, height=720, codec_name='mpeg2video')
        cls._client_mxf_video = {
            'video': video,
            'image_sizes': image_sizes,
        }

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()

        dirs = set()
        for filename in cls._share_files:
            dirs.add(os.path.dirname(filename))
            os.remove(os.path.join(settings.SHARE_ROOT, filename))

        for dirname in sorted(dirs, reverse=True):
            path = os.path.join(settings.SHARE_ROOT, dirname)
            if not os.listdir(path):
                os.rmdir(path)

    def _run_api_v2_tasks_id_data_post(self, tid, user, data, *, headers=None):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/tasks/{}/data'.format(tid),
                data=data, **{'HTTP_' + k: v for k, v in (headers or {}).items()})

        return response

    def _get_task_creation_status(self, tid, user, *, headers=None):
        with ForceLogin(user, self.client):
            response = self.client.get('/api/tasks/{}/status'.format(tid),
                **{'HTTP_' + k: v for k, v in (headers or {}).items()})

        return response

    def _create_task(self, user, data):
        with ForceLogin(user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
        return response

    def _get_task(self, user, tid):
        with ForceLogin(user, self.client):
            return self.client.get("/api/tasks/{}".format(tid))

    def _run_api_v2_task_id_data_get(self, tid, user, data_type, data_quality=None, data_number=None):
        url = '/api/tasks/{}/data?type={}'.format(tid, data_type)
        if data_quality is not None:
            url += '&quality={}'.format(data_quality)
        if data_number is not None:
            url += '&number={}'.format(data_number)
        with ForceLogin(user, self.client):
            return self.client.get(url)

    def _get_preview(self, tid, user):
        url = '/api/tasks/{}/preview'.format(tid)
        with ForceLogin(user, self.client):
            return self.client.get(url)

    def _get_compressed_chunk(self, tid, user, number):
        return self._run_api_v2_task_id_data_get(tid, user, "chunk", "compressed", number)

    def _get_original_chunk(self, tid, user, number):
        return self._run_api_v2_task_id_data_get(tid, user, "chunk", "original", number)

    def _get_compressed_frame(self, tid, user, number):
        return self._run_api_v2_task_id_data_get(tid, user, "frame", "compressed", number)

    def _get_original_frame(self, tid, user, number):
        return self._run_api_v2_task_id_data_get(tid, user, "frame", "original", number)

    @staticmethod
    def _extract_zip_archive(archive, dimension=DimensionType.DIM_2D):
        chunk = zipfile.ZipFile(archive, mode='r')
        if dimension == DimensionType.DIM_3D:
            return [(f, BytesIO(chunk.read(f)))
                for f in sorted(chunk.namelist())
                if f.rsplit(".", maxsplit=1)[-1] == "pcd"
            ]
        return [(f, Image.open(BytesIO(chunk.read(f))))
            for f in sorted(chunk.namelist())
        ]

    @staticmethod
    def _extract_rar_archive(archive):
        with tempfile.TemporaryDirectory(dir=settings.TMP_FILES_ROOT) as archive_dir:
            patool_path = os.path.join(sysconfig.get_path('scripts'), 'patool')
            Archive(archive).extractall_patool(archive_dir, patool_path)

            images = [(image, Image.open(os.path.join(archive_dir, image)))
                for image in os.listdir(archive_dir)
            ]
            return images

    @classmethod
    def _extract_zip_chunk(cls, chunk_buffer, dimension=DimensionType.DIM_2D):
        return [f[1] for f in cls._extract_zip_archive(chunk_buffer, dimension=dimension)]

    @staticmethod
    def _extract_video_chunk(chunk_buffer):
        container = av.open(chunk_buffer)
        stream = container.streams.video[0]
        return [f.to_image() for f in container.decode(stream)]

    def _test_api_v2_tasks_id_data_spec(self, user, spec, data,
                                        expected_compressed_type,
                                        expected_original_type,
                                        expected_image_sizes,
                                        expected_storage_method=None,
                                        expected_uploaded_data_location=StorageChoice.LOCAL,
                                        dimension=DimensionType.DIM_2D,
                                        expected_task_creation_status_state='Finished',
                                        expected_task_creation_status_reason=None,
                                        *,
                                        send_data_callback=None,
                                        get_status_callback=None,
                                        ):
        if send_data_callback is None:
            send_data_callback = self._run_api_v2_tasks_id_data_post

        if get_status_callback is None:
            get_status_callback = self._get_task_creation_status

        if expected_storage_method is None:
            if settings.MEDIA_CACHE_ALLOW_STATIC_CACHE:
                expected_storage_method = StorageMethodChoice.FILE_SYSTEM
            else:
                expected_storage_method = StorageMethodChoice.CACHE

        # create task
        response = self._create_task(user, spec)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

        task_id = response.data["id"]

        # post data for the task
        response = send_data_callback(task_id, user, data)
        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED, response.reason_phrase)

        if get_status_callback:
            max_number_of_attempt = 100
            state = None
            while state not in ('Failed', 'Finished'):
                assert max_number_of_attempt, "Too much time to create a task"
                response = get_status_callback(task_id, user)
                state = response.data['state']
                sleep(0.1)
                max_number_of_attempt -= 1
            self.assertEqual(state, expected_task_creation_status_state)
            if expected_task_creation_status_state == 'Failed':
                self.assertIn(expected_task_creation_status_reason, response.data['message'])
                return

        response = self._get_task(user, task_id)

        expected_status_code = status.HTTP_200_OK
        if user == self.user and "owner_id" in spec and spec["owner_id"] != user.id and \
           "assignee_id" in spec and spec["assignee_id"] != user.id:
            expected_status_code = status.HTTP_403_FORBIDDEN
        self.assertEqual(response.status_code, expected_status_code)

        if expected_status_code == status.HTTP_200_OK:
            task = response.json()
            self.assertEqual(expected_compressed_type, task["data_compressed_chunk_type"])
            self.assertEqual(expected_original_type, task["data_original_chunk_type"])
            self.assertEqual(len(expected_image_sizes), task["size"])
            db_data = Task.objects.get(pk=task_id).data
            self.assertEqual(expected_storage_method, db_data.storage_method)
            self.assertEqual(expected_uploaded_data_location, db_data.storage)
            # check if used share without copying inside and files doesn`t exist in ../raw/ and exist in share
            if expected_uploaded_data_location is StorageChoice.SHARE:
                filename = next(
                    (v for k, v in data.items() if k.startswith('server_files[') ),
                    None
                )
                raw_file_path = os.path.join(db_data.get_upload_dirname(), filename)
                share_file_path = os.path.join(settings.SHARE_ROOT, filename)
                self.assertEqual(False, os.path.exists(raw_file_path))
                self.assertEqual(True, os.path.exists(share_file_path))

        # check preview
        response = self._get_preview(task_id, user)
        self.assertEqual(response.status_code, expected_status_code)
        if expected_status_code == status.HTTP_200_OK:
            if dimension == DimensionType.DIM_2D:
                preview = Image.open(io.BytesIO(response.content))
                self.assertLessEqual(preview.size, expected_image_sizes[0])

        # check compressed chunk
        response = self._get_compressed_chunk(task_id, user, 0)
        self.assertEqual(response.status_code, expected_status_code)
        if expected_status_code == status.HTTP_200_OK:
            if isinstance(response, HttpResponse):
                compressed_chunk = io.BytesIO(response.content)
            else:
                compressed_chunk = io.BytesIO(b"".join(response.streaming_content))
            if task["data_compressed_chunk_type"] == self.ChunkType.IMAGESET:
                images = self._extract_zip_chunk(compressed_chunk, dimension=dimension)
            else:
                images = self._extract_video_chunk(compressed_chunk)

            self.assertEqual(len(images), min(task["data_chunk_size"], len(expected_image_sizes)))

            for image_idx, received_image in enumerate(images):
                if dimension == DimensionType.DIM_3D:
                    properties = ValidateDimension.get_pcd_properties(received_image)
                    self.assertEqual((int(properties["WIDTH"]),int(properties["HEIGHT"])), expected_image_sizes[image_idx])
                else:
                    self.assertEqual(received_image.size, expected_image_sizes[image_idx])

        # check original chunk
        response = self._get_original_chunk(task_id, user, 0)
        self.assertEqual(response.status_code, expected_status_code)
        if expected_status_code == status.HTTP_200_OK:
            if isinstance(response, HttpResponse):
                original_chunk = io.BytesIO(response.getvalue())
            else:
                original_chunk  = io.BytesIO(b"".join(response.streaming_content))
            if task["data_original_chunk_type"] == self.ChunkType.IMAGESET:
                images = self._extract_zip_chunk(original_chunk, dimension=dimension)
            else:
                images = self._extract_video_chunk(original_chunk)

            for image_idx, received_image in enumerate(images):
                if dimension == DimensionType.DIM_3D:
                    properties = ValidateDimension.get_pcd_properties(received_image)
                    self.assertEqual((int(properties["WIDTH"]), int(properties["HEIGHT"])), expected_image_sizes[image_idx])
                else:
                    self.assertEqual(received_image.size, expected_image_sizes[image_idx])

            self.assertEqual(len(images), min(task["data_chunk_size"], len(expected_image_sizes)))

            if task["data_original_chunk_type"] == self.ChunkType.IMAGESET:
                server_files = [img for key, img in data.items() if key.startswith("server_files")]
                client_files = [img for key, img in data.items() if key.startswith("client_files")]

                _name_key = lambda x: getattr(x, 'name', x)

                if server_files:
                    _add_prefix = lambda x: os.path.join(settings.SHARE_ROOT, x)
                    source_files = server_files
                else:
                    _add_prefix = lambda x: x
                    source_files = client_files

                manifest = next((v for v in source_files if _name_key(v).endswith('.jsonl')), None)
                source_files = [_add_prefix(f)
                    for f in source_files if not _name_key(f).endswith('jsonl')]

                # Load images
                source_images = {}
                for f in source_files:
                    if zipfile.is_zipfile(f):
                        for frame_name, frame in self._extract_zip_archive(f, dimension=dimension):
                            source_images[frame_name] = frame
                    elif isinstance(f, str) and f.endswith('.rar'):
                        archive_frames = self._extract_rar_archive(f)
                        for fn, frame in archive_frames:
                            source_images[fn] = frame
                    elif isinstance(f, str) and f.endswith('.pdf'):
                        with open(f, 'rb') as pdf_file:
                            for i, frame in enumerate(convert_from_bytes(pdf_file.read(), fmt='png')):
                                source_images[f"frame_{i}"] = frame
                    elif isinstance(f, IOBase) and getattr(f, 'name', '').endswith('.pdf'):
                        for i, frame in enumerate(convert_from_bytes(f.getvalue(), fmt='png')):
                            source_images[f"frame_{i}"] = frame
                    elif isinstance(f, str) and not f.endswith('.jsonl'):
                        source_images[f] = Image.open(f)
                    elif isinstance(f, IOBase) and not f.name.endswith('.jsonl'):
                        source_images[f.name] = Image.open(f)

                # Apply the requested sorting to the expected results
                sorting = data.get('sorting_method', SortingMethod.LEXICOGRAPHICAL)
                if sorting == SortingMethod.PREDEFINED and manifest:
                    manifest = _add_prefix(_name_key(manifest))
                    manifest_root = os.path.dirname(manifest)
                    manifest_files = get_manifest_images_list(manifest)
                    assert len(manifest_files) == len(source_images)
                    source_images = [
                        source_images.get(os.path.join(manifest_root, f)) or source_images[f]
                        for f in manifest_files
                    ]
                else:
                    source_images = [v[1] for v in sort(
                        source_images.items(),
                        sorting_method=sorting,
                        func=lambda e: _name_key(e[0])
                    )]

                for (received_image, source_image) in zip(images, source_images):
                    if dimension == DimensionType.DIM_3D:
                        server_image = np.array(received_image.getbuffer())
                        source_image = np.array(source_image.getbuffer())
                        self.assertTrue(np.array_equal(source_image, server_image))
                    else:
                        server_image = np.array(received_image)
                        source_image = np.array(source_image)
                        self.assertTrue(np.array_equal(source_image, server_image))

    def _test_api_v2_tasks_id_data_create_can_upload_local_images(self, user):
        task_spec = {
            "name": "my task #1",
            "owner_id": user.id,
            "assignee_id": user.id,
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        images = copy.deepcopy(self._client_images['images'])
        image_sizes = self._client_images['image_sizes']
        task_data = {
            "client_files[0]": images[0],
            "client_files[1]": images[1],
            "client_files[2]": images[2],
            "image_quality": 75,
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)

    def _test_api_v2_tasks_id_data_create_can_use_server_images(self, user):
        task_spec = {
            "name": "my task without copying #2",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_1.jpg",
            "server_files[1]": "test_2.jpg",
            "server_files[2]": "test_3.jpg",
            "server_files[3]": os.path.join("data", "test_3.jpg"),
            "image_quality": 75,
        }
        image_sizes = [
            self._share_image_sizes[task_data["server_files[3]"]],
            self._share_image_sizes[task_data["server_files[0]"]],
            self._share_image_sizes[task_data["server_files[1]"]],
            self._share_image_sizes[task_data["server_files[2]"]],
        ]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes,
                                             expected_uploaded_data_location=StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my task #3')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                                             image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_local_video(self, user):
        task_spec = {
            "name": "my video task #4",
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        video = copy.deepcopy(self._client_mp4_video['video'])
        image_sizes = self._client_mp4_video['image_sizes']
        task_data = {
            "client_files[0]": video,
            "image_quality": 43,
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes)

    def _test_api_v2_tasks_id_data_create_can_use_server_video(self, user):
        task_spec = {
            "name": "my video task without copying #5",
            "overlap": 0,
            "segment_size": 5,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_video_1.mp4",
            "image_quality": 57,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes,
                                             expected_uploaded_data_location=StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my video task #6')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO,
                                             image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_server_video_default_segment_size(self, user):
        task_spec = {
            "name": "my video task without copying #7",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        task_data = {
            "server_files[0]": os.path.join("videos", "test_video_1.mp4"),
            "image_quality": 57,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes,
                                             expected_uploaded_data_location=StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([("name", "my video task #8")])
            task_data.update([("copy_data", True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO,
                                             image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_compress_server_video(self, user):
        task_spec = {
            "name": "my video task without copying #9",
            "overlap": 0,
            "segment_size": 5,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_video_1.mp4",
            "image_quality": 12,
            "use_zip_chunks": True,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.VIDEO, image_sizes,
                                             expected_uploaded_data_location=StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my video task #10')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.VIDEO,
                                             image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_server_zip_archive(self, user):
        task_spec = {
            "name": "my archive task without copying #11",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        task_data = {
            "server_files[0]": "test_archive_1.zip",
            "image_quality": 88,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes,
                                             expected_uploaded_data_location=StorageChoice.LOCAL)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my archive task #12')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                                             image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_local_archive(self, user):
        task_spec = {
            "name": "my archive task #13",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        archive = copy.deepcopy(self._client_archive['archive'])
        image_sizes = self._client_archive['image_sizes']
        task_data = {
            "client_files[0]": archive,
            "image_quality": 100,
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)

    def _test_api_v2_tasks_id_data_create_can_use_cached_server_video(self, user):
        task_spec = {
            "name": "cached video task without copying #14",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": 'test_video_1.mp4',
            "image_quality": 70,
            "use_cache": True,
        }

        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO,
                self.ChunkType.VIDEO, image_sizes, StorageMethodChoice.CACHE, StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'cached video task #15')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO,
                self.ChunkType.VIDEO, image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_cached_server_images(self, user):
        task_spec = {
            "name": "cached images task with default sorting data and without copying #16",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_1.jpg",
            "server_files[1]": "test_2.jpg",
            "server_files[2]": "test_10.jpg",
            "image_quality": 70,
            "use_cache": True,
        }
        image_sizes = [
            self._share_image_sizes[task_data["server_files[0]"]],
            self._share_image_sizes[task_data["server_files[2]"]],
            self._share_image_sizes[task_data["server_files[1]"]],
        ]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
                self.ChunkType.IMAGESET, image_sizes, StorageMethodChoice.CACHE, StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'cached images task #17')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                                             image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_cached_server_zip_archive(self, user):
        task_spec = {
            "name": "my cached zip archive task without copying #18",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_archive_1.zip",
            "image_quality": 70,
            "use_cache": True
        }

        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
                self.ChunkType.IMAGESET, image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my cached zip archive task #19')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                                             image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_cached_local_pdf(self, user):
        task_spec = {
            "name": "my cached pdf task #20",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        document = copy.deepcopy(self._client_pdf['pdf'])
        image_sizes = self._client_pdf['image_sizes']

        task_data = {
            "client_files[0]": document,
            "image_quality": 70,
            "use_cache": True
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
            self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
            image_sizes, StorageMethodChoice.CACHE)

    def _test_api_v2_tasks_id_data_create_can_use_local_pdf(self, user):
        task_spec = {
            "name": "my pdf task #21",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        document = copy.deepcopy(self._client_pdf['pdf'])
        image_sizes = self._client_pdf['image_sizes']
        task_data = {
            "client_files[0]": document,
            "image_quality": 70,
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
            self.ChunkType.IMAGESET, self.ChunkType.IMAGESET, image_sizes)

    def _test_api_v2_tasks_id_data_create_can_use_server_video_with_meta(self, user):
        task_spec = {
            "name": "my video with meta info task without copying #22",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        task_data = {
            "server_files[0]": os.path.join("videos", "test_video_1.mp4"),
            "server_files[1]": os.path.join("videos", "manifest.jsonl"),
            "image_quality": 70,
            "use_cache": True
        }
        image_sizes = self._share_image_sizes[task_data['server_files[0]']]

        with self.subTest(current_function_name() + " no copy"):
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO,
                                            self.ChunkType.VIDEO, image_sizes, StorageMethodChoice.CACHE,
                                            StorageChoice.SHARE)

        with self.subTest(current_function_name() + " with copy"):
            task_spec.update([('name', 'my video with meta info task #23')])
            task_data.update([('copy_data', True)])
            self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO,
                                             image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_chunked_local_video(self, user):
        task_spec = {
            "name": "my cached video task #14",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "client_files[0]": open(os.path.join(os.path.dirname(__file__), 'assets', 'test_rotated_90_video.mp4'), 'rb'),
            "image_quality": 70,
            "use_zip_chunks": True
        }

        image_sizes = self._share_image_sizes['test_rotated_90_video.mp4']
        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
            self.ChunkType.VIDEO, image_sizes, StorageMethodChoice.CACHE)

    def _test_api_v2_tasks_id_data_create_can_use_chunked_cached_local_video(self, user):
        task_spec = {
            "name": "my video task #15",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "client_files[0]": open(os.path.join(os.path.dirname(__file__), 'assets', 'test_rotated_90_video.mp4'), 'rb'),
            "image_quality": 70,
            "use_cache": True,
            "use_zip_chunks": True
        }

        image_sizes = self._share_image_sizes['test_rotated_90_video.mp4']
        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
            self.ChunkType.VIDEO, image_sizes, StorageMethodChoice.CACHE)

    def _test_api_v2_tasks_id_data_create_can_use_mxf_video(self, user):
        task_spec = {
            "name": "test mxf format",
            "use_zip_chunks": False,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ],
        }

        video = copy.deepcopy(self._client_mxf_video['video'])
        image_sizes = self._client_mxf_video['image_sizes']
        task_data = {
            "client_files[0]": video,
            "image_quality": 51,
        }

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.VIDEO, self.ChunkType.VIDEO, image_sizes)

    def _test_api_v2_tasks_id_data_create_can_use_local_pcd_zip(self, user):
        task_spec = {
            "name": "my archive task #24",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "client_files[0]": open(os.path.join(os.path.dirname(__file__), 'assets', 'test_pointcloud_pcd.zip'), 'rb'),
            "image_quality": 100,
        }
        image_sizes = self._share_image_sizes["test_pointcloud_pcd.zip"]
        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
                                             self.ChunkType.IMAGESET,
                                             image_sizes, dimension=DimensionType.DIM_3D)

    def _test_api_v2_tasks_id_data_create_can_use_local_pcd_kitti(self, user):
        task_spec = {
            "name": "my archive task #25",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "client_files[0]": open(os.path.join(os.path.dirname(__file__), 'assets', 'test_velodyne_points.zip'),
                                    'rb'),
            "image_quality": 100,
        }
        image_sizes = self._share_image_sizes["test_velodyne_points.zip"]
        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET,
                                             self.ChunkType.IMAGESET,
                                             image_sizes, dimension=DimensionType.DIM_3D)

    def _test_api_v2_tasks_id_data_create_can_use_server_images_and_manifest(self, user):
        task_spec_common = {
            "name": "images+manifest #26",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "image_quality": 70,
        }

        manifest_name = "images_manifest_sorted.jsonl"
        images = get_manifest_images_list(os.path.join(settings.SHARE_ROOT, manifest_name))
        image_sizes = [self._share_image_sizes[fn] for fn in images]
        task_data.update({
            f"server_files[{i}]": fn
            for i, fn in enumerate(images + [manifest_name])
        })

        for use_cache in [True, False]:
            task_data['use_cache'] = use_cache

            for copy_data in [True, False]:
                with self.subTest(current_function_name(), copy=copy_data, use_cache=use_cache):
                    task_spec = task_spec_common.copy()
                    task_spec['name'] = task_spec['name'] + f' copy={copy_data}'
                    task_data_copy = task_data.copy()
                    task_data_copy['copy_data'] = copy_data
                    self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data_copy,
                        self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                        image_sizes,
                        expected_uploaded_data_location=(
                            StorageChoice.LOCAL if copy_data else StorageChoice.SHARE
                        )
                    )

            with self.subTest(current_function_name() + ' file order mismatch', use_cache=use_cache):
                task_spec = task_spec_common.copy()
                task_spec['name'] = task_spec['name'] + f' mismatching file order'
                task_data_copy = task_data.copy()
                task_data_copy[f'server_files[{len(images)}]'] = "images_manifest.jsonl"
                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data_copy,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes,
                    expected_uploaded_data_location=StorageChoice.SHARE,
                    expected_task_creation_status_state='Failed',
                    expected_task_creation_status_reason='Incorrect file mapping to manifest content')

    def _test_api_v2_tasks_id_data_create_can_use_server_images_with_predefined_sorting(self, user):
        task_spec = {
            "name": 'task custom data sequence server files #28-1',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data_common = {
            "image_quality": 70,
            "sorting_method": SortingMethod.PREDEFINED
        }

        manifest_name = "images_manifest.jsonl"
        images = get_manifest_images_list(os.path.join(settings.SHARE_ROOT, manifest_name))
        image_sizes = [self._share_image_sizes[v] for v in images]

        for caching_enabled, manifest in product(
            [True, False], [True, False]
        ):
            with self.subTest(current_function_name(),
                manifest=manifest,
                caching_enabled=caching_enabled,
            ):
                task_data = task_data_common.copy()

                task_data["use_cache"] = caching_enabled
                if caching_enabled or not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE:
                    storage_method = StorageMethodChoice.CACHE
                else:
                    storage_method = StorageMethodChoice.FILE_SYSTEM

                if manifest:
                    task_data.update(
                        (f"server_files[{i}]", f)
                        for i, f in enumerate(reversed(images + [manifest_name]))
                        # Use a different order from what we have in the manifest.
                        # The files should be sorted during the task creation.
                        # Then we compare them with the original manifest order
                    )
                else:
                    task_data.update(
                        (f"server_files[{i}]", f)
                        for i, f in enumerate(images)
                    )

                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes, storage_method, StorageChoice.SHARE)

    def _test_api_v2_tasks_id_data_create_can_use_local_images_with_predefined_sorting(self, user):
        task_spec = {
            "name": 'task custom data sequence client files single request #28-2',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        with TestDir() as test_dir:
            image_sizes, image_files = generate_random_image_files(
                "test_1.jpg", "test_3.jpg", "test_5.jpg", "test_4.jpg", "test_2.jpg"
            )
            image_paths = []
            for image in image_files:
                fp = os.path.join(test_dir, image.name)
                with open(fp, 'wb') as f:
                    f.write(image.getvalue())
                image_paths.append(fp)
            del image_files

            task_data_common = {
                "image_quality": 75,
                "sorting_method": SortingMethod.PREDEFINED
            }

            for (caching_enabled, manifest) in product(
                [True, False], [True, False]
            ):
                manifest_path = os.path.join(test_dir, "manifest.jsonl")
                generate_manifest_file("images", manifest_path, image_paths,
                    sorting_method=SortingMethod.PREDEFINED)

                task_data_common["use_cache"] = caching_enabled
                if caching_enabled or not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE:
                    storage_method = StorageMethodChoice.CACHE
                else:
                    storage_method = StorageMethodChoice.FILE_SYSTEM

                with self.subTest(current_function_name(),
                    manifest=manifest,
                    caching_enabled=caching_enabled,
                ), ExitStack() as es:
                    images = [es.enter_context(open(p, 'rb')) for p in image_paths]

                    task_data = task_data_common.copy()
                    expected_image_sizes = image_sizes

                    if manifest:
                        manifest_file = es.enter_context(open(manifest_path))
                        task_data.update(
                            (f"client_files[{i}]", f)
                            for i, f in enumerate(reversed(images))
                            # Use a different order from what we have in the manifest.
                            # The files should be sorted during the task creation.
                            # Then we compare them with the original manifest order
                        )
                        task_data[f"client_files[{len(images)}]"] = manifest_file
                    else:
                        task_data.update(
                            (f"client_files[{i}]", f)
                            for i, f in enumerate(images)
                        )

                    self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                        self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                        expected_image_sizes, storage_method, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_use_server_archive_with_predefined_sorting(self, user):
        task_spec = {
            "name": 'task custom data sequence server files single request #28-3',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data_common = {
            "image_quality": 75,
            "sorting_method": SortingMethod.PREDEFINED,
        }
        archive_name = "test_archive_2.zip"

        for (caching_enabled, manifest) in product(
            [True, False], [True, False]
        ):
            with self.subTest(current_function_name(),
                manifest=manifest,
                caching_enabled=caching_enabled,
            ):
                task_data = task_data_common.copy()

                task_data["use_cache"] = caching_enabled
                if caching_enabled or not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE:
                    storage_method = StorageMethodChoice.CACHE
                else:
                    storage_method = StorageMethodChoice.FILE_SYSTEM

                task_data["server_files[0]"] = archive_name

                manifest_name = "images_manifest.jsonl"
                images = get_manifest_images_list(os.path.join(settings.SHARE_ROOT, manifest_name))
                image_sizes = [self._share_image_sizes[v] for v in images]

                kwargs = {}
                if manifest:
                    task_data["server_files[1]"] = manifest_name
                else:
                    kwargs.update({
                        'expected_task_creation_status_state': 'Failed',
                        'expected_task_creation_status_reason': "Can't find upload manifest file",
                    })

                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes, storage_method, StorageChoice.LOCAL, **kwargs)

    def _test_api_v2_tasks_id_data_create_can_use_local_archive_with_predefined_sorting(self, user):
        task_spec = {
            "name": 'task custom data sequence client files single request #28-4',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        with TestDir() as test_dir:
            image_sizes, image_files = generate_random_image_files(
                "test_1.jpg", "test_3.jpg", "test_5.jpg", "test_4.jpg", "test_2.jpg"
            )
            image_paths = []
            for image in image_files:
                fp = os.path.join(test_dir, image.name)
                with open(fp, 'wb') as f:
                    f.write(image.getvalue())
                image_paths.append(fp)

            archive_path = os.path.join(test_dir, 'archive.zip')
            with zipfile.ZipFile(archive_path, 'x') as archive:
                for image_path in image_paths:
                    archive.write(image_path, os.path.relpath(image_path, test_dir))

            del image_files

            task_data_common = {
                "image_quality": 75,
                "sorting_method": SortingMethod.PREDEFINED,
            }

            for (caching_enabled, include_image_info, manifest) in product(
                [True, False], [True, False], [True, False]
            ):
                with self.subTest(current_function_name(),
                    manifest=manifest,
                    caching_enabled=caching_enabled,
                    include_image_info=include_image_info
                ), ExitStack() as es:
                    task_data = task_data_common.copy()

                    manifest_path = os.path.join(test_dir, "manifest.jsonl")
                    generate_manifest_file("images", manifest_path, image_paths,
                        sorting_method=SortingMethod.PREDEFINED)

                    task_data["use_cache"] = caching_enabled
                    if caching_enabled or not settings.MEDIA_CACHE_ALLOW_STATIC_CACHE:
                        storage_method = StorageMethodChoice.CACHE
                    else:
                        storage_method = StorageMethodChoice.FILE_SYSTEM

                    task_data[f"client_files[0]"] = es.enter_context(open(archive_path, 'rb'))

                    kwargs = {}
                    if manifest:
                        task_data[f"client_files[1]"] = es.enter_context(open(manifest_path))
                    else:
                        kwargs.update({
                            'expected_task_creation_status_state': 'Failed',
                            'expected_task_creation_status_reason': "Can't find upload manifest file",
                        })
                    self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                        self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                        image_sizes, storage_method, StorageChoice.LOCAL, **kwargs)

    def _test_api_v2_tasks_id_data_create_can_use_server_images_with_natural_sorting(self, user):
        task_spec = {
            "name": 'task native data sequence #29',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_10.jpg",
            "server_files[1]": "test_2.jpg",
            "server_files[2]": "test_1.jpg",
            "image_quality": 70,
            "use_cache": True,
            "sorting_method": SortingMethod.NATURAL
        }
        image_sizes = [
            self._share_image_sizes[task_data["server_files[2]"]],
            self._share_image_sizes[task_data["server_files[1]"]],
            self._share_image_sizes[task_data["server_files[0]"]],
        ]

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
            image_sizes, StorageMethodChoice.CACHE, StorageChoice.SHARE)

    def _test_api_v2_tasks_id_data_create_can_use_server_pdf(self, user):
        task_spec = {
            "name": 'task pdf in the shared folder #30',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_1.pdf",
            "image_quality": 70,
            "copy_data": False,
            "use_cache": True,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
            image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create_can_send_ordered_images_with_multifile_requests(self, user):
        task_spec = {
            "name": 'task custom data sequence client files multi request #31',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data_common = {
            "image_quality": 70,
            "sorting_method": SortingMethod.PREDEFINED,
        }

        def _send_data(tid, user, data):
            response = self._run_api_v2_tasks_id_data_post(tid, user,
                data={ 'image_quality': task_data["image_quality"] },
                headers={ 'Upload-Start': True })
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code

            for group_idx, file_group in enumerate(file_groups):
                request_data = {k: v for k, v in data.items() if '_files' not in k}
                request_data.update({
                    f'client_files[{i}]': images[f] for i, f in enumerate(file_group)
                })

                if group_idx == len(file_groups) - 1:
                    headers = { 'Upload-Finish': True }
                    request_data['upload_file_order'] = upload_info
                else:
                    headers = { 'Upload-Multiple': True }

                response = self._run_api_v2_tasks_id_data_post(tid, user, data=request_data,
                    headers=headers)

                if group_idx != len(file_groups) - 1:
                    assert response.status_code == status.HTTP_200_OK, response.status_code
            return response

        def _send_data_and_fail(*args, **kwargs):
            response = _send_data(*args, **kwargs)
            assert response.status_code == status.HTTP_400_BAD_REQUEST
            raise Exception(response.data)

        filenames = [
            "test_1.jpg", "test_3.jpg", "test_5.jpg", "test_qwe.jpg", "test_4.jpg", "test_2.jpg"
        ]

        for name, upload_info, file_groups in (
            (
                "input ordering, multiple requests, the last one has data",
                [],
                [
                    [ 0, 1, 2 ],
                    [ 3, 4 ],
                    [ 5, ],
                ]
            ),

            (
                "input ordering, multiple requests, the last one has no data",
                [],
                [
                    [ 0, 1, 2 ],
                    [ 3, 4, 5 ],
                    [ ],
                ]
            ),

            (
                "input ordering, multiple requests, the last one has data, has an empty request",
                [],
                [
                    [ 0, 1, 2 ],
                    [ ],
                    [ 3, 4, 5 ],
                ]
            ),

            (
                "custom ordering, multiple requests, the last one has no data, files unordered",
                filenames,
                [
                    [ 2, 4, 0 ],
                    [ 3, 5, 1 ],
                    [  ],
                ]
            ),

            (
                "custom ordering, multiple requests, the last one has data, files unordered",
                filenames,
                [
                    [ 2, 0 ],
                    [ 3, 5, 4 ],
                    [ 1, ],
                ]
            ),
        ):
            with self.subTest(current_function_name() + ' ' + name):
                image_sizes, images = generate_random_image_files(*filenames)

                task_data = task_data_common.copy()
                task_data.update((f"client_files[{i}]", f) for i, f in enumerate(images))

                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL,
                    send_data_callback=_send_data)

        with self.subTest(current_function_name() + ' mismatching file sets - extra files'):
            upload_info = [filenames[0]]
            file_groups = [[ 0, 1 ]]
            image_sizes, images = generate_random_image_files(*filenames[:2])

            task_data = task_data_common.copy()
            task_data.update((f"client_files[{i}]", f) for i, f in enumerate(images))

            with self.assertRaisesMessage(Exception, "(extra)"):
                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL,
                    send_data_callback=_send_data_and_fail)

        with self.subTest(current_function_name() + ' mismatching file sets - missing files'):
            upload_info = filenames[0:3]
            file_groups = [[ 0, 1 ]]
            image_sizes, images = generate_random_image_files(*upload_info)

            task_data = task_data_common.copy()
            task_data.update((f"client_files[{i}]", f) for i, f in enumerate(images))

            with self.assertRaisesMessage(Exception, "(missing)"):
                self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data,
                    self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
                    image_sizes, expected_uploaded_data_location=StorageChoice.LOCAL,
                    send_data_callback=_send_data_and_fail)

    def _test_api_v2_tasks_id_data_create_can_use_server_rar(self, user):
        task_spec = {
            "name": 'task rar in the shared folder #32',
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

        task_data = {
            "server_files[0]": "test_rar.rar",
            "image_quality": 75,
            "copy_data": False,
            "use_cache": True,
        }
        image_sizes = self._share_image_sizes[task_data["server_files[0]"]]

        self._test_api_v2_tasks_id_data_spec(user, task_spec, task_data, self.ChunkType.IMAGESET, self.ChunkType.IMAGESET,
            image_sizes, StorageMethodChoice.CACHE, StorageChoice.LOCAL)

    def _test_api_v2_tasks_id_data_create(self, user):
        method_list = {
            func: getattr(self, func) for func in dir(self)
            if func.startswith('_test_api_v2_tasks_id_data_create_') and
            callable(getattr(self, func))
        }
        assert method_list
        for name, func in method_list.items():
            with self.subTest(name):
                func(user)

    def test_api_v2_tasks_id_data_admin(self):
        self._test_api_v2_tasks_id_data_create(self.admin)

    def test_api_v2_tasks_id_data_owner(self):
        self._test_api_v2_tasks_id_data_create(self.owner)

    def test_api_v2_tasks_id_data_user(self):
        self._test_api_v2_tasks_id_data_create(self.user)

    def test_api_v2_tasks_id_data_no_auth(self):
        data = {
            "name": "my task #3",
            "owner_id": self.owner.id,
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        response = self._create_task(None, data)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

def compare_objects(self, obj1, obj2, ignore_keys, fp_tolerance=.001,
        current_key=None):
    key_info = "{}: ".format(current_key) if current_key else ""

    if isinstance(obj1, dict):
        self.assertTrue(isinstance(obj2, dict),
            "{}{} != {}".format(key_info, obj1, obj2))
        for k, v1 in obj1.items():
            if k in ignore_keys:
                continue
            v2 = obj2[k]
            if k == 'attributes':
                key = lambda a: a['spec_id'] if 'spec_id' in a else a['id']
                v1.sort(key=key)
                v2.sort(key=key)
            compare_objects(self, v1, v2, ignore_keys, current_key=k)
    elif isinstance(obj1, list):
        self.assertTrue(isinstance(obj2, list),
            "{}{} != {}".format(key_info, obj1, obj2))
        self.assertEqual(len(obj1), len(obj2),
            "{}{} != {}".format(key_info, obj1, obj2))
        for v1, v2 in zip(obj1, obj2):
            compare_objects(self, v1, v2, ignore_keys, current_key=current_key)
    else:
        if isinstance(obj1, float) or isinstance(obj2, float):
            self.assertAlmostEqual(obj1, obj2, delta=fp_tolerance,
                msg=current_key)
        else:
            self.assertEqual(obj1, obj2, msg=current_key)

class JobAnnotationAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _create_task(self, owner, assignee, annotation_format=""):
        dimension = DimensionType.DIM_2D
        data = {
            "name": "my task #1",
            "owner_id": owner.id,
            "assignee_id": assignee.id,
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {
                    "name": "car",
                    "attributes": [
                        {
                            "name": "model",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "mazda",
                            "values": ["bmw", "mazda", "renault"]
                        },
                        {
                            "name": "parked",
                            "mutable": True,
                            "input_type": "checkbox",
                            "default_value": "false",
                            "values": [],
                        },
                    ]
                },
                {"name": "person"},
                {
                    "name": "widerface",
                    "attributes": [
                        {
                            "name": "blur",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "0",
                            "values": ["0", "1", "2"]
                        },
                        {
                            "name": "expression",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "0",
                            "values": ["0", "1"]
                        },
                        {
                            "name": "illumination",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "0",
                            "values": ["0", "1"]
                        },
                    ]
                },
            ]
        }
        if annotation_format == "Market-1501 1.0":
            data["labels"] = [{
                "name": "market-1501",
                "attributes": [
                    {
                        "name": "query",
                        "mutable": False,
                        "input_type": "select",
                        "values": ["True", "False"]
                    },
                    {
                        "name": "camera_id",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["0", "1", "2", "3", "4", "5"]
                    },
                    {
                        "name": "person_id",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["1", "2", "3"]
                    },
                ]
            }]
        elif annotation_format in ["ICDAR Recognition 1.0",
                "ICDAR Localization 1.0"]:
            data["labels"] = [{
                "name": "icdar",
                "attributes": [
                    {
                        "name": "text",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["word_1", "word_2", "word_3"]
                    },
                ]
            }]
        elif annotation_format in ['Kitti Raw Format 1.0', 'Sly Point Cloud Format 1.0']:
            data["labels"] = [{
                "name": "car"},
                {"name": "bus"}
            ]
            dimension = DimensionType.DIM_3D
        elif annotation_format == "ICDAR Segmentation 1.0":
            data["labels"] = [{
                "name": "icdar",
                "attributes": [
                    {
                        "name": "text",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["word_1", "word_2", "word_3"]
                    },
                    {
                        "name": "index",
                        "mutable": False,
                        "input_type": "number",
                        "values": ["0", "1", "2"]
                    },
                    {
                        "name": "color",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["100 110 240", "10 15 20", "120 128 64"]
                    },
                    {
                        "name": "center",
                        "mutable": False,
                        "input_type": "text",
                        "values": ["1 2", "2 4", "10 45"]
                    },
                ]
            }]

        with ForceLogin(owner, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED
            tid = response.data["id"]

            images = {
                "client_files[0]": generate_random_image_file("test_1.jpg")[1],
                "client_files[1]": generate_random_image_file("test_2.jpg")[1],
                "client_files[2]": generate_random_image_file("test_3.jpg")[1],
                "client_files[4]": generate_random_image_file("test_4.jpg")[1],
                "client_files[5]": generate_random_image_file("test_5.jpg")[1],
                "client_files[6]": generate_random_image_file("test_6.jpg")[1],
                "client_files[7]": generate_random_image_file("test_7.jpg")[1],
                "client_files[8]": generate_random_image_file("test_8.jpg")[1],
                "client_files[9]": generate_random_image_file("test_9.jpg")[1],
                "image_quality": 75,
                "frame_filter": "step=3",
            }
            if dimension == DimensionType.DIM_3D:
                images = {
                    "client_files[0]": open(
                        os.path.join(os.path.dirname(__file__), 'assets', 'test_pointcloud_pcd.zip'
                        if annotation_format == 'Sly Point Cloud Format 1.0' else 'test_velodyne_points.zip'),
                        'rb'),
                    "image_quality": 100,
                }

            response = self.client.post("/api/tasks/{}/data".format(tid), data=images)
            assert response.status_code == status.HTTP_202_ACCEPTED

            response = self.client.get("/api/tasks/{}".format(tid))
            task = response.data

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get(
                        "/api/labels?task_id=%s&page=%s" % (response.data["id"], page)
                    )
                ))
                response.data["labels"] = labels_response

            jobs = get_paginated_collection(lambda page:
                self.client.get("/api/jobs?task_id={}&page={}".format(tid, page))
            )

        return (task, jobs)

    @staticmethod
    def _get_default_attr_values(task):
        default_attr_values = {}
        for label in task["labels"]:
            default_attr_values[label["id"]] = {
                "mutable": [],
                "immutable": [],
                "all": [],
            }
            for attr in label["attributes"]:
                default_value = {
                    "spec_id": attr["id"],
                    "value": attr["default_value"],
                }
                if attr["mutable"]:
                    default_attr_values[label["id"]]["mutable"].append(default_value)
                else:
                    default_attr_values[label["id"]]["immutable"].append(default_value)
                default_attr_values[label["id"]]["all"].append(default_value)
        return default_attr_values

    def _put_api_v2_jobs_id_data(self, jid, user, data):
        with ForceLogin(user, self.client):
            response = self.client.put("/api/jobs/{}/annotations".format(jid),
                data=data, format="json")

        return response

    def _get_api_v2_jobs_id_data(self, jid, user):
        with ForceLogin(user, self.client):
            response = self.client.get("/api/jobs/{}/annotations".format(jid))

        return response

    def _delete_api_v2_jobs_id_data(self, jid, user):
        with ForceLogin(user, self.client):
            response = self.client.delete("/api/jobs/{}/annotations".format(jid),
            format="json")

        return response

    def _patch_api_v2_jobs_id_data(self, jid, user, action, data):
        with ForceLogin(user, self.client):
            response = self.client.patch(
                "/api/jobs/{}/annotations?action={}".format(jid, action),
                data=data, format="json")

        return response

    def _check_response(self, response, data):
        if not response.status_code in [
            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:
            compare_objects(self, data, response.data, ignore_keys=["id", "version"])

    def _run_api_v2_jobs_id_annotations(self, owner, assignee, annotator):
        task, jobs = self._create_task(owner, assignee)
        if annotator:
            HTTP_200_OK = status.HTTP_200_OK
            HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT
            HTTP_400_BAD_REQUEST = status.HTTP_400_BAD_REQUEST
        else:
            HTTP_200_OK = status.HTTP_401_UNAUTHORIZED
            HTTP_204_NO_CONTENT = status.HTTP_401_UNAUTHORIZED
            HTTP_400_BAD_REQUEST = status.HTTP_401_UNAUTHORIZED

        job = jobs[0]
        data = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._put_api_v2_jobs_id_data(job["id"], annotator, data)
        self.assertEqual(response.status_code, HTTP_200_OK)

        data = {
            "version": 1,
            "tags": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": []
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False
                },
                {
                    "frame": 2,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                },
                            ]
                        },
                        {
                            "frame": 2,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True
                        },
                    ]
                },
                {
                    "frame": 2,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 2,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False
                        }
                    ]
                },
            ]
        }

        default_attr_values = self._get_default_attr_values(task)
        response = self._put_api_v2_jobs_id_data(job["id"], annotator, data)
        data["version"] += 1 # need to update the version
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._get_api_v2_jobs_id_data(job["id"], annotator)
        self.assertEqual(response.status_code, HTTP_200_OK)
        # server should add default attribute values if puted data doesn't contain it
        data["tags"][0]["attributes"] = default_attr_values[data["tags"][0]["label_id"]]["all"]
        data["tracks"][0]["shapes"][1]["attributes"] = default_attr_values[data["tracks"][0]["label_id"]]["mutable"]
        self._check_response(response, data)

        response = self._delete_api_v2_jobs_id_data(job["id"], annotator)
        data["version"] += 1 # need to update the version
        self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)

        data = {
            "version": data["version"],
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._get_api_v2_jobs_id_data(job["id"], annotator)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": []
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False,
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                },
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,
                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                },
            ]
        }
        response = self._patch_api_v2_jobs_id_data(job["id"], annotator,
            "create", data)
        data["version"] += 1
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._get_api_v2_jobs_id_data(job["id"], annotator)
        self.assertEqual(response.status_code, HTTP_200_OK)
        # server should add default attribute values if puted data doesn't contain it
        data["tags"][0]["attributes"] = default_attr_values[data["tags"][0]["label_id"]]["all"]
        data["tracks"][0]["shapes"][1]["attributes"] = default_attr_values[data["tracks"][0]["label_id"]]["mutable"]
        self._check_response(response, data)

        data = response.data
        if not response.status_code in [
            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:
            data["tags"][0]["label_id"] = task["labels"][0]["id"]
            data["shapes"][0]["points"] = [1, 2, 3.0, 100, 120, 1, 2, 4.0]
            data["shapes"][0]["type"] = "polygon"
            data["tracks"][0]["group"] = 10
            data["tracks"][0]["shapes"][0]["outside"] = False
            data["tracks"][0]["shapes"][0]["occluded"] = False

        response = self._patch_api_v2_jobs_id_data(job["id"], annotator,
            "update", data)
        data["version"] = data.get("version", 0) + 1 # need to update the version
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._get_api_v2_jobs_id_data(job["id"], annotator)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._patch_api_v2_jobs_id_data(job["id"], annotator,
            "delete", data)
        data["version"] += 1 # need to update the version
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._get_api_v2_jobs_id_data(job["id"], annotator)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [
                {
                    "frame": 0,
                    "label_id": 11010101,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": 32234234,
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][0]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False,
                },
                {
                    "frame": 1,
                    "label_id": 1212121,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": 0,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": 10000,
                                    "value": task["labels"][0]["attributes"][0]["values"][0]
                                },
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,
                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                },
            ]
        }
        response = self._patch_api_v2_jobs_id_data(job["id"], annotator,
            "create", data)
        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)

    def test_api_v2_jobs_id_annotations_admin(self):
        self._run_api_v2_jobs_id_annotations(self.admin, self.assignee,
            self.assignee)

    def test_api_v2_jobs_id_annotations_user(self):
        self._run_api_v2_jobs_id_annotations(self.user, self.user,
            self.user)

    def test_api_v2_jobs_id_annotations_somebody(self):
        _, jobs = self._create_task(self.user, self.user)
        job = jobs[0]
        data = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": []
        }

        response = self._get_api_v2_jobs_id_data(job["id"], self.somebody)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._put_api_v2_jobs_id_data(job["id"], self.somebody, data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._patch_api_v2_jobs_id_data(job["id"], self.somebody, "create", data)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

        response = self._delete_api_v2_jobs_id_data(job["id"], self.somebody)
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)


    def test_api_v2_jobs_id_annotations_no_auth(self):
        self._run_api_v2_jobs_id_annotations(self.user, self.user, None)

class TaskAnnotationAPITestCase(ExportApiTestBase, JobAnnotationAPITestCase):
    def _put_api_v2_tasks_id_annotations(self, pk, user, data):
        with ForceLogin(user, self.client):
            response = self.client.put("/api/tasks/{}/annotations".format(pk),
                data=data, format="json")

        return response

    def _get_api_v2_tasks_id_annotations(self, pk, user):
        with ForceLogin(user, self.client):
            response = self.client.get("/api/tasks/{}/annotations".format(pk))

        return response

    def _delete_api_v2_tasks_id_annotations(self, pk, user):
        with ForceLogin(user, self.client):
            response = self.client.delete("/api/tasks/{}/annotations".format(pk),
            format="json")

        return response

    def _patch_api_v2_tasks_id_annotations(self, pk, user, action, data):
        with ForceLogin(user, self.client):
            response = self.client.patch(
                "/api/tasks/{}/annotations?action={}".format(pk, action),
                data=data, format="json")

        return response

    def _upload_api_v2_tasks_id_annotations(self, pk, user, data, query_params=""):
        with ForceLogin(user, self.client):
            response = self.client.put(
                path="/api/tasks/{0}/annotations?{1}".format(pk, query_params),
                data=data,
                format="multipart",
                )

        return response

    def _get_formats(self, user):
        with ForceLogin(user, self.client):
            response = self.client.get(
                path="/api/server/annotation/formats"
            )
        return response

    def _check_response(self, response, data):
        if not response.status_code in [
            status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN]:
            try:
                compare_objects(self, data, response.data, ignore_keys=["id", "version"])
            except AssertionError as e:
                print("Objects are not equal: ", data, response.data)
                print(e)
                raise

    def _run_api_v2_tasks_id_annotations(self, owner, assignee):
        task, _ = self._create_task(owner, assignee)
        HTTP_200_OK = status.HTTP_200_OK
        HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT
        HTTP_400_BAD_REQUEST = status.HTTP_400_BAD_REQUEST

        data = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._put_api_v2_tasks_id_annotations(task["id"], owner, data)
        data["version"] += 1
        self.assertEqual(response.status_code, HTTP_200_OK)

        data = {
            "version": data["version"],
            "tags": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][0]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False,
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,

                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                },
            ]
        }
        response = self._put_api_v2_tasks_id_annotations(task["id"], owner, data)
        data["version"] += 1

        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        default_attr_values = self._get_default_attr_values(task)
        response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
        # server should add default attribute values if puted data doesn't contain it
        data["tags"][0]["attributes"] = default_attr_values[data["tags"][0]["label_id"]]["all"]
        data["tracks"][0]["shapes"][1]["attributes"] = default_attr_values[data["tracks"][0]["label_id"]]["mutable"]
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._delete_api_v2_tasks_id_annotations(task["id"], owner)
        data["version"] += 1
        self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)

        data = {
            "version": data["version"],
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][0]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False,
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,
                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                },
            ]
        }
        response = self._patch_api_v2_tasks_id_annotations(task["id"], owner,
            "create", data)
        data["version"] += 1
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
        # server should add default attribute values if puted data doesn't contain it
        data["tags"][0]["attributes"] = default_attr_values[data["tags"][0]["label_id"]]["all"]
        data["tracks"][0]["shapes"][1]["attributes"] = default_attr_values[data["tracks"][0]["label_id"]]["mutable"]
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = response.data
        if not response.status_code in [
            status.HTTP_403_FORBIDDEN, status.HTTP_401_UNAUTHORIZED]:
            data["tags"][0]["label_id"] = task["labels"][0]["id"]
            data["shapes"][0]["points"] = [1, 2, 3.0, 100, 120, 1, 2, 4.0]
            data["shapes"][0]["type"] = "polygon"
            data["tracks"][0]["group"] = 10
            data["tracks"][0]["shapes"][0]["outside"] = False
            data["tracks"][0]["shapes"][0]["occluded"] = False

        response = self._patch_api_v2_tasks_id_annotations(task["id"], owner,
            "update", data)
        data["version"] = data.get("version", 0) + 1
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        response = self._patch_api_v2_tasks_id_annotations(task["id"], owner,
            "delete", data)
        data["version"] += 1
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [],
            "shapes": [],
            "tracks": []
        }
        response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
        self.assertEqual(response.status_code, HTTP_200_OK)
        self._check_response(response, data)

        data = {
            "version": data["version"],
            "tags": [
                {
                    "frame": 0,
                    "label_id": 11010101,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                }
            ],
            "shapes": [
                {
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": 32234234,
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][0]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 100, 300.222],
                    "type": "rectangle",
                    "occluded": False,
                },
                {
                    "frame": 1,
                    "label_id": 1212121,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 300.222, 400, 500, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                },
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": 0,
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": 10000,
                                    "value": task["labels"][0]["attributes"][0]["values"][0]
                                },
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][0]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [2.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,
                        },
                    ]
                },
                {
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": None,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 100, 300.222],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                },
            ]
        }
        response = self._patch_api_v2_tasks_id_annotations(task["id"], owner,
            "create", data)
        self.assertEqual(response.status_code, HTTP_400_BAD_REQUEST)

    def _run_api_v2_tasks_id_annotations_dump_load(self, owner):
        if owner:
            HTTP_200_OK = status.HTTP_200_OK
            HTTP_204_NO_CONTENT = status.HTTP_204_NO_CONTENT
            HTTP_202_ACCEPTED = status.HTTP_202_ACCEPTED
            HTTP_201_CREATED = status.HTTP_201_CREATED
        else:
            HTTP_200_OK = status.HTTP_401_UNAUTHORIZED
            HTTP_204_NO_CONTENT = status.HTTP_401_UNAUTHORIZED
            HTTP_202_ACCEPTED = status.HTTP_401_UNAUTHORIZED
            HTTP_201_CREATED = status.HTTP_401_UNAUTHORIZED

        def _get_initial_annotation(annotation_format):
            if annotation_format not in ["Market-1501 1.0", "ICDAR Recognition 1.0",
                                         "ICDAR Localization 1.0", "ICDAR Segmentation 1.0",
                                         'Kitti Raw Format 1.0', 'Sly Point Cloud Format 1.0',
                                         'Datumaro 3D 1.0']:
                rectangle_tracks_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "shapes": [
                        {
                            "frame": 0,
                            "points": [1.0, 2.1, 50.1, 30.22],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 1,
                            "points": [2.0, 2.1, 77.2, 36.22],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": False,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                        {
                            "frame": 2,
                            "points": [2.0, 2.1, 77.2, 36.22],
                            "type": "rectangle",
                            "occluded": True,
                            "outside": True,
                            "attributes": [
                                {
                                    "spec_id": task["labels"][0]["attributes"][1]["id"],
                                    "value": task["labels"][0]["attributes"][1]["default_value"]
                                }
                            ]
                        },
                    ]
                }]
                rectangle_tracks_wo_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 0,
                            "attributes": [],
                            "points": [1.0, 2.1, 50.2, 36.6],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False,
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 51, 36.6],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": False
                        },
                        {
                            "frame": 2,
                            "attributes": [],
                            "points": [1.0, 2.1, 51, 36.6],
                            "type": "rectangle",
                            "occluded": False,
                            "outside": True,
                        }
                    ]
                }]
                polygon_tracks_wo_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "shapes": [
                        {
                            "frame": 0,
                            "attributes": [],
                            "points": [1.0, 2.1, 50.2, 36.6, 7.0, 10.0],
                            "type": "polygon",
                            "occluded": False,
                            "outside": False,
                        },
                        {
                            "frame": 1,
                            "attributes": [],
                            "points": [1.0, 2.1, 51, 36.6, 8.0, 11.0],
                            "type": "polygon",
                            "occluded": False,
                            "outside": True
                        },
                        {
                            "frame": 2,
                            "attributes": [],
                            "points": [1.0, 2.1, 51, 36.6, 14.0, 15.0],
                            "type": "polygon",
                            "occluded": False,
                            "outside": False,
                        }
                    ]
                }]

                rectangle_shapes_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 10.6, 53.22],
                    "type": "rectangle",
                    "occluded": False,
                }]

                rectangle_shapes_with_wider_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][2]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][2]["attributes"][0]["id"],
                            "value": task["labels"][2]["attributes"][0]["default_value"]
                        },
                        {
                            "spec_id": task["labels"][2]["attributes"][1]["id"],
                            "value": task["labels"][2]["attributes"][1]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][2]["attributes"][2]["id"],
                            "value": task["labels"][2]["attributes"][2]["default_value"]
                        }
                    ],
                    "points": [1.0, 2.1, 10.6, 53.22],
                    "type": "rectangle",
                    "occluded": False,
                }]

                rectangle_shapes_wo_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 40, 50.7],
                    "type": "rectangle",
                    "occluded": False,
                }]

                polygon_shapes_wo_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "points": [2.0, 2.1, 100, 30.22, 40, 77, 1, 3],
                    "type": "polygon",
                    "occluded": False,
                }]

                polygon_shapes_with_attrs = [{
                    "frame": 2,
                    "label_id": task["labels"][0]["id"],
                    "group": 1,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["default_value"]
                        }
                    ],
                    "points": [20.0, 0.1, 10, 3.22, 4, 7, 10, 30, 1, 2, 4.44, 5.55],
                    "type": "polygon",
                    "occluded": True,
                },
                {
                    "frame": 2,
                    "label_id": task["labels"][1]["id"],
                    "group": 1,
                    "source": "manual",
                    "attributes": [],
                    "points": [4, 7, 10, 30, 4, 5.55],
                    "type": "polygon",
                    "occluded": False,
                }]

                points_wo_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "points": [20.0, 0.1, 10, 3.22, 4, 7, 10, 30, 1, 2],
                    "type": "points",
                    "occluded": False,
                }]

                tags_wo_attrs = [{
                    "frame": 2,
                    "label_id": task["labels"][1]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                }]
                tags_with_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][0]["id"],
                    "group": 3,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["default_value"]
                        }
                    ],
                }]

            annotations = {
                "version": 0,
                "tags": [],
                "shapes": [],
                "tracks": [],
            }
            if annotation_format == "CVAT for video 1.1":
                annotations["tracks"] = rectangle_tracks_with_attrs \
                                      + rectangle_tracks_wo_attrs \
                                      + polygon_tracks_wo_attrs

            elif annotation_format == "CVAT for images 1.1":
                annotations["shapes"] = rectangle_shapes_with_attrs \
                                      + rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs \
                                      + polygon_shapes_with_attrs
                annotations["tags"] = tags_with_attrs + tags_wo_attrs

            elif annotation_format == "PASCAL VOC 1.1":
                annotations["shapes"] = rectangle_shapes_wo_attrs
                annotations["tags"] = tags_wo_attrs

            elif annotation_format == "YOLO 1.1":
                annotations["shapes"] = rectangle_shapes_wo_attrs

            elif annotation_format == "Ultralytics YOLO Detection 1.0":
                annotations["shapes"] = rectangle_shapes_wo_attrs

            elif annotation_format == "Ultralytics YOLO Oriented Bounding Boxes 1.0":
                annotations["shapes"] = rectangle_shapes_wo_attrs

            elif annotation_format == "Ultralytics YOLO Segmentation 1.0":
                annotations["shapes"] = polygon_shapes_wo_attrs

            elif annotation_format == "COCO 1.0":
                annotations["shapes"] = polygon_shapes_wo_attrs

            elif annotation_format == "COCO Keypoints 1.0":
                annotations["shapes"] = points_wo_attrs

            elif annotation_format == "Segmentation mask 1.1":
                annotations["shapes"] = rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs
                annotations["tracks"] = rectangle_tracks_wo_attrs

            elif annotation_format == "MOT 1.1":
                annotations["shapes"] = rectangle_shapes_wo_attrs
                annotations["tracks"] = rectangle_tracks_wo_attrs

            elif annotation_format == "MOTS PNG 1.0":
                annotations["tracks"] = polygon_tracks_wo_attrs

            elif annotation_format == "LabelMe 3.0":
                annotations["shapes"] = rectangle_shapes_with_attrs \
                                      + rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs \
                                      + polygon_shapes_with_attrs

            elif annotation_format == "Datumaro 1.0":
                annotations["shapes"] = rectangle_shapes_with_attrs \
                                      + rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs \
                                      + polygon_shapes_with_attrs
                annotations["tags"] = tags_with_attrs + tags_wo_attrs

            elif annotation_format == "ImageNet 1.0":
                annotations["tags"] = tags_wo_attrs

            elif annotation_format == "CamVid 1.0":
                annotations["shapes"] = rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs

            elif annotation_format == "WiderFace 1.0":
                annotations["tags"] = tags_wo_attrs
                annotations["shapes"] = rectangle_shapes_with_wider_attrs

            elif annotation_format == "VGGFace2 1.0":
                annotations["tags"] = tags_wo_attrs
                annotations["shapes"] = points_wo_attrs \
                                      + rectangle_shapes_wo_attrs
            elif annotation_format == "Cityscapes 1.0":
                annotations["shapes"] = points_wo_attrs \
                                      + rectangle_shapes_wo_attrs
            elif annotation_format == "Open Images V6 1.0":
                annotations["tags"] = tags_wo_attrs
                annotations["shapes"] = rectangle_shapes_wo_attrs \
                                      + polygon_shapes_wo_attrs

            elif annotation_format == "LFW 1.0":
                annotations["shapes"] = points_wo_attrs \
                                      + tags_wo_attrs

            elif annotation_format == "KITTI 1.0":
                annotations["shapes"] = rectangle_shapes_wo_attrs \
                                            + polygon_shapes_wo_attrs

            elif annotation_format == "Market-1501 1.0":
                tags_with_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["values"][2]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][2]["id"],
                            "value": task["labels"][0]["attributes"][2]["values"][0]
                        }
                    ],
                }]
                annotations["tags"] = tags_with_attrs
            elif annotation_format in ['Kitti Raw Format 1.0',
                    'Sly Point Cloud Format 1.0', 'Datumaro 3D 1.0']:
                velodyne_wo_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                    ],
                    "points": [-3.62, 7.95, -1.03, 0.0, 0.0, 0.0, 1.0, 1.0,
                               1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                    "type": "cuboid_3d",
                    "occluded": False,
                },
                    {
                        "frame": 0,
                        "label_id": task["labels"][0]["id"],
                        "group": 0,
                        "source": "manual",
                        "attributes": [],
                        "points": [23.01, 8.34, -0.76, 0.0, 0.0, 0.0, 1.0, 1.0,
                                   1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
                        "type": "cuboid_3d",
                        "occluded": False,
                    }
                ]
                annotations["shapes"] = velodyne_wo_attrs
            elif annotation_format == "ICDAR Recognition 1.0":
                tags_with_attrs = [{
                    "frame": 1,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        }
                    ],
                }]

                annotations["tags"] = tags_with_attrs

            elif annotation_format == "ICDAR Localization 1.0":
                rectangle_shapes_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                    ],
                    "points": [1.0, 2.1, 10.6, 53.22],
                    "type": "rectangle",
                    "occluded": False,
                }]
                polygon_shapes_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        },
                    ],
                    "points": [20.0, 0.1, 10, 3.22, 4, 7, 10, 30],
                    "type": "polygon",
                    "occluded": False,
                }]

                annotations["shapes"] = rectangle_shapes_with_attrs \
                                      + polygon_shapes_with_attrs

            elif annotation_format == "ICDAR Segmentation 1.0":
                rectangle_shapes_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][2]["id"],
                            "value": task["labels"][0]["attributes"][2]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][3]["id"],
                            "value": task["labels"][0]["attributes"][3]["values"][2]
                        }
                    ],
                    "points": [1.0, 2.1, 10.6, 53.22],
                    "type": "rectangle",
                    "occluded": False,
                }]
                polygon_shapes_with_attrs = [{
                    "frame": 0,
                    "label_id": task["labels"][0]["id"],
                    "group": 0,
                    "source": "manual",
                    "attributes": [
                        {
                            "spec_id": task["labels"][0]["attributes"][0]["id"],
                            "value": task["labels"][0]["attributes"][0]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][1]["id"],
                            "value": task["labels"][0]["attributes"][1]["values"][1]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][2]["id"],
                            "value": task["labels"][0]["attributes"][2]["values"][0]
                        },
                        {
                            "spec_id": task["labels"][0]["attributes"][3]["id"],
                            "value": task["labels"][0]["attributes"][3]["values"][1]
                        }
                    ],
                    "points": [20.0, 0.1, 10, 3.22, 4, 7, 10, 30],
                    "type": "polygon",
                    "occluded": False,
                }]

                annotations["shapes"] = rectangle_shapes_with_attrs \
                                      + polygon_shapes_with_attrs

            else:
                raise Exception("Unknown format {}".format(annotation_format))

            return annotations

        response = self._get_formats(owner)
        self.assertEqual(response.status_code, HTTP_200_OK)
        if owner is not None:
            data = response.data
        else:
            data = self._get_formats(owner).data
        import_formats = data['importers']
        export_formats = data['exporters']
        self.assertTrue(isinstance(import_formats, list) and import_formats)
        self.assertTrue(isinstance(export_formats, list) and export_formats)
        import_formats = { v['name']: v for v in import_formats }
        export_formats = { v['name']: v for v in export_formats }

        formats = { exp: exp if exp in import_formats else None
            for exp in export_formats }
        if 'CVAT 1.1' in import_formats:
            if 'CVAT for video 1.1' in export_formats:
                formats['CVAT for video 1.1'] = 'CVAT 1.1'
            if 'CVAT for images 1.1' in export_formats:
                formats['CVAT for images 1.1'] = 'CVAT 1.1'
        if 'Ultralytics YOLO Detection 1.0' in import_formats:
            if 'Ultralytics YOLO Detection Track 1.0' in export_formats:
                formats['Ultralytics YOLO Detection Track 1.0'] = 'Ultralytics YOLO Detection 1.0'
        if set(import_formats) ^ set(export_formats):
            # NOTE: this may not be an error, so we should not fail
            print("The following import formats have no pair:",
                set(import_formats) - set(export_formats))
            print("The following export formats have no pair:",
                set(export_formats) - set(import_formats))

        for export_format, import_format in formats.items():
            with self.subTest(export_format=export_format,
                    import_format=import_format):
                # 1. create task
                task, jobs = self._create_task(owner, owner, import_format)

                # 2. add annotation
                data = _get_initial_annotation(export_format)
                response = self._put_api_v2_tasks_id_annotations(task["id"], owner, data)
                data["version"] += 1

                self.assertEqual(response.status_code, HTTP_200_OK)
                self._check_response(response, data)

                # 3. download annotation
                if not export_formats[export_format]['enabled']:
                    self._export_task_annotations(
                        owner, task["id"], query_params={"format": export_format},
                        download_locally=False,
                        expected_4xx_status_code=status.HTTP_405_METHOD_NOT_ALLOWED
                    )
                    continue

                response = self._export_task_annotations(
                    owner, task["id"], query_params={"format": export_format},
                )

                # 4. check downloaded data
                self.assertTrue(response.streaming)
                content = io.BytesIO(b"".join(response.streaming_content))
                self._check_dump_content(content, task, jobs, data, export_format)
                content.seek(0)

                # 5. remove annotation form the task
                response = self._delete_api_v2_tasks_id_annotations(task["id"], owner)
                data["version"] += 1
                self.assertEqual(response.status_code, HTTP_204_NO_CONTENT)

                # 6. upload annotation
                if not import_format:
                    continue

                uploaded_data = {
                    "annotation_file": content,
                }
                response = self._upload_api_v2_tasks_id_annotations(
                    task["id"], owner, uploaded_data,
                    "format={}".format(import_format))
                self.assertEqual(response.status_code, HTTP_202_ACCEPTED)

                response = self._upload_api_v2_tasks_id_annotations(
                    task["id"], owner, {},
                    "format={}".format(import_format))
                self.assertEqual(response.status_code, HTTP_201_CREATED)

                # 7. check annotation
                if export_format in {"Segmentation mask 1.1", "MOTS PNG 1.0",
                        "CamVid 1.0", "ICDAR Segmentation 1.0"}:
                    continue # can't really predict the result to check
                response = self._get_api_v2_tasks_id_annotations(task["id"], owner)
                self.assertEqual(response.status_code, HTTP_200_OK)

                data["version"] += 2 # upload is delete + put
                self._check_response(response, data)

                break
    def _check_dump_content(self, content, task, jobs, data, format_name):
        def etree_to_dict(t):
            d = {t.tag: {} if t.attrib else None}
            children = list(t)
            if children:
                dd = defaultdict(list)
                for dc in map(etree_to_dict, children):
                    for k, v in dc.items():
                        dd[k].append(v)
                d = {t.tag: {k: v[0] if len(v) == 1 else v
                    for k, v in dd.items()}}
            if t.attrib:
                d[t.tag].update(('@' + k, v) for k, v in t.attrib.items())
            if t.text:
                text = t.text.strip()
                if not (children or t.attrib):
                    d[t.tag] = text
            return d

        if format_name in {"CVAT for video 1.1", "CVAT for images 1.1"}:
            with tempfile.TemporaryDirectory() as tmp_dir:
                zipfile.ZipFile(content).extractall(tmp_dir)
                xmls = glob(os.path.join(tmp_dir, '**', '*.xml'), recursive=True)
                self.assertTrue(xmls)
                for xml in xmls:
                    xmlroot = ET.parse(xml).getroot()
                    self.assertEqual(xmlroot.tag, "annotations")
                    tags = xmlroot.findall("./meta")
                    self.assertEqual(len(tags), 1)
                    meta = etree_to_dict(tags[0])["meta"]
                    self.assertEqual(meta["task"]["name"], task["name"])
        elif format_name == "PASCAL VOC 1.1":
            self.assertTrue(zipfile.is_zipfile(content))
        elif format_name in [
            "YOLO 1.1", "Ultralytics YOLO Detection 1.0", "Ultralytics YOLO Segmentation 1.0",
            "Ultralytics YOLO Oriented Bounding Boxes 1.0", "Ultralytics YOLO Pose 1.0",
        ]:
            self.assertTrue(zipfile.is_zipfile(content))
        elif format_name in ['Kitti Raw Format 1.0','Sly Point Cloud Format 1.0']:
            self.assertTrue(zipfile.is_zipfile(content))
        elif format_name in ["COCO 1.0", "COCO Keypoints 1.0"]:
            with tempfile.TemporaryDirectory() as tmp_dir:
                zipfile.ZipFile(content).extractall(tmp_dir)
                jsons = glob(os.path.join(tmp_dir, '**', '*.json'), recursive=True)
                self.assertTrue(jsons)
                for json in jsons:
                    coco = coco_loader.COCO(json)
                    self.assertTrue(coco.getAnnIds())
        elif format_name == "Segmentation mask 1.1":
            self.assertTrue(zipfile.is_zipfile(content))


    def _run_coco_annotation_upload_test(self, user):
        def generate_coco_anno():
            return b"""{
            "licenses": [],
            "info": {},
            "categories": [
                {
                "id": 1,
                "name": "car",
                "supercategory": ""
                },
                {
                "id": 2,
                "name": "person",
                "supercategory": ""
                }
            ],
            "images": [
                {
                "coco_url": "",
                "date_captured": "",
                "flickr_url": "",
                "license": 0,
                "id": 0,
                "file_name": "test_1.jpg",
                "height": 720,
                "width": 1280
                }
            ],
            "annotations": [
                {
                "category_id": 1,
                "id": 1,
                "image_id": 0,
                "iscrowd": 0,
                "segmentation": [
                    []
                ],
                "area": 17702.0,
                "bbox": [
                    574.0,
                    407.0,
                    167.0,
                    106.0
                ]
                }
            ]
            }"""

        task, _ = self._create_task(user, user)

        content = io.BytesIO(generate_coco_anno())
        content.seek(0)

        format_name = "COCO 1.0"
        uploaded_data = {
            "annotation_file": content,
        }
        response = self._upload_api_v2_tasks_id_annotations(
            task["id"], user, uploaded_data,
            "format={}".format(format_name))
        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)

        response = self._upload_api_v2_tasks_id_annotations(
            task["id"], user, {}, "format={}".format(format_name))
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

        response = self._get_api_v2_tasks_id_annotations(task["id"], user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_api_v2_tasks_id_annotations_admin(self):
        self._run_api_v2_tasks_id_annotations(self.admin, self.assignee)

    def test_api_v2_tasks_id_annotations_user(self):
        self._run_api_v2_tasks_id_annotations(self.user, self.user)

    def test_api_v2_tasks_id_annotations_dump_load_admin(self):
        self._run_api_v2_tasks_id_annotations_dump_load(self.admin)

    def test_api_v2_tasks_id_annotations_dump_load_user(self):
        self._run_api_v2_tasks_id_annotations_dump_load(self.user)

    def test_api_v2_tasks_id_annotations_dump_load_no_auth(self):
        self._run_api_v2_tasks_id_annotations_dump_load(self.user)

    def test_api_v2_tasks_id_annotations_upload_coco_user(self):
        self._run_coco_annotation_upload_test(self.user)

class ServerShareAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        path = os.path.join(settings.SHARE_ROOT, "file0.txt")
        open(path, "w").write("test string")
        path = os.path.join(settings.SHARE_ROOT, "test1")
        os.makedirs(path)
        path = os.path.join(path, "file1.txt")
        open(path, "w").write("test string")
        directory = os.path.join(settings.SHARE_ROOT, "test1", "test3")
        os.makedirs(directory)
        path = os.path.join(settings.SHARE_ROOT, "test2")
        os.makedirs(path)
        path = os.path.join(path, "file2.txt")
        open(path, "w").write("test string")

    @classmethod
    def tearDownClass(cls):
        super().tearDownClass()
        path = os.path.join(settings.SHARE_ROOT, "file0.txt")
        os.remove(path)
        path = os.path.join(settings.SHARE_ROOT, "test1")
        shutil.rmtree(path)
        path = os.path.join(settings.SHARE_ROOT, "test2")
        shutil.rmtree(path)

    def _run_api_v2_server_share(self, user, directory):
        with ForceLogin(user, self.client):
            response = self.client.get(
                '/api/server/share?directory={}'.format(directory))

        return response

    def _test_api_v2_server_share(self, user):
        data = [
            {"name": "test1", "type": "DIR"},
            {"name": "test2", "type": "DIR"},
            {"name": "file0.txt", "type": "REG"},
        ]

        response = self._run_api_v2_server_share(user, "/")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        compare_objects(
            self=self,
            obj1=sorted(data, key=lambda d: d["name"]),
            obj2=sorted(response.data, key=lambda d: d["name"]),
            ignore_keys=[]
        )

        data = [
            {"name": "file1.txt", "type": "REG"},
            {"name": "test3", "type": "DIR"},
        ]
        response = self._run_api_v2_server_share(user, "/test1")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        compare_objects(
            self=self,
            obj1=sorted(data, key=lambda d: d["name"]),
            obj2=sorted(response.data, key=lambda d: d["name"]),
            ignore_keys=[]
        )

        data = []
        response = self._run_api_v2_server_share(user, "/test1/test3")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        compare_objects(
            self=self,
            obj1=sorted(data, key=lambda d: d["name"]),
            obj2=sorted(response.data, key=lambda d: d["name"]),
            ignore_keys=[]
        )

        data = [
            {"name": "file2.txt", "type": "REG"},
        ]
        response = self._run_api_v2_server_share(user, "/test2")
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        compare_objects(
            self=self,
            obj1=sorted(data, key=lambda d: d["name"]),
            obj2=sorted(response.data, key=lambda d: d["name"]),
            ignore_keys=[]
        )

        response = self._run_api_v2_server_share(user, "/test4")
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_server_share_admin(self):
        self._test_api_v2_server_share(self.admin)

    def test_api_v2_server_share_owner(self):
        self._test_api_v2_server_share(self.owner)

    def test_api_v2_server_share_assignee(self):
        self._test_api_v2_server_share(self.assignee)

    def test_api_v2_server_share_user(self):
        self._test_api_v2_server_share(self.user)

    def test_api_v2_server_share_annotator(self):
        self._test_api_v2_server_share(self.annotator)

    def test_api_v2_server_share_somebody(self):
        self._test_api_v2_server_share(self.somebody)

    def test_api_v2_server_share_no_auth(self):
        response = self._run_api_v2_server_share(None, "/")
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)


class ServerShareDifferentTypesAPITestCase(ApiTestBase):
    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    @staticmethod
    def _create_shared_files(shared_images):
        image = Image.new('RGB', size=(100, 50))
        for img in shared_images:
            img_path = os.path.join(settings.SHARE_ROOT, img)
            if not os.path.exists(os.path.dirname(img_path)):
                os.makedirs(os.path.dirname(img_path))
            image.save(img_path)

    def _run_api_v2_server_share(self, directory):
        with ForceLogin(self.user, self.client):
            response = self.client.get(
                '/api/server/share?directory={}'.format(directory))

        return response

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            self.assertEqual(response.status_code, status.HTTP_201_CREATED)
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid,
                                        data=image_data)
            self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)
            response = self.client.get("/api/tasks/%s" % tid)
            task = response.data

        return task

    def test_api_v2_combined_image_and_directory_extractors(self):
        shared_images = ["data1/street.png", "data1/people.jpeg", "data1/street_1.jpeg", "data1/street_2.jpeg",
                         "data1/street_3.jpeg", "data1/subdir/image_4.jpeg", "data1/subdir/image_5.jpeg",
                         "data1/subdir/image_6.jpeg"]
        images_count = len(shared_images)
        self._create_shared_files(shared_images)
        response = self._run_api_v2_server_share("/data1")
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        shared_images = [img for img in shared_images if os.path.dirname(img) != "/data1/subdir"]
        shared_images.append("/data1/subdir/")
        shared_images.append("/data1/")
        remote_files = {"server_files[%d]" % i: shared_images[i] for i in range(len(shared_images))}

        task = {
            "name": "task combined image and directory extractors",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        image_data = {
            "size": 0,
            "image_quality": 70,
            "compressed_chunk_type": "imageset",
            "original_chunk_type": "imageset",
            "client_files": [],
            "remote_files": [],
            "use_zip_chunks": False,
            "use_cache": False,
            "copy_data": False
        }
        image_data.update(remote_files)
        # create task with server
        task = self._create_task(task, image_data)
        response = self._get_request("/api/tasks/%s/data/meta" % task["id"], self.user)
        self.assertEqual(len(response.data["frames"]), images_count)


class TaskAnnotation2DContext(ApiTestBase):
    def setUp(self):
        super().setUp()
        self.task = {
            "name": "my archive task without copying #11",
            "overlap": 0,
            "segment_size": 0,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }

    @classmethod
    def setUpTestData(cls):
        create_db_users(cls)

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid,
                                            data=image_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code

            response = self.client.get("/api/tasks/%s" % tid)
            task = response.data

        return task

    def create_zip_archive_with_related_images(self, file_name, test_dir, context_images_info):
        with tempfile.TemporaryDirectory() as tmp_dir:
            for img in context_images_info:
                image = Image.new('RGB', size=(100, 50))
                image.save(os.path.join(tmp_dir, img), 'png')
                if context_images_info[img]:
                    related_path = os.path.join(tmp_dir, "related_images", img.replace(".", "_"))
                    os.makedirs(related_path)
                    image.save(os.path.join(related_path, f"related_{img}"), 'png')

            zip_file_path = os.path.join(test_dir, file_name)
            shutil.make_archive(zip_file_path, 'zip', tmp_dir)
        return f"{zip_file_path}.zip"

    def test_check_flag_has_related_context(self):
        with TestDir() as test_dir:
            test_cases = {
                "All images with context": {"image_1.png": True, "image_2.png": True},
                "One image with context": {"image_1.png": True, "image_2.png": False}
            }
            for test_case, context_img_data in test_cases.items():
                filename = self.create_zip_archive_with_related_images(test_case, test_dir, context_img_data)
                img_data = {
                    "client_files[0]": open(filename, 'rb'),
                    "image_quality": 75,
                }
                task = self._create_task(self.task, img_data)
                task_id = task["id"]

                response = self._get_request("/api/tasks/%s/data/meta" % task_id, self.admin)
                for frame in response.data["frames"]:
                    self.assertEqual(context_img_data[frame["name"]], frame["has_related_context"])

    def test_fetch_related_image_from_server(self):
        test_name = self._testMethodName
        context_img_data ={"image_1.png": True}
        with TestDir() as test_dir:
            filename = self.create_zip_archive_with_related_images(test_name, test_dir, context_img_data)
            img_data = {
                "client_files[0]": open(filename, 'rb'),
                "image_quality": 75,
            }
            task = self._create_task(self.task , img_data)
            task_id = task["id"]
            query_params = {
                "quality": "original",
                "type": "context_image",
                "number": 0
            }
            response = self._get_request(
                "/api/tasks/%s/data" % task_id, self.admin, query_params=query_params
            )
            self.assertEqual(response.status_code, status.HTTP_200_OK)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\tests\test_rest_api_3D.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


import copy
import itertools
import os
import os.path as osp
import tempfile
import xml.etree.ElementTree as ET
import zipfile
from collections import defaultdict
from glob import glob
from shutil import copyfile

from django.contrib.auth.models import Group, User
from rest_framework import status

from cvat.apps.dataset_manager.task import TaskAnnotation
from cvat.apps.dataset_manager.tests.utils import TestDir
from cvat.apps.engine.media_extractors import ValidateDimension
from cvat.apps.engine.tests.utils import ExportApiTestBase, ForceLogin, get_paginated_collection

CREATE_ACTION = "create"
UPDATE_ACTION = "update"
DELETE_ACTION = "delete"


class _DbTestBase(ExportApiTestBase):
    @classmethod
    def setUpTestData(cls):
        cls.create_db_users()

    @classmethod
    def create_db_users(cls):
        (group_admin, _) = Group.objects.get_or_create(name="admin")
        (group_user, _) = Group.objects.get_or_create(name="user")

        user_admin = User.objects.create_superuser(username="admin", email="",
            password="admin")
        user_admin.groups.add(group_admin)
        user_dummy = User.objects.create_user(username="user", password="user")
        user_dummy.groups.add(group_user)

        cls.admin = user_admin
        cls.user = user_dummy

    def _put_api_v2_task_id_annotations(self, tid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.put("/api/tasks/%s/annotations" % tid,
                                       data=data, format="json")

        return response

    def _put_api_v2_job_id_annotations(self, jid, data):
        with ForceLogin(self.admin, self.client):
            response = self.client.put("/api/jobs/%s/annotations" % jid,
                                       data=data, format="json")

        return response

    def _patch_api_v2_task_id_annotations(self, tid, data, action, user):
        with ForceLogin(user, self.client):
            response = self.client.patch(
                "/api/tasks/{}/annotations?action={}".format(tid, action),
                data=data, format="json")

        return response

    def _patch_api_v2_job_id_annotations(self, jid, data, action, user):
        with ForceLogin(user, self.client):
            response = self.client.patch(
                "/api/jobs/{}/annotations?action={}".format(jid, action),
                data=data, format="json")

        return response

    def _create_task(self, data, image_data):
        with ForceLogin(self.user, self.client):
            response = self.client.post('/api/tasks', data=data, format="json")
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post("/api/tasks/%s/data" % tid, data=image_data)
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get("/api/tasks/%s" % tid)

            if 200 <= response.status_code < 400:
                labels_response = list(get_paginated_collection(
                    lambda page: self.client.get("/api/labels?task_id=%s&page=%s" % (tid, page))
                ))
                response.data["labels"] = labels_response

            task = response.data

        return task

    @staticmethod
    def _get_tmp_annotation(task, annotation):
        tmp_annotations = copy.deepcopy(annotation)
        for item in tmp_annotations:
            if item in ["tags", "shapes", "tracks"]:
                for index_elem, _ in enumerate(tmp_annotations[item]):
                    tmp_annotations[item][index_elem]["label_id"] = task["labels"][0]["id"]

                    for index_attribute, attribute in enumerate(task["labels"][0]["attributes"]):
                        spec_id = task["labels"][0]["attributes"][index_attribute]["id"]

                        value = attribute["default_value"]

                        if item == "tracks" and attribute["mutable"]:
                            for index_shape, _ in enumerate(tmp_annotations[item][index_elem]["shapes"]):
                                tmp_annotations[item][index_elem]["shapes"][index_shape]["attributes"].append({
                                    "spec_id": spec_id,
                                    "value": value,
                                })
                        else:
                            tmp_annotations[item][index_elem]["attributes"].append({
                                "spec_id": spec_id,
                                "value": value,
                            })
        return tmp_annotations

    def _get_jobs(self, task_id):
        with ForceLogin(self.admin, self.client):
            values = get_paginated_collection(lambda page:
                self.client.get("/api/jobs?task_id={}&page={}".format(task_id, page))
            )
        return values

    def _upload_file(self, url, data, user):
        response = self._put_request(url, user, data={"annotation_file": data}, format="multipart")
        self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)
        response = self._put_request(url, user)
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)

    def _generate_url_upload_tasks_annotations(self, task_id, upload_format_name):
        return f"/api/tasks/{task_id}/annotations?format={upload_format_name}"

    def _generate_url_upload_job_annotations(self, job_id, upload_format_name):
        return f"/api/jobs/{job_id}/annotations?format={upload_format_name}"

    def _remove_annotations(self, tid):
        response = self._delete_request(f"/api/tasks/{tid}/annotations", self.admin)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        return response

    def _delete_task(self, tid):
        response = self._delete_request('/api/tasks/{}'.format(tid), self.admin)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        return response

    def _check_dump_content(self, content, task_data, format_name, related_files=True):
        def etree_to_dict(t):
            d = {t.tag: {} if t.attrib else None}
            children = list(t)
            if children:
                dd = defaultdict(list)
                for dc in map(etree_to_dict, children):
                    for k, v in dc.items():
                        dd[k].append(v)
                d = {t.tag: {k: v[0] if len(v) == 1 else v
                             for k, v in dd.items()}}
            if t.attrib:
                d[t.tag].update(('@' + k, v)
                                for k, v in t.attrib.items())
            if t.text:
                text = t.text.strip()
                if children or t.attrib:
                    if text:
                        d[t.tag]['#text'] = text
                else:
                    d[t.tag] = text
            return d
        if format_name == "Kitti Raw Format 1.0":
            with tempfile.TemporaryDirectory() as tmp_dir:
                zipfile.ZipFile(content).extractall(tmp_dir)
                xmls = glob(osp.join(tmp_dir, '**', '*.xml'), recursive=True)
                self.assertTrue(xmls)
                for xml in xmls:
                    xmlroot = ET.parse(xml).getroot()
                    self.assertEqual(xmlroot.tag, "boost_serialization")
                    items = xmlroot.findall("./tracklets/item")
                    self.assertEqual(len(items), len(task_data["shapes"]))
        elif format_name == "Sly Point Cloud Format 1.0":
            with tempfile.TemporaryDirectory() as tmp_dir:
                checking_files = [osp.join(tmp_dir, "key_id_map.json"),
                                  osp.join(tmp_dir, "meta.json"),
                                  osp.join(tmp_dir, "ds0", "ann", "000001.pcd.json"),
                                  osp.join(tmp_dir, "ds0", "ann", "000002.pcd.json"),
                                  osp.join(tmp_dir, "ds0", "ann","000003.pcd.json")]
                if related_files:
                    checking_files.extend([osp.join(tmp_dir, "ds0", "related_images", "000001.pcd_pcd", "000001.png.json"),
                                  osp.join(tmp_dir, "ds0", "related_images", "000002.pcd_pcd", "000002.png.json"),
                                  osp.join(tmp_dir, "ds0", "related_images", "000003.pcd_pcd", "000003.png.json")])
                zipfile.ZipFile(content).extractall(tmp_dir)
                jsons = glob(osp.join(tmp_dir, '**', '*.json'), recursive=True)
                self.assertTrue(jsons)
                self.assertTrue(set(checking_files).issubset(set(jsons)))


class Task3DTest(_DbTestBase):
    @classmethod
    def setUpClass(cls):
        super().setUpClass()
        cls.format_names = ["Sly Point Cloud Format 1.0", "Kitti Raw Format 1.0"]
        cls._image_sizes = {}
        cls.pointcloud_pcd_filename = "test_canvas3d.zip"
        cls.pointcloud_pcd_path = osp.join(os.path.dirname(__file__), 'assets', cls.pointcloud_pcd_filename)
        image_sizes = []
        zip_file = zipfile.ZipFile(cls.pointcloud_pcd_path )
        for info in zip_file.namelist():
            if info.rsplit(".", maxsplit=1)[-1] == "pcd":
                with zip_file.open(info, "r") as file:
                    data = ValidateDimension.get_pcd_properties(file)
                    image_sizes.append((int(data["WIDTH"]), int(data["HEIGHT"])))
        cls.task = {
            "name": "main task",
            "owner_id": 2,
            "assignee_id": 2,
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "car"},
                {"name": "person"},
            ]
        }
        cls.task_with_attributes = {
            "name": "task with attributes",
            "owner_id": 2,
            "assignee_id": 2,
            "overlap": 0,
            "segment_size": 100,
            "labels": [
                {"name": "car",
                 "color": "#2080c0",
                 "attributes": [
                     {
                         "name": "radio_name",
                         "mutable": False,
                         "input_type": "radio",
                         "default_value": "x1",
                         "values": ["x1", "x2", "x3"]
                     },
                     {
                         "name": "check_name",
                         "mutable": True,
                         "input_type": "checkbox",
                         "default_value": "false",
                         "values": ["false"]
                     },
                     {
                         "name": "text_name",
                         "mutable": False,
                         "input_type": "text",
                         "default_value": "qwerty",
                         "values": ["qwerty"]
                     },
                     {
                         "name": "number_name",
                         "mutable": False,
                         "input_type": "number",
                         "default_value": "-4.0",
                         "values": ["-4", "4", "1"]
                     }
                 ]
                 },
                {"name": "person",
                 "color": "#c06060",
                 "attributes": []
                 },
            ]
        }
        cls.task_many_jobs = {
            "name": "task several jobs",
            "owner_id": 2,
            "assignee_id": 2,
            "overlap": 3,
            "segment_size": 1,
            "labels": [
                {
                    "name": "car",
                    "color": "#c06060",
                    "id": 1,
                    "attributes": []
                }
            ]
        }
        cls.cuboid_example = {
            "version": 0,
            "tags": [],
            "shapes": [
                {
                    "type": "cuboid",
                    "occluded": False,
                    "outside": False,
                    "z_order": 0,
                    "points": [0.16, 0.20, -0.26, 0, -0.14, 0, 4.84, 4.48, 4.12, 0, 0, 0, 0, 0, 0, 0],
                    "rotation": 0,
                    "frame": 0,
                    "label_id": None,
                    "group": 0,
                    "source": "manual",
                    "elements": [],
                    "attributes": []
                },
            ],
            "tracks": []
        }
        cls._image_sizes[cls.pointcloud_pcd_filename] = image_sizes
        cls.expected_action = {
            cls.admin: {'name': 'admin', 'code': status.HTTP_200_OK, 'annotation_changed': True},
            cls.user: {'name': 'user', 'code': status.HTTP_200_OK, 'annotation_changed': True},
            None: {'name': 'none', 'code': status.HTTP_401_UNAUTHORIZED, 'annotation_changed': False},
        }
        cls.expected_dump_upload = {
            cls.admin: {'name': 'admin', 'file_exists': True, 'annotation_loaded': True},
            cls.user: {'name': 'user', 'file_exists': True, 'annotation_loaded': True},
            None: {'name': 'none', 'file_exists': False, 'annotation_loaded': False},
        }

    def copy_pcd_file_and_get_task_data(self, test_dir):
        tmp_file = osp.join(test_dir, self.pointcloud_pcd_filename)
        copyfile(self.pointcloud_pcd_path, tmp_file)
        task_data = {
            "client_files[0]": open(tmp_file, 'rb'),
            "image_quality": 100,
        }
        return task_data

    def test_api_v2_create_annotation_in_job(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            annotation = self._get_tmp_annotation(task, self.cuboid_example)
            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    response = self._patch_api_v2_task_id_annotations(task_id, annotation, CREATE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"])
                    if edata["annotation_changed"]:
                        task_ann = TaskAnnotation(task_id)
                        task_ann.init_from_db()
                        task_shape = task_ann.data["shapes"][0]
                        task_shape.pop("id")
                        self.assertEqual(task_shape, annotation["shapes"][0])
                        self._remove_annotations(task_id)

    def test_api_v2_update_annotation_in_task(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            annotation = self._get_tmp_annotation(task, self.cuboid_example)
            response = self._put_api_v2_task_id_annotations(task_id, annotation)
            self.assertEqual(response.status_code, status.HTTP_200_OK)

            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()
                    annotation["shapes"][0]["points"] = [x + 0.1 for x in annotation["shapes"][0]["points"]]
                    annotation["shapes"][0]["id"] = task_ann_prev.data["shapes"][0]["id"]
                    response = self._patch_api_v2_task_id_annotations(task_id, annotation, UPDATE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"], task_id)

                    if edata["annotation_changed"]:
                        task_ann = TaskAnnotation(task_id)
                        task_ann.init_from_db()
                        self.assertEqual(task_ann.data["shapes"], annotation["shapes"])

    def test_api_v2_remove_annotation_in_task(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            annotation = self._get_tmp_annotation(task, self.cuboid_example)

            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    response = self._patch_api_v2_task_id_annotations(task_id, annotation, CREATE_ACTION, self.admin)
                    self.assertEqual(response.status_code, status.HTTP_200_OK)
                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()
                    annotation["shapes"][0]["id"] = task_ann_prev.data["shapes"][0]["id"]

                    response = self._patch_api_v2_task_id_annotations(task_id, annotation, DELETE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"])

                    if edata["annotation_changed"]:
                        task_ann = TaskAnnotation(task_id)
                        task_ann.init_from_db()
                        self.assertTrue(len(task_ann.data["shapes"]) == 0)

    def test_api_v2_create_annotation_in_jobs(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            annotation = self._get_tmp_annotation(task, self.cuboid_example)
            jobs = self._get_jobs(task_id)
            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    response = self._patch_api_v2_job_id_annotations(jobs[0]["id"], annotation, CREATE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"])

                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()
                    if len(task_ann.data["shapes"]):
                        task_shape = task_ann.data["shapes"][0]
                        task_shape.pop("id")
                        self.assertEqual(task_shape, annotation["shapes"][0])
                        self._remove_annotations(task_id)

    def test_api_v2_update_annotation_in_job(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            jobs = self._get_jobs(task_id)
            annotation = self._get_tmp_annotation(task, self.cuboid_example)

            response = self._put_api_v2_task_id_annotations(task_id, annotation)
            self.assertEqual(response.status_code, status.HTTP_200_OK)

            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()

                    annotation["shapes"][0]["points"] = [x + 0.1 for x in annotation["shapes"][0]["points"]]
                    annotation["shapes"][0]["id"] = task_ann_prev.data["shapes"][0]["id"]

                    response = self._patch_api_v2_job_id_annotations(jobs[0]["id"], annotation, UPDATE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"])

                    if edata["annotation_changed"]:
                        task_ann = TaskAnnotation(task_id)
                        task_ann.init_from_db()
                        self.assertEqual(task_ann.data["shapes"], annotation["shapes"])

    def test_api_v2_remove_annotation_in_job(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            jobs = self._get_jobs(task_id)
            annotation = self._get_tmp_annotation(task, self.cuboid_example)

            for user, edata in list(self.expected_action.items()):
                with self.subTest(format=edata["name"]):
                    response = self._patch_api_v2_job_id_annotations(jobs[0]["id"], annotation, CREATE_ACTION, self.admin)
                    self.assertEqual(response.status_code, status.HTTP_200_OK)

                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()
                    annotation["shapes"][0]["id"] = task_ann_prev.data["shapes"][0]["id"]
                    response = self._patch_api_v2_job_id_annotations(jobs[0]["id"], annotation, DELETE_ACTION, user)
                    self.assertEqual(response.status_code, edata["code"])

                    if edata["annotation_changed"]:
                        task_ann = TaskAnnotation(task_id)
                        task_ann.init_from_db()
                        self.assertTrue(len(task_ann.data["shapes"]) == 0)

    def test_api_v2_dump_and_upload_annotation(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]

            for format_name in self.format_names:
                annotation = self._get_tmp_annotation(task, self.cuboid_example)
                response = self._put_api_v2_task_id_annotations(task_id, annotation)
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                task_ann_prev = TaskAnnotation(task_id)
                task_ann_prev.init_from_db()

                for user, edata in list(self.expected_dump_upload.items()):
                    with self.subTest(format=f"{format_name}_{edata['name']}_dump"):
                        self._clear_temp_data() # clean up from previous tests and iterations
                        file_name = osp.join(test_dir, f"{format_name}_{edata['name']}.zip")
                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        self._export_task_annotations(user, task_id, query_params={"format": format_name}, expected_4xx_status_code=expected_4xx_status_code, file_path=file_name)

                        if bool(user):
                            with open(file_name, "rb") as f:
                                self._check_dump_content(f, task_ann_prev.data, format_name, related_files=False)
                        self.assertEqual(osp.exists(file_name), edata['file_exists'])

                self._remove_annotations(task_id)
                with self.subTest(format=f"{format_name}_upload"):
                    file_name = osp.join(test_dir, f"{format_name}_admin.zip")
                    url = self._generate_url_upload_tasks_annotations(task_id, format_name)

                    with open(file_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)
                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()

                    task_ann_prev.data["shapes"][0].pop("id")
                    task_ann.data["shapes"][0].pop("id")
                    self.assertEqual(len(task_ann_prev.data["shapes"]), len(task_ann.data["shapes"]))
                    self.assertEqual(task_ann_prev.data["shapes"], task_ann.data["shapes"])

    def test_api_v2_rewrite_annotation(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            for format_name in self.format_names:
                with self.subTest(format=f"{format_name}"):
                    annotation = self._get_tmp_annotation(task, self.cuboid_example)
                    response = self._put_api_v2_task_id_annotations(task_id, annotation)
                    self.assertEqual(response.status_code, status.HTTP_200_OK)
                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()
                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    self._export_task_annotations(
                        self.admin, task_id, query_params={"format": format_name},
                        file_path=file_name
                    )
                    self.assertTrue(osp.exists(file_name))

                    self._remove_annotations(task_id)
                    # rewrite annotation
                    annotation_copy = copy.deepcopy(annotation)
                    annotation_copy["shapes"][0]["points"] = [0] * 16
                    response = self._put_api_v2_task_id_annotations(task_id, annotation_copy)
                    self.assertEqual(response.status_code, status.HTTP_200_OK)

                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    url = self._generate_url_upload_tasks_annotations(task_id, format_name)

                    with open(file_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)
                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()

                    task_ann_prev.data["shapes"][0].pop("id")
                    task_ann.data["shapes"][0].pop("id")
                    self.assertEqual(len(task_ann_prev.data["shapes"]), len(task_ann.data["shapes"]))
                    self.assertEqual(task_ann_prev.data["shapes"], task_ann.data["shapes"])

    def test_api_v2_dump_and_upload_empty_annotation(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]
            task_ann_prev = TaskAnnotation(task_id)
            task_ann_prev.init_from_db()

            for format_name in self.format_names:
                with self.subTest(format=f"{format_name}"):
                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    self._export_task_annotations(
                        self.admin, task_id, query_params={"format": format_name},
                        file_path=file_name
                    )
                    self.assertTrue(osp.exists(file_name))

                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    url = self._generate_url_upload_tasks_annotations(task_id, format_name)

                    with open(file_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)

                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()

                    self.assertEqual(len(task_ann.data["shapes"]), 0)
                    self.assertEqual(task_ann_prev.data["shapes"], task_ann.data["shapes"])

    def test_api_v2_dump_and_upload_several_jobs(self):
        job_test_cases = ["first", "all"]
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task_many_jobs, task_data)
            task_id = task["id"]
            annotation = self._get_tmp_annotation(task, self.cuboid_example)

            for format_name, job_test_case in itertools.product(self.format_names, job_test_cases):
                with self.subTest(format=f"{format_name}_{job_test_case}"):
                    jobs = self._get_jobs(task_id)
                    if job_test_case == "all":
                        for job in jobs:
                            response = self._put_api_v2_job_id_annotations(job["id"], annotation)
                            self.assertEqual(response.status_code, status.HTTP_200_OK)
                    else:
                        response = self._put_api_v2_job_id_annotations(jobs[1]["id"], annotation)
                        self.assertEqual(response.status_code, status.HTTP_200_OK)
                    task_ann_prev = TaskAnnotation(task_id)
                    task_ann_prev.init_from_db()
                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    self._export_task_annotations(
                        self.admin, task_id, query_params={"format": format_name},
                        file_path=file_name
                    )
                    self._remove_annotations(task_id)

    def test_api_v2_upload_annotation_with_attributes(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task_with_attributes, task_data)
            task_id = task["id"]

            for format_name in self.format_names:
                annotation = self._get_tmp_annotation(task, self.cuboid_example)
                response = self._put_api_v2_task_id_annotations(task_id, annotation)
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                task_ann_prev = TaskAnnotation(task_id)
                task_ann_prev.init_from_db()

                with self.subTest(format=f"{format_name}_dump"):
                    file_name = osp.join(test_dir, f"{format_name}.zip")

                    self._export_task_annotations(
                        self.admin, task_id, query_params={"format": format_name},
                        file_path=file_name
                    )
                    self.assertTrue(osp.exists(file_name))

                self._remove_annotations(task_id)
                with self.subTest(format=f"{format_name}_upload"):
                    file_name = osp.join(test_dir, f"{format_name}.zip")
                    url = self._generate_url_upload_tasks_annotations(task_id, format_name)

                    with open(file_name, 'rb') as binary_file:
                        self._upload_file(url, binary_file, self.admin)
                    task_ann = TaskAnnotation(task_id)
                    task_ann.init_from_db()

                    task_ann_prev.data["shapes"][0].pop("id")
                    task_ann.data["shapes"][0].pop("id")
                    self.assertEqual(task_ann_prev.data["shapes"][0]["attributes"],
                                     task_ann.data["shapes"][0]["attributes"])

    def test_api_v2_export_dataset(self):
        with TestDir() as test_dir:
            task_data = self.copy_pcd_file_and_get_task_data(test_dir)
            task = self._create_task(self.task, task_data)
            task_id = task["id"]

            for format_name in self.format_names:
                annotation = self._get_tmp_annotation(task, self.cuboid_example)
                response = self._put_api_v2_task_id_annotations(task_id, annotation)
                self.assertEqual(response.status_code, status.HTTP_200_OK)
                task_ann_prev = TaskAnnotation(task_id)
                task_ann_prev.init_from_db()

                for user, edata in list(self.expected_dump_upload.items()):
                    with self.subTest(format=f"{format_name}_{edata['name']}_export"):
                        self._clear_temp_data() # clean up from previous tests and iterations

                        file_name = osp.join(test_dir, f"{format_name}_{edata['name']}.zip")
                        expected_4xx_status_code = None if user else status.HTTP_401_UNAUTHORIZED
                        self._export_task_dataset(
                            user, task_id, query_params={"format": format_name},
                            expected_4xx_status_code=expected_4xx_status_code, file_path=file_name
                        )

                        if bool(user):
                            with open(file_name, "rb") as f:
                                self._check_dump_content(
                                    f, task_ann_prev.data, format_name, related_files=False
                                )
                        self.assertEqual(osp.exists(file_name), edata['file_exists'])



# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\tests\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import itertools
import logging
import os
import shutil
from collections.abc import Iterator, Sequence
from contextlib import contextmanager
from io import BytesIO
from pathlib import Path
from typing import Any, Callable, TypeVar
from urllib.parse import urlencode

import av
import django_rq
import numpy as np
from django.conf import settings
from django.core.cache import caches
from django.http.response import HttpResponse
from PIL import Image
from rest_framework import status
from rest_framework.response import Response
from rest_framework.test import APITestCase

T = TypeVar('T')


@contextmanager
def logging_disabled():
    old_level = logging.getLogger().manager.disable

    try:
        logging.disable(logging.CRITICAL)
        yield
    finally:
        logging.disable(old_level)


class ForceLogin:
    def __init__(self, user, client):
        self.user = user
        self.client = client

    def __enter__(self):
        if self.user:
            self.client.force_login(self.user, backend='django.contrib.auth.backends.ModelBackend')

        return self

    def __exit__(self, exception_type, exception_value, traceback):
        if self.user:
            self.client.logout()


def clear_rq_jobs():
    for queue_name in settings.RQ_QUEUES:
        queue = django_rq.get_queue(queue_name)

        # Remove actual jobs
        queue.empty()

        # Clean up the registries
        for registry in [
            queue.failed_job_registry,
            queue.finished_job_registry,
            queue.started_job_registry,
            queue.scheduled_job_registry,
        ]:
            for job_id in registry.get_job_ids():
                registry.remove(job_id)

        # Remove orphaned jobs that can't be normally reported by DjangoRQ
        # https://github.com/rq/django-rq/issues/73
        for key in queue.connection.keys('rq:job:*'):
            job_id = key.decode().split('rq:job:', maxsplit=1)[1]
            job = queue.fetch_job(job_id)
            if not job:
                # The job can belong to a different queue, using the same connection
                continue

            job.delete()

        # Clean up the scheduler, if any
        try:
            scheduler = django_rq.get_scheduler(queue_name, queue)
        except ImportError:
            # If the scheduler is not enabled, an exception is thrown
            continue

        try:
            scheduler.acquire_lock()
            for job in scheduler.get_jobs():
                scheduler.cancel(job)
        finally:
            scheduler.remove_lock()


class ApiTestBase(APITestCase):
    def _clear_temp_data(self):
        # Clear server frame/chunk cache.
        # The parent class clears DB changes, and it can lead to under-cleaned task data,
        # which can affect other tests.
        # This situation is not expected to happen on a real server, because
        # cache keys include Data object ids, which cannot be reused or freed
        # in real scenarios
        for cache in caches.all(initialized_only=True):
            cache.clear()

        # Clear any remaining RQ jobs produced by the tests executed
        self._clear_rq_jobs()

        # clear cache files created after previous exports
        export_cache_dir = Path(settings.EXPORT_CACHE_ROOT)
        for child in export_cache_dir.iterdir():
            if child.is_dir():
                shutil.rmtree(child)
            else:
                os.remove(child)

    def _clear_rq_jobs(self):
        clear_rq_jobs()

    def setUp(self):
        self._clear_temp_data()

        super().setUp()
        self.client = self.client_class()

    def _get_request(self, path: str, user: str, *, query_params: dict[str, Any] | None = None) -> Response:
        with ForceLogin(user, self.client):
            response = self.client.get(path, data=query_params)
        return response

    def _delete_request(self, path: str, user: str):
        with ForceLogin(user, self.client):
            response = self.client.delete(path)
        return response

    def _post_request(
        self,
        path: str,
        user: str,
        *,
        format: str = "json",  # pylint: disable=redefined-builtin
        query_params: dict[str, Any] = None,
        data: dict[str, Any] | None = None
    ):
        if query_params:
            # Note: once we upgrade to Django 5.1+, this should be changed to pass query_params
            # directly to self.client.
            assert "?" not in path
            path += "?" + urlencode(query_params)
        with ForceLogin(user, self.client):
            response = self.client.post(path, data=data, format=format)
        return response

    def _patch_request(self, path: str, user: str, *, data: dict[str, Any] | None = None):
        with ForceLogin(user, self.client):
            response = self.client.patch(path, data=data, format="json")
        return response

    def _put_request(
        self,
        url: str,
        user: str,
        *,
        format: str = "json",  # pylint: disable=redefined-builtin
        data: dict[str, Any] | None = None,
    ):
        with ForceLogin(user, self.client):
            response = self.client.put(url, data=data, format=format)
        return response

    def _check_request_status(
        self,
        user: str,
        rq_id: str,
        *,
        expected_4xx_status_code: int | None = None,
    ):
        response = self._get_request(f"/api/requests/{rq_id}", user)
        self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_200_OK)
        if expected_4xx_status_code is not None:
            return

        request_status = response.json()["status"]
        assert request_status == "finished", f"The last request status was {request_status}"
        return response


class ExportApiTestBase(ApiTestBase):
    def _export(
        self,
        user: str,
        api_path: str,
        *,
        query_params: dict[str, Any] | None = None,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        response = self._post_request(api_path, user, query_params=query_params)
        self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_202_ACCEPTED)

        rq_id = response.json().get("rq_id")
        if expected_4xx_status_code:
            # export task by admin to get real rq_id
            response = self._post_request(api_path, self.admin, query_params=query_params)
            self.assertEqual(response.status_code, status.HTTP_202_ACCEPTED)
            rq_id = response.json().get("rq_id")

        assert rq_id, "The rq_id param was not found in the server response"

        response = self._check_request_status(user, rq_id, expected_4xx_status_code=expected_4xx_status_code)

        if not download_locally:
            return response

        # get actual result URL to check that server returns 401/403 when a user tries to download prepared file
        if expected_4xx_status_code:
            response = self._check_request_status(self.admin, rq_id)

        result_url = response.json().get("result_url")
        assert result_url, "The result_url param was not found in the server response"

        response = self._get_request(result_url, user)
        self.assertEqual(response.status_code, expected_4xx_status_code or status.HTTP_200_OK)

        if not expected_4xx_status_code and file_path:
            with open(file_path, "wb") as f:
                f.write(response.getvalue())

        return response

    def _export_task_backup(
        self,
        user: str,
        task_id: int,
        *,
        query_params: dict | None = None,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        return self._export(
            user, f"/api/tasks/{task_id}/backup/export",
            query_params=query_params,
            download_locally=download_locally, file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_project_backup(
        self,
        user: str,
        project_id: int,
        *,
        query_params: dict | None = None,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        return self._export(
            user, f"/api/projects/{project_id}/backup/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_project_dataset(
        self,
        user: str,
        project_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = True

        return self._export(
            user, f"/api/projects/{project_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_project_annotations(
        self,
        user: str,
        project_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = False

        return self._export(
            user, f"/api/projects/{project_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_task_dataset(
        self,
        user: str,
        task_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = True

        return self._export(
            user, f"/api/tasks/{task_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_task_annotations(
        self,
        user: str,
        task_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = False

        return self._export(
            user, f"/api/tasks/{task_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_job_dataset(
        self,
        user: str,
        job_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = True

        return self._export(
            user, f"/api/jobs/{job_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

    def _export_job_annotations(
        self,
        user: str,
        job_id: int,
        *,
        query_params: dict,
        download_locally: bool = True,
        file_path: str | None = None,
        expected_4xx_status_code: int | None = None,
    ):
        query_params["save_images"] = False

        return self._export(
            user, f"/api/jobs/{job_id}/dataset/export",
            query_params=query_params,
            download_locally=download_locally,
            file_path=file_path,
            expected_4xx_status_code=expected_4xx_status_code
        )

def generate_image_file(filename, size=(100, 100)):
    assert os.path.splitext(filename)[-1].lower() in ['', '.jpg', '.jpeg'], \
        "This function supports only jpeg images. Please add the .jpg extension to the file name"

    f = BytesIO()
    image = Image.new('RGB', size=size)
    image.save(f, 'jpeg')
    f.name = filename
    f.seek(0)
    return f


def generate_video_file(filename, width=1920, height=1080, duration=1, fps=25, codec_name='mpeg4'):
    f = BytesIO()
    total_frames = duration * fps
    file_ext = os.path.splitext(filename)[1][1:]
    container = av.open(f, mode='w', format=file_ext)

    stream = container.add_stream(codec_name=codec_name, rate=fps)
    stream.width = width
    stream.height = height
    stream.pix_fmt = 'yuv420p'

    for frame_i in range(total_frames):
        img = np.empty((stream.width, stream.height, 3))
        img[:, :, 0] = 0.5 + 0.5 * np.sin(2 * np.pi * (0 / 3 + frame_i / total_frames))
        img[:, :, 1] = 0.5 + 0.5 * np.sin(2 * np.pi * (1 / 3 + frame_i / total_frames))
        img[:, :, 2] = 0.5 + 0.5 * np.sin(2 * np.pi * (2 / 3 + frame_i / total_frames))

        img = np.round(255 * img).astype(np.uint8)
        img = np.clip(img, 0, 255)

        frame = av.VideoFrame.from_ndarray(img, format='rgb24')
        for packet in stream.encode(frame):
            container.mux(packet)

    # Flush stream
    for packet in stream.encode():
        container.mux(packet)

    # Close the file
    container.close()
    f.name = filename
    f.seek(0)

    return [(width, height)] * total_frames, f

def get_paginated_collection(
    request_chunk_callback: Callable[[int], HttpResponse]
) -> Iterator[T]:
    values = []

    for page in itertools.count(start=1):
        response = request_chunk_callback(page)
        data = response.json()
        values.extend(data["results"])
        if not data.get('next'):
            break

    return values


def filter_dict(
    d: dict[str, Any], *, keep: Sequence[str] = None, drop: Sequence[str] = None
) -> dict[str, Any]:
    return {k: v for k, v in d.items() if (not keep or k in keep) and (not drop or k not in drop)}


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\engine\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class EventsConfig(AppConfig):
    name = "cvat.apps.events"

    def ready(self):
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)

        from . import signals  # pylint: disable=unused-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\cache.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

_caches = {}


class DeleteCache:
    def __init__(self, cache_id):
        from cvat.apps.engine.models import Comment, Issue, Job, Task

        self._cache = _caches.setdefault(
            cache_id,
            {
                Task: {},
                Job: {},
                Issue: {},
                Comment: {},
            },
        )

    def set(self, instance_class, instance_id, value):
        self._cache[instance_class][instance_id] = value

    def pop(self, instance_class, instance_id, default=None):
        if instance_class in self._cache and instance_id in self._cache[instance_class]:
            return self._cache[instance_class].pop(instance_id, default)

    def has_key(self, instance_class, instance_id):
        if instance_class in self._cache and instance_id in self._cache[instance_class]:
            return True
        return False

    def clear(self):
        self._cache.clear()


def get_cache():
    from .handlers import request_info

    return DeleteCache(request_info()["id"])


def clear_cache():
    get_cache().clear()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\const.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import datetime

MAX_EVENT_DURATION = datetime.timedelta(seconds=100)
WORKING_TIME_RESOLUTION = datetime.timedelta(milliseconds=1)
WORKING_TIME_SCOPE = "send:working_time"
USER_ACTIVITY_SCOPE = "user:activity"
COMPRESSED_EVENT_SCOPES = frozenset(("change:frame",))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\event.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datetime import datetime, timezone
from typing import Optional

from django.db import transaction
from rest_framework.renderers import JSONRenderer

from cvat.apps.engine.log import vlogger


def event_scope(action, resource):
    return f"{action}:{resource}"


class EventScopes:
    RESOURCES = {
        "project": ["create", "update", "delete"],
        "task": ["create", "update", "delete"],
        "job": ["create", "update", "delete"],
        "organization": ["create", "update", "delete"],
        "membership": ["create", "update", "delete"],
        "invitation": ["create", "delete"],
        "user": ["create", "update", "delete"],
        "cloudstorage": ["create", "update", "delete"],
        "issue": ["create", "update", "delete"],
        "comment": ["create", "update", "delete"],
        "annotations": ["create", "update", "delete"],
        "label": ["create", "update", "delete"],
        "dataset": ["export", "import"],
        "function": ["call"],
        "webhook": ["create", "update", "delete"],
    }

    @classmethod
    def select(cls, resources):
        return [
            f"{event_scope(action, resource)}"
            for resource in resources
            for action in cls.RESOURCES.get(resource, [])
        ]


def record_server_event(
    *,
    scope: str,
    request_info: dict[str, str],
    payload: Optional[dict] = None,
    on_commit: bool = False,
    **kwargs,
) -> None:
    payload = payload or {}

    payload_with_request_info = {
        **payload,
        "request": {
            **payload.get("request", {}),
            **request_info,
        },
    }

    data = {
        "scope": scope,
        "timestamp": str(datetime.now(timezone.utc).timestamp()),
        "source": "server",
        "payload": JSONRenderer().render(payload_with_request_info).decode("UTF-8"),
        **kwargs,
    }

    rendered_data = JSONRenderer().render(data).decode("UTF-8")

    if on_commit:
        transaction.on_commit(lambda: vlogger.info(rendered_data), robust=True)
    else:
        vlogger.info(rendered_data)


class EventScopeChoice:
    @classmethod
    def choices(cls):
        return sorted((val, val.upper()) for val in AllEvents.events)


class AllEvents:
    events = list(
        event_scope(action, resource)
        for resource, actions in EventScopes.RESOURCES.items()
        for action in actions
    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\export.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import os
import uuid
from datetime import datetime, timedelta, timezone
from logging import Logger

import clickhouse_connect
import django_rq
from dateutil import parser
from django.conf import settings
from rest_framework import serializers, status
from rest_framework.response import Response

from cvat.apps.dataset_manager.views import log_exception
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.rq import BaseRQMeta, RQMetaWithFailureInfo
from cvat.apps.engine.utils import sendfile

slogger = ServerLogManager(__name__)

DEFAULT_CACHE_TTL = timedelta(hours=1)


def _create_csv(query_params, output_filename, cache_ttl):
    try:
        clickhouse_settings = settings.CLICKHOUSE["events"]

        time_filter = {
            "from": query_params.pop("from"),
            "to": query_params.pop("to"),
        }

        query = "SELECT * FROM events"
        conditions = []
        parameters = {}

        if time_filter["from"]:
            conditions.append(f"timestamp >= {{from:DateTime64}}")
            parameters["from"] = time_filter["from"]

        if time_filter["to"]:
            conditions.append(f"timestamp <= {{to:DateTime64}}")
            parameters["to"] = time_filter["to"]

        for param, value in query_params.items():
            if value:
                conditions.append(f"{param} = {{{param}:UInt64}}")
                parameters[param] = value

        if conditions:
            query += " WHERE " + " AND ".join(conditions)

        query += " ORDER BY timestamp ASC"

        with clickhouse_connect.get_client(
            host=clickhouse_settings["HOST"],
            database=clickhouse_settings["NAME"],
            port=clickhouse_settings["PORT"],
            username=clickhouse_settings["USER"],
            password=clickhouse_settings["PASSWORD"],
        ) as client:
            result = client.query(query, parameters=parameters)

        with open(output_filename, "w", encoding="UTF8") as f:
            writer = csv.writer(f)
            writer.writerow(result.column_names)
            writer.writerows(result.result_rows)

        archive_ctime = os.path.getctime(output_filename)
        scheduler = django_rq.get_scheduler(settings.CVAT_QUEUES.EXPORT_DATA.value)
        cleaning_job = scheduler.enqueue_in(
            time_delta=cache_ttl,
            func=_clear_export_cache,
            file_path=output_filename,
            file_ctime=archive_ctime,
            logger=slogger.glob,
        )
        slogger.glob.info(
            f"The {output_filename} is created "
            f"and available for downloading for the next {cache_ttl}. "
            f"Export cache cleaning job is enqueued, id '{cleaning_job.id}'"
        )
        return output_filename
    except Exception:
        log_exception(slogger.glob)
        raise


def export(request, filter_query, queue_name):
    action = request.query_params.get("action", None)
    filename = request.query_params.get("filename", None)

    query_params = {
        "org_id": filter_query.get("org_id", None),
        "project_id": filter_query.get("project_id", None),
        "task_id": filter_query.get("task_id", None),
        "job_id": filter_query.get("job_id", None),
        "user_id": filter_query.get("user_id", None),
        "from": filter_query.get("from", None),
        "to": filter_query.get("to", None),
    }

    try:
        if query_params["from"]:
            query_params["from"] = parser.parse(query_params["from"]).timestamp()
    except parser.ParserError:
        raise serializers.ValidationError(
            f"Cannot parse 'from' datetime parameter: {query_params['from']}"
        )
    try:
        if query_params["to"]:
            query_params["to"] = parser.parse(query_params["to"]).timestamp()
    except parser.ParserError:
        raise serializers.ValidationError(
            f"Cannot parse 'to' datetime parameter: {query_params['to']}"
        )

    if query_params["from"] and query_params["to"] and query_params["from"] > query_params["to"]:
        raise serializers.ValidationError("'from' must be before than 'to'")

    # Set the default time interval to last 30 days
    if not query_params["from"] and not query_params["to"]:
        query_params["to"] = datetime.now(timezone.utc)
        query_params["from"] = query_params["to"] - timedelta(days=30)

    if action not in (None, "download"):
        raise serializers.ValidationError("Unexpected action specified for the request")

    query_id = request.query_params.get("query_id", None) or uuid.uuid4()
    rq_id = f"export:csv-logs-{query_id}-by-{request.user}"
    response_data = {
        "query_id": query_id,
    }

    queue: django_rq.queues.DjangoRQ = django_rq.get_queue(queue_name)
    rq_job = queue.fetch_job(rq_id)

    if rq_job:
        if rq_job.is_finished:
            file_path = rq_job.return_value()
            if action == "download" and os.path.exists(file_path):
                rq_job.delete()
                timestamp = datetime.strftime(datetime.now(), "%Y_%m_%d_%H_%M_%S")
                filename = filename or f"logs_{timestamp}.csv"

                return sendfile(request, file_path, attachment=True, attachment_filename=filename)
            else:
                if os.path.exists(file_path):
                    return Response(status=status.HTTP_201_CREATED)
        elif rq_job.is_failed:
            rq_job_meta = RQMetaWithFailureInfo.for_job(rq_job)
            exc_info = rq_job_meta.formatted_exception or str(rq_job.exc_info)
            rq_job.delete()
            return Response(exc_info, status=status.HTTP_500_INTERNAL_SERVER_ERROR)
        else:
            return Response(data=response_data, status=status.HTTP_202_ACCEPTED)

    ttl = DEFAULT_CACHE_TTL.total_seconds()
    output_filename = os.path.join(settings.TMP_FILES_ROOT, f"{query_id}.csv")
    queue.enqueue_call(
        func=_create_csv,
        args=(query_params, output_filename, DEFAULT_CACHE_TTL),
        job_id=rq_id,
        meta=BaseRQMeta.build(request=request, db_obj=None),
        result_ttl=ttl,
        failure_ttl=ttl,
    )

    return Response(data=response_data, status=status.HTTP_202_ACCEPTED)


def _clear_export_cache(file_path: str, file_ctime: float, logger: Logger) -> None:
    try:
        if os.path.exists(file_path) and os.path.getctime(file_path) == file_ctime:
            os.remove(file_path)

            logger.info("Export cache file '{}' successfully removed".format(file_path))
    except Exception:
        log_exception(logger)
        raise


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\handlers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import traceback
from typing import Any, Optional, Union

import rq
from crum import get_current_request, get_current_user
from rest_framework import status
from rest_framework.exceptions import NotAuthenticated
from rest_framework.views import exception_handler

from cvat.apps.dataset_manager.tracks_counter import TracksCounter
from cvat.apps.engine.models import (
    CloudStorage,
    Comment,
    Issue,
    Job,
    Label,
    Project,
    ShapeType,
    Task,
    User,
)
from cvat.apps.engine.rq import BaseRQMeta
from cvat.apps.engine.serializers import (
    BasicUserSerializer,
    CloudStorageReadSerializer,
    CommentReadSerializer,
    IssueReadSerializer,
    JobReadSerializer,
    LabelSerializer,
    ProjectReadSerializer,
    TaskReadSerializer,
)
from cvat.apps.organizations.models import Invitation, Membership, Organization
from cvat.apps.organizations.serializers import (
    InvitationReadSerializer,
    MembershipReadSerializer,
    OrganizationReadSerializer,
)
from cvat.apps.webhooks.models import Webhook
from cvat.apps.webhooks.serializers import WebhookReadSerializer

from .cache import get_cache
from .const import WORKING_TIME_RESOLUTION, WORKING_TIME_SCOPE
from .event import event_scope, record_server_event
from .utils import compute_working_time_per_ids


def project_id(instance):
    if isinstance(instance, Project):
        return instance.id

    try:
        pid = getattr(instance, "project_id", None)
        if pid is None:
            return instance.get_project_id()
        return pid
    except Exception:
        return None


def organization_id(instance):
    if isinstance(instance, Organization):
        return instance.id

    try:
        return getattr(instance, "organization_id", None)
    except Exception:
        return None


def task_id(instance):
    if isinstance(instance, Task):
        return instance.id

    try:
        tid = getattr(instance, "task_id", None)
        if tid is None:
            return instance.get_task_id()
        return tid
    except Exception:
        return None


def job_id(instance):
    if isinstance(instance, Job):
        return instance.id

    try:
        jid = getattr(instance, "job_id", None)
        if jid is None:
            return instance.get_job_id()
        return jid
    except Exception:
        return None


def get_user(instance=None) -> User | dict | None:
    def _get_user_from_rq_job(rq_job: rq.job.Job) -> dict | None:
        if user := BaseRQMeta.for_job(rq_job).user:
            return user.to_dict()
        return None

    # Try to get current user from request
    user = get_current_user()
    if user is not None:
        return user

    # Try to get user from rq_job
    if isinstance(instance, rq.job.Job):
        return _get_user_from_rq_job(instance)
    else:
        rq_job = rq.get_current_job()
        if rq_job:
            return _get_user_from_rq_job(rq_job)

    if isinstance(instance, User):
        return instance

    return None


def get_request(instance=None):
    def _get_request_from_rq_job(rq_job: rq.job.Job) -> dict | None:
        if request := BaseRQMeta.for_job(rq_job).request:
            return request.to_dict()
        return None

    request = get_current_request()
    if request is not None:
        return request

    if isinstance(instance, rq.job.Job):
        return _get_request_from_rq_job(instance)
    else:
        rq_job = rq.get_current_job()
        if rq_job:
            return _get_request_from_rq_job(rq_job)

    return None


def _get_value(obj, key):
    if obj is not None:
        if isinstance(obj, dict):
            return obj.get(key, None)
        return getattr(obj, key, None)

    return None


def request_info(instance=None):
    request = get_request(instance)
    request_headers = _get_value(request, "headers")
    return {
        "id": _get_value(request, "uuid"),
        "user_agent": request_headers.get("User-Agent") if request_headers is not None else None,
    }


def user_id(instance=None):
    current_user = get_user(instance)
    return _get_value(current_user, "id")


def user_name(instance=None):
    current_user = get_user(instance)
    return _get_value(current_user, "username")


def user_email(instance=None):
    current_user = get_user(instance)
    return _get_value(current_user, "email") or None


def organization_slug(instance):
    if isinstance(instance, Organization):
        return instance.slug

    try:
        org = getattr(instance, "organization", None)
        if org is None:
            return instance.get_organization_slug()
        return org.slug
    except Exception:
        return None


def get_instance_diff(old_data, data):
    ignore_related_fields = ("labels",)
    diff = {}
    for prop, value in data.items():
        if prop in ignore_related_fields:
            continue
        old_value = old_data.get(prop)
        if old_value != value:
            diff[prop] = {
                "old_value": old_value,
                "new_value": value,
            }

    return diff


def _cleanup_fields(obj: dict[str, Any]) -> dict[str, Any]:
    fields = (
        "slug",
        "id",
        "name",
        "username",
        "display_name",
        "message",
        "organization",
        "project",
        "size",
        "task",
        "tasks",
        "job",
        "jobs",
        "comments",
        "url",
        "issues",
        "attributes",
        "key",
    )
    subfields = ("url",)

    data = {}
    for k, v in obj.items():
        if k in fields:
            continue
        if isinstance(v, dict):
            data[k] = {kk: vv for kk, vv in v.items() if kk not in subfields}
        else:
            data[k] = v
    return data


def _get_object_name(instance):
    if (
        isinstance(instance, Organization)
        or isinstance(instance, Project)
        or isinstance(instance, Task)
        or isinstance(instance, Job)
        or isinstance(instance, Label)
    ):
        return getattr(instance, "name", None)

    if isinstance(instance, User):
        return getattr(instance, "username", None)

    if isinstance(instance, CloudStorage):
        return getattr(instance, "display_name", None)

    if isinstance(instance, Comment):
        return getattr(instance, "message", None)

    return None


SERIALIZERS = [
    (Organization, OrganizationReadSerializer),
    (Project, ProjectReadSerializer),
    (Task, TaskReadSerializer),
    (Job, JobReadSerializer),
    (User, BasicUserSerializer),
    (CloudStorage, CloudStorageReadSerializer),
    (Issue, IssueReadSerializer),
    (Comment, CommentReadSerializer),
    (Label, LabelSerializer),
    (Membership, MembershipReadSerializer),
    (Invitation, InvitationReadSerializer),
    (Webhook, WebhookReadSerializer),
]


def get_serializer(instance):
    context = {"request": get_current_request()}

    serializer = None
    for model, serializer_class in SERIALIZERS:
        if isinstance(instance, model):
            serializer = serializer_class(instance=instance, context=context)

    return serializer


def get_serializer_without_url(instance):
    serializer = get_serializer(instance)
    if serializer:
        serializer.fields.pop("url", None)
    return serializer


from cvat.apps.engine.log import ServerLogManager

slogger = ServerLogManager(__name__)


def handle_create(scope, instance, **kwargs):
    oid = organization_id(instance)
    oslug = organization_slug(instance)
    pid = project_id(instance)
    tid = task_id(instance)
    jid = job_id(instance)
    uid = user_id(instance)
    uname = user_name(instance)
    uemail = user_email(instance)

    serializer = get_serializer_without_url(instance=instance)
    try:
        payload = serializer.data
    except Exception:
        payload = {}

    payload = _cleanup_fields(obj=payload)
    record_server_event(
        scope=scope,
        request_info=request_info(),
        on_commit=True,
        obj_id=getattr(instance, "id", None),
        obj_name=_get_object_name(instance),
        org_id=oid,
        org_slug=oslug,
        project_id=pid,
        task_id=tid,
        job_id=jid,
        user_id=uid,
        user_name=uname,
        user_email=uemail,
        payload=payload,
    )


def handle_update(scope, instance, old_instance, **kwargs):
    oid = organization_id(instance)
    oslug = organization_slug(instance)
    pid = project_id(instance)
    tid = task_id(instance)
    jid = job_id(instance)
    uid = user_id(instance)
    uname = user_name(instance)
    uemail = user_email(instance)

    old_serializer = get_serializer_without_url(instance=old_instance)
    serializer = get_serializer_without_url(instance=instance)
    diff = get_instance_diff(old_data=old_serializer.data, data=serializer.data)

    for prop, change in diff.items():
        change = _cleanup_fields(change)
        record_server_event(
            scope=scope,
            request_info=request_info(),
            on_commit=True,
            obj_name=prop,
            obj_id=getattr(instance, f"{prop}_id", None),
            obj_val=str(change["new_value"]),
            org_id=oid,
            org_slug=oslug,
            project_id=pid,
            task_id=tid,
            job_id=jid,
            user_id=uid,
            user_name=uname,
            user_email=uemail,
            payload={"old_value": change["old_value"]},
        )


def handle_delete(scope, instance, store_in_deletion_cache=False, **kwargs):
    deletion_cache = get_cache()
    instance_id = getattr(instance, "id", None)
    if store_in_deletion_cache:
        deletion_cache.set(
            instance.__class__,
            instance_id,
            {
                "oid": organization_id(instance),
                "oslug": organization_slug(instance),
                "pid": project_id(instance),
                "tid": task_id(instance),
                "jid": job_id(instance),
            },
        )
        return

    instance_meta_info = deletion_cache.pop(instance.__class__, instance_id)
    if instance_meta_info:
        oid = instance_meta_info["oid"]
        oslug = instance_meta_info["oslug"]
        pid = instance_meta_info["pid"]
        tid = instance_meta_info["tid"]
        jid = instance_meta_info["jid"]
    else:
        oid = organization_id(instance)
        oslug = organization_slug(instance)
        pid = project_id(instance)
        tid = task_id(instance)
        jid = job_id(instance)

    uid = user_id(instance)
    uname = user_name(instance)
    uemail = user_email(instance)

    record_server_event(
        scope=scope,
        request_info=request_info(),
        on_commit=True,
        obj_id=instance_id,
        obj_name=_get_object_name(instance),
        org_id=oid,
        org_slug=oslug,
        project_id=pid,
        task_id=tid,
        job_id=jid,
        user_id=uid,
        user_name=uname,
        user_email=uemail,
    )


def handle_annotations_change(instance: Job, annotations, action, **kwargs):
    def filter_data(data):
        return {
            "id": data["id"],
        }

    in_mem_counter = TracksCounter()
    in_mem_counter.load_tracks_from_job(instance.id, annotations.get("tracks", []))

    in_db_counter = TracksCounter()
    if action == "update" and annotations.get("tracks", []):
        in_db_counter.load_tracks_from_db(
            parent_labeledtrack_qs_filter=lambda x: x.filter(
                pk__in=(track["id"] for track in annotations["tracks"])
            ),
            child_labeledtrack_qs_filter=lambda x: x.filter(
                parent_id__in=(track["id"] for track in annotations["tracks"])
            ),
        )

    def filter_track(track):
        job_id = instance.id
        track_id = track["id"]

        in_mem_shapes = in_mem_counter.count_track_shapes(job_id, track_id)
        in_mem_visible_shapes = in_mem_shapes["manual"] + in_mem_shapes["interpolated"]
        filtered_data = filter_data(track)

        if action == "create":
            filtered_data["visible_shapes_count_diff"] = in_mem_visible_shapes
        elif action == "delete":
            filtered_data["visible_shapes_count_diff"] = -in_mem_visible_shapes
        elif action == "update":
            # when track is just updated, it may lead to both new or deleted visible shapes
            in_db_shapes = in_db_counter.count_track_shapes(job_id, track_id)
            in_db_visible_shapes = in_db_shapes["manual"] + in_db_shapes["interpolated"]
            filtered_data["visible_shapes_count_diff"] = (
                in_db_visible_shapes - in_mem_visible_shapes
            )

        filtered_data["shapes"] = [filter_data(s) for s in track["shapes"]]
        return filtered_data

    oid = organization_id(instance)
    oslug = organization_slug(instance)
    pid = project_id(instance)
    tid = task_id(instance)
    jid = job_id(instance)
    uid = user_id(instance)
    uname = user_name(instance)
    uemail = user_email(instance)
    request_info_ = request_info()

    tags = [filter_data(tag) for tag in annotations.get("tags", [])]
    if tags:
        record_server_event(
            scope=event_scope(action, "tags"),
            request_info=request_info_,
            on_commit=True,
            count=len(tags),
            org_id=oid,
            org_slug=oslug,
            project_id=pid,
            task_id=tid,
            job_id=jid,
            user_id=uid,
            user_name=uname,
            user_email=uemail,
            payload={"tags": tags},
        )

    shapes_by_type = {shape_type[0]: [] for shape_type in ShapeType.choices()}
    for shape in annotations.get("shapes", []):
        shapes_by_type[shape["type"]].append(filter_data(shape))

    scope = event_scope(action, "shapes")
    for shape_type, shapes in shapes_by_type.items():
        if shapes:
            record_server_event(
                scope=scope,
                request_info=request_info_,
                on_commit=True,
                obj_name=shape_type,
                count=len(shapes),
                org_id=oid,
                org_slug=oslug,
                project_id=pid,
                task_id=tid,
                job_id=jid,
                user_id=uid,
                user_name=uname,
                user_email=uemail,
                payload={"shapes": shapes},
            )

    tracks_by_type = {shape_type[0]: [] for shape_type in ShapeType.choices()}
    for track in annotations.get("tracks", []):
        filtered_track = filter_track(track)
        tracks_by_type[track["shapes"][0]["type"]].append(filtered_track)

    scope = event_scope(action, "tracks")
    for track_type, tracks in tracks_by_type.items():
        if tracks:
            record_server_event(
                scope=scope,
                request_info=request_info_,
                on_commit=True,
                obj_name=track_type,
                count=len(tracks),
                org_id=oid,
                org_slug=oslug,
                project_id=pid,
                task_id=tid,
                job_id=jid,
                user_id=uid,
                user_name=uname,
                user_email=uemail,
                payload={"tracks": tracks},
            )


def handle_dataset_io(
    instance: Union[Project, Task, Job],
    action: str,
    *,
    format_name: str,
    cloud_storage_id: Optional[int],
    **payload_fields,
) -> None:
    payload = {"format": format_name, **payload_fields}

    if cloud_storage_id:
        payload["cloud_storage"] = {"id": cloud_storage_id}

    record_server_event(
        scope=event_scope(action, "dataset"),
        request_info=request_info(),
        org_id=organization_id(instance),
        org_slug=organization_slug(instance),
        project_id=project_id(instance),
        task_id=task_id(instance),
        job_id=job_id(instance),
        user_id=user_id(instance),
        user_name=user_name(instance),
        user_email=user_email(instance),
        payload=payload,
    )


def handle_dataset_export(
    instance: Union[Project, Task, Job],
    *,
    format_name: str,
    cloud_storage_id: Optional[int],
    save_images: bool,
) -> None:
    handle_dataset_io(
        instance,
        "export",
        format_name=format_name,
        cloud_storage_id=cloud_storage_id,
        save_images=save_images,
    )


def handle_dataset_import(
    instance: Union[Project, Task, Job],
    *,
    format_name: str,
    cloud_storage_id: Optional[int],
) -> None:
    handle_dataset_io(
        instance, "import", format_name=format_name, cloud_storage_id=cloud_storage_id
    )


def handle_function_call(
    function_id: str,
    target: Union[Task, Job],
    **payload_fields,
) -> None:
    record_server_event(
        scope=event_scope("call", "function"),
        request_info=request_info(),
        project_id=project_id(target),
        task_id=task_id(target),
        job_id=job_id(target),
        user_id=user_id(),
        user_name=user_name(),
        user_email=user_email(),
        payload={
            "function": {"id": function_id},
            **payload_fields,
        },
    )


def handle_rq_exception(rq_job, exc_type, exc_value, tb):
    rq_job_meta = BaseRQMeta.for_job(rq_job)
    oid = rq_job_meta.org_id
    oslug = rq_job_meta.org_slug
    pid = rq_job_meta.project_id
    tid = rq_job_meta.task_id
    jid = rq_job_meta.job_id
    uid = user_id(rq_job)
    uname = user_name(rq_job)
    uemail = user_email(rq_job)
    tb_strings = traceback.format_exception(exc_type, exc_value, tb)

    payload = {
        "message": tb_strings[-1].rstrip("\n"),
        "stack": "".join(tb_strings),
    }

    record_server_event(
        scope="send:exception",
        request_info=request_info(instance=rq_job),
        count=1,
        org_id=oid,
        org_slug=oslug,
        project_id=pid,
        task_id=tid,
        job_id=jid,
        user_id=uid,
        user_name=uname,
        user_email=uemail,
        payload=payload,
    )

    return False


def handle_viewset_exception(exc, context):
    response = exception_handler(exc, context)

    IGNORED_EXCEPTION_CLASSES = (NotAuthenticated,)
    if isinstance(exc, IGNORED_EXCEPTION_CLASSES):
        return response
    # the standard DRF exception handler only handle APIException, Http404 and PermissionDenied
    # exceptions types, any other will cause a 500 error
    status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
    if response is not None:
        status_code = response.status_code
    request = context["request"]
    view = context["view"]

    tb_strings = traceback.format_exception(type(exc), exc, exc.__traceback__)

    payload = {
        "basename": getattr(view, "basename", None),
        "action": getattr(view, "action", None),
        "request": {
            "url": request.get_full_path(),
            "query_params": request.query_params,
            "content_type": request.content_type,
            "method": request.method,
        },
        "message": tb_strings[-1].rstrip("\n"),
        "stack": "".join(tb_strings),
        "status_code": status_code,
    }

    record_server_event(
        scope="send:exception",
        request_info=request_info(),
        count=1,
        user_id=getattr(request.user, "id", None),
        user_name=getattr(request.user, "username", None),
        user_email=getattr(request.user, "email", None),
        payload=payload,
    )

    return response


def handle_client_events_push(request, data: dict):
    org = request.iam_context["organization"]

    working_time_per_ids = compute_working_time_per_ids(data)

    if data["events"]:
        common = {
            "user_id": request.user.id,
            "user_name": request.user.username,
            "user_email": request.user.email or None,
            "org_id": getattr(org, "id", None),
            "org_slug": getattr(org, "slug", None),
        }

        for ids, working_time in working_time_per_ids.items():
            job_id, task_id, project_id = ids
            if working_time["value"].total_seconds():
                value = working_time["value"] // WORKING_TIME_RESOLUTION
                record_server_event(
                    scope=WORKING_TIME_SCOPE,
                    request_info=request_info(),
                    # keep it in payload for backward compatibility
                    # but in the future it is much better to use a "duration" field
                    # because parsing JSON in SQL query is very slow
                    payload={"working_time": value},
                    timestamp=str(working_time["timestamp"].timestamp()),
                    duration=value,
                    project_id=project_id,
                    task_id=task_id,
                    job_id=job_id,
                    count=1,
                    **common,
                )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings
from rest_framework.exceptions import PermissionDenied

from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum
from cvat.utils.http import make_requests_session


class EventsPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        SEND_EVENTS = "send:events"
        DUMP_EVENTS = "dump:events"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "events":
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/events/allow"

    def filter(self, query_params):
        url = self.url.replace("/allow", "/filter")

        with make_requests_session() as session:
            r = session.post(url, json=self.payload).json()["result"]

        filter_params = query_params.copy()
        for query in r:
            for attr, value in query.items():
                if str(filter_params.get(attr, value)) != str(value):
                    raise PermissionDenied(
                        f"You don't have permission to view events with {attr}={filter_params.get(attr)}"
                    )
                else:
                    filter_params[attr] = str(value)
        return filter_params

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                ("create", "POST"): Scopes.SEND_EVENTS,
                ("list", "GET"): Scopes.DUMP_EVENTS,
            }[(view.action, request.method)]
        ]

    def get_resource(self):
        return None


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\serializers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import datetime
import json

from rest_framework import serializers

from .const import USER_ACTIVITY_SCOPE


class EventSerializer(serializers.Serializer):
    scope = serializers.CharField(required=True)
    obj_name = serializers.CharField(required=False, allow_null=True)
    obj_id = serializers.IntegerField(required=False, allow_null=True)
    obj_val = serializers.CharField(required=False, allow_null=True)
    source = serializers.CharField(required=False, allow_null=True)
    timestamp = serializers.DateTimeField(required=True)
    count = serializers.IntegerField(required=False, allow_null=True)
    duration = serializers.IntegerField(required=False, default=0)
    project_id = serializers.IntegerField(required=False, allow_null=True)
    task_id = serializers.IntegerField(required=False, allow_null=True)
    job_id = serializers.IntegerField(required=False, allow_null=True)
    user_id = serializers.IntegerField(required=False, allow_null=True)
    user_name = serializers.CharField(required=False, allow_null=True)
    user_email = serializers.CharField(required=False, allow_null=True)
    org_id = serializers.IntegerField(required=False, allow_null=True)
    org_slug = serializers.CharField(required=False, allow_null=True)
    payload = serializers.CharField(required=False, allow_null=True)


class ClientEventsSerializer(serializers.Serializer):
    ALLOWED_SCOPES = {
        "client": frozenset(
            (
                "load:cvat",
                "load:job",
                "save:job",
                "load:workspace",
                "send:exception",
                "join:objects",
                "change:frame",
                "draw:object",
                "paste:object",
                "copy:object",
                "propagate:object",
                "drag:object",
                "resize:object",
                "delete:object",
                "merge:objects",
                "split:objects",
                "group:objects",
                "slice:object",
                "zoom:image",
                "fit:image",
                "rotate:image",
                "action:undo",
                "action:redo",
                "debug:info",
                "run:annotations_action",
                "click:element",
                USER_ACTIVITY_SCOPE,
            )
        ),
    }

    events = EventSerializer(many=True, default=[])
    previous_event = EventSerializer(default=None, allow_null=True, write_only=True)
    timestamp = serializers.DateTimeField()

    def to_internal_value(self, data):
        data = super().to_internal_value(data)
        request = self.context.get("request")
        org = request.iam_context["organization"]
        user_and_org_data = {
            "org_id": getattr(org, "id", None),
            "org_slug": getattr(org, "slug", None),
            "user_id": request.user.id,
            "user_name": request.user.username,
            "user_email": request.user.email,
        }

        send_time = data["timestamp"]
        receive_time = datetime.datetime.now(datetime.timezone.utc)
        time_correction = receive_time - send_time

        if data["previous_event"]:
            data["previous_event"]["timestamp"] += time_correction

        for event in data["events"]:
            scope = event["scope"]
            source = event.get("source", "client")
            if scope not in ClientEventsSerializer.ALLOWED_SCOPES.get(source, []):
                raise serializers.ValidationError(
                    {"scope": f"Event scope **{scope}** is not allowed from {source}"}
                )

            try:
                payload = json.loads(event.get("payload", "{}"))
            except json.JSONDecodeError:
                raise serializers.ValidationError(
                    {"payload": "JSON payload is not valid in passed event"}
                )

            event.update(
                {
                    "timestamp": event["timestamp"] + time_correction,
                    "source": source,
                    "payload": json.dumps(payload),
                    **(user_and_org_data if source == "client" else {}),
                }
            )

        return data


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\signals.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.core.exceptions import ObjectDoesNotExist
from django.db.models.signals import post_delete, post_save, pre_save
from django.dispatch import receiver

from cvat.apps.engine.models import (
    CloudStorage,
    Comment,
    Issue,
    Job,
    Label,
    Project,
    Task,
    TimestampedModel,
    User,
)
from cvat.apps.organizations.models import Invitation, Membership, Organization
from cvat.apps.webhooks.models import Webhook

from .event import EventScopeChoice, event_scope
from .handlers import handle_create, handle_delete, handle_update


@receiver(pre_save, sender=Webhook, dispatch_uid="webhook:update_receiver")
@receiver(pre_save, sender=Membership, dispatch_uid="membership:update_receiver")
@receiver(pre_save, sender=Organization, dispatch_uid="organization:update_receiver")
@receiver(pre_save, sender=Project, dispatch_uid="project:update_receiver")
@receiver(pre_save, sender=Task, dispatch_uid="task:update_receiver")
@receiver(pre_save, sender=Job, dispatch_uid="job:update_receiver")
@receiver(pre_save, sender=User, dispatch_uid="user:update_receiver")
@receiver(pre_save, sender=CloudStorage, dispatch_uid="cloudstorage:update_receiver")
@receiver(pre_save, sender=Issue, dispatch_uid="issue:update_receiver")
@receiver(pre_save, sender=Comment, dispatch_uid="comment:update_receiver")
@receiver(pre_save, sender=Label, dispatch_uid="label:update_receiver")
def resource_update(sender, *, instance, update_fields, **kwargs):
    if (
        isinstance(instance, TimestampedModel)
        and update_fields
        and list(update_fields) == ["updated_date"]
    ):
        # This is an optimization for the common case where only the date is bumped
        # (see `TimestampedModel.touch`). Since the actual update of the field will
        # be performed _after_ this signal is sent (in `DateTimeField.pre_save`),
        # and no other fields are updated, there is guaranteed to be no difference
        # between the old and current states of the instance. Therefore, no events
        # will need be logged, so we can just exit immediately.
        return

    resource_name = instance.__class__.__name__.lower()

    try:
        old_instance = sender.objects.get(id=instance.id)
    except ObjectDoesNotExist:
        return

    scope = event_scope("update", resource_name)
    if scope not in map(lambda a: a[0], EventScopeChoice.choices()):
        return

    handle_update(scope=scope, instance=instance, old_instance=old_instance, **kwargs)


@receiver(post_save, sender=Webhook, dispatch_uid="webhook:create_receiver")
@receiver(post_save, sender=Membership, dispatch_uid="membership:create_receiver")
@receiver(post_save, sender=Invitation, dispatch_uid="invitation:create_receiver")
@receiver(post_save, sender=Organization, dispatch_uid="organization:create_receiver")
@receiver(post_save, sender=Project, dispatch_uid="project:create_receiver")
@receiver(post_save, sender=Task, dispatch_uid="task:create_receiver")
@receiver(post_save, sender=Job, dispatch_uid="job:create_receiver")
@receiver(post_save, sender=User, dispatch_uid="user:create_receiver")
@receiver(post_save, sender=CloudStorage, dispatch_uid="cloudstorage:create_receiver")
@receiver(post_save, sender=Issue, dispatch_uid="issue:create_receiver")
@receiver(post_save, sender=Comment, dispatch_uid="comment:create_receiver")
@receiver(post_save, sender=Label, dispatch_uid="label:create_receiver")
def resource_create(sender, instance, created, **kwargs):
    if not created:
        return

    resource_name = instance.__class__.__name__.lower()

    scope = event_scope("create", resource_name)
    if scope not in map(lambda a: a[0], EventScopeChoice.choices()):
        return

    handle_create(scope=scope, instance=instance, **kwargs)


@receiver(post_delete, sender=Webhook, dispatch_uid="webhook:delete_receiver")
@receiver(post_delete, sender=Membership, dispatch_uid="membership:delete_receiver")
@receiver(post_delete, sender=Invitation, dispatch_uid="invitation:delete_receiver")
@receiver(post_delete, sender=Organization, dispatch_uid="organization:delete_receiver")
@receiver(post_delete, sender=Project, dispatch_uid="project:delete_receiver")
@receiver(post_delete, sender=Task, dispatch_uid="task:delete_receiver")
@receiver(post_delete, sender=Job, dispatch_uid="job:delete_receiver")
@receiver(post_delete, sender=User, dispatch_uid="user:delete_receiver")
@receiver(post_delete, sender=CloudStorage, dispatch_uid="cloudstorage:delete_receiver")
@receiver(post_delete, sender=Issue, dispatch_uid="issue:delete_receiver")
@receiver(post_delete, sender=Comment, dispatch_uid="comment:delete_receiver")
@receiver(post_delete, sender=Label, dispatch_uid="label:delete_receiver")
def resource_delete(sender, instance, **kwargs):
    resource_name = instance.__class__.__name__.lower()
    scope = event_scope("delete", resource_name)
    if scope not in map(lambda a: a[0], EventScopeChoice.choices()):
        return

    handle_delete(scope=scope, instance=instance, **kwargs)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\urls.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework import routers

from . import views

router = routers.DefaultRouter(trailing_slash=False)
router.register("events", views.EventsViewSet, basename="events")

urlpatterns = router.urls


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import datetime

from .cache import clear_cache
from .const import COMPRESSED_EVENT_SCOPES, MAX_EVENT_DURATION


def _prepare_objects_to_delete(object_to_delete):
    from cvat.apps.engine.models import Comment, Issue, Job, Project, Segment, Task

    relation_chain = (Project, Task, Segment, Job, Issue, Comment)
    related_field_names = ("task_set", "segment_set", "job_set", "issues", "comments")
    field_names = tuple(m._meta.model_name for m in relation_chain)

    # Find object Model
    index = relation_chain.index(object_to_delete.__class__)

    # Need to prefetch 'next' Model objects in the chain
    index += 1
    if index >= len(relation_chain):
        return []

    # Fill filter param
    filter_params = {
        f"{object_to_delete.__class__._meta.model_name}_id": object_to_delete.id,
    }

    # Fill prefetch
    prefetch = []
    if index < len(relation_chain) - 1:
        forward_prefetch = "__".join(related_field_names[index:])
        prefetch.append(forward_prefetch)

    if index > 0:
        backward_prefetch = "__".join(reversed(field_names[:index]))
        prefetch.append(backward_prefetch)

    # make queryset
    objects = relation_chain[index].objects.filter(**filter_params).prefetch_related(*prefetch)

    # list of objects which will be deleted with current object
    objects_to_delete = list(objects)
    parents = objects
    for rn in related_field_names[index:]:
        related = [related for parent in parents for related in getattr(parent, rn).all()]
        objects_to_delete.extend(related)
        parents = related

    return objects_to_delete


def cache_deleted(method):
    def wrap(self, *args, **kwargs):
        from .signals import resource_delete

        objects = _prepare_objects_to_delete(self)
        try:
            for obj in objects:
                resource_delete(obj.__class__, obj, store_in_deletion_cache=True)

            method(self, *args, **kwargs)
        finally:
            clear_cache()

    return wrap


def get_end_timestamp(event: dict) -> datetime.datetime:
    if event["scope"] in COMPRESSED_EVENT_SCOPES:
        return event["timestamp"] + datetime.timedelta(milliseconds=event["duration"])
    return event["timestamp"]


def is_contained(event1: dict, event2: dict) -> bool:
    return event1["timestamp"] < get_end_timestamp(event2)


def compute_working_time_per_ids(data: dict) -> dict:
    def read_ids(event: dict) -> tuple[int | None, int | None, int | None]:
        return event.get("job_id"), event.get("task_id"), event.get("project_id")

    if previous_event := data["previous_event"]:
        previous_end_timestamp = get_end_timestamp(previous_event)
        previous_ids = read_ids(previous_event)
    elif data["events"]:
        previous_end_timestamp = data["events"][0]["timestamp"]
        previous_ids = read_ids(data["events"][0])

    working_time_per_ids = {}
    for event in data["events"]:
        working_time = datetime.timedelta()
        timestamp = event["timestamp"]

        if timestamp > previous_end_timestamp:
            t_diff = timestamp - previous_end_timestamp
            if t_diff < MAX_EVENT_DURATION:
                working_time += t_diff

            previous_end_timestamp = timestamp

        end_timestamp = get_end_timestamp(event)
        if end_timestamp > previous_end_timestamp:
            working_time += end_timestamp - previous_end_timestamp
            previous_end_timestamp = end_timestamp

        if previous_ids not in working_time_per_ids:
            working_time_per_ids[previous_ids] = {
                "value": datetime.timedelta(),
                "timestamp": timestamp,
            }

        working_time_per_ids[previous_ids]["value"] += working_time
        previous_ids = read_ids(event)

    return working_time_per_ids


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\views.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import OpenApiParameter, OpenApiResponse, extend_schema
from rest_framework import status, viewsets
from rest_framework.renderers import JSONRenderer
from rest_framework.response import Response

from cvat.apps.engine.log import vlogger
from cvat.apps.events.permissions import EventsPermission
from cvat.apps.events.serializers import ClientEventsSerializer
from cvat.apps.iam.filters import ORGANIZATION_OPEN_API_PARAMETERS

from .const import USER_ACTIVITY_SCOPE
from .export import export
from .handlers import handle_client_events_push


class EventsViewSet(viewsets.ViewSet):
    serializer_class = None

    @extend_schema(
        summary="Log client events",
        methods=["POST"],
        description="Sends logs to the Clickhouse if it is connected",
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        request=ClientEventsSerializer(),
        responses={
            "201": ClientEventsSerializer(),
        },
    )
    def create(self, request):
        serializer = ClientEventsSerializer(data=request.data, context={"request": request})
        serializer.is_valid(raise_exception=True)

        handle_client_events_push(request, serializer.validated_data)
        for event in serializer.validated_data["events"]:
            if event["scope"] == USER_ACTIVITY_SCOPE:
                # do not record these events, we only need them for correct working time computation
                continue

            message = (
                JSONRenderer()
                .render({**event, "timestamp": str(event["timestamp"].timestamp())})
                .decode("UTF-8")
            )
            vlogger.info(message)

        return Response(serializer.validated_data, status=status.HTTP_201_CREATED)

    @extend_schema(
        summary="Get an event log",
        methods=["GET"],
        description="The log is returned in the CSV format.",
        parameters=[
            OpenApiParameter(
                "org_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.INT,
                required=False,
                description="Filter events by organization ID",
            ),
            OpenApiParameter(
                "project_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.INT,
                required=False,
                description="Filter events by project ID",
            ),
            OpenApiParameter(
                "task_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.INT,
                required=False,
                description="Filter events by task ID",
            ),
            OpenApiParameter(
                "job_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.INT,
                required=False,
                description="Filter events by job ID",
            ),
            OpenApiParameter(
                "user_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.INT,
                required=False,
                description="Filter events by user ID",
            ),
            OpenApiParameter(
                "from",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.DATETIME,
                required=False,
                description="Filter events after the datetime. If no 'from' or 'to' parameters are passed, the last 30 days will be set.",
            ),
            OpenApiParameter(
                "to",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.DATETIME,
                required=False,
                description="Filter events before the datetime. If no 'from' or 'to' parameters are passed, the last 30 days will be set.",
            ),
            OpenApiParameter(
                "filename",
                description="Desired output file name",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.STR,
                required=False,
            ),
            OpenApiParameter(
                "action",
                location=OpenApiParameter.QUERY,
                description="Used to start downloading process after annotation file had been created",
                type=OpenApiTypes.STR,
                required=False,
                enum=["download"],
            ),
            OpenApiParameter(
                "query_id",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.STR,
                required=False,
                description="ID of query request that need to check or download",
            ),
        ],
        responses={
            "200": OpenApiResponse(description="Download of file started"),
            "201": OpenApiResponse(description="CSV log file is ready for downloading"),
            "202": OpenApiResponse(description="Creating a CSV log file has been started"),
        },
    )
    def list(self, request):
        perm = EventsPermission.create_scope_list(request)
        filter_query = perm.filter(request.query_params)
        return export(
            request=request,
            filter_query=filter_query,
            queue_name=settings.CVAT_QUEUES.EXPORT_DATA.value,
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\rules\tests\generators\events_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

random.seed(42)

NAME = "events"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            found = False
            for col, val in row.items():
                if col in ["method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]


def RESOURCES(scope):
    return [None]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        print("scopes", SCOPES)
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\tests\test_events.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import unittest
from datetime import datetime, timedelta, timezone
from typing import Optional

from django.contrib.auth import get_user_model
from django.test import RequestFactory

from cvat.apps.events.const import MAX_EVENT_DURATION, WORKING_TIME_RESOLUTION
from cvat.apps.events.serializers import ClientEventsSerializer
from cvat.apps.events.utils import compute_working_time_per_ids, is_contained
from cvat.apps.organizations.models import Organization


class WorkingTimeTestCase(unittest.TestCase):
    _START_TIMESTAMP = datetime(2024, 1, 1, 12)
    _SHORT_GAP = MAX_EVENT_DURATION - timedelta(milliseconds=1)
    _SHORT_GAP_INT = _SHORT_GAP / WORKING_TIME_RESOLUTION
    _LONG_GAP = MAX_EVENT_DURATION
    _LONG_GAP_INT = _LONG_GAP / WORKING_TIME_RESOLUTION

    @staticmethod
    def _instant_event(timestamp: datetime) -> dict:
        return {
            "scope": "click:element",
            "timestamp": timestamp.isoformat(),
            "duration": 123,
        }

    @staticmethod
    def _compressed_event(timestamp: datetime, duration: timedelta) -> dict:
        return {
            "scope": "change:frame",
            "timestamp": timestamp.isoformat(),
            "duration": duration // WORKING_TIME_RESOLUTION,
        }

    @staticmethod
    def _get_actual_working_times(data: dict) -> list[int]:
        data_copy = data.copy()
        working_times = []
        for event in data["events"]:
            data_copy["events"] = [event]
            event_working_time = compute_working_time_per_ids(data_copy)
            for working_time in event_working_time.values():
                working_times.append((working_time["value"] // WORKING_TIME_RESOLUTION))
            if data_copy["previous_event"] and is_contained(event, data_copy["previous_event"]):
                continue
            data_copy["previous_event"] = event
        return working_times

    @staticmethod
    def _deserialize(events: list[dict], previous_event: Optional[dict] = None) -> dict:
        request = RequestFactory().post("/api/events")
        request.user = get_user_model()(id=100, username="testuser", email="testuser@example.org")
        request.iam_context = {
            "organization": Organization(id=101, slug="testorg", name="Test Organization"),
        }

        s = ClientEventsSerializer(
            data={
                "events": events,
                "previous_event": previous_event,
                "timestamp": datetime.now(timezone.utc),
            },
            context={"request": request},
        )

        s.is_valid(raise_exception=True)

        return s.validated_data

    def test_instant(self):
        data = self._deserialize(
            [
                self._instant_event(self._START_TIMESTAMP),
            ]
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 0)

    def test_compressed(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, self._LONG_GAP),
            ]
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], self._LONG_GAP_INT)

    def test_instants_with_short_gap(self):
        data = self._deserialize(
            [
                self._instant_event(self._START_TIMESTAMP),
                self._instant_event(self._START_TIMESTAMP + self._SHORT_GAP),
            ]
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 0)
        self.assertEqual(event_times[1], self._SHORT_GAP_INT)

    def test_instants_with_long_gap(self):
        data = self._deserialize(
            [
                self._instant_event(self._START_TIMESTAMP),
                self._instant_event(self._START_TIMESTAMP + self._LONG_GAP),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 0)
        self.assertEqual(event_times[1], 0)

    def test_compressed_with_short_gap(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=1)),
                self._compressed_event(
                    self._START_TIMESTAMP + timedelta(seconds=1) + self._SHORT_GAP,
                    timedelta(seconds=5),
                ),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 1000)
        self.assertEqual(event_times[1], self._SHORT_GAP_INT + 5000)

    def test_compressed_with_long_gap(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=1)),
                self._compressed_event(
                    self._START_TIMESTAMP + timedelta(seconds=1) + self._LONG_GAP,
                    timedelta(seconds=5),
                ),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 1000)
        self.assertEqual(event_times[1], 5000)

    def test_compressed_contained(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=5)),
                self._compressed_event(
                    self._START_TIMESTAMP + timedelta(seconds=3), timedelta(seconds=1)
                ),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 5000)
        self.assertEqual(event_times[1], 0)

    def test_compressed_overlapping(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=5)),
                self._compressed_event(
                    self._START_TIMESTAMP + timedelta(seconds=3), timedelta(seconds=6)
                ),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 5000)
        self.assertEqual(event_times[1], 4000)

    def test_instant_inside_compressed(self):
        data = self._deserialize(
            [
                self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=5)),
                self._instant_event(self._START_TIMESTAMP + timedelta(seconds=3)),
                self._instant_event(self._START_TIMESTAMP + timedelta(seconds=6)),
            ]
        )

        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 5000)
        self.assertEqual(event_times[1], 0)
        self.assertEqual(event_times[2], 1000)

    def test_previous_instant_short_gap(self):
        data = self._deserialize(
            [self._instant_event(self._START_TIMESTAMP + self._SHORT_GAP)],
            previous_event=self._instant_event(self._START_TIMESTAMP),
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], self._SHORT_GAP_INT)

    def test_previous_instant_long_gap(self):
        data = self._deserialize(
            [self._instant_event(self._START_TIMESTAMP + self._LONG_GAP)],
            previous_event=self._instant_event(self._START_TIMESTAMP),
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 0)

    def test_previous_compressed_short_gap(self):
        data = self._deserialize(
            [self._instant_event(self._START_TIMESTAMP + timedelta(seconds=1) + self._SHORT_GAP)],
            previous_event=self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=1)),
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], self._SHORT_GAP_INT)

    def test_previous_compressed_long_gap(self):
        data = self._deserialize(
            [self._instant_event(self._START_TIMESTAMP + timedelta(seconds=1) + self._LONG_GAP)],
            previous_event=self._compressed_event(self._START_TIMESTAMP, timedelta(seconds=1)),
        )
        event_times = self._get_actual_working_times(data)
        self.assertEqual(event_times[0], 0)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\events\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig
from health_check.plugins import plugin_dir


class HealthConfig(AppConfig):
    name = "cvat.apps.health"

    def ready(self):
        from .backends import OPAHealthCheck

        plugin_dir.register(OPAHealthCheck)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\backends.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import requests
from django.conf import settings
from health_check.backends import BaseHealthCheckBackend
from health_check.exceptions import HealthCheckException

from cvat.utils.http import make_requests_session


class OPAHealthCheck(BaseHealthCheckBackend):
    critical_service = True

    def check_status(self):
        opa_health_url = f"{settings.IAM_OPA_HOST}/health?bundles"
        try:
            with make_requests_session() as session:
                response = session.get(opa_health_url)
                response.raise_for_status()
        except requests.RequestException as e:
            raise HealthCheckException(str(e))

    def identifier(self):
        return self.__class__.__name__


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\__init__.py =====
#  Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\management\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\management\commands\workerprobe.py =====
import os
import platform
from datetime import datetime, timedelta

import django_rq
from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from rq.worker import Worker


class Command(BaseCommand):
    help = "Check worker liveness in specified queues"

    def add_arguments(self, parser):
        parser.add_argument("queue_names", nargs="+", type=str)

    def handle(self, *args, **options):
        hostname = platform.node()
        for queue_name in options["queue_names"]:
            if queue_name not in settings.RQ_QUEUES:
                raise CommandError(f"Queue {queue_name} is not defined")

            connection = django_rq.get_connection(queue_name)
            workers = [
                w
                for w in Worker.all(connection)
                if queue_name in w.queue_names() and w.hostname == hostname
            ]

            expected_workers = int(os.getenv("NUMPROCS", 1))

            if len(workers) != expected_workers:
                raise CommandError(
                    "Number of registered workers does not match the expected number, "
                    f"actual: {len(workers)}, expected: {expected_workers}"
                )
            for worker in workers:
                if datetime.now() - worker.last_heartbeat > timedelta(seconds=worker.worker_ttl):
                    raise CommandError(
                        f"It seems that worker {worker.name}, pid: {worker.pid} is dead"
                    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\health\management\commands\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\adapters.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from allauth.account.adapter import DefaultAccountAdapter
from django.conf import settings
from django.http import HttpResponseRedirect


class DefaultAccountAdapterEx(DefaultAccountAdapter):
    def respond_email_verification_sent(self, request, user):
        return HttpResponseRedirect(settings.ACCOUNT_EMAIL_VERIFICATION_SENT_REDIRECT_URL)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\admin.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.contrib import admin
from django.contrib.auth.admin import GroupAdmin, UserAdmin
from django.contrib.auth.models import Group, User
from django.utils.translation import gettext_lazy as _

from cvat.apps.engine.models import Profile


class ProfileInline(admin.StackedInline):
    model = Profile

    fieldsets = ((None, {"fields": ("has_analytics_access",)}),)


class CustomUserAdmin(UserAdmin):
    inlines = (ProfileInline,)
    list_display = ("username", "email", "first_name", "last_name", "is_active", "is_staff")
    fieldsets = (
        (None, {"fields": ("username", "password")}),
        (_("Personal info"), {"fields": ("first_name", "last_name", "email")}),
        (
            _("Permissions"),
            {
                "fields": (
                    "is_active",
                    "is_staff",
                    "is_superuser",
                    "groups",
                )
            },
        ),
        (_("Important dates"), {"fields": ("last_login", "date_joined")}),
    )
    add_fieldsets = (
        (
            None,
            {
                "classes": ("wide",),
                "fields": ("username", "email", "password1", "password2"),
            },
        ),
    )
    actions = ["user_activate", "user_deactivate"]

    @admin.action(permissions=["change"], description=_("Mark selected users as active"))
    def user_activate(self, request, queryset):
        queryset.update(is_active=True)

    @admin.action(permissions=["change"], description=_("Mark selected users as not active"))
    def user_deactivate(self, request, queryset):
        queryset.update(is_active=False)


class CustomGroupAdmin(GroupAdmin):
    fieldsets = ((None, {"fields": ("name",)}),)


admin.site.unregister(User)
admin.site.unregister(Group)
admin.site.register(User, CustomUserAdmin)
admin.site.register(Group, CustomGroupAdmin)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\apps.py =====
# Copyright (C) 2021 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class IAMConfig(AppConfig):
    name = "cvat.apps.iam"

    def ready(self):
        from .signals import register_signals

        register_signals(self)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\authentication.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import hashlib

from django.contrib.auth import get_user_model
from django.core import signing
from furl import furl
from rest_framework import exceptions
from rest_framework.authentication import BaseAuthentication


# Got implementation ideas in https://github.com/marcgibbons/drf_signed_auth
class Signer:
    QUERY_PARAM = "sign"
    MAX_AGE = 30

    @classmethod
    def get_salt(cls, url):
        normalized_url = furl(url).remove(cls.QUERY_PARAM).url.encode("utf-8")
        salt = hashlib.sha256(normalized_url).hexdigest()
        return salt

    def sign(self, user, url):
        """
        Create a signature for a user object.
        """
        data = {"user_id": user.pk, "username": user.get_username()}

        return signing.dumps(data, salt=self.get_salt(url))

    def unsign(self, signature, url):
        """
        Return a user object for a valid signature.
        """
        User = get_user_model()
        data = signing.loads(signature, salt=self.get_salt(url), max_age=self.MAX_AGE)

        if not isinstance(data, dict):
            raise signing.BadSignature()

        try:
            return User.objects.get(
                **{"pk": data.get("user_id"), User.USERNAME_FIELD: data.get("username")}
            )
        except User.DoesNotExist:
            raise signing.BadSignature()


class SignatureAuthentication(BaseAuthentication):
    """
    Authentication backend for signed URLs.
    """

    def authenticate(self, request):
        """
        Returns authenticated user if URL signature is valid.
        """
        signer = Signer()
        sign = request.query_params.get(Signer.QUERY_PARAM)
        if not sign:
            return

        try:
            user = signer.unsign(sign, request.build_absolute_uri())
        except signing.SignatureExpired:
            raise exceptions.AuthenticationFailed("This URL has expired.")
        except signing.BadSignature:
            raise exceptions.AuthenticationFailed("Invalid signature.")
        if not user.is_active:
            raise exceptions.AuthenticationFailed("User inactive or deleted.")

        return (user, None)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\filters.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from collections.abc import Iterable

from django.db.models import Q
from drf_spectacular.utils import OpenApiParameter
from rest_framework.filters import BaseFilterBackend

ORGANIZATION_OPEN_API_PARAMETERS = [
    OpenApiParameter(
        name="org",
        type=str,
        required=False,
        location=OpenApiParameter.QUERY,
        description="Organization unique slug",
    ),
    OpenApiParameter(
        name="org_id",
        type=int,
        required=False,
        location=OpenApiParameter.QUERY,
        description="Organization identifier",
    ),
    OpenApiParameter(
        name="X-Organization",
        type=str,
        required=False,
        location=OpenApiParameter.HEADER,
        description="Organization unique slug",
    ),
]


class OrganizationFilterBackend(BaseFilterBackend):

    def _parameter_is_provided(self, request):
        for parameter in ORGANIZATION_OPEN_API_PARAMETERS:
            if parameter.location == "header" and parameter.name in request.headers:
                return True
            elif parameter.location == "query" and parameter.name in request.query_params:
                return True
        return False

    def _construct_filter_query(self, organization_fields, org_id):
        if isinstance(organization_fields, str):
            return Q(**{organization_fields: org_id})

        if isinstance(organization_fields, Iterable):
            # we select all db records where AT LEAST ONE organization field is equal org_id
            operation = Q.OR

            if org_id is None:
                # but to get all non-org objects we need select db records where ALL organization fields are None
                operation = Q.AND

            filter_query = Q()
            for org_field in organization_fields:
                filter_query.add(Q(**{org_field: org_id}), operation)

            return filter_query

        return Q()

    def filter_queryset(self, request, queryset, view):
        # Filter works only for "list" requests and allows to return
        # only non-organization objects if org isn't specified

        if (
            view.detail
            or not view.iam_organization_field
            or
            # FIXME:  It should be handled in another way. For example, if we try to get information for a specific job
            # and org isn't specified, we need to return the full list of labels, issues, comments.
            # Allow crowdsourcing users to get labels/issues/comments related to specific job.
            # Crowdsourcing user always has worker group and isn't a member of an organization.
            (
                view.__class__.__name__ in ("LabelViewSet", "IssueViewSet", "CommentViewSet")
                and request.query_params.get("job_id")
                and request.iam_context.get("organization") is None
                and request.iam_context.get("privilege") == "worker"
            )
        ):
            return queryset

        visibility = None
        org = request.iam_context["organization"]

        if org:
            visibility = {"organization": org.id}

        elif not org and self._parameter_is_provided(request):
            visibility = {"organization": None}

        if visibility:
            org_id = visibility.pop("organization")
            query = self._construct_filter_query(view.iam_organization_field, org_id)

            return queryset.filter(query).distinct()

        return queryset

    def get_schema_operation_parameters(self, view):
        if not view.iam_organization_field or view.detail:
            return []

        parameters = []
        for parameter in ORGANIZATION_OPEN_API_PARAMETERS:
            parameter_type = None

            if parameter.type == int:
                parameter_type = "integer"
            elif parameter.type == str:
                parameter_type = "string"

            parameters.append(
                {
                    "name": parameter.name,
                    "in": parameter.location,
                    "description": parameter.description,
                    "schema": {"type": parameter_type},
                }
            )

        return parameters


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\forms.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from allauth.account.adapter import get_adapter
from allauth.account.forms import default_token_generator
from allauth.account.utils import user_pk_to_url_str
from dj_rest_auth.forms import AllAuthPasswordResetForm
from django.contrib.auth import get_user_model
from django.contrib.sites.shortcuts import get_current_site

UserModel = get_user_model()


class ResetPasswordFormEx(AllAuthPasswordResetForm):
    def save(
        self,
        request=None,
        domain_override=None,
        email_template_prefix="authentication/password_reset_key",
        use_https=False,
        token_generator=default_token_generator,
        extra_email_context=None,
        **kwargs,
    ):
        """
        Generate a one-use only link for resetting password and send it to the
        user.
        """
        email = self.cleaned_data["email"]
        if not domain_override:
            current_site = get_current_site(request)
            site_name = current_site.name
            domain = current_site.domain
        else:
            site_name = domain = domain_override
        email_field_name = UserModel.get_email_field_name()
        for user in self.users:
            user_email = getattr(user, email_field_name)
            context = {
                "email": user_email,
                "domain": domain,
                "site_name": site_name,
                "uid": user_pk_to_url_str(user),
                "user": user,
                "token": token_generator.make_token(user),
                "protocol": "https" if use_https else "http",
                **(extra_email_context or {}),
            }

            get_adapter(request).send_mail(email_template_prefix, email, context)

        return self.cleaned_data["email"]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\middleware.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datetime import timedelta
from typing import Any, Callable, Protocol

from django.conf import settings
from django.http import HttpRequest, HttpResponse
from django.utils.functional import SimpleLazyObject
from rest_framework.exceptions import NotFound, ValidationError


class WithIAMContext(Protocol):
    iam_context: dict[str, Any]


def get_organization(request: HttpRequest):
    from cvat.apps.organizations.models import Organization

    IAM_ROLES = {role: priority for priority, role in enumerate(settings.IAM_ROLES)}
    groups = list(request.user.groups.filter(name__in=list(IAM_ROLES.keys())))
    groups.sort(key=lambda group: IAM_ROLES[group.name])
    privilege = groups[0] if groups else None

    organization = None

    try:
        org_slug = request.GET.get("org")
        org_id = request.GET.get("org_id")
        org_header = request.headers.get("X-Organization")

        if org_id is not None and (org_slug is not None or org_header is not None):
            raise ValidationError(
                'You cannot specify "org_id" query parameter with '
                '"org" query parameter or "X-Organization" HTTP header at the same time.'
            )

        if org_slug is not None and org_header is not None and org_slug != org_header:
            raise ValidationError(
                'You cannot specify "org" query parameter and '
                '"X-Organization" HTTP header with different values.'
            )

        org_slug = org_slug if org_slug is not None else org_header

        if org_slug:
            organization = Organization.objects.select_related("owner").get(slug=org_slug)
        elif org_id:
            organization = Organization.objects.select_related("owner").get(id=int(org_id))
    except Organization.DoesNotExist:
        raise NotFound(f"{org_slug or org_id} organization does not exist.")

    context = {"organization": organization, "privilege": getattr(privilege, "name", None)}

    return context


class ContextMiddleware:
    def __init__(self, get_response):
        self.get_response = get_response

    def __call__(self, request: HttpRequest):

        # https://stackoverflow.com/questions/26240832/django-and-middleware-which-uses-request-user-is-always-anonymous
        request.iam_context = SimpleLazyObject(lambda: get_organization(request))

        return self.get_response(request)


class SessionRefreshMiddleware:
    """
    Implements behavior similar to SESSION_SAVE_EVERY_REQUEST=True, but instead of
    saving the session on every request, saves it at most once per REFRESH_INTERVAL.
    This is accomplished by setting a parallel cookie that expires whenever the session
    needs to be prolonged.

    This ensures that user sessions are automatically prolonged while in use,
    but avoids making an extra DB query on every HTTP request.

    Must be listed after SessionMiddleware in the MIDDLEWARE list.
    """

    _REFRESH_INTERVAL = timedelta(days=1)
    _COOKIE_NAME = "sessionfresh"

    def __init__(self, get_response: Callable[[HttpRequest], HttpResponse]) -> None:
        self.get_response = get_response

    def __call__(self, request: HttpRequest) -> HttpResponse:
        response = self.get_response(request)

        shared_cookie_args = {
            "key": self._COOKIE_NAME,
            "domain": getattr(settings, "SESSION_COOKIE_DOMAIN"),
            "path": getattr(settings, "SESSION_COOKIE_PATH", "/"),
            "samesite": getattr(settings, "SESSION_COOKIE_SAMESITE", "Lax"),
        }

        if request.session.is_empty():
            if self._COOKIE_NAME in request.COOKIES:
                response.delete_cookie(**shared_cookie_args)
            return response

        if self._COOKIE_NAME in request.COOKIES:
            # Session is still fresh.
            return response

        if response.status_code >= 500:
            # SessionMiddleware does not save the session for 5xx responses,
            # so we should not set our cookie either.
            return response

        response.set_cookie(
            **shared_cookie_args,
            value="1",
            max_age=min(
                self._REFRESH_INTERVAL,
                # Refresh no later than after half of the session lifetime.
                timedelta(seconds=request.session.get_expiry_age() // 2),
            ),
            httponly=True,
            secure=getattr(settings, "SESSION_COOKIE_SECURE", False),
        )

        # Force SessionMiddleware to re-save the session.
        request.session.modified = True

        return response


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\models.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import importlib
import operator
from abc import ABCMeta, abstractmethod
from collections.abc import Sequence
from enum import Enum
from pathlib import Path
from typing import Any, Optional, TypeVar

from attrs import define, field
from django.apps import AppConfig
from django.conf import settings
from django.db.models import Model, Q
from rest_framework.exceptions import PermissionDenied
from rest_framework.permissions import BasePermission

from cvat.apps.organizations.models import Membership, Organization
from cvat.utils.http import make_requests_session

from .utils import add_opa_rules_path


class StrEnum(str, Enum):
    def __str__(self) -> str:
        return self.value


@define
class PermissionResult:
    allow: bool
    reasons: list[str] = field(factory=list)


def get_organization(request, obj):
    # Try to get organization from an object otherwise, return the organization that is specified in query parameters
    if isinstance(obj, Organization):
        return obj

    if obj:
        try:
            org_id = obj.organization_id
        except AttributeError as exc:
            # Skip initialization of organization for those objects that don't related with organization
            view = request.parser_context.get("view")
            if view and view.basename in settings.OBJECTS_NOT_RELATED_WITH_ORG:
                return request.iam_context["organization"]

            raise exc

        if not org_id:
            return None

        try:
            # If the object belongs to an organization transitively via the parent object
            # there might be no organization field, because it has to be defined and implemented
            # manually
            try:
                return obj.organization
            except AttributeError:
                return Organization.objects.get(id=org_id)
        except Organization.DoesNotExist:
            return None

    return request.iam_context["organization"]


def get_membership(request, organization):
    if organization is None:
        return None

    return Membership.objects.filter(
        organization=organization, user=request.user, is_active=True
    ).first()


def build_iam_context(
    request, organization: Optional[Organization], membership: Optional[Membership]
):
    return {
        "user_id": request.user.id,
        "group_name": request.iam_context["privilege"],
        "org_id": getattr(organization, "id", None),
        "org_slug": getattr(organization, "slug", None),
        "org_owner_id": organization.owner_id if organization else None,
        "org_role": getattr(membership, "role", None),
    }


def get_iam_context(request, obj) -> dict[str, Any]:
    organization = get_organization(request, obj)
    membership = get_membership(request, organization)

    return build_iam_context(request, organization, membership)


class OpenPolicyAgentPermission(metaclass=ABCMeta):
    url: str
    user_id: int
    group_name: Optional[str]
    org_id: Optional[int]
    org_owner_id: Optional[int]
    org_role: Optional[str]
    scope: str
    obj: Optional[Any]

    @classmethod
    @abstractmethod
    def create(cls, request, view, obj, iam_context) -> Sequence[OpenPolicyAgentPermission]: ...

    @classmethod
    def create_base_perm(cls, request, view, scope, iam_context, obj=None, **kwargs):
        if not iam_context and request:
            iam_context = get_iam_context(request, obj)
        return cls(scope=scope, obj=obj, **iam_context, **kwargs)

    @classmethod
    def create_scope_list(cls, request, iam_context=None):
        if not iam_context and request:
            iam_context = get_iam_context(request, None)
        return cls(**iam_context, scope="list")

    def __init__(self, **kwargs):
        self.obj = None
        for name, val in kwargs.items():
            setattr(self, name, val)

        self.payload = {
            "input": {
                "scope": self.scope,
                "auth": {
                    "user": {
                        "id": self.user_id,
                        "privilege": self.group_name,
                    },
                    "organization": (
                        {
                            "id": self.org_id,
                            "owner": {
                                "id": self.org_owner_id,
                            },
                            "user": {
                                "role": self.org_role,
                            },
                        }
                        if self.org_id is not None
                        else None
                    ),
                },
            }
        }

        self.payload["input"]["resource"] = self.get_resource()

    @abstractmethod
    def get_resource(self):
        return None

    def check_access(self) -> PermissionResult:
        with make_requests_session() as session:
            response = session.post(self.url, json=self.payload)
            output = response.json()["result"]

        allow = False
        reasons = []
        if isinstance(output, dict):
            allow = output["allow"]
            reasons = output.get("reasons", [])
        elif isinstance(output, bool):
            allow = output
        else:
            raise ValueError("Unexpected response format")

        return PermissionResult(allow=allow, reasons=reasons)

    def filter(self, queryset):
        url = self.url.replace("/allow", "/filter")

        with make_requests_session() as session:
            r = session.post(url, json=self.payload).json()["result"]

        q_objects = []
        ops_dict = {
            "|": operator.or_,
            "&": operator.and_,
            "~": operator.not_,
        }
        for item in r:
            if isinstance(item, str):
                val1 = q_objects.pop()
                if item == "~":
                    q_objects.append(ops_dict[item](val1))
                else:
                    val2 = q_objects.pop()
                    q_objects.append(ops_dict[item](val1, val2))
            else:
                q_objects.append(Q(**item))

        if q_objects:
            assert len(q_objects) == 1
        else:
            q_objects.append(Q())

        # By default, a QuerySet will not eliminate duplicate rows. If your
        # query spans multiple tables (e.g. members__user_id, owner_id), it's
        # possible to get duplicate results when a QuerySet is evaluated.
        # That's when you'd use distinct().
        return queryset.filter(q_objects[0]).distinct()

    @classmethod
    def get_per_field_update_scopes(cls, request, scopes_per_field):
        """
        Returns the list of required scopes for a PATCH endpoint where different
        request body fields are associated with different scopes.
        """

        assert request.method == "PATCH"

        # Even if no fields are modified, a PATCH request typically returns the
        # new state of the object, so we need to make sure the user has permissions
        # to view it.
        scopes = [cls.Scopes.VIEW]

        try:
            scopes.extend({scopes_per_field[field_name] for field_name in request.data})
        except KeyError as ex:
            raise PermissionDenied("Attempted to update an unknown field") from ex

        return scopes


T = TypeVar("T", bound=Model)


def is_public_obj(obj: T) -> bool:
    return getattr(obj, "is_public", False)


class PolicyEnforcer(BasePermission):
    # pylint: disable=no-self-use
    def check_permission(self, request, view, obj) -> bool:
        # DRF can send OPTIONS request. Internally it will try to get
        # information about serializers for PUT and POST requests (clone
        # request and replace the http method). To avoid handling
        # ('POST', 'metadata') and ('PUT', 'metadata') in every request,
        # the condition below is enough.
        if self.is_metadata_request(request, view) or obj and is_public_obj(obj):
            return True

        iam_context = get_iam_context(request, obj)
        for perm_class in OpenPolicyAgentPermission.__subclasses__():
            for perm in perm_class.create(request, view, obj, iam_context):
                result = perm.check_access()
                if not result.allow:
                    return False

        return True

    def has_permission(self, request, view):
        if not view.detail:
            return self.check_permission(request, view, None)
        else:
            return True  # has_object_permission will be called later

    def has_object_permission(self, request, view, obj):
        return self.check_permission(request, view, obj)

    @staticmethod
    def is_metadata_request(request, view):
        return request.method == "OPTIONS" or (
            request.method == "POST" and view.action == "metadata" and len(request.data) == 0
        )


class IsAuthenticatedOrReadPublicResource(BasePermission):
    def has_object_permission(self, request, view, obj) -> bool:
        return bool(
            (request.user and request.user.is_authenticated)
            or (request.method == "GET" and is_public_obj(obj))
        )


def load_app_permissions(config: AppConfig) -> None:
    """
    Ensures that permissions and OPA rules from the given app are loaded.

    This function should be called from the AppConfig.ready() method of every
    app that defines a permissions module.
    """
    permissions_module = importlib.import_module(config.name + ".permissions")

    assert any(
        isinstance(attr, type) and issubclass(attr, OpenPolicyAgentPermission)
        for attr in vars(permissions_module).values()
    )

    add_opa_rules_path(Path(config.path, "rules"))


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\schema.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import re
import textwrap

from drf_spectacular.authentication import SessionScheme, TokenScheme
from drf_spectacular.extensions import OpenApiAuthenticationExtension
from drf_spectacular.openapi import AutoSchema
from rest_framework import serializers


class SignatureAuthenticationScheme(OpenApiAuthenticationExtension):
    """
    Adds the signature auth method to schema
    """

    target_class = "cvat.apps.iam.authentication.SignatureAuthentication"
    name = "signatureAuth"  # name used in the schema

    def get_security_definition(self, auto_schema):
        return {
            "type": "apiKey",
            "in": "query",
            "name": "sign",
            "description": "Can be used to share URLs to private links",
        }


class TokenAuthenticationScheme(TokenScheme):
    """
    Adds the token auth method to schema. The description includes extra info
    comparing to what is generated by default.
    """

    name = "tokenAuth"
    priority = 0
    match_subclasses = True

    def get_security_requirement(self, auto_schema):
        # These schemes must be used together
        return {"sessionAuth": [], "csrfAuth": [], self.name: []}

    def get_security_definition(self, auto_schema):
        schema = super().get_security_definition(auto_schema)
        schema["x-token-prefix"] = self.target.keyword
        schema["description"] = textwrap.dedent(
            f"""
            To authenticate using a token (or API key), you need to have 3 components in a request:
            - the 'sessionid' cookie
            - the 'csrftoken' cookie or 'X-CSRFTOKEN' header
            - the 'Authentication' header with the '{self.target.keyword} ' prefix

            You can obtain an API key (the token) from the server response on
            the basic auth request.
        """
        )
        return schema


class CookieAuthenticationScheme(SessionScheme):
    """
    This class adds csrftoken cookie into security sections. It must be used together with
    the 'sessionid' cookie.
    """

    name = ["sessionAuth", "csrfAuth"]
    priority = 0

    def get_security_requirement(self, auto_schema):
        # These schemes cannot be used separately
        return None

    def get_security_definition(self, auto_schema):
        sessionid_schema = super().get_security_definition(auto_schema)
        csrftoken_schema = {
            "type": "apiKey",
            "in": "cookie",
            "name": "csrftoken",
            "description": "Can be sent as a cookie or as the X-CSRFTOKEN header",
        }
        return [sessionid_schema, csrftoken_schema]


class CustomAutoSchema(AutoSchema):
    def get_operation_id(self):
        # Change style of operation ids to [viewset _ action _ object]
        # This form is simpler to handle during SDK generation

        tokenized_path = self._tokenize_path()
        # replace dashes as they can be problematic later in code generation
        tokenized_path = [t.replace("-", "_") for t in tokenized_path]

        if self.method == "GET" and self._is_list_view():
            action = "list"
        else:
            action = self.method_mapping[self.method.lower()]

        if not tokenized_path:
            tokenized_path.append("root")

        if re.search(r"<drf_format_suffix\w*:\w+>", self.path_regex):
            tokenized_path.append("formatted")

        return "_".join([tokenized_path[0]] + [action] + tokenized_path[1:])

    def _get_request_for_media_type(self, serializer, *args, **kwargs):
        # Enables support for required=False serializers in request body specification
        # in drf-spectacular. Doesn't block other extensions on the target serializer.
        # This is supported by OpenAPI and by SDK generator, but not by drf-spectacular

        schema, required = super()._get_request_for_media_type(serializer, *args, **kwargs)

        if isinstance(serializer, serializers.Serializer):
            if not serializer.required:
                required = False

        return schema, required


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\serializers.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import Optional, Union

from allauth.account import app_settings as allauth_settings
from allauth.account.adapter import get_adapter
from allauth.account.models import EmailAddress
from allauth.account.utils import filter_users_by_email, setup_user_email
from dj_rest_auth.registration.serializers import RegisterSerializer
from dj_rest_auth.serializers import LoginSerializer, PasswordResetSerializer
from django.conf import settings
from django.contrib.auth import get_user_model
from django.contrib.auth.models import User
from django.core.exceptions import ValidationError as DjangoValidationError
from drf_spectacular.utils import extend_schema_field
from rest_framework import serializers
from rest_framework.exceptions import ValidationError

from cvat.apps.iam.forms import ResetPasswordFormEx
from cvat.apps.iam.utils import get_dummy_user


class RegisterSerializerEx(RegisterSerializer):
    first_name = serializers.CharField(required=False)
    last_name = serializers.CharField(required=False)
    email_verification_required = serializers.SerializerMethodField()
    key = serializers.SerializerMethodField()

    @extend_schema_field(serializers.BooleanField)
    def get_email_verification_required(self, obj: Union[dict, User]) -> bool:
        return (
            allauth_settings.EMAIL_VERIFICATION
            == allauth_settings.EmailVerificationMethod.MANDATORY
        )

    @extend_schema_field(serializers.CharField(allow_null=True))
    def get_key(self, obj: Union[dict, User]) -> Optional[str]:
        key = None
        if (
            isinstance(obj, User)
            and allauth_settings.EMAIL_VERIFICATION
            != allauth_settings.EmailVerificationMethod.MANDATORY
        ):
            key = obj.auth_token.key
        return key

    def get_cleaned_data(self):
        data = super().get_cleaned_data()
        data.update(
            {
                "first_name": self.validated_data.get("first_name", ""),
                "last_name": self.validated_data.get("last_name", ""),
            }
        )

        return data

    def validate_email(self, email):
        def email_address_exists(email) -> bool:
            if EmailAddress.objects.filter(email__iexact=email).exists():
                return True

            if email_field := allauth_settings.USER_MODEL_EMAIL_FIELD:
                users = get_user_model().objects
                return users.filter(**{email_field + "__iexact": email}).exists()
            return False

        email = get_adapter().clean_email(email)
        if allauth_settings.UNIQUE_EMAIL:
            if email and email_address_exists(email):
                user = get_dummy_user(email)
                if not user:
                    raise serializers.ValidationError(
                        ("A user is already registered with this e-mail address."),
                    )

        return email

    def save(self, request):
        adapter = get_adapter()
        self.cleaned_data = self.get_cleaned_data()

        # Allow to overwrite data for dummy users
        dummy_user = get_dummy_user(self.cleaned_data["email"])
        user = dummy_user if dummy_user else adapter.new_user(request)

        user = adapter.save_user(request, user, self, commit=False)
        if "password1" in self.cleaned_data:
            try:
                adapter.clean_password(self.cleaned_data["password1"], user=user)
            except DjangoValidationError as exc:
                raise serializers.ValidationError(detail=serializers.as_serializer_error(exc))
        user.save()
        self.custom_signup(request, user)

        if not dummy_user:
            setup_user_email(request, user, [])
        return user


class PasswordResetSerializerEx(PasswordResetSerializer):
    @property
    def password_reset_form_class(self):
        return ResetPasswordFormEx

    def get_email_options(self):
        domain = None
        if hasattr(settings, "UI_HOST") and settings.UI_HOST:
            domain = settings.UI_HOST
            if hasattr(settings, "UI_PORT") and settings.UI_PORT:
                domain += ":{}".format(settings.UI_PORT)
        return {"domain_override": domain}


class LoginSerializerEx(LoginSerializer):
    def get_auth_user_using_allauth(self, username, email, password):

        def is_email_authentication():
            return (
                settings.ACCOUNT_AUTHENTICATION_METHOD
                == allauth_settings.AuthenticationMethod.EMAIL
            )

        def is_username_authentication():
            return (
                settings.ACCOUNT_AUTHENTICATION_METHOD
                == allauth_settings.AuthenticationMethod.USERNAME
            )

        # check that the server settings match the request
        if is_username_authentication() and not username and email:
            raise ValidationError(
                "Attempt to authenticate with email/password. "
                "But username/password are used for authentication on the server. "
                "Please check your server configuration ACCOUNT_AUTHENTICATION_METHOD."
            )

        if is_email_authentication() and not email and username:
            raise ValidationError(
                "Attempt to authenticate with username/password. "
                "But email/password are used for authentication on the server. "
                "Please check your server configuration ACCOUNT_AUTHENTICATION_METHOD."
            )

        # Authentication through email
        if settings.ACCOUNT_AUTHENTICATION_METHOD == allauth_settings.AuthenticationMethod.EMAIL:
            return self._validate_email(email, password)

        # Authentication through username
        if settings.ACCOUNT_AUTHENTICATION_METHOD == allauth_settings.AuthenticationMethod.USERNAME:
            return self._validate_username(username, password)

        # Authentication through either username or email
        if email:
            users = filter_users_by_email(email)
            if not users or len(users) > 1:
                raise ValidationError("Unable to login with provided credentials")

        return self._validate_username_email(username, email, password)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\signals.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings
from django.contrib.auth.models import Group, User
from django.db.models.signals import post_migrate, post_save


def register_groups(sender, **kwargs):
    # Create all groups which corresponds system roles
    for role in settings.IAM_ROLES:
        Group.objects.get_or_create(name=role)


if settings.IAM_TYPE == "BASIC":

    def create_user(sender, instance, created: bool, raw: bool, **kwargs):
        if created and raw:
            return

        from allauth.account import app_settings as allauth_settings
        from allauth.account.models import EmailAddress

        if instance.is_superuser and instance.is_staff:
            db_group = Group.objects.get(name=settings.IAM_ADMIN_ROLE)
            instance.groups.add(db_group)

            # create and verify EmailAddress for superuser accounts
            if allauth_settings.EMAIL_REQUIRED:
                EmailAddress.objects.get_or_create(
                    user=instance, email=instance.email, primary=True, verified=True
                )
        else:  # don't need to add default groups for superuser
            if created and not getattr(instance, "skip_group_assigning", None):
                db_group = Group.objects.get(name=settings.IAM_DEFAULT_ROLE)
                instance.groups.add(db_group)

elif settings.IAM_TYPE == "LDAP":

    def create_user(sender, created: bool, raw: bool, user=None, ldap_user=None, **kwargs):
        if created and raw:
            return

        user_groups = []
        for role in settings.IAM_ROLES:
            db_group = Group.objects.get(name=role)

            for ldap_group in settings.DJANGO_AUTH_LDAP_GROUPS[role]:
                if ldap_group.lower() in ldap_user.group_dns:
                    user_groups.append(db_group)
                    if role == settings.IAM_ADMIN_ROLE:
                        user.is_staff = user.is_superuser = True
                    break
        # add default group if no other group has been assigned
        if not len(user_groups):
            user_groups.append(Group.objects.get(name=settings.IAM_DEFAULT_ROLE))

        # It is important to save the user before adding groups. Please read
        # https://django-auth-ldap.readthedocs.io/en/latest/users.html#populating-users
        # The user instance will be saved automatically after the signal handler
        # is run.
        user.save()
        user.groups.set(user_groups)


def register_signals(app_config):
    post_migrate.connect(register_groups, app_config)
    if settings.IAM_TYPE == "BASIC":
        # Add default groups and add admin rights to super users.
        post_save.connect(create_user, sender=User)
    elif settings.IAM_TYPE == "LDAP":
        import django_auth_ldap.backend

        # Map groups from LDAP to roles, convert a user to super user if he/she
        # has an admin group.
        django_auth_ldap.backend.populate_user.connect(create_user)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\urls.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from allauth.account import app_settings as allauth_settings
from dj_rest_auth.views import (
    LogoutView,
    PasswordChangeView,
    PasswordResetConfirmView,
    PasswordResetView,
)
from django.conf import settings
from django.urls import path, re_path
from django.urls.conf import include

from cvat.apps.iam.views import (
    ConfirmEmailViewEx,
    LoginViewEx,
    RegisterViewEx,
    RulesView,
    SigningView,
)

BASIC_LOGIN_PATH_NAME = "rest_login"
BASIC_REGISTER_PATH_NAME = "rest_register"

urlpatterns = [
    path("login", LoginViewEx.as_view(), name=BASIC_LOGIN_PATH_NAME),
    path("logout", LogoutView.as_view(), name="rest_logout"),
    path("signing", SigningView.as_view(), name="signing"),
    path("rules", RulesView.as_view(), name="rules"),
]

if settings.IAM_TYPE == "BASIC":
    urlpatterns += [
        path("register", RegisterViewEx.as_view(), name=BASIC_REGISTER_PATH_NAME),
        # password
        path("password/reset", PasswordResetView.as_view(), name="rest_password_reset"),
        path(
            "password/reset/confirm",
            PasswordResetConfirmView.as_view(),
            name="rest_password_reset_confirm",
        ),
        path("password/change", PasswordChangeView.as_view(), name="rest_password_change"),
    ]
    if allauth_settings.EMAIL_VERIFICATION != allauth_settings.EmailVerificationMethod.NONE:
        # emails
        urlpatterns += [
            re_path(
                r"^account-confirm-email/(?P<key>[-:\w]+)/$",
                ConfirmEmailViewEx.as_view(),
                name="account_confirm_email",
            ),
        ]

urlpatterns = [path("auth/", include(urlpatterns))]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\utils.py =====
import functools
import hashlib
import importlib
import io
import tarfile
from pathlib import Path

from django.conf import settings
from django.contrib.sessions.backends.base import SessionBase

_OPA_RULES_PATHS = {
    Path(__file__).parent / "rules",
}


@functools.lru_cache(maxsize=None)
def get_opa_bundle() -> tuple[bytes, str]:
    bundle_file = io.BytesIO()

    with tarfile.open(fileobj=bundle_file, mode="w:gz") as tar:
        for p in _OPA_RULES_PATHS:
            for f in p.glob("*[!.gen].rego"):
                tar.add(name=f, arcname=f.relative_to(p.parent))

    bundle = bundle_file.getvalue()
    etag = hashlib.blake2b(bundle).hexdigest()
    return bundle, etag


def add_opa_rules_path(path: Path) -> None:
    _OPA_RULES_PATHS.add(path)
    get_opa_bundle.cache_clear()


def get_dummy_user(email):
    from allauth.account import app_settings
    from allauth.account.models import EmailAddress
    from allauth.account.utils import filter_users_by_email

    users = filter_users_by_email(email)
    if not users or len(users) > 1:
        return None
    user = users[0]
    if user.has_usable_password():
        return None
    if app_settings.EMAIL_VERIFICATION == app_settings.EmailVerificationMethod.MANDATORY:
        email = EmailAddress.objects.get_for_user(user, email)
        if email.verified:
            return None
    return user


def clean_up_sessions() -> None:
    SessionStore: type[SessionBase] = importlib.import_module(settings.SESSION_ENGINE).SessionStore
    SessionStore.clear_expired()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\views.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import functools

from allauth.account import app_settings as allauth_settings
from allauth.account.utils import complete_signup, has_verified_email, send_email_confirmation
from allauth.account.views import ConfirmEmailView
from dj_rest_auth.app_settings import api_settings as dj_rest_auth_settings
from dj_rest_auth.registration.views import RegisterView
from dj_rest_auth.utils import jwt_encode
from dj_rest_auth.views import LoginView
from django.conf import settings
from django.http import Http404, HttpResponse, HttpResponseBadRequest, HttpResponseRedirect
from django.views.decorators.http import etag as django_etag
from drf_spectacular.contrib.rest_auth import get_token_serializer_class
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import (
    OpenApiResponse,
    extend_schema,
    extend_schema_view,
    inline_serializer,
)
from furl import furl
from rest_framework import serializers, views
from rest_framework.exceptions import ValidationError
from rest_framework.permissions import AllowAny
from rest_framework.response import Response

from .authentication import Signer
from .utils import get_opa_bundle


@extend_schema(tags=["auth"])
@extend_schema_view(
    post=extend_schema(
        summary="This method signs URL for access to the server",
        description="Signed URL contains a token which authenticates a user on the server."
        "Signed URL is valid during 30 seconds since signing.",
        request=inline_serializer(
            name="Signing",
            fields={
                "url": serializers.CharField(),
            },
        ),
        responses={"200": OpenApiResponse(response=OpenApiTypes.STR, description="text URL")},
    )
)
class SigningView(views.APIView):

    def post(self, request):
        url = request.data.get("url")
        if not url:
            raise ValidationError("Please provide `url` parameter")

        signer = Signer()
        url = self.request.build_absolute_uri(url)
        sign = signer.sign(self.request.user, url)

        url = furl(url).add({Signer.QUERY_PARAM: sign}).url
        return Response(url)


class LoginViewEx(LoginView):
    """
    Check the credentials and return the REST Token
    if the credentials are valid and authenticated.
    If email verification is enabled and the user has the unverified email,
    an email with a confirmation link will be sent.
    Calls Django Auth login method to register User ID
    in Django session framework.

    Accept the following POST parameters: username, email, password
    Return the REST Framework Token Object's key.
    """

    @extend_schema(responses=get_token_serializer_class())
    def post(self, request, *args, **kwargs):
        self.request = request
        self.serializer = self.get_serializer(data=self.request.data)
        try:
            self.serializer.is_valid(raise_exception=True)
        except ValidationError:
            user = self.serializer.get_auth_user(
                self.serializer.data.get("username"),
                self.serializer.data.get("email"),
                self.serializer.data.get("password"),
            )
            if not user:
                raise

            # Check that user's email is verified.
            # If not, send a verification email.
            if not has_verified_email(user):
                send_email_confirmation(request, user)
                # we cannot use redirect to ACCOUNT_EMAIL_VERIFICATION_SENT_REDIRECT_URL here
                # because redirect will make a POST request and we'll get a 404 code
                # (although in the browser request method will be displayed like GET)
                return HttpResponseBadRequest("Unverified email")
        except Exception:  # nosec
            pass

        self.login()
        return self.get_response()


class RegisterViewEx(RegisterView):
    def get_response_data(self, user):
        serializer = self.get_serializer(user)
        return serializer.data

    # NOTE: we should reimplement this method to fix the following issue:
    # In the previous used version of dj-rest-auth 2.2.7, if the REST_SESSION_LOGIN setting was not defined in the settings file,
    # the default value specified in the documentation (https://dj-rest-auth.readthedocs.io/en/2.2.7/configuration.html)
    # was not applied for some unknown reason, and an authentication token was added to a user.
    # With the dj-rest-auth version 5.0.2, there have been changes to how settings are handled,
    # and now the default value is properly taken into account.
    # However, even with the updated code, it still does not handle the scenario
    # of handling two authentication flows simultaneously during registration process.
    # Since there is no mention in the dj-rest-auth documentation that session authentication
    # cannot be used alongside token authentication (https://dj-rest-auth.readthedocs.io/en/latest/configuration.html),
    # and given the login implementation (https://github.com/iMerica/dj-rest-auth/blob/c6b6530eb0bfa5b10fd7b9e955a39301156e49d2/dj_rest_auth/views.py#L69-L75),
    # this situation appears to be a bug.
    # Link to the issue: https://github.com/iMerica/dj-rest-auth/issues/604
    def perform_create(self, serializer):
        user = serializer.save(self.request)
        if (
            allauth_settings.EMAIL_VERIFICATION
            != allauth_settings.EmailVerificationMethod.MANDATORY
        ):
            if dj_rest_auth_settings.USE_JWT:
                self.access_token, self.refresh_token = jwt_encode(user)
            elif self.token_model:
                dj_rest_auth_settings.TOKEN_CREATOR(self.token_model, user, serializer)

        complete_signup(
            self.request._request,
            user,
            allauth_settings.EMAIL_VERIFICATION,
            None,
        )
        return user


def _etag(etag_func):
    """
    Decorator to support conditional retrieval (or change)
    for a Django Rest Framework's ViewSet.
    It calls Django's original decorator but pass correct request object to it.
    Django's original decorator doesn't work with DRF request object.
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(obj_self, request, *args, **kwargs):
            drf_request = request
            wsgi_request = request._request

            @django_etag(etag_func=etag_func)
            def patched_viewset_method(*_args, **_kwargs):
                """Call original viewset method with correct type of request"""
                return func(obj_self, drf_request, *args, **kwargs)

            return patched_viewset_method(wsgi_request, *args, **kwargs)

        return wrapper

    return decorator


class RulesView(views.APIView):
    serializer_class = None
    permission_classes = [AllowAny]
    authentication_classes = []
    iam_organization_field = None

    @_etag(lambda request: get_opa_bundle()[1])
    def get(self, request):
        return HttpResponse(get_opa_bundle()[0], content_type="application/x-tar")


class ConfirmEmailViewEx(ConfirmEmailView):
    template_name = "account/email/email_confirmation_signup_message.html"

    def get(self, *args, **kwargs):
        try:
            if not allauth_settings.CONFIRM_EMAIL_ON_GET:
                return super().get(*args, **kwargs)
            return self.post(*args, **kwargs)
        except Http404:
            return HttpResponseRedirect(settings.INCORRECT_EMAIL_CONFIRMATION_URL)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\__init__.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\migrations\0001_remove_business_group.py =====
# Generated by Django 4.2.16 on 2024-10-30 12:03
from django.conf import settings
from django.db import migrations

BUSINESS_GROUP_NAME = "business"
USER_GROUP_NAME = "user"


def delete_business_group(apps, schema_editor):
    Group = apps.get_model("auth", "Group")
    User = apps.get_model(settings.AUTH_USER_MODEL)

    if user_group := Group.objects.filter(name=USER_GROUP_NAME).first():
        user_group.user_set.add(*User.objects.filter(groups__name=BUSINESS_GROUP_NAME))

    Group.objects.filter(name=BUSINESS_GROUP_NAME).delete()


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.RunPython(
            delete_business_group,
            reverse_code=migrations.RunPython.noop,
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\migrations\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\rules\tests\generate_tests.py =====
#!/usr/bin/env python3
#
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import subprocess
import sys
from argparse import ArgumentParser, Namespace
from collections.abc import Sequence
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from pathlib import Path
from typing import Optional

REPO_ROOT = Path(__file__).resolve().parents[5]


def create_arg_parser() -> ArgumentParser:
    parser = ArgumentParser(add_help=True)
    parser.add_argument(
        "-a",
        "--apps-dir",
        type=Path,
        default=REPO_ROOT / "cvat/apps",
        help="The directory with Django apps (default: cvat/apps)",
    )
    return parser


def parse_args(args: Optional[Sequence[str]] = None) -> Namespace:
    parser = create_arg_parser()
    parsed_args = parser.parse_args(args)
    return parsed_args


def call_generator(generator_path: Path, gen_params: Namespace) -> None:
    rules_dir = generator_path.parents[2]
    subprocess.check_call(
        [sys.executable, generator_path.relative_to(rules_dir), "tests/configs"], cwd=rules_dir
    )


def main(args: Optional[Sequence[str]] = None) -> int:
    args = parse_args(args)

    generator_paths = list(args.apps_dir.glob("*/rules/tests/generators/*_test.gen.rego.py"))

    if not generator_paths:
        sys.exit("error: no generators found")

    with ThreadPoolExecutor() as pool:
        for _ in pool.map(
            partial(call_generator, gen_params=args),
            generator_paths,
        ):
            pass  # consume all results in order to propagate exceptions


if __name__ == "__main__":
    sys.exit(main())


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\tests\test_rest_api.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from allauth.account.views import EmailVerificationSentView
from django.test import override_settings
from django.urls import path, re_path, reverse
from rest_framework import status
from rest_framework.authtoken.models import Token
from rest_framework.test import APIClient, APITestCase

from cvat.apps.iam.urls import urlpatterns as iam_url_patterns
from cvat.apps.iam.views import ConfirmEmailViewEx

urlpatterns = iam_url_patterns + [
    re_path(
        r"^account-confirm-email/(?P<key>[-:\w]+)/$",
        ConfirmEmailViewEx.as_view(),
        name="account_confirm_email",
    ),
    path(
        "register/account-email-verification-sent",
        EmailVerificationSentView.as_view(),
        name="account_email_verification_sent",
    ),
]


class ForceLogin:
    def __init__(self, user, client):
        self.user = user
        self.client = client

    def __enter__(self):
        if self.user:
            self.client.force_login(self.user, backend="django.contrib.auth.backends.ModelBackend")

        return self

    def __exit__(self, exception_type, exception_value, traceback):
        if self.user:
            self.client.logout()


class UserRegisterAPITestCase(APITestCase):

    user_data = {
        "first_name": "test_first",
        "last_name": "test_last",
        "username": "test_username",
        "email": "test_email@test.com",
        "password1": "$Test357Test%",
        "password2": "$Test357Test%",
        "confirmations": [],
    }

    def setUp(self):
        self.client = APIClient()

    def _run_api_v2_user_register(self, data):
        url = reverse("rest_register")
        response = self.client.post(url, data, format="json")
        return response

    def _check_response(self, response, data):
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        self.assertEqual(response.data, data)

    @override_settings(ACCOUNT_EMAIL_VERIFICATION="none")
    def test_api_v2_user_register_with_email_verification_none(self):
        """
        Ensure we can register a user and get auth token key when email verification is none
        """
        response = self._run_api_v2_user_register(self.user_data)
        user_token = Token.objects.get(user__username=response.data["username"])
        self._check_response(
            response,
            {
                "first_name": "test_first",
                "last_name": "test_last",
                "username": "test_username",
                "email": "test_email@test.com",
                "email_verification_required": False,
                "key": user_token.key,
            },
        )

    # Since URLConf is executed before running the tests, so we have to manually configure the url patterns for
    # the tests and pass it using ROOT_URLCONF in the override settings decorator

    @override_settings(ACCOUNT_EMAIL_VERIFICATION="optional", ROOT_URLCONF=__name__)
    def test_api_v2_user_register_with_email_verification_optional(self):
        """
        Ensure we can register a user and get auth token key when email verification is optional
        """
        response = self._run_api_v2_user_register(self.user_data)
        user_token = Token.objects.get(user__username=response.data["username"])
        self._check_response(
            response,
            {
                "first_name": "test_first",
                "last_name": "test_last",
                "username": "test_username",
                "email": "test_email@test.com",
                "email_verification_required": False,
                "key": user_token.key,
            },
        )

    @override_settings(
        ACCOUNT_EMAIL_REQUIRED=True,
        ACCOUNT_EMAIL_VERIFICATION="mandatory",
        EMAIL_BACKEND="django.core.mail.backends.console.EmailBackend",
        ROOT_URLCONF=__name__,
    )
    def test_register_account_with_email_verification_mandatory(self):
        """
        Ensure we can register a user and it does not return auth token key when email verification is mandatory
        """
        response = self._run_api_v2_user_register(self.user_data)
        self._check_response(
            response,
            {
                "first_name": "test_first",
                "last_name": "test_last",
                "username": "test_username",
                "email": "test_email@test.com",
                "email_verification_required": True,
                "key": None,
            },
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\iam\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\apps.py =====
# Copyright (C) 2020-2021 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class LambdaManagerConfig(AppConfig):
    name = "cvat.apps.lambda_manager"

    def ready(self) -> None:
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\models.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import django.db.models as models


class FunctionKind(models.TextChoices):
    DETECTOR = "detector"
    INTERACTOR = "interactor"
    REID = "reid"
    TRACKER = "tracker"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings

from cvat.apps.engine.permissions import JobPermission, TaskPermission
from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum


class LambdaPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        LIST = "list"
        VIEW = "view"
        CALL_ONLINE = "call:online"
        CALL_OFFLINE = "call:offline"
        LIST_OFFLINE = "list:offline"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "lambda_function" or view.basename == "lambda_request":
            scopes = cls.get_scopes(request, view, obj)
            for scope in scopes:
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

            if job_id := request.data.get("job"):
                perm = JobPermission.create_scope_view_data(iam_context, job_id)
                permissions.append(perm)
            elif task_id := request.data.get("task"):
                perm = TaskPermission.create_scope_view_data(iam_context, task_id)
                permissions.append(perm)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/lambda/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                ("lambda_function", "list"): Scopes.LIST,
                ("lambda_function", "retrieve"): Scopes.VIEW,
                ("lambda_function", "call"): Scopes.CALL_ONLINE,
                ("lambda_request", "create"): Scopes.CALL_OFFLINE,
                ("lambda_request", "list"): Scopes.LIST_OFFLINE,
                ("lambda_request", "retrieve"): Scopes.CALL_OFFLINE,
                ("lambda_request", "destroy"): Scopes.CALL_OFFLINE,
            }[(view.basename, view.action)]
        ]

    def get_resource(self):
        return None


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\rq.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from django.db.models import Model

from cvat.apps.engine.rq import (
    BaseRQMeta,
    ImmutableRQMetaAttribute,
    MutableRQMetaAttribute,
    RQJobMetaField,
)
from cvat.apps.engine.types import ExtendedRequest


class LambdaRQMeta(BaseRQMeta):
    # immutable fields
    function_id: str = ImmutableRQMetaAttribute(RQJobMetaField.FUNCTION_ID)
    lambda_: bool | None = ImmutableRQMetaAttribute(RQJobMetaField.LAMBDA, optional=True)
    # FUTURE-FIXME: progress should be in [0, 1] range
    progress: int | None = MutableRQMetaAttribute(
        RQJobMetaField.PROGRESS, validator=lambda x: isinstance(x, int), optional=True
    )

    @classmethod
    def build_for(
        cls,
        *,
        request: ExtendedRequest,
        db_obj: Model,
        function_id: str,
    ):
        base_meta = BaseRQMeta.build(request=request, db_obj=db_obj)
        return {
            **base_meta,
            RQJobMetaField.FUNCTION_ID: function_id,
            RQJobMetaField.LAMBDA: True,
        }


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\serializers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from drf_spectacular.utils import extend_schema_serializer
from rest_framework import serializers


class SublabelMappingEntrySerializer(serializers.Serializer):
    name = serializers.CharField()
    attributes = serializers.DictField(child=serializers.CharField(), required=False)


class LabelMappingEntrySerializer(serializers.Serializer):
    name = serializers.CharField()
    attributes = serializers.DictField(child=serializers.CharField(), required=False)
    sublabels = serializers.DictField(
        child=SublabelMappingEntrySerializer(),
        required=False,
        help_text="Label mapping for from the model to the task sublabels within a parent label",
    )


@extend_schema_serializer(
    # The "Request" suffix is added by drf-spectacular automatically
    component_name="FunctionCall"
)
class FunctionCallRequestSerializer(serializers.Serializer):
    function = serializers.CharField(help_text="The name of the function to execute")
    task = serializers.IntegerField(help_text="The id of the task to be annotated")
    job = serializers.IntegerField(required=False, help_text="The id of the job to be annotated")
    max_distance = serializers.IntegerField(required=False)
    threshold = serializers.FloatField(required=False)
    cleanup = serializers.BooleanField(
        help_text="Whether existing annotations should be removed", default=False
    )
    convMaskToPoly = serializers.BooleanField(
        required=False,
        source="conv_mask_to_poly",
        write_only=True,
        help_text="Deprecated; use conv_mask_to_poly instead",
    )
    conv_mask_to_poly = serializers.BooleanField(
        required=False, help_text="Convert mask shapes to polygons"
    )
    mapping = serializers.DictField(
        child=LabelMappingEntrySerializer(),
        required=False,
        help_text="Label mapping from the model to the task labels",
    )


class FunctionCallParamsSerializer(serializers.Serializer):
    id = serializers.CharField(allow_null=True, help_text="The name of the function")

    task = serializers.IntegerField(allow_null=True, help_text="The id of the task")
    job = serializers.IntegerField(required=False, help_text="The id of the job")

    threshold = serializers.FloatField(allow_null=True)


class FunctionCallSerializer(serializers.Serializer):
    id = serializers.CharField(help_text="Request id")

    function = FunctionCallParamsSerializer()
    status = serializers.CharField(allow_null=True)
    progress = serializers.IntegerField(default=0, allow_null=True)
    enqueued = serializers.DateTimeField(allow_null=True)
    started = serializers.DateTimeField(allow_null=True)
    ended = serializers.DateTimeField(allow_null=True)
    exc_info = serializers.CharField(required=False, allow_null=True)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\signals.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.dispatch import Signal

interactive_function_call_signal = Signal()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\urls.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from django.urls import include, path
from rest_framework import routers

from . import views

router = routers.DefaultRouter(trailing_slash=False)
# https://github.com/encode/django-rest-framework/issues/6645
# I want to "call" my functions. To do that need to map my call method to
# POST (like get HTTP method is mapped to list(...)). One way is to implement
# own CustomRouter. But it is simpler just patch the router instance here.
router.routes[2].mapping.update({"post": "call"})
router.register("functions", views.FunctionViewSet, basename="lambda_function")
router.register("requests", views.RequestViewSet, basename="lambda_request")

# GET  /api/lambda/functions - get list of functions
# GET  /api/lambda/functions/<int:fid> - get information about the function
# POST /api/lambda/requests - call a function
# { "function": "<id>", "mode": "online|offline", "job": "<jid>", "frame": "<n>",
#   "points": [...], }
# GET  /api/lambda/requests - get list of requests
# GET  /api/lambda/requests/<int:rid> - get status of the request
# DEL  /api/lambda/requests/<int:rid> - cancel a request (don't delete)
urlpatterns = [path("api/lambda/", include(router.urls))]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\views.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import base64
import json
import os
import textwrap
from copy import deepcopy
from datetime import timedelta
from functools import wraps
from typing import Any, Optional

import datumaro.util.mask_tools as mask_tools
import django_rq
import numpy as np
import requests
import rq
from django.conf import settings
from django.core.exceptions import ObjectDoesNotExist, ValidationError
from django.core.signing import BadSignature, TimestampSigner
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import (
    OpenApiParameter,
    OpenApiResponse,
    extend_schema,
    extend_schema_view,
    inline_serializer,
)
from rest_framework import serializers, status, viewsets
from rest_framework.response import Response

import cvat.apps.dataset_manager as dm
from cvat.apps.dataset_manager.task import PatchAction
from cvat.apps.engine.frame_provider import TaskFrameProvider
from cvat.apps.engine.log import ServerLogManager
from cvat.apps.engine.models import (
    Job,
    Label,
    RequestAction,
    RequestTarget,
    ShapeType,
    SourceType,
    Task,
)
from cvat.apps.engine.rq import RQId, define_dependent_job
from cvat.apps.engine.serializers import LabeledDataSerializer
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import get_rq_lock_by_user, get_rq_lock_for_job
from cvat.apps.events.handlers import handle_function_call
from cvat.apps.iam.filters import ORGANIZATION_OPEN_API_PARAMETERS
from cvat.apps.lambda_manager.models import FunctionKind
from cvat.apps.lambda_manager.permissions import LambdaPermission
from cvat.apps.lambda_manager.rq import LambdaRQMeta
from cvat.apps.lambda_manager.serializers import (
    FunctionCallRequestSerializer,
    FunctionCallSerializer,
)
from cvat.apps.lambda_manager.signals import interactive_function_call_signal
from cvat.utils.http import make_requests_session

slogger = ServerLogManager(__name__)


class LambdaGateway:
    NUCLIO_ROOT_URL = "/api/functions"

    def _http(
        self,
        method="get",
        scheme=None,
        host=None,
        port=None,
        function_namespace=None,
        url=None,
        headers=None,
        data=None,
    ):
        NUCLIO_GATEWAY = "{}://{}:{}".format(
            scheme or settings.NUCLIO["SCHEME"],
            host or settings.NUCLIO["HOST"],
            port or settings.NUCLIO["PORT"],
        )
        NUCLIO_FUNCTION_NAMESPACE = function_namespace or settings.NUCLIO["FUNCTION_NAMESPACE"]
        NUCLIO_TIMEOUT = settings.NUCLIO["DEFAULT_TIMEOUT"]
        extra_headers = {
            "x-nuclio-project-name": "cvat",
            "x-nuclio-function-namespace": NUCLIO_FUNCTION_NAMESPACE,
            "x-nuclio-invoke-via": "domain-name",
            "X-Nuclio-Invoke-Timeout": f"{NUCLIO_TIMEOUT}s",
        }
        if headers:
            extra_headers.update(headers)

        if url:
            url = "{}{}".format(NUCLIO_GATEWAY, url)
        else:
            url = NUCLIO_GATEWAY

        with make_requests_session() as session:
            reply = session.request(
                method, url, headers=extra_headers, timeout=NUCLIO_TIMEOUT, json=data
            )
            reply.raise_for_status()
            response = reply.json()

        return response

    def list(self):
        data = self._http(url=self.NUCLIO_ROOT_URL)
        for item in data.values():
            try:
                yield LambdaFunction(self, item)
            except InvalidFunctionMetadataError:
                slogger.glob.error("Failed to parse lambda function metadata", exc_info=True)

    def get(self, func_id):
        data = self._http(url=self.NUCLIO_ROOT_URL + "/" + func_id)
        response = LambdaFunction(self, data)
        return response

    def invoke(self, func, payload):
        invoke_method = {
            "dashboard": self._invoke_via_dashboard,
            "direct": self._invoke_directly,
        }

        return invoke_method[settings.NUCLIO["INVOKE_METHOD"]](func, payload)

    def _invoke_via_dashboard(self, func, payload):
        return self._http(
            method="post",
            url="/api/function_invocations",
            data=payload,
            headers={"x-nuclio-function-name": func.id, "x-nuclio-path": "/"},
        )

    def _invoke_directly(self, func, payload):
        # host.docker.internal for Linux will work only with Docker 20.10+
        NUCLIO_TIMEOUT = settings.NUCLIO["DEFAULT_TIMEOUT"]
        if os.path.exists("/.dockerenv"):  # inside a docker container
            url = f"http://host.docker.internal:{func.port}"
        else:
            url = f"http://localhost:{func.port}"

        with make_requests_session() as session:
            reply = session.post(url, timeout=NUCLIO_TIMEOUT, json=payload)
            reply.raise_for_status()
            response = reply.json()

        return response


class InvalidFunctionMetadataError(Exception):
    pass


class LambdaFunction:
    FRAME_PARAMETERS = (
        ("frame", "frame"),
        ("frame0", "start frame"),
        ("frame1", "end frame"),
    )

    TRACKER_STATE_MAX_AGE = timedelta(hours=8)

    def __init__(self, gateway, data):
        # ID of the function (e.g. omz.public.yolo-v3)
        self.id = data["metadata"]["name"]
        # type of the function (e.g. detector, interactor)
        meta_anno = data["metadata"]["annotations"]
        kind = meta_anno.get("type")
        try:
            self.kind = FunctionKind(kind)
        except ValueError as e:
            raise InvalidFunctionMetadataError(
                f"{self.id} lambda function has unknown type: {kind!r}"
            ) from e
        # dictionary of labels for the function (e.g. car, person)
        spec = json.loads(meta_anno.get("spec") or "[]")

        def parse_labels(spec):
            def parse_attributes(attrs_spec):
                parsed_attributes = [
                    {
                        "name": attr["name"],
                        "input_type": attr["input_type"],
                        "values": attr["values"],
                    }
                    for attr in attrs_spec
                ]

                if len(parsed_attributes) != len({attr["name"] for attr in attrs_spec}):
                    raise InvalidFunctionMetadataError(
                        f"{self.id} lambda function has non-unique attributes"
                    )

                return parsed_attributes

            parsed_labels = []
            for label in spec:
                parsed_label = {
                    "name": label["name"],
                    "type": label.get("type", "any"),
                    "attributes": parse_attributes(label.get("attributes", [])),
                }
                if parsed_label["type"] == "skeleton":
                    parsed_label.update(
                        {"sublabels": parse_labels(label["sublabels"]), "svg": label["svg"]}
                    )
                parsed_labels.append(parsed_label)

            if len(parsed_labels) != len({label["name"] for label in spec}):
                raise InvalidFunctionMetadataError(
                    f"{self.id} lambda function has non-unique labels"
                )

            return parsed_labels

        self.labels = parse_labels(spec)
        # mapping of labels and corresponding supported attributes
        self.func_attributes = {item["name"]: item.get("attributes", []) for item in spec}
        for label, attributes in self.func_attributes.items():
            if len([attr["name"] for attr in attributes]) != len(
                set([attr["name"] for attr in attributes])
            ):
                raise InvalidFunctionMetadataError(
                    "`{}` lambda function has non-unique attributes for label {}".format(
                        self.id, label
                    )
                )
        # description of the function
        self.description = data["spec"]["description"]
        # http port to access the serverless function
        self.port = data["status"].get("httpPort")
        # display name for the function
        self.name = meta_anno.get("name", self.id)
        self.min_pos_points = int(meta_anno.get("min_pos_points", 1))
        self.min_neg_points = int(meta_anno.get("min_neg_points", -1))
        self.startswith_box = bool(meta_anno.get("startswith_box", False))
        self.startswith_box_optional = bool(meta_anno.get("startswith_box_optional", False))
        self.animated_gif = meta_anno.get("animated_gif", "")
        self.version = int(meta_anno.get("version", "1"))
        self.help_message = meta_anno.get("help_message", "")
        self.gateway = gateway

    def to_dict(self):
        response = {
            "id": self.id,
            "kind": str(self.kind),
            "labels_v2": self.labels,
            "description": self.description,
            "name": self.name,
            "version": self.version,
        }

        if self.kind is FunctionKind.INTERACTOR:
            response.update(
                {
                    "min_pos_points": self.min_pos_points,
                    "min_neg_points": self.min_neg_points,
                    "startswith_box": self.startswith_box,
                    "startswith_box_optional": self.startswith_box_optional,
                    "help_message": self.help_message,
                    "animated_gif": self.animated_gif,
                }
            )

        return response

    def invoke(
        self,
        db_task: Task,
        data: dict[str, Any],
        *,
        db_job: Optional[Job] = None,
        is_interactive: Optional[bool] = False,
        request: Optional[ExtendedRequest] = None,
        converter: Optional[DetectionResultConverter] = None,
    ):
        if db_job is not None and db_job.get_task_id() != db_task.id:
            raise ValidationError(
                "Job task id does not match task id", code=status.HTTP_400_BAD_REQUEST
            )

        payload = {}
        data = {k: v for k, v in data.items() if v is not None}

        def mandatory_arg(name: str) -> Any:
            try:
                return data[name]
            except KeyError:
                raise ValidationError(
                    "`{}` lambda function was called without mandatory argument: {}".format(
                        self.id, name
                    ),
                    code=status.HTTP_400_BAD_REQUEST,
                )

        threshold = data.get("threshold")
        if threshold:
            payload.update({"threshold": threshold})
        mapping = data.get("mapping", {})

        model_labels = self.labels
        task_labels = db_task.get_labels(prefetch=True)

        def labels_compatible(model_label: dict, task_label: Label) -> bool:
            model_type = model_label["type"]
            db_type = task_label.type
            compatible_types = [[ShapeType.MASK, ShapeType.POLYGON]]
            return (
                model_type == db_type
                or (db_type == "any" and model_type != "skeleton")
                or (model_type == "any" and db_type != "skeleton")
                or any(
                    [
                        model_type in compatible and db_type in compatible
                        for compatible in compatible_types
                    ]
                )
            )

        def make_default_mapping(model_labels, task_labels):
            mapping_by_default = {}
            for model_label in model_labels:
                for task_label in task_labels:
                    if task_label.name == model_label["name"] and labels_compatible(
                        model_label, task_label
                    ):
                        attributes_default_mapping = {}
                        for model_attr in model_label.get("attributes", {}):
                            for db_attr in task_label.attributespec_set.all():
                                if db_attr.name == model_attr["name"]:
                                    attributes_default_mapping[model_attr["name"]] = db_attr.name

                        mapping_by_default[model_label["name"]] = {
                            "name": task_label.name,
                            "attributes": attributes_default_mapping,
                        }

                        if model_label["type"] == "skeleton" and task_label.type == "skeleton":
                            mapping_by_default[model_label["name"]]["sublabels"] = (
                                make_default_mapping(
                                    model_label["sublabels"],
                                    task_label.sublabels.all(),
                                )
                            )

            return mapping_by_default

        def update_mapping(_mapping, _model_labels, _db_labels):
            copy = deepcopy(_mapping)
            for model_label_name, mapping_item in copy.items():
                md_label = next(filter(lambda x: x["name"] == model_label_name, _model_labels))
                db_label = next(filter(lambda x: x.name == mapping_item["name"], _db_labels))
                mapping_item.setdefault("attributes", {})
                mapping_item["md_label"] = md_label
                mapping_item["db_label"] = db_label
                if md_label["type"] == "skeleton" and db_label.type == "skeleton":
                    mapping_item["sublabels"] = update_mapping(
                        mapping_item["sublabels"], md_label["sublabels"], db_label.sublabels.all()
                    )
            return copy

        def validate_labels_mapping(_mapping, _model_labels, _db_labels):
            def validate_attributes_mapping(attributes_mapping, model_attributes, db_attributes):
                db_attr_names = [attr.name for attr in db_attributes]
                model_attr_names = [attr["name"] for attr in model_attributes]
                for model_attr in attributes_mapping:
                    task_attr = attributes_mapping[model_attr]
                    if model_attr not in model_attr_names:
                        raise ValidationError(
                            f'Invalid mapping. Unknown model attribute "{model_attr}"'
                        )
                    if task_attr not in db_attr_names:
                        raise ValidationError(
                            f'Invalid mapping. Unknown db attribute "{task_attr}"'
                        )

            for model_label_name, mapping_item in _mapping.items():
                db_label_name = mapping_item["name"]

                md_label = None
                db_label = None
                try:
                    md_label = next(x for x in _model_labels if x["name"] == model_label_name)
                except StopIteration:
                    raise ValidationError(
                        f'Invalid mapping. Unknown model label "{model_label_name}"'
                    )

                try:
                    db_label = next(x for x in _db_labels if x.name == db_label_name)
                except StopIteration:
                    raise ValidationError(f'Invalid mapping. Unknown db label "{db_label_name}"')

                if not labels_compatible(md_label, db_label):
                    raise ValidationError(
                        f'Invalid mapping. Model label "{model_label_name}" and'
                        + f' database label "{db_label_name}" are not compatible'
                    )

                validate_attributes_mapping(
                    mapping_item.get("attributes", {}),
                    md_label["attributes"],
                    db_label.attributespec_set.all(),
                )

                if md_label["type"] == "skeleton" and db_label.type == "skeleton":
                    if "sublabels" not in mapping_item:
                        raise ValidationError(
                            f'Mapping for elements was not specified in skeleton "{model_label_name}" '
                        )

                    validate_labels_mapping(
                        mapping_item["sublabels"], md_label["sublabels"], db_label.sublabels.all()
                    )

        if not mapping:
            mapping = make_default_mapping(model_labels, task_labels)
        else:
            validate_labels_mapping(mapping, self.labels, task_labels)

        mapping = update_mapping(mapping, self.labels, task_labels)

        # Check job frame boundaries
        if db_job:
            task_data = db_task.data
            data_start_frame = task_data.start_frame
            step = task_data.get_frame_step()

            for key, desc in self.FRAME_PARAMETERS:
                if key not in data:
                    continue

                abs_frame_id = data_start_frame + data[key] * step
                if not db_job.segment.contains_frame(abs_frame_id):
                    raise ValidationError(
                        f"The {desc} is outside the job range", code=status.HTTP_400_BAD_REQUEST
                    )

        if self.kind == FunctionKind.DETECTOR:
            payload.update({"image": self._get_image(db_task, mandatory_arg("frame"))})
        elif self.kind == FunctionKind.INTERACTOR:
            payload.update(
                {
                    "image": self._get_image(db_task, mandatory_arg("frame")),
                    "pos_points": mandatory_arg("pos_points"),
                    "neg_points": mandatory_arg("neg_points"),
                    "obj_bbox": data.get("obj_bbox", None),
                }
            )
        elif self.kind == FunctionKind.REID:
            payload.update(
                {
                    "image0": self._get_image(db_task, mandatory_arg("frame0")),
                    "image1": self._get_image(db_task, mandatory_arg("frame1")),
                    "boxes0": mandatory_arg("boxes0"),
                    "boxes1": mandatory_arg("boxes1"),
                }
            )
            max_distance = data.get("max_distance")
            if max_distance:
                payload.update({"max_distance": max_distance})
        elif self.kind == FunctionKind.TRACKER:
            signer = TimestampSigner(salt=f"cvat-tracker-state:{self.id}")

            try:
                payload.update(
                    {
                        "image": self._get_image(db_task, mandatory_arg("frame")),
                        "shapes": data.get("shapes", []),
                        "states": [
                            (
                                None
                                if state is None
                                else json.loads(
                                    signer.unsign(state, max_age=self.TRACKER_STATE_MAX_AGE)
                                )
                            )
                            for state in data.get("states", [])
                        ],
                    }
                )
            except BadSignature as ex:
                raise ValidationError("Invalid or expired tracker state") from ex
        else:
            raise ValidationError(
                "`{}` lambda function has incorrect type: {}".format(self.id, self.kind),
                code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            )

        if is_interactive and request:
            interactive_function_call_signal.send(sender=self, request=request)

        response = self.gateway.invoke(self, payload)

        def check_attr_value(value, db_attr):
            if db_attr is None:
                return False

            db_attr_type = db_attr["input_type"]
            if db_attr_type == "number":
                min_value, max_value, step = map(int, db_attr["values"].split("\n"))

                try:
                    value_num = int(value)
                except ValueError:
                    return False

                return min_value <= value_num <= max_value and (value_num - min_value) % step == 0
            elif db_attr_type == "checkbox":
                return value in ["true", "false"]
            elif db_attr_type == "text":
                return True
            elif db_attr_type in ["select", "radio"]:
                return value in db_attr["values"]
            else:
                return False

        def transform_attributes(input_attributes, attr_mapping, db_attributes):
            attributes = []
            for attr in input_attributes:
                if attr["name"] not in attr_mapping:
                    continue
                db_attr_name = attr_mapping[attr["name"]]
                db_attr = next(filter(lambda x: x["name"] == db_attr_name, db_attributes), None)
                if db_attr is not None and check_attr_value(attr["value"], db_attr):
                    attributes.append({"name": db_attr["name"], "value": attr["value"]})
            return attributes

        if self.kind == FunctionKind.DETECTOR:
            response_filtered = []

            for item in response:
                item_label = item["label"]
                if item_label not in mapping:
                    continue
                db_label = mapping[item_label]["db_label"]
                item["label"] = db_label.name
                item["attributes"] = transform_attributes(
                    item.get("attributes", {}),
                    mapping[item_label]["attributes"],
                    db_label.attributespec_set.values(),
                )

                if "elements" in item:
                    sublabels = mapping[item_label]["sublabels"]
                    item["elements"] = [x for x in item["elements"] if x["label"] in sublabels]
                    for element in item["elements"]:
                        element_label = element["label"]
                        db_label = sublabels[element_label]["db_label"]
                        element["label"] = db_label.name
                        element["attributes"] = transform_attributes(
                            element.get("attributes", {}),
                            sublabels[element_label]["attributes"],
                            db_label.attributespec_set.values(),
                        )
                response_filtered.append(item)

            response = converter.convert(
                conv_mask_to_poly=data.get("conv_mask_to_poly", False),
                frame=mandatory_arg("frame"),
                annotations=response_filtered,
            )
        elif self.kind == FunctionKind.TRACKER:
            response["states"] = [
                # We could've used .sign_object, but that unconditionally applies
                # an extra layer of Base64 encoding, bloating each state by 33%.
                # So we just encode the state manually instead.
                signer.sign(json.dumps(state, separators=(",", ":")))
                for state in response["states"]
            ]

        return response

    def _get_image(self, db_task, frame):
        frame_provider = TaskFrameProvider(db_task)
        image = frame_provider.get_frame(frame)

        return base64.b64encode(image.data.getvalue()).decode("utf-8")


class LambdaQueue:
    RESULT_TTL = timedelta(minutes=30)
    FAILED_TTL = timedelta(hours=3)

    def _get_queue(self):
        return django_rq.get_queue(settings.CVAT_QUEUES.AUTO_ANNOTATION.value)

    def get_jobs(self):
        queue = self._get_queue()
        # Only failed jobs are not included in the list below.
        job_ids = set(
            queue.get_job_ids()
            + queue.started_job_registry.get_job_ids()
            + queue.finished_job_registry.get_job_ids()
            + queue.scheduled_job_registry.get_job_ids()
            + queue.deferred_job_registry.get_job_ids()
        )
        jobs = queue.job_class.fetch_many(job_ids, queue.connection)

        return [LambdaJob(job) for job in jobs if job and LambdaRQMeta.for_job(job).lambda_]

    def enqueue(
        self,
        lambda_func,
        threshold,
        task,
        mapping,
        cleanup,
        conv_mask_to_poly,
        max_distance,
        request,
        *,
        job: Optional[int] = None,
    ) -> LambdaJob:
        queue = self._get_queue()
        rq_id = RQId(RequestAction.AUTOANNOTATE, RequestTarget.TASK, task).render()

        # Ensure that there is no race condition when processing parallel requests.
        # Enqueuing an RQ job with (queue, user) lock  but without (queue, rq_id) lock
        # may lead to queue jamming for a user due to self-dependencies.
        with get_rq_lock_for_job(queue, rq_id):
            if rq_job := queue.fetch_job(rq_id):
                if rq_job.get_status(refresh=False) not in {
                    rq.job.JobStatus.FAILED,
                    rq.job.JobStatus.FINISHED,
                }:
                    raise ValidationError(
                        "Only one running request is allowed for the same task #{}".format(task),
                        code=status.HTTP_409_CONFLICT,
                    )
                rq_job.delete()

            # LambdaJob(None) is a workaround for python-rq. It has multiple issues
            # with invocation of non-trivial functions. For example, it cannot run
            # staticmethod, it cannot run a callable class. Thus I provide an object
            # which has __call__ function.
            user_id = request.user.id

            with get_rq_lock_by_user(queue, user_id):
                meta = LambdaRQMeta.build_for(
                    request=request,
                    db_obj=Job.objects.get(pk=job) if job else Task.objects.get(pk=task),
                    function_id=lambda_func.id,
                )
                rq_job = queue.create_job(
                    LambdaJob(None),
                    job_id=rq_id,
                    meta=meta,
                    kwargs={
                        "function": lambda_func,
                        "threshold": threshold,
                        "task": task,
                        "job": job,
                        "cleanup": cleanup,
                        "conv_mask_to_poly": conv_mask_to_poly,
                        "mapping": mapping,
                        "max_distance": max_distance,
                    },
                    depends_on=define_dependent_job(queue, user_id),
                    result_ttl=self.RESULT_TTL.total_seconds(),
                    failure_ttl=self.FAILED_TTL.total_seconds(),
                )

                queue.enqueue_job(rq_job)

        return LambdaJob(rq_job)

    def fetch_job(self, pk):
        queue = self._get_queue()
        rq_job = queue.fetch_job(pk)
        if rq_job is None or not LambdaRQMeta.for_job(rq_job).lambda_:
            raise ValidationError(
                "{} lambda job is not found".format(pk), code=status.HTTP_404_NOT_FOUND
            )

        return LambdaJob(rq_job)


class DetectionResultConverter:
    def __init__(self, db_task: Task) -> None:
        self._labels = self._convert_labels(db_task.get_labels(prefetch=True))

    @classmethod
    def _convert_labels(cls, db_labels) -> dict:
        labels = {}
        for label in db_labels:
            labels[label.name] = {"id": label.id, "attributes": {}, "type": label.type}
            if label.type == "skeleton":
                labels[label.name]["sublabels"] = cls._convert_labels(label.sublabels.all())
            for attr in label.attributespec_set.values():
                labels[label.name]["attributes"][attr["name"]] = attr["id"]
        return labels

    def convert(self, *, conv_mask_to_poly: bool, frame: int, annotations: list) -> dict:
        data = {"tags": [], "shapes": []}

        for anno in annotations:
            if parsed := self._parse_anno(
                labels=self._labels, conv_mask_to_poly=conv_mask_to_poly, frame=frame, anno=anno
            ):
                if anno["type"].lower() == "tag":
                    data["tags"].append(parsed)
                else:
                    data["shapes"].append(parsed)

        serializer = LabeledDataSerializer(data=data)
        serializer.is_valid(raise_exception=True)
        return serializer.validated_data

    def _parse_anno(
        self, *, labels: dict, conv_mask_to_poly: bool, frame: int, anno: dict
    ) -> Optional[dict]:
        label = labels.get(anno["label"])
        if label is None:
            # Invalid label provided
            return None

        attrs = [
            {"spec_id": label["attributes"][attr["name"]], "value": attr["value"]}
            for attr in anno.get("attributes", [])
            if attr["name"] in label["attributes"]
        ]

        if anno["type"].lower() == "tag":
            return {
                "frame": frame,
                "label_id": label["id"],
                "source": "auto",
                "attributes": attrs,
                "group": None,
            }
        else:
            shape = {
                "frame": frame,
                "label_id": label["id"],
                "source": "auto",
                "attributes": attrs,
                "group": anno["group_id"] if "group_id" in anno else None,
                "type": anno["type"],
                "occluded": False,
                "outside": anno.get("outside", False),
                "points": (
                    anno.get("mask", []) if anno["type"] == "mask" else anno.get("points", [])
                ),
                "z_order": 0,
            }

            if shape["type"] in ("rectangle", "ellipse"):
                shape["rotation"] = anno.get("rotation", 0)

            if anno["type"] == "mask" and "points" in anno and conv_mask_to_poly:
                shape["type"] = "polygon"
                shape["points"] = anno["points"]
            elif anno["type"] == "mask":
                [xtl, ytl, xbr, ybr] = shape["points"][-4:]
                cut_points = shape["points"][:-4]
                rle = mask_tools.mask_to_rle(np.array(cut_points)[:, np.newaxis])["counts"].tolist()
                rle.extend([xtl, ytl, xbr, ybr])
                shape["points"] = rle

            if shape["type"] == "skeleton":
                parsed_elements = [
                    self._parse_anno(
                        labels=label["sublabels"],
                        conv_mask_to_poly=conv_mask_to_poly,
                        frame=frame,
                        anno=x,
                    )
                    for x in anno["elements"]
                ]

                # find a center to set position of missing points
                center = [0, 0]
                for element in parsed_elements:
                    center[0] += element["points"][0]
                    center[1] += element["points"][1]
                center[0] /= len(parsed_elements) or 1
                center[1] /= len(parsed_elements) or 1

                def _map(sublabel_body):
                    try:
                        return next(
                            filter(lambda x: x["label_id"] == sublabel_body["id"], parsed_elements)
                        )
                    except StopIteration:
                        return {
                            "frame": frame,
                            "label_id": sublabel_body["id"],
                            "source": "auto",
                            "attributes": [],
                            "group": None,
                            "type": sublabel_body["type"],
                            "occluded": False,
                            "points": center,
                            "outside": True,
                            "z_order": 0,
                        }

                shape["elements"] = list(map(_map, label["sublabels"].values()))
                if all(element["outside"] for element in shape["elements"]):
                    return None

            return shape


class DetectionResultCollector:
    def __init__(self, task: Task, job: Optional[Job]) -> None:
        self._task = task
        self._job = job

        self._reset()

    def add(self, data: dict) -> None:
        self._data["tags"] += data["tags"]
        self._data["shapes"] += data["shapes"]

        assert not data["tracks"]

    def submit(self):
        if self._is_empty():
            return

        if self._job:
            dm.task.patch_job_data(self._job.id, self._data, PatchAction.CREATE)
        else:
            dm.task.patch_task_data(self._task.id, self._data, PatchAction.CREATE)

        self._reset()

    def _is_empty(self) -> bool:
        return not (self._data["tags"] or self._data["shapes"])

    def _reset(self) -> None:
        s = LabeledDataSerializer(data={})
        s.is_valid(raise_exception=True)

        self._data = s.validated_data


class LambdaJob:
    def __init__(self, job):
        self.job = job

    def to_dict(self):
        lambda_func = self.job.kwargs.get("function")
        dict_ = {
            "id": self.job.id,
            "function": {
                "id": lambda_func.id if lambda_func else None,
                "threshold": self.job.kwargs.get("threshold"),
                "task": self.job.kwargs.get("task"),
                **(
                    {
                        "job": self.job.kwargs["job"],
                    }
                    if self.job.kwargs.get("job")
                    else {}
                ),
            },
            "status": self.job.get_status(),
            "progress": LambdaRQMeta.for_job(self.job).progress,
            "enqueued": self.job.enqueued_at,
            "started": self.job.started_at,
            "ended": self.job.ended_at,
            "exc_info": self.job.exc_info,
        }
        if dict_["status"] == rq.job.JobStatus.DEFERRED:
            dict_["status"] = rq.job.JobStatus.QUEUED.value

        return dict_

    def get_task(self):
        return self.job.kwargs.get("task")

    def get_status(self):
        return self.job.get_status()

    @property
    def is_finished(self):
        return self.get_status() == rq.job.JobStatus.FINISHED

    @property
    def is_queued(self):
        return self.get_status() == rq.job.JobStatus.QUEUED

    @property
    def is_failed(self):
        return self.get_status() == rq.job.JobStatus.FAILED

    @property
    def is_started(self):
        return self.get_status() == rq.job.JobStatus.STARTED

    @property
    def is_deferred(self):
        return self.get_status() == rq.job.JobStatus.DEFERRED

    @property
    def is_scheduled(self):
        return self.get_status() == rq.job.JobStatus.SCHEDULED

    def delete(self):
        self.job.delete()

    @classmethod
    def _call_detector(
        cls,
        function: LambdaFunction,
        db_task: Task,
        threshold: float,
        mapping: Optional[dict[str, str]],
        conv_mask_to_poly: bool,
        *,
        db_job: Optional[Job] = None,
    ):
        collector = DetectionResultCollector(db_task, db_job)

        converter = DetectionResultConverter(db_task)

        frame_set = cls._get_frame_set(db_task, db_job)

        for frame in frame_set:
            if frame in db_task.data.deleted_frames:
                continue

            annotations = function.invoke(
                db_task,
                db_job=db_job,
                data={
                    "frame": frame,
                    "mapping": mapping,
                    "threshold": threshold,
                    "conv_mask_to_poly": conv_mask_to_poly,
                },
                converter=converter,
            )

            progress = (frame + 1) / db_task.data.size
            if not cls._update_progress(progress):
                break

            collector.add(annotations)

            # Accumulate data during 100 frames before submitting results.
            # It is optimization to make fewer calls to our server. Also
            # it isn't possible to keep all results in memory.
            if frame and frame % 100 == 0:
                collector.submit()

        collector.submit()

    @staticmethod
    # progress is in [0, 1] range
    def _update_progress(progress):
        job = rq.get_current_job()
        rq_job_meta = LambdaRQMeta.for_job(job)
        # If the job has been deleted, get_status will return None. Thus it will
        # exist the loop.
        rq_job_meta.progress = int(progress * 100)
        rq_job_meta.save()

        return job.get_status()

    @classmethod
    def _get_frame_set(cls, db_task: Task, db_job: Optional[Job]):
        if db_job:
            task_data = db_task.data
            data_start_frame = task_data.start_frame
            step = task_data.get_frame_step()
            frame_set = sorted(
                (abs_id - data_start_frame) // step for abs_id in db_job.segment.frame_set
            )
        else:
            frame_set = range(db_task.data.size)

        return frame_set

    @classmethod
    def _call_reid(
        cls,
        function: LambdaFunction,
        db_task: Task,
        threshold: float,
        max_distance: int,
        *,
        db_job: Optional[Job] = None,
    ):
        if db_job:
            data = dm.task.get_job_data(db_job.id)
        else:
            data = dm.task.get_task_data(db_task.id)

        frame_set = cls._get_frame_set(db_task, db_job)

        boxes_by_frame = {frame: [] for frame in frame_set}
        shapes_without_boxes = []
        for shape in data["shapes"]:
            if shape["type"] == str(ShapeType.RECTANGLE):
                boxes_by_frame[shape["frame"]].append(shape)
            else:
                shapes_without_boxes.append(shape)

        paths = {}
        for i, (frame0, frame1) in enumerate(zip(frame_set[:-1], frame_set[1:])):
            boxes0 = boxes_by_frame[frame0]
            for box in boxes0:
                if "path_id" not in box:
                    path_id = len(paths)
                    paths[path_id] = [box]
                    box["path_id"] = path_id

            boxes1 = boxes_by_frame[frame1]
            if boxes0 and boxes1:
                matching = function.invoke(
                    db_task,
                    db_job=db_job,
                    data={
                        "frame0": frame0,
                        "frame1": frame1,
                        "boxes0": boxes0,
                        "boxes1": boxes1,
                        "threshold": threshold,
                        "max_distance": max_distance,
                    },
                )

                for idx0, idx1 in enumerate(matching):
                    if idx1 >= 0:
                        path_id = boxes0[idx0]["path_id"]
                        boxes1[idx1]["path_id"] = path_id
                        paths[path_id].append(boxes1[idx1])

            if not LambdaJob._update_progress((i + 1) / len(frame_set)):
                break

        for box in boxes_by_frame[frame_set[-1]]:
            if "path_id" not in box:
                path_id = len(paths)
                paths[path_id] = [box]
                box["path_id"] = path_id

        tracks = []
        for path_id in paths:
            box0 = paths[path_id][0]
            tracks.append(
                {
                    "label_id": box0["label_id"],
                    "group": None,
                    "attributes": [],
                    "frame": box0["frame"],
                    "shapes": paths[path_id],
                    "source": str(SourceType.AUTO),
                }
            )

            for box in tracks[-1]["shapes"]:
                box.pop("id", None)
                box.pop("path_id")
                box.pop("group")
                box.pop("label_id")
                box.pop("source")
                box["outside"] = False
                box["attributes"] = []

        for track in tracks:
            if track["shapes"][-1]["frame"] != frame_set[-1]:
                box = track["shapes"][-1].copy()
                box["outside"] = True
                box["frame"] += 1
                track["shapes"].append(box)

        if tracks:
            data["shapes"] = shapes_without_boxes
            data["tracks"].extend(tracks)

            serializer = LabeledDataSerializer(data=data)
            if serializer.is_valid(raise_exception=True):
                if db_job:
                    dm.task.put_job_data(db_job.id, serializer.data)
                else:
                    dm.task.put_task_data(db_task.id, serializer.data)

    @classmethod
    def __call__(cls, function, task: int, cleanup: bool, **kwargs):
        # TODO: need logging
        db_job = None
        if job := kwargs.get("job"):
            db_job = Job.objects.select_related("segment", "segment__task").get(pk=job)
            db_task = db_job.segment.task
        else:
            db_task = Task.objects.get(pk=task)

        if cleanup:
            if db_job:
                dm.task.delete_job_data(db_job.id)
            elif db_task:
                dm.task.delete_task_data(db_task.id)
            else:
                assert False

        if function.kind == FunctionKind.DETECTOR:
            cls._call_detector(
                function,
                db_task,
                kwargs.get("threshold"),
                kwargs.get("mapping"),
                kwargs.get("conv_mask_to_poly"),
                db_job=db_job,
            )
        elif function.kind == FunctionKind.REID:
            cls._call_reid(
                function,
                db_task,
                kwargs.get("threshold"),
                kwargs.get("max_distance"),
                db_job=db_job,
            )


def return_response(success_code=status.HTTP_200_OK):
    def wrap_response(func):
        @wraps(func)
        def func_wrapper(*args, **kwargs):
            data = None
            status_code = success_code
            try:
                data = func(*args, **kwargs)
            except requests.ConnectionError as err:
                status_code = status.HTTP_503_SERVICE_UNAVAILABLE
                data = str(err)
            except requests.HTTPError as err:
                status_code = err.response.status_code
                data = str(err)
            except requests.Timeout as err:
                status_code = status.HTTP_504_GATEWAY_TIMEOUT
                data = str(err)
            except requests.RequestException as err:
                status_code = status.HTTP_500_INTERNAL_SERVER_ERROR
                data = str(err)
            except ValidationError as err:
                status_code = err.code or status.HTTP_400_BAD_REQUEST
                data = err.message
            except ObjectDoesNotExist as err:
                status_code = status.HTTP_400_BAD_REQUEST
                data = str(err)

            return Response(data=data, status=status_code)

        return func_wrapper

    return wrap_response


@extend_schema(tags=["lambda"])
@extend_schema_view(
    retrieve=extend_schema(
        operation_id="lambda_retrieve_functions",
        summary="Method returns the information about the function",
        responses={
            "200": OpenApiResponse(
                response=OpenApiTypes.OBJECT, description="Information about the function"
            ),
        },
    ),
    list=extend_schema(
        operation_id="lambda_list_functions", summary="Method returns a list of functions"
    ),
)
class FunctionViewSet(viewsets.ViewSet):
    lookup_value_regex = "[a-zA-Z0-9_.-]+"
    lookup_field = "func_id"
    iam_organization_field = None
    serializer_class = None

    @return_response()
    def list(self, request):
        gateway = LambdaGateway()
        return [f.to_dict() for f in gateway.list()]

    @return_response()
    def retrieve(self, request, func_id):
        self.check_object_permissions(request, func_id)
        gateway = LambdaGateway()
        return gateway.get(func_id).to_dict()

    @extend_schema(
        description=textwrap.dedent(
            """\
        Allows to execute a function for immediate computation.

        Intended for short-lived executions, useful for interactive calls.

        When executed for interactive annotation, the job id must be specified
        in the 'job' input field. The task id is not required in this case,
        but if it is specified, it must match the job task id.
        """
        ),
        request=inline_serializer(
            "OnlineFunctionCall",
            fields={
                "job": serializers.IntegerField(required=False),
                "task": serializers.IntegerField(required=False),
            },
        ),
        responses=OpenApiResponse(description="Returns function invocation results"),
    )
    @return_response()
    def call(self, request, func_id):
        self.check_object_permissions(request, func_id)
        try:
            job_id = request.data.get("job")
            job = None
            if job_id is not None:
                job = Job.objects.get(id=job_id)
                task_id = job.get_task_id()
            else:
                task_id = request.data["task"]

            db_task = Task.objects.get(pk=task_id)
        except (KeyError, ObjectDoesNotExist) as err:
            raise ValidationError(
                "`{}` lambda function was run ".format(func_id)
                + "with wrong arguments ({})".format(str(err)),
                code=status.HTTP_400_BAD_REQUEST,
            )

        gateway = LambdaGateway()
        lambda_func = gateway.get(func_id)

        converter = None

        if lambda_func.kind == FunctionKind.DETECTOR:
            converter = DetectionResultConverter(db_task)

        response = lambda_func.invoke(
            db_task,
            request.data,  # TODO: better to add validation via serializer for these data
            db_job=job,
            converter=converter,
            is_interactive=True,
            request=request,
        )

        handle_function_call(
            func_id,
            db_task,
            category="interactive",
            parameters={
                param_name: param_value
                for param_name, _ in LambdaFunction.FRAME_PARAMETERS
                for param_value in [request.data.get(param_name)]
                if param_value is not None
            },
        )

        return response


@extend_schema(tags=["lambda"])
@extend_schema_view(
    retrieve=extend_schema(
        operation_id="lambda_retrieve_requests",
        summary="Method returns the status of the request",
        parameters=[
            OpenApiParameter(
                "id",
                location=OpenApiParameter.PATH,
                type=OpenApiTypes.STR,
                description="Request id",
            ),
        ],
        responses={"200": FunctionCallSerializer},
    ),
    list=extend_schema(
        operation_id="lambda_list_requests",
        summary="Method returns a list of requests",
        responses={"200": FunctionCallSerializer(many=True)},
    ),
    create=extend_schema(
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        summary="Method calls the function",
        request=FunctionCallRequestSerializer,
        responses={"200": FunctionCallSerializer},
    ),
    destroy=extend_schema(
        operation_id="lambda_delete_requests",
        summary="Method cancels the request",
        parameters=[
            OpenApiParameter(
                "id",
                location=OpenApiParameter.PATH,
                type=OpenApiTypes.STR,
                description="Request id",
            ),
        ],
    ),
)
class RequestViewSet(viewsets.ViewSet):
    iam_organization_field = None
    serializer_class = None

    @return_response()
    def list(self, request):
        queryset = Task.objects.select_related(
            "assignee",
            "owner",
            "organization",
        ).prefetch_related(
            "project__owner",
            "project__assignee",
            "project__organization",
        )

        perm = LambdaPermission.create_scope_list(request)
        queryset = perm.filter(queryset)
        task_ids = set(queryset.values_list("id", flat=True))

        queue = LambdaQueue()
        rq_jobs = [job.to_dict() for job in queue.get_jobs() if job.get_task() in task_ids]

        response_serializer = FunctionCallSerializer(rq_jobs, many=True)
        return response_serializer.data

    @return_response()
    def create(self, request):
        request_serializer = FunctionCallRequestSerializer(data=request.data)
        request_serializer.is_valid(raise_exception=True)
        request_data = request_serializer.validated_data

        try:
            function = request_data["function"]
            threshold = request_data.get("threshold")
            task = request_data["task"]
            job = request_data.get("job", None)
            cleanup = request_data.get("cleanup", False)
            conv_mask_to_poly = request_data.get("conv_mask_to_poly", False)
            mapping = request_data.get("mapping")
            max_distance = request_data.get("max_distance")
        except KeyError as err:
            raise ValidationError(
                "`{}` lambda function was run ".format(request_data.get("function", "undefined"))
                + "with wrong arguments ({})".format(str(err)),
                code=status.HTTP_400_BAD_REQUEST,
            )

        gateway = LambdaGateway()
        queue = LambdaQueue()
        lambda_func = gateway.get(function)
        rq_job = queue.enqueue(
            lambda_func,
            threshold,
            task,
            mapping,
            cleanup,
            conv_mask_to_poly,
            max_distance,
            request,
            job=job,
        )

        handle_function_call(function, job or task, category="batch")

        response_serializer = FunctionCallSerializer(rq_job.to_dict())
        return response_serializer.data

    @return_response()
    def retrieve(self, request, pk):
        self.check_object_permissions(request, pk)
        queue = LambdaQueue()
        rq_job = queue.fetch_job(pk)

        response_serializer = FunctionCallSerializer(rq_job.to_dict())
        return response_serializer.data

    @return_response(status.HTTP_204_NO_CONTENT)
    def destroy(self, request, pk):
        self.check_object_permissions(request, pk)
        queue = LambdaQueue()
        rq_job = queue.fetch_job(pk)
        rq_job.delete()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\rules\tests\generators\lambda_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "lambda"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]


def RESOURCES(scope):
    return [None]


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES
        ):
            for resource in RESOURCES(scope):
                if not is_valid(scope, context, ownership, privilege, membership, resource):
                    continue

                data = get_data(scope, context, ownership, privilege, membership, resource)
                test_name = get_name(scope, context, ownership, privilege, membership, resource)
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\tests\test_lambda.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import os
from collections import Counter, OrderedDict
from itertools import groupby
from typing import Optional
from unittest import mock, skip

import requests
from django.contrib.auth.models import Group, User
from django.core.signing import TimestampSigner
from django.http import HttpResponseNotFound, HttpResponseServerError
from rest_framework import status

from cvat.apps.engine.tests.utils import (
    ApiTestBase,
    ForceLogin,
    filter_dict,
    generate_image_file,
    get_paginated_collection,
)

LAMBDA_ROOT_PATH = "/api/lambda"
LAMBDA_FUNCTIONS_PATH = f"{LAMBDA_ROOT_PATH}/functions"
LAMBDA_REQUESTS_PATH = f"{LAMBDA_ROOT_PATH}/requests"

id_function_detector = "test-openvino-omz-public-yolo-v3-tf"
id_function_reid_with_response_data = "test-openvino-omz-intel-person-reidentification-retail-0300"
id_function_reid_with_no_response_data = (
    "test-openvino-omz-intel-person-reidentification-retail-1234"
)
id_function_interactor = "test-openvino-dextr"
id_function_tracker = "test-pth-foolwood-siammask"
id_function_non_type = "test-model-has-non-type"
id_function_wrong_type = "test-model-has-wrong-type"
id_function_unknown_type = "test-model-has-unknown-type"
id_function_non_unique_labels = "test-model-has-non-unique-labels"
id_function_state_building = "test-model-has-state-building"
id_function_state_error = "test-model-has-state-error"

expected_keys_in_response_all_functions = ["id", "kind", "labels_v2", "description", "name"]
expected_keys_in_response_function_interactor = expected_keys_in_response_all_functions + [
    "min_pos_points",
    "startswith_box",
]
expected_keys_in_response_requests = [
    "id",
    "function",
    "status",
    "progress",
    "enqueued",
    "started",
    "ended",
    "exc_info",
]

path = os.path.join(os.path.dirname(__file__), "assets", "tasks.json")
with open(path) as f:
    tasks = json.load(f)

# removed unnecessary data
path = os.path.join(os.path.dirname(__file__), "assets", "functions.json")
with open(path) as f:
    functions = json.load(f)


class _LambdaTestCaseBase(ApiTestBase):
    def setUp(self):
        super().setUp()

        self.client = self.client_class(raise_request_exception=False)

        http_patcher = mock.patch(
            "cvat.apps.lambda_manager.views.LambdaGateway._http",
            side_effect=self._get_data_from_lambda_manager_http,
        )
        self.addCleanup(http_patcher.stop)
        http_patcher.start()

        invoke_patcher = mock.patch(
            "cvat.apps.lambda_manager.views.LambdaGateway.invoke", side_effect=self._invoke_function
        )
        self.addCleanup(invoke_patcher.stop)
        invoke_patcher.start()

    def _get_data_from_lambda_manager_http(self, **kwargs):
        url = kwargs["url"]
        if url == "/api/functions":
            return functions["positive"]
        else:
            func_id = url.split("/")[-1]
            if func_id in functions["positive"]:
                if func_id in [id_function_state_building, id_function_state_error]:
                    r = requests.RequestException()
                    r.response = HttpResponseServerError()
                    raise r  # raise 500 Internal_Server error

                return functions["positive"][func_id]
            else:
                r = requests.HTTPError()
                r.response = HttpResponseNotFound()
                raise r  # raise 404 Not Found error

    def _invoke_function(self, func, payload):
        data = []
        func_id = func.id
        type_function = functions["positive"][func_id]["metadata"]["annotations"]["type"]
        if type_function == "reid":
            if func_id == id_function_reid_with_response_data:
                data = [0, 1]
            else:
                data = []
        elif type_function == "tracker":
            data = {
                "shapes": [[12.34, 34.0, 35.01, 41.99]],
                "states": [{"key": "value"}],
            }
        elif type_function == "interactor":
            data = [
                [8, 12],
                [34, 56],
                [77, 77],
            ]
        elif type_function == "detector":
            data = [
                {
                    "confidence": "0.9959098",
                    "label": "car",
                    "points": [3, 3, 15, 15],
                    "type": "rectangle",
                },
                {
                    "confidence": "0.89535173",
                    "label": "car",
                    "points": [20, 25, 30, 35],
                    "type": "rectangle",
                },
                {
                    "confidence": "0.59464583",
                    "label": "car",
                    "points": [12.17, 45.0, 69.80, 18.99],
                    "type": "polygon",
                },
                {
                    "confidence": "0.59464583",
                    "label": "car",
                    "mask": [255, 255, 0, 0, 255, 255, 0, 0, 255, 255, 0, 0, 0, 0, 2, 3],
                    "type": "mask",
                },
            ]

        return data

    @classmethod
    def _create_db_users(cls):
        (group_admin, _) = Group.objects.get_or_create(name="admin")
        (group_user, _) = Group.objects.get_or_create(name="user")

        user_admin = User.objects.create_superuser(username="admin", email="", password="admin")
        user_admin.groups.add(group_admin)
        user_dummy = User.objects.create_user(
            username="user", password="user", email="user@example.com"
        )
        user_dummy.groups.add(group_user)

        cls.admin = user_admin
        cls.user = user_dummy

    def _create_task(self, task_spec, data, *, owner=None, org_id=None):
        with ForceLogin(owner or self.admin, self.client):
            response = self.client.post(
                "/api/tasks",
                data=task_spec,
                format="json",
                QUERY_STRING=f"org_id={org_id}" if org_id is not None else None,
            )
            assert response.status_code == status.HTTP_201_CREATED, response.status_code
            tid = response.data["id"]

            response = self.client.post(
                "/api/tasks/%s/data" % tid,
                data=data,
                QUERY_STRING=f"org_id={org_id}" if org_id is not None else None,
            )
            assert response.status_code == status.HTTP_202_ACCEPTED, response.status_code
            rq_id = response.json()["rq_id"]

            response = self.client.get(f"/api/requests/{rq_id}")
            assert response.status_code == status.HTTP_200_OK, response.status_code
            assert response.json()["status"] == "finished", response.json().get("status")

            response = self.client.get(
                "/api/tasks/%s" % tid,
                QUERY_STRING=f"org_id={org_id}" if org_id is not None else None,
            )
            task = response.data

        return task

    def _generate_task_images(self, count):  # pylint: disable=no-self-use
        images = {
            "client_files[%d]" % i: generate_image_file("image_%d.jpg" % i) for i in range(count)
        }
        images["image_quality"] = 75
        return images

    @classmethod
    def setUpTestData(cls):
        cls._create_db_users()

    def _check_expected_keys_in_response_function(self, data):
        kind = data["kind"]
        if kind == "interactor":
            for key in expected_keys_in_response_function_interactor:
                self.assertIn(key, data)
        else:
            for key in expected_keys_in_response_all_functions:
                self.assertIn(key, data)

    def _delete_lambda_request(self, request_id: str, user: Optional[User] = None) -> None:
        response = self._delete_request(f"{LAMBDA_REQUESTS_PATH}/{request_id}", user or self.admin)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)


class LambdaTestCases(_LambdaTestCaseBase):
    def setUp(self):
        super().setUp()

        images_main_task = self._generate_task_images(3)
        images_assigneed_to_user_task = self._generate_task_images(3)
        self.main_task = self._create_task(tasks["main"], images_main_task)
        self.assigneed_to_user_task = self._create_task(
            tasks["assigneed_to_user"], images_assigneed_to_user_task
        )

    def test_api_v2_lambda_functions_list(self):
        response = self._get_request(LAMBDA_FUNCTIONS_PATH, self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for data in response.data:
            self._check_expected_keys_in_response_function(data)

        response = self._get_request(LAMBDA_FUNCTIONS_PATH, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for data in response.data:
            self._check_expected_keys_in_response_function(data)

        response = self._get_request(LAMBDA_FUNCTIONS_PATH, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    @mock.patch("cvat.apps.lambda_manager.views.LambdaGateway._http", return_value={})
    def test_api_v2_lambda_functions_list_empty(self, mock_http):
        response = self._get_request(LAMBDA_FUNCTIONS_PATH, self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(response.data), 0)

        response = self._get_request(LAMBDA_FUNCTIONS_PATH, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(response.data), 0)

        response = self._get_request(LAMBDA_FUNCTIONS_PATH, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    @mock.patch(
        "cvat.apps.lambda_manager.views.LambdaGateway._http",
        return_value={
            **functions["negative"],
            id_function_detector: functions["positive"][id_function_detector],
        },
    )
    def test_api_v2_lambda_functions_list_negative(self, mock_http):
        response = self._get_request(LAMBDA_FUNCTIONS_PATH, self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        # the positive function must remain visible
        visible_ids = {f["id"] for f in response.data}
        self.assertEqual(visible_ids, {id_function_detector})

    def test_api_v2_lambda_functions_read(self):
        ids_functions = [
            id_function_detector,
            id_function_interactor,
            id_function_tracker,
            id_function_reid_with_response_data,
        ]

        for id_func in ids_functions:
            path = f"{LAMBDA_FUNCTIONS_PATH}/{id_func}"

            response = self._get_request(path, self.admin)
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            self._check_expected_keys_in_response_function(response.data)

            response = self._get_request(path, self.user)
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            self._check_expected_keys_in_response_function(response.data)

            response = self._get_request(path, None)
            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_functions_read_wrong_id(self):
        id_wrong_function = "test-functions-wrong-id"
        response = self._get_request(f"{LAMBDA_FUNCTIONS_PATH}/{id_wrong_function}", self.admin)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

        response = self._get_request(f"{LAMBDA_FUNCTIONS_PATH}/{id_wrong_function}", self.user)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

        response = self._get_request(f"{LAMBDA_FUNCTIONS_PATH}/{id_wrong_function}", None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_functions_read_negative(self):
        for id_func in [
            id_function_non_type,
            id_function_wrong_type,
            id_function_unknown_type,
            id_function_non_unique_labels,
        ]:
            with mock.patch(
                "cvat.apps.lambda_manager.views.LambdaGateway._http",
                return_value=functions["negative"][id_func],
            ):
                response = self._get_request(f"{LAMBDA_FUNCTIONS_PATH}/{id_func}", self.admin)
                self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

    @skip("Fail: add mock")
    def test_api_v2_lambda_requests_list(self):
        response = self._get_request(LAMBDA_REQUESTS_PATH, self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data[0])

        response = self._get_request(LAMBDA_REQUESTS_PATH, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data[0])

        response = self._get_request(LAMBDA_REQUESTS_PATH, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_requests_list_empty(self):
        response = self._get_request(LAMBDA_REQUESTS_PATH, self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(response.data), 0)

        response = self._get_request(LAMBDA_REQUESTS_PATH, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        self.assertEqual(len(response.data), 0)

        response = self._get_request(LAMBDA_REQUESTS_PATH, None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_requests_read(self):
        # create request
        data_main_task = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
            "threshold": 55,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data_main_task)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        id_request = response.data["id"]

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data)

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data)

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_requests_read_wrong_id(self):
        id_request = "cf343b95-afeb-475e-ab53-8d7e64991d30-wrong-id"

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.admin)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.user)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_requests_delete_finished_request(self):
        data = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        id_request = response.data["id"]

        response = self._delete_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", None)
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

        response = self._delete_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.admin)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.admin)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        id_request = response.data["id"]
        response = self._delete_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.user)
        self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT)
        response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.user)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    @skip("Fail: add mock")
    def test_api_v2_lambda_requests_delete_not_finished_request(self):
        pass

    def test_api_v2_lambda_requests_create(self):
        ids_functions = [
            id_function_detector,
            id_function_interactor,
            id_function_tracker,
            id_function_reid_with_response_data,
            id_function_detector,
            id_function_reid_with_no_response_data,
        ]

        for id_func in ids_functions:
            data_main_task = {
                "function": id_func,
                "task": self.main_task["id"],
                "cleanup": True,
                "threshold": 55,
                "mapping": {
                    "car": {"name": "car"},
                },
            }
            data_assigneed_to_user_task = {
                "function": id_func,
                "task": self.assigneed_to_user_task["id"],
                "cleanup": False,
                "max_distance": 70,
                "mapping": {
                    "car": {"name": "car"},
                },
            }

            response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data_main_task)
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            for key in expected_keys_in_response_requests:
                self.assertIn(key, response.data)

            self._delete_lambda_request(response.data["id"])

            response = self._post_request(
                LAMBDA_REQUESTS_PATH, self.user, data=data_assigneed_to_user_task
            )
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            for key in expected_keys_in_response_requests:
                self.assertIn(key, response.data)

            self._delete_lambda_request(response.data["id"], self.user)

            response = self._post_request(LAMBDA_REQUESTS_PATH, self.user, data=data_main_task)
            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

            response = self._post_request(LAMBDA_REQUESTS_PATH, None, data=data_main_task)
            self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_requests_create_negative(self):
        for id_func in [
            id_function_non_type,
            id_function_wrong_type,
            id_function_unknown_type,
            id_function_non_unique_labels,
        ]:
            data = {
                "function": id_func,
                "task": self.main_task["id"],
                "cleanup": True,
                "mapping": {
                    "car": {"name": "car"},
                },
            }

            with mock.patch(
                "cvat.apps.lambda_manager.views.LambdaGateway._http",
                return_value=functions["negative"][id_func],
            ):
                response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
                self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

    def test_api_v2_lambda_requests_create_empty_data(self):
        data = {}
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_requests_create_without_function(self):
        data = {
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_requests_create_wrong_id_function(self):
        data = {
            "function": "test-requests-wrong-id",
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    @skip("Fail: add mock")
    def test_api_v2_lambda_requests_create_two_requests(self):
        data = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        request_id = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data).data["id"]
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_409_CONFLICT)

        self._delete_lambda_request(request_id)

    def test_api_v2_lambda_requests_create_empty_mapping(self):
        data = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {},
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data)

        self._delete_lambda_request(response.data["id"])

    def test_api_v2_lambda_requests_create_without_cleanup(self):
        data = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data)

        self._delete_lambda_request(response.data["id"])

    def test_api_v2_lambda_requests_create_without_mapping(self):
        data = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        for key in expected_keys_in_response_requests:
            self.assertIn(key, response.data)

        self._delete_lambda_request(response.data["id"])

    def test_api_v2_lambda_requests_create_without_task(self):
        data = {
            "function": id_function_detector,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_requests_create_wrong_id_task(self):
        data = {
            "function": id_function_detector,
            "task": 12345,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_requests_create_is_not_ready(self):
        ids_functions = [id_function_state_building, id_function_state_error]

        for id_func in ids_functions:
            data = {
                "function": id_func,
                "task": self.main_task["id"],
                "cleanup": True,
                "mapping": {
                    "car": {"name": "car"},
                },
            }

            response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data)
            self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

    def test_api_v2_lambda_functions_create_detector(self):
        data_main_task = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "threshold": 0.55,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        data_assigneed_to_user_task = {
            "task": self.assigneed_to_user_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}",
            self.user,
            data=data_assigneed_to_user_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", None, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    @skip(
        "Fail: expected result != actual result"
    )  # TODO move test to test_api_v2_lambda_functions_create
    def test_api_v2_lambda_functions_create_user_assigned_to_no_user(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.user, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_api_v2_lambda_functions_create_interactor(self):
        data_main_task = {
            "task": self.main_task["id"],
            "frame": 0,
            "pos_points": [
                [3.45, 6.78],
                [12.1, 12.1],
                [34.1, 41.0],
                [43.01, 43.99],
            ],
            "neg_points": [
                [3.25, 6.58],
                [11.1, 11.0],
                [35.5, 44.44],
                [45.01, 45.99],
            ],
        }
        data_assigneed_to_user_task = {
            "task": self.assigneed_to_user_task["id"],
            "frame": 0,
            "threshold": 0.1,
            "pos_points": [
                [3.45, 6.78],
                [12.1, 12.1],
                [34.1, 41.0],
                [43.01, 43.99],
            ],
            "neg_points": [
                [3.25, 6.58],
                [11.1, 11.0],
                [35.5, 44.44],
                [45.01, 45.99],
            ],
        }

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_interactor}", self.admin, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_interactor}",
            self.user,
            data=data_assigneed_to_user_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_interactor}", None, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_functions_create_tracker(self):
        data_main_task = {
            "task": self.main_task["id"],
            "frame": 0,
            "shapes": [[12.12, 34.45, 54.0, 76.12]],
        }
        data_assigneed_to_user_task = {
            "task": self.assigneed_to_user_task["id"],
            "frame": 0,
            "shapes": [[12.12, 34.45, 54.0, 76.12]],
        }

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_tracker}", self.admin, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_tracker}",
            self.user,
            data=data_assigneed_to_user_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_tracker}", None, data=data_main_task
        )
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_functions_create_tracker_bad_signature(self):
        signer = TimestampSigner(key="bad key")

        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "shapes": [[12.12, 34.45, 54.0, 76.12]],
            "states": [signer.sign("{}")],
        }

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_tracker}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)
        self.assertIn("Invalid or expired tracker state", response.content.decode("UTF-8"))

    def test_api_v2_lambda_functions_create_reid(self):
        data_main_task = {
            "task": self.main_task["id"],
            "frame0": 0,
            "frame1": 1,
            "boxes0": [
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 0),
                        ("group", None),
                        ("id", 11258),
                        ("label_id", 8),
                        ("occluded", False),
                        ("path_id", 0),
                        ("points", [137.0, 129.0, 457.0, 676.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 0),
                        ("group", None),
                        ("id", 11259),
                        ("label_id", 8),
                        ("occluded", False),
                        ("path_id", 1),
                        ("points", [1511.0, 224.0, 1537.0, 437.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
            ],
            "boxes1": [
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 1),
                        ("group", None),
                        ("id", 11260),
                        ("label_id", 8),
                        ("occluded", False),
                        ("points", [1076.0, 199.0, 1218.0, 593.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 1),
                        ("group", None),
                        ("id", 11261),
                        ("label_id", 8),
                        ("occluded", False),
                        ("points", [924.0, 177.0, 1090.0, 615.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
            ],
            "threshold": 0.5,
            "max_distance": 55,
        }
        data_assigneed_to_user_task = {
            "task": self.assigneed_to_user_task["id"],
            "frame0": 0,
            "frame1": 1,
            "boxes0": [
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 0),
                        ("group", None),
                        ("id", 11258),
                        ("label_id", 8),
                        ("occluded", False),
                        ("path_id", 0),
                        ("points", [137.0, 129.0, 457.0, 676.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 0),
                        ("group", None),
                        ("id", 11259),
                        ("label_id", 8),
                        ("occluded", False),
                        ("path_id", 1),
                        ("points", [1511.0, 224.0, 1537.0, 437.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
            ],
            "boxes1": [
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 1),
                        ("group", None),
                        ("id", 11260),
                        ("label_id", 8),
                        ("occluded", False),
                        ("points", [1076.0, 199.0, 1218.0, 593.0]),
                        ("source", "auto"),
                        ("type", "rectangle"),
                        ("z_order", 0),
                    ]
                ),
                OrderedDict(
                    [
                        ("attributes", []),
                        ("frame", 1),
                        ("group", 0),
                        ("id", 11398),
                        ("label_id", 8),
                        ("occluded", False),
                        (
                            "points",
                            [
                                184.3935546875,
                                211.5048828125,
                                331.64968722073354,
                                97.27792672028772,
                                445.87667560321825,
                                126.17873100983161,
                                454.13404825737416,
                                691.8087578194827,
                                180.26452189455085,
                            ],
                        ),
                        ("source", "manual"),
                        ("type", "polygon"),
                        ("z_order", 0),
                    ]
                ),
            ],
        }

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_response_data}",
            self.admin,
            data=data_main_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_response_data}",
            self.user,
            data=data_assigneed_to_user_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_response_data}",
            None,
            data=data_main_task,
        )
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_no_response_data}",
            self.admin,
            data=data_main_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_no_response_data}",
            self.user,
            data=data_assigneed_to_user_task,
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_reid_with_no_response_data}",
            None,
            data=data_main_task,
        )
        self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED)

    def test_api_v2_lambda_functions_create_negative(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }

        for id_func in [
            id_function_non_type,
            id_function_wrong_type,
            id_function_unknown_type,
            id_function_non_unique_labels,
        ]:
            with mock.patch(
                "cvat.apps.lambda_manager.views.LambdaGateway._http",
                return_value=functions["negative"][id_func],
            ):
                response = self._post_request(
                    f"{LAMBDA_FUNCTIONS_PATH}/{id_func}", self.admin, data=data
                )
                self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

    def test_api_v2_lambda_functions_convert_mask_to_rle(self):
        data_main_task = {
            "function": id_function_detector,
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(LAMBDA_REQUESTS_PATH, self.admin, data=data_main_task)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        id_request = response.data["id"]

        request_status = "started"
        while request_status != "finished" and request_status != "failed":
            response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{id_request}", self.admin)
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            request_status = response.json().get("status")
        self.assertEqual(request_status, "finished")

        self._delete_lambda_request(id_request)

        response = self._get_request(f'/api/tasks/{self.main_task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        masks = [shape for shape in response.json().get("shapes", []) if shape["type"] == "mask"]

        # [1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0] -> [0, 2, 2, 2, 2, 2, 2]
        self.assertEqual(masks[0].get("points"), [0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 3])

    def test_api_v2_lambda_functions_create_empty_data(self):
        data = {}
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_functions_create_detector_empty_mapping(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {},
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_api_v2_lambda_functions_create_detector_without_cleanup(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_api_v2_lambda_functions_create_detector_without_mapping(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_api_v2_lambda_functions_create_detector_all_shapes_unmapped(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "mapping": {"person": {"name": "person"}},
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        annotations = response.json()
        self.assertEqual(annotations["shapes"], [])

    def test_api_v2_lambda_functions_create_detector_without_task(self):
        data = {
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_functions_create_detector_without_id_frame(self):
        data = {
            "task": self.main_task["id"],
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_api_v2_lambda_functions_create_wrong_id_function(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/test-functions-wrong-id", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND)

    def test_api_v2_lambda_functions_create_wrong_id_task(self):
        data = {
            "task": 12345,
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    @skip("Fail: expected result != actual result, issue #2770")
    def test_api_v2_lambda_functions_create_detector_wrong_id_frame(self):
        data = {
            "task": self.main_task["id"],
            "frame": 12345,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    @skip("Fail: add mock and expected result != actual result")
    def test_api_v2_lambda_functions_create_two_functions(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        self._post_request(f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data)
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_409_CONFLICT)

    def test_api_v2_lambda_functions_create_function_is_not_ready(self):
        data = {
            "task": self.main_task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }
        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_state_building}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)

        response = self._post_request(
            f"{LAMBDA_FUNCTIONS_PATH}/{id_function_state_error}", self.admin, data=data
        )
        self.assertEqual(response.status_code, status.HTTP_500_INTERNAL_SERVER_ERROR)


class TestComplexFrameSetupCases(_LambdaTestCaseBase):
    def _invoke_function(self, func, payload):
        data = []
        func_id = func.id
        type_function = functions["positive"][func_id]["metadata"]["annotations"]["type"]
        if type_function == "reid":
            if func_id == id_function_reid_with_response_data:
                data = [0]
            else:
                data = []
        elif type_function == "tracker":
            data = {
                "shape": [12.34, 34.0, 35.01, 41.99],
                "state": {"key": "value"},
            }
        elif type_function == "interactor":
            data = [
                [8, 12],
                [34, 56],
                [77, 77],
            ]
        elif type_function == "detector":
            data = [
                {
                    "confidence": "0.9959098",
                    "label": "car",
                    "points": [3, 3, 15, 15],
                    "type": "rectangle",
                },
            ]

        return data

    def setUp(self):
        super().setUp()

        image_count = 50
        frame_step = 5
        start_frame = 3
        stop_frame = image_count - 4
        segment_size = 2

        data = self._generate_task_images(image_count)
        data["frame_filter"] = f"step={frame_step}"
        data["start_frame"] = start_frame
        data["stop_frame"] = stop_frame

        self.task = self._create_task(
            task_spec={
                "name": "test_task",
                "labels": [{"name": "car"}],
                "segment_size": segment_size,
            },
            data=data,
            owner=self.user,
        )
        self.task_rel_frame_range = range(len(range(start_frame, stop_frame, frame_step)))
        self.start_frame = start_frame
        self.frame_step = frame_step
        self.segment_size = segment_size

        self.labels = get_paginated_collection(
            lambda page: self._get_request(
                f"/api/labels?task_id={self.task['id']}&page={page}&sort=id", self.admin
            )
        )

        self.jobs = get_paginated_collection(
            lambda page: self._get_request(
                f"/api/jobs?task_id={self.task['id']}&page={page}", self.admin
            )
        )

        self.detector_function_id = id_function_detector
        self.reid_function_id = id_function_reid_with_response_data

        self.common_request_data = {
            "task": self.task["id"],
            "cleanup": True,
        }

    def _run_offline_function(self, function_id, data, user):
        data["function"] = function_id
        response = self._post_request(LAMBDA_REQUESTS_PATH, user, data=data)
        self.assertEqual(response.status_code, status.HTTP_200_OK, response.content)
        request_id = response.json()["id"]

        request_status = self._wait_request(request_id)
        self.assertEqual(request_status, "finished")
        self._delete_lambda_request(request_id, user)

    def _wait_request(self, request_id: str) -> str:
        request_status = "started"
        while request_status != "finished" and request_status != "failed":
            response = self._get_request(f"{LAMBDA_REQUESTS_PATH}/{request_id}", self.admin)
            self.assertEqual(response.status_code, status.HTTP_200_OK)
            request_status = response.json().get("status")

        return request_status

    def _run_online_function(self, function_id, data, user):
        response = self._post_request(f"{LAMBDA_FUNCTIONS_PATH}/{function_id}", user, data=data)
        return response

    def test_can_run_offline_detector_function_on_whole_task(self):
        data = self.common_request_data.copy()
        self._run_offline_function(self.detector_function_id, data, self.user)

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["tracks"]), 0)

        requested_frame_range = self.task_rel_frame_range
        self.assertEqual(
            {frame: 1 for frame in requested_frame_range},
            {
                frame: len(list(group))
                for frame, group in groupby(annotations["shapes"], key=lambda a: a["frame"])
            },
        )

    def test_can_run_offline_reid_function_on_whole_task(self):
        # Add starting shapes to be tracked on following frames
        requested_frame_range = self.task_rel_frame_range
        shape_template = {
            "attributes": [],
            "group": None,
            "label_id": self.labels[0]["id"],
            "occluded": False,
            "points": [0, 5, 5, 0],
            "source": "manual",
            "type": "rectangle",
            "z_order": 0,
        }
        response = self._put_request(
            f'/api/tasks/{self.task["id"]}/annotations',
            self.admin,
            data={
                "tags": [],
                "shapes": [{"frame": frame, **shape_template} for frame in requested_frame_range],
                "tracks": [],
            },
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        data = self.common_request_data.copy()
        data["cleanup"] = False  # cleanup is not compatible with reid
        self._run_offline_function(self.reid_function_id, data, self.user)

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["shapes"]), 0)
        self.assertEqual(
            [
                # The single track will be split by job segments
                {
                    "frame": job["start_frame"],
                    "shapes": [
                        {"frame": frame, "outside": frame > job["stop_frame"]}
                        for frame in requested_frame_range
                        if frame in range(job["start_frame"], job["stop_frame"] + self.segment_size)
                    ],
                }
                for job in sorted(self.jobs, key=lambda j: j["start_frame"])
            ],
            [
                {
                    "frame": track["frame"],
                    "shapes": [
                        filter_dict(shape, keep=["frame", "outside"]) for shape in track["shapes"]
                    ],
                }
                for track in annotations["tracks"]
            ],
        )

    def test_can_run_offline_detector_function_on_whole_job(self):
        data = self.common_request_data.copy()
        job = self.jobs[3]
        data["job"] = job["id"]
        self._run_offline_function(self.detector_function_id, data, self.user)

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["tracks"]), 0)

        requested_frame_range = range(job["start_frame"], job["stop_frame"] + 1)
        self.assertEqual(
            {frame: 1 for frame in requested_frame_range},
            {
                frame: len(list(group))
                for frame, group in groupby(annotations["shapes"], key=lambda a: a["frame"])
            },
        )

    def test_can_run_offline_reid_function_on_whole_job(self):
        job = self.jobs[3]
        requested_frame_range = range(job["start_frame"], job["stop_frame"] + 1)

        # Add starting shapes to be tracked on following frames
        shape_template = {
            "attributes": [],
            "group": None,
            "label_id": self.labels[0]["id"],
            "occluded": False,
            "points": [0, 5, 5, 0],
            "source": "manual",
            "type": "rectangle",
            "z_order": 0,
        }
        response = self._put_request(
            f'/api/jobs/{job["id"]}/annotations',
            self.admin,
            data={
                "tags": [],
                "shapes": [{"frame": frame, **shape_template} for frame in requested_frame_range],
                "tracks": [],
            },
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        data = self.common_request_data.copy()
        data["cleanup"] = False  # cleanup is not compatible with reid
        data["job"] = job["id"]
        self._run_offline_function(self.reid_function_id, data, self.user)

        response = self._get_request(f'/api/jobs/{job["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["shapes"]), 0)
        self.assertEqual(
            [
                {
                    "frame": job["start_frame"],
                    "shapes": [
                        {"frame": frame, "outside": frame > job["stop_frame"]}
                        for frame in requested_frame_range
                        if frame in range(job["start_frame"], job["stop_frame"] + self.segment_size)
                    ],
                }
            ],
            [
                {
                    "frame": track["frame"],
                    "shapes": [
                        filter_dict(shape, keep=["frame", "outside"]) for shape in track["shapes"]
                    ],
                }
                for track in annotations["tracks"]
            ],
        )

    def test_can_run_offline_detector_function_on_whole_gt_job(self):
        requested_frame_range = self.task_rel_frame_range[::3]
        response = self._post_request(
            "/api/jobs",
            self.admin,
            data={
                "type": "ground_truth",
                "task_id": self.task["id"],
                "frame_selection_method": "manual",
                "frames": list(requested_frame_range),
            },
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        job = response.json()

        data = self.common_request_data.copy()
        data["job"] = job["id"]
        self._run_offline_function(self.detector_function_id, data, self.user)

        response = self._get_request(f'/api/jobs/{job["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["tracks"]), 0)

        self.assertEqual(
            {frame: 1 for frame in requested_frame_range},
            Counter(a["frame"] for a in annotations["shapes"]),
        )

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()
        self.assertEqual(annotations, {"version": 0, "tags": [], "shapes": [], "tracks": []})

    def test_can_run_offline_reid_function_on_whole_gt_job(self):
        requested_frame_range = self.task_rel_frame_range[::3]
        response = self._post_request(
            "/api/jobs",
            self.admin,
            data={
                "type": "ground_truth",
                "task_id": self.task["id"],
                "frame_selection_method": "manual",
                "frames": list(requested_frame_range),
            },
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        job = response.json()

        # Add starting shapes to be tracked on following frames
        shape_template = {
            "attributes": [],
            "group": None,
            "label_id": self.labels[0]["id"],
            "occluded": False,
            "points": [0, 5, 5, 0],
            "source": "manual",
            "type": "rectangle",
            "z_order": 0,
        }
        response = self._put_request(
            f'/api/jobs/{job["id"]}/annotations',
            self.admin,
            data={
                "tags": [],
                "shapes": [{"frame": frame, **shape_template} for frame in requested_frame_range],
                "tracks": [],
            },
        )
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        data = self.common_request_data.copy()
        data["cleanup"] = False  # cleanup is not compatible with reid
        data["job"] = job["id"]
        self._run_offline_function(self.reid_function_id, data, self.user)

        response = self._get_request(f'/api/jobs/{job["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["shapes"]), 0)
        self.assertEqual(
            [
                {
                    "frame": job["start_frame"],
                    "shapes": [
                        {"frame": frame, "outside": frame > job["stop_frame"]}
                        for frame in requested_frame_range
                        if frame in range(job["start_frame"], job["stop_frame"] + self.segment_size)
                    ],
                }
            ],
            [
                {
                    "frame": track["frame"],
                    "shapes": [
                        filter_dict(shape, keep=["frame", "outside"]) for shape in track["shapes"]
                    ],
                }
                for track in annotations["tracks"]
            ],
        )

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()
        self.assertEqual(annotations, {"version": 0, "tags": [], "shapes": [], "tracks": []})

    def test_offline_function_run_on_task_does_not_affect_gt_job(self):
        response = self._post_request(
            "/api/jobs",
            self.admin,
            data={
                "type": "ground_truth",
                "task_id": self.task["id"],
                "frame_selection_method": "manual",
                "frames": list(self.task_rel_frame_range[::3]),
            },
        )
        self.assertEqual(response.status_code, status.HTTP_201_CREATED)
        job = response.json()

        data = self.common_request_data.copy()
        self._run_offline_function(self.detector_function_id, data, self.user)

        response = self._get_request(f'/api/tasks/{self.task["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()

        self.assertEqual(len(annotations["tags"]), 0)
        self.assertEqual(len(annotations["tracks"]), 0)

        requested_frame_range = self.task_rel_frame_range
        self.assertEqual(
            {frame: 1 for frame in requested_frame_range},
            Counter(a["frame"] for a in annotations["shapes"]),
        )

        response = self._get_request(f'/api/jobs/{job["id"]}/annotations', self.admin)
        self.assertEqual(response.status_code, status.HTTP_200_OK)
        annotations = response.json()
        self.assertEqual(annotations, {"version": 0, "tags": [], "shapes": [], "tracks": []})

    def test_can_run_online_function_on_valid_task_frame(self):
        data = self.common_request_data.copy()
        requested_frame = self.task_rel_frame_range[4]
        data["frame"] = requested_frame

        response = self._run_online_function(self.detector_function_id, data, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        annotations = response.json()
        self.assertEqual(1, len(annotations["shapes"]))

    def test_can_run_online_function_on_invalid_task_frame(self):
        data = self.common_request_data.copy()
        requested_frame = self.task_rel_frame_range[-1] + 1
        data["frame"] = requested_frame

        response = self._run_online_function(self.detector_function_id, data, self.user)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_can_run_online_function_on_valid_job_frame(self):
        data = self.common_request_data.copy()
        job = self.jobs[2]
        requested_frame = job["start_frame"] + 1
        data["frame"] = requested_frame
        data["job"] = job["id"]

        response = self._run_online_function(self.detector_function_id, data, self.user)
        self.assertEqual(response.status_code, status.HTTP_200_OK)

        annotations = response.json()
        self.assertEqual(1, len(annotations["shapes"]))

    def test_can_run_online_function_on_invalid_job_frame(self):
        data = self.common_request_data.copy()
        job = self.jobs[2]
        requested_frame = job["stop_frame"] + 1
        data["frame"] = requested_frame
        data["job"] = job["id"]

        response = self._run_online_function(self.detector_function_id, data, self.user)
        self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)


class Issue4996_Cases(_LambdaTestCaseBase):
    # Check regressions for https://github.com/cvat-ai/cvat/issues/4996#issuecomment-1266123032
    # We need to check that job assignee can call functions in the assigned jobs
    # This requires to pass the job id in the call request.

    def _create_org(self, *, owner: int, members: dict[int, str] = None) -> dict:
        org = self._post_request(
            "/api/organizations",
            user=owner,
            data={
                "slug": "testorg",
                "name": "test Org",
            },
        )
        assert org.status_code == status.HTTP_201_CREATED
        org = org.json()

        for uid, role in members.items():
            user = self._get_request("/api/users/self", user=uid)
            assert user.status_code == status.HTTP_200_OK
            user = user.json()

            invitation = self._post_request(
                "/api/invitations",
                user=owner,
                data={
                    "email": user["email"],
                    "role": role,
                },
                query_params={"org_id": org["id"]},
            )
            assert invitation.status_code == status.HTTP_201_CREATED

        return org

    def _set_task_assignee(self, task: int, assignee: Optional[int]):
        response = self._patch_request(
            f"/api/tasks/{task}",
            user=self.admin,
            data={
                "assignee_id": assignee,
            },
        )
        assert response.status_code == status.HTTP_200_OK

    def _set_job_assignee(self, job: int, assignee: Optional[int]):
        response = self._patch_request(
            f"/api/jobs/{job}",
            user=self.admin,
            data={
                "assignee": assignee,
            },
        )
        assert response.status_code == status.HTTP_200_OK

    def setUp(self):
        super().setUp()

        self.org = self._create_org(owner=self.admin, members={self.user: "worker"})

        task = self._create_task(
            task_spec={"name": "test_task", "labels": [{"name": "car"}], "segment_size": 2},
            data=self._generate_task_images(6),
            owner=self.admin,
            org_id=self.org["id"],
        )
        self.task = task

        jobs = get_paginated_collection(
            lambda page: self._get_request(
                "/api/jobs",
                self.admin,
                query_params={"task_id": self.task["id"], "page": page, "org_id": self.org["id"]},
            )
        )
        self.job = jobs[1]

        self.common_request_data = {
            "task": self.task["id"],
            "frame": 0,
            "cleanup": True,
            "mapping": {
                "car": {"name": "car"},
            },
        }

        self.function_url = f"{LAMBDA_FUNCTIONS_PATH}/{id_function_detector}"

    def _get_valid_job_request_data(self):
        data = self.common_request_data.copy()
        data.update({"job": self.job["id"], "frame": 2})
        return data

    def _get_invalid_job_request_data(self):
        data = self.common_request_data.copy()
        data.update({"job": self.job["id"], "frame": 0})
        return data

    def test_can_call_function_for_job_worker_in_org__deny_unassigned_worker_with_task_request(
        self,
    ):
        data = self.common_request_data.copy()
        with self.subTest(job=None, assignee=None):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_can_call_function_for_job_worker_in_org__deny_unassigned_worker_with_job_request(self):
        data = self._get_valid_job_request_data()
        with self.subTest(job="defined", assignee=None):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_can_call_function_for_job_worker_in_org__allow_task_assigned_worker_with_task_request(
        self,
    ):
        self._set_task_assignee(self.task["id"], self.user.id)

        data = self.common_request_data.copy()
        with self.subTest(job=None, assignee="task"):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_can_call_function_for_job_worker_in_org__deny_job_assigned_worker_with_task_request(
        self,
    ):
        self._set_job_assignee(self.job["id"], self.user.id)

        data = self.common_request_data.copy()
        with self.subTest(job=None, assignee="job"):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_403_FORBIDDEN)

    def test_can_call_function_for_job_worker_in_org__allow_job_assigned_worker_with_job_request(
        self,
    ):
        self._set_job_assignee(self.job["id"], self.user.id)

        data = self._get_valid_job_request_data()
        with self.subTest(job="defined", assignee="job"):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_200_OK)

    def test_can_check_job_boundaries_in_function_call__fail_for_frame_outside_job(self):
        self._set_job_assignee(self.job["id"], self.user.id)

        data = self._get_invalid_job_request_data()
        with self.subTest(job="defined", frame="outside"):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_400_BAD_REQUEST)

    def test_can_check_job_boundaries_in_function_call__ok_for_frame_inside_job(self):
        self._set_job_assignee(self.job["id"], self.user.id)

        data = self._get_valid_job_request_data()
        with self.subTest(job="defined", frame="inside"):
            response = self._post_request(
                self.function_url, self.user, data=data, query_params={"org_id": self.org["id"]}
            )
            self.assertEqual(response.status_code, status.HTTP_200_OK)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\lambda_manager\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class LogViewerConfig(AppConfig):
    name = "cvat.apps.log_viewer"

    def ready(self) -> None:
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings

from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum, get_iam_context


class LogViewerPermission(OpenPolicyAgentPermission):
    has_analytics_access: bool

    class Scopes(StrEnum):
        VIEW = "view"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "analytics":
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    @classmethod
    def create_base_perm(cls, request, view, scope, iam_context, obj=None, **kwargs):
        if not iam_context and request:
            iam_context = get_iam_context(request, obj)
        return cls(
            scope=scope,
            obj=obj,
            has_analytics_access=request.user.profile.has_analytics_access,
            **iam_context,
            **kwargs,
        )

    def __init__(self, has_analytics_access=False, **kwargs):
        super().__init__(**kwargs)
        self.payload["input"]["auth"]["user"]["has_analytics_access"] = has_analytics_access
        self.url = settings.IAM_OPA_DATA_URL + "/analytics/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.VIEW,
            }[view.action]
        ]

    def get_resource(self):
        return None


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\urls.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework import routers

from . import views

router = routers.DefaultRouter(trailing_slash=False)
router.register("analytics", views.LogViewerAccessViewSet, basename="analytics")

urlpatterns = router.urls


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\views.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings
from django.http import HttpResponsePermanentRedirect
from drf_spectacular.utils import extend_schema
from rest_framework import status, viewsets
from rest_framework.decorators import action
from rest_framework.response import Response


@extend_schema(exclude=True)
class LogViewerAccessViewSet(viewsets.ViewSet):
    serializer_class = None

    def list(self, request):
        return Response(status=status.HTTP_200_OK)

    # All log view requests are proxied by Traefik in production mode which is not available in debug mode,
    # In order not to duplicate settings, let's just redirect to the default page in debug mode
    @action(detail=False, url_path="dashboards")
    def redirect(self, request):
        if settings.DEBUG:
            return HttpResponsePermanentRedirect("http://localhost:3001/dashboards")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\log_viewer\rules\tests\generators\analytics_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "analytics"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
HAS_ANALYTICS_ACCESS = [True, False]


def RESOURCES(scope):
    return [None]


def eval_rule(scope, context, ownership, privilege, membership, data, has_analytics_access):
    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    rules = list(
        filter(
            lambda r: r["hasanalyticsaccess"] in ("na", str(has_analytics_access).lower()), rules
        )
    )
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, has_analytics_access):
    data = {
        "scope": scope,
        "auth": {
            "user": {
                "id": random.randrange(0, 100),  # nosec B311 NOSONAR
                "privilege": privilege,
                "has_analytics_access": has_analytics_access,
            },
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        if k == "resource":
            continue
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )

    return name


def get_name(scope, context, ownership, privilege, membership, resource, has_analytics_access):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, has_analytics_access in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, HAS_ANALYTICS_ACCESS
        ):
            for resource in RESOURCES(scope):
                if not is_valid(scope, context, ownership, privilege, membership, resource):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, has_analytics_access
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, has_analytics_access
                )
                result = eval_rule(
                    scope, context, ownership, privilege, membership, data, has_analytics_access
                )
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\admin.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.contrib import admin

from .models import Membership, Organization


class MembershipInline(admin.TabularInline):
    model = Membership
    extra = 0

    radio_fields = {
        "role": admin.VERTICAL,
    }

    autocomplete_fields = ("user",)


class OrganizationAdmin(admin.ModelAdmin):
    search_fields = ("slug", "name", "owner__username")
    list_display = ("id", "slug", "name")

    autocomplete_fields = ("owner",)

    inlines = [MembershipInline]


admin.site.register(Organization, OrganizationAdmin)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\apps.py =====
# Copyright (C) 2021 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class OrganizationsConfig(AppConfig):
    name = "cvat.apps.organizations"

    def ready(self) -> None:
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\models.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datetime import timedelta

from allauth.account.adapter import get_adapter
from django.conf import settings
from django.contrib.auth import get_user_model
from django.contrib.sites.shortcuts import get_current_site
from django.core.exceptions import ImproperlyConfigured
from django.db import models
from django.utils import timezone
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import extend_schema_field

from cvat.apps.engine.models import TimestampedModel


class Organization(TimestampedModel):
    slug = models.SlugField(max_length=16, blank=False, unique=True)
    name = models.CharField(max_length=64, blank=True)
    description = models.TextField(blank=True)
    contact = models.JSONField(blank=True, default=dict)

    owner = models.ForeignKey(
        get_user_model(), null=True, blank=True, on_delete=models.SET_NULL, related_name="+"
    )

    def __str__(self):
        return self.slug

    class Meta:
        default_permissions = ()


class Membership(models.Model):
    WORKER = "worker"
    SUPERVISOR = "supervisor"
    MAINTAINER = "maintainer"
    OWNER = "owner"

    user = models.ForeignKey(
        get_user_model(), on_delete=models.CASCADE, null=True, related_name="memberships"
    )
    organization = models.ForeignKey(Organization, on_delete=models.CASCADE, related_name="members")
    is_active = models.BooleanField(default=False)
    joined_date = models.DateTimeField(null=True)
    role = models.CharField(
        max_length=16,
        choices=[
            (WORKER, "Worker"),
            (SUPERVISOR, "Supervisor"),
            (MAINTAINER, "Maintainer"),
            (OWNER, "Owner"),
        ],
    )

    class Meta:
        default_permissions = ()
        unique_together = ("user", "organization")


# Inspried by https://github.com/bee-keeper/django-invitations
class Invitation(models.Model):
    key = models.CharField(max_length=64, primary_key=True)
    created_date = models.DateTimeField(auto_now_add=True)
    sent_date = models.DateTimeField(null=True)
    owner = models.ForeignKey(get_user_model(), null=True, on_delete=models.SET_NULL)
    membership = models.OneToOneField(Membership, on_delete=models.CASCADE)

    @property
    def organization_id(self):
        return self.membership.organization_id

    @property
    @extend_schema_field(OpenApiTypes.BOOL)
    def expired(self):
        if self.sent_date:
            expiration_date = self.sent_date + timedelta(
                days=settings.ORG_INVITATION_EXPIRY_DAYS,
            )
            return expiration_date <= timezone.now()
        return None

    @property
    def accepted(self):
        return self.membership.is_active

    @property
    def organization_slug(self):
        return self.membership.organization.slug

    def send(self, request):
        if settings.EMAIL_BACKEND is None:
            raise ImproperlyConfigured("Email backend is not configured")

        target_email = self.membership.user.email
        current_site = get_current_site(request)
        site_name = current_site.name
        domain = current_site.domain
        context = {
            "email": target_email,
            "invitation_key": self.key,
            "domain": domain,
            "site_name": site_name,
            "invitation_owner": self.owner.get_username(),
            "organization_name": self.membership.organization.slug,
            "protocol": "https" if request.is_secure() else "http",
        }

        get_adapter(request).send_mail("invitation/invitation", target_email, context)

        self.sent_date = timezone.now()
        self.save()

    def accept(self, date=None):
        if not self.membership.is_active:
            self.membership.is_active = True
            if date is None:
                self.membership.joined_date = timezone.now()
            else:
                self.membership.joined_date = date
            self.membership.save()

    class Meta:
        default_permissions = ()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.conf import settings

from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum

from .models import Membership


class OrganizationPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        LIST = "list"
        CREATE = "create"
        DELETE = "delete"
        UPDATE = "update"
        VIEW = "view"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "organization":
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(request, view, scope, iam_context, obj)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/organizations/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
                "create": Scopes.CREATE,
                "destroy": Scopes.DELETE,
                "partial_update": Scopes.UPDATE,
                "retrieve": Scopes.VIEW,
            }[view.action]
        ]

    def get_resource(self):
        if self.obj:
            membership = Membership.objects.filter(organization=self.obj, user=self.user_id).first()
            return {
                "id": self.obj.id,
                "owner": {"id": self.obj.owner_id},
                "user": {"role": membership.role if membership else None},
            }
        elif self.scope.startswith(__class__.Scopes.CREATE.value):
            return {"id": None, "owner": {"id": self.user_id}, "user": {"role": "owner"}}
        else:
            return None


class InvitationPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        LIST = "list"
        CREATE = "create"
        DELETE = "delete"
        ACCEPT = "accept"
        DECLINE = "decline"
        RESEND = "resend"
        VIEW = "view"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "invitation":
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(
                    request, view, scope, iam_context, obj, role=request.data.get("role")
                )
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.role = kwargs.get("role")
        self.url = settings.IAM_OPA_DATA_URL + "/invitations/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
                "create": Scopes.CREATE,
                "destroy": Scopes.DELETE,
                "partial_update": (
                    Scopes.ACCEPT if "accepted" in request.query_params else Scopes.RESEND
                ),
                "retrieve": Scopes.VIEW,
                "accept": Scopes.ACCEPT,
                "decline": Scopes.DECLINE,
                "resend": Scopes.RESEND,
            }[view.action]
        ]

    def get_resource(self):
        data = None
        if self.obj:
            data = {
                "owner": {"id": self.obj.owner_id},
                "invitee": {"id": self.obj.membership.user_id},
                "role": self.obj.membership.role,
                "organization": {"id": self.obj.membership.organization_id},
            }
        elif self.scope.startswith(__class__.Scopes.CREATE.value):
            data = {
                "owner": {"id": self.user_id},
                "invitee": {"id": None},  # unknown yet
                "role": self.role,
                "organization": {"id": self.org_id} if self.org_id is not None else None,
            }

        return data


class MembershipPermission(OpenPolicyAgentPermission):
    class Scopes(StrEnum):
        LIST = "list"
        UPDATE = "change"
        UPDATE_ROLE = "change:role"
        VIEW = "view"
        DELETE = "delete"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "membership":
            for scope in cls.get_scopes(request, view, obj):
                params = {}
                if scope == "change:role":
                    params["role"] = request.data.get("role")

                self = cls.create_base_perm(request, view, scope, iam_context, obj, **params)
                permissions.append(self)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/memberships/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        scopes = []

        scope = {
            "list": Scopes.LIST,
            "partial_update": Scopes.UPDATE,
            "retrieve": Scopes.VIEW,
            "destroy": Scopes.DELETE,
        }[view.action]

        if scope == Scopes.UPDATE:
            scopes.extend(
                __class__.get_per_field_update_scopes(
                    request,
                    {
                        "role": Scopes.UPDATE_ROLE,
                    },
                )
            )
        else:
            scopes.append(scope)

        return scopes

    def get_resource(self):
        if self.obj:
            return {
                "role": self.obj.role,
                "is_active": self.obj.is_active,
                "user": {"id": self.obj.user_id},
                "organization": {"id": self.obj.organization_id},
            }
        else:
            return None


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\serializers.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from allauth.account.models import EmailAddress
from attr.converters import to_bool
from django.conf import settings
from django.contrib.auth import get_user_model
from django.contrib.auth.models import User
from django.core.exceptions import ObjectDoesNotExist
from django.db import transaction
from rest_framework import serializers

from cvat.apps.engine.serializers import BasicUserSerializer
from cvat.apps.iam.utils import get_dummy_user

from .models import Invitation, Membership, Organization


class OrganizationReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(allow_null=True)

    class Meta:
        model = Organization
        fields = [
            "id",
            "slug",
            "name",
            "description",
            "created_date",
            "updated_date",
            "contact",
            "owner",
        ]
        read_only_fields = fields


class BasicOrganizationSerializer(serializers.ModelSerializer):
    class Meta:
        model = Organization
        fields = ["id", "slug"]
        read_only_fields = fields


class OrganizationWriteSerializer(serializers.ModelSerializer):
    def to_representation(self, instance):
        serializer = OrganizationReadSerializer(instance, context=self.context)
        return serializer.data

    class Meta:
        model = Organization
        fields = ["slug", "name", "description", "contact", "owner"]

        # TODO: at the moment isn't possible to change the owner. It should
        # be a separate feature. Need to change it together with corresponding
        # Membership. Also such operation should be well protected.
        read_only_fields = ["owner"]

    def create(self, validated_data):
        organization = super().create(validated_data)
        Membership.objects.create(
            user=organization.owner,
            organization=organization,
            is_active=True,
            joined_date=organization.created_date,
            role=Membership.OWNER,
        )

        return organization


class InvitationReadSerializer(serializers.ModelSerializer):
    role = serializers.ChoiceField(Membership.role.field.choices, source="membership.role")
    user = BasicUserSerializer(source="membership.user")
    organization = serializers.PrimaryKeyRelatedField(
        queryset=Organization.objects.all(), source="membership.organization"
    )
    organization_info = BasicOrganizationSerializer(source="membership.organization")
    owner = BasicUserSerializer(allow_null=True)

    class Meta:
        model = Invitation
        fields = [
            "key",
            "created_date",
            "owner",
            "role",
            "user",
            "organization",
            "expired",
            "organization_info",
        ]
        read_only_fields = fields
        extra_kwargs = {
            "expired": {
                "allow_null": True,
            }
        }


class InvitationWriteSerializer(serializers.ModelSerializer):
    role = serializers.ChoiceField(Membership.role.field.choices, source="membership.role")
    email = serializers.EmailField(source="membership.user.email")
    organization = serializers.PrimaryKeyRelatedField(
        source="membership.organization", read_only=True
    )

    def to_representation(self, instance):
        serializer = InvitationReadSerializer(instance, context=self.context)
        return serializer.data

    class Meta:
        model = Invitation
        fields = ["key", "created_date", "owner", "role", "organization", "email"]
        read_only_fields = ["key", "created_date", "owner", "organization"]

    @transaction.atomic
    def create(self, validated_data):
        membership_data = validated_data.pop("membership")
        organization = validated_data.pop("organization")
        try:
            user = get_user_model().objects.get(email__iexact=membership_data["user"]["email"])
            del membership_data["user"]
        except ObjectDoesNotExist:
            user_email = membership_data["user"]["email"]
            user = User.objects.create_user(username=user_email, email=user_email)
            user.set_unusable_password()
            # User.objects.create_user(...) normalizes passed email and user.email can be different from original user_email
            email = EmailAddress.objects.create(
                user=user, email=user.email, primary=True, verified=False
            )
            user.save()
            email.save()
            del membership_data["user"]
        membership, created = Membership.objects.get_or_create(
            defaults=membership_data, user=user, organization=organization
        )
        if not created:
            raise serializers.ValidationError(
                "The user is a member of " "the organization already."
            )
        invitation = Invitation.objects.create(**validated_data, membership=membership)

        return invitation

    def update(self, instance, validated_data):
        return super().update(instance, {})

    def save(self, request, **kwargs):
        invitation = super().save(**kwargs)
        dummy_user = get_dummy_user(invitation.membership.user.email)
        if not to_bool(settings.ORG_INVITATION_CONFIRM) and not dummy_user:
            invitation.accept()
        else:
            invitation.send(request)

        return invitation


class MembershipReadSerializer(serializers.ModelSerializer):
    user = BasicUserSerializer()

    class Meta:
        model = Membership
        fields = ["id", "user", "organization", "is_active", "joined_date", "role", "invitation"]
        read_only_fields = fields
        extra_kwargs = {
            "invitation": {
                "allow_null": True,  # owner of an organization does not have an invitation
            }
        }


class MembershipWriteSerializer(serializers.ModelSerializer):
    def to_representation(self, instance):
        serializer = MembershipReadSerializer(instance, context=self.context)
        return serializer.data

    class Meta:
        model = Membership
        fields = ["id", "user", "organization", "is_active", "joined_date", "role"]
        read_only_fields = ["user", "organization", "is_active", "joined_date"]


class AcceptInvitationReadSerializer(serializers.Serializer):
    organization_slug = serializers.CharField()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\throttle.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework.throttling import UserRateThrottle


class ResendOrganizationInvitationThrottle(UserRateThrottle):
    rate = "5/hour"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\urls.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework.routers import DefaultRouter

from .views import InvitationViewSet, MembershipViewSet, OrganizationViewSet

router = DefaultRouter(trailing_slash=False)
router.register("organizations", OrganizationViewSet)
router.register("invitations", InvitationViewSet)
router.register("memberships", MembershipViewSet)

urlpatterns = router.urls


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\views.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.core.exceptions import ImproperlyConfigured
from django.db import transaction
from django.utils.crypto import get_random_string
from drf_spectacular.utils import OpenApiResponse, extend_schema, extend_schema_view
from rest_framework import mixins, status, viewsets
from rest_framework.decorators import action
from rest_framework.permissions import SAFE_METHODS
from rest_framework.response import Response

from cvat.apps.engine.mixins import PartialUpdateModelMixin
from cvat.apps.iam.filters import ORGANIZATION_OPEN_API_PARAMETERS
from cvat.apps.organizations.permissions import (
    InvitationPermission,
    MembershipPermission,
    OrganizationPermission,
)
from cvat.apps.organizations.throttle import ResendOrganizationInvitationThrottle

from .models import Invitation, Membership, Organization
from .serializers import (
    AcceptInvitationReadSerializer,
    InvitationReadSerializer,
    InvitationWriteSerializer,
    MembershipReadSerializer,
    MembershipWriteSerializer,
    OrganizationReadSerializer,
    OrganizationWriteSerializer,
)


@extend_schema(tags=["organizations"])
@extend_schema_view(
    retrieve=extend_schema(
        summary="Get organization details",
        responses={
            "200": OrganizationReadSerializer,
        },
    ),
    list=extend_schema(
        summary="List organizations",
        responses={
            "200": OrganizationReadSerializer(many=True),
        },
    ),
    partial_update=extend_schema(
        summary="Update an organization",
        request=OrganizationWriteSerializer(partial=True),
        responses={
            "200": OrganizationReadSerializer,  # check OrganizationWriteSerializer.to_representation
        },
    ),
    create=extend_schema(
        summary="Create an organization",
        request=OrganizationWriteSerializer,
        responses={
            "201": OrganizationReadSerializer,  # check OrganizationWriteSerializer.to_representation
        },
    ),
    destroy=extend_schema(
        summary="Delete an organization",
        responses={
            "204": OpenApiResponse(description="The organization has been deleted"),
        },
    ),
)
class OrganizationViewSet(
    viewsets.GenericViewSet,
    mixins.RetrieveModelMixin,
    mixins.ListModelMixin,
    mixins.CreateModelMixin,
    mixins.DestroyModelMixin,
    PartialUpdateModelMixin,
):
    queryset = Organization.objects.select_related("owner").all()
    search_fields = ("name", "owner", "slug")
    filter_fields = list(search_fields) + ["id"]
    simple_filters = list(search_fields)
    lookup_fields = {"owner": "owner__username"}
    ordering_fields = list(filter_fields)
    ordering = "-id"
    http_method_names = ["get", "post", "patch", "delete", "head", "options"]
    iam_organization_field = None

    def get_queryset(self):
        queryset = super().get_queryset()

        permission = OrganizationPermission.create_scope_list(self.request)
        return permission.filter(queryset)

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return OrganizationReadSerializer
        else:
            return OrganizationWriteSerializer

    def perform_create(self, serializer):
        extra_kwargs = {"owner": self.request.user}
        if not serializer.validated_data.get("name"):
            extra_kwargs.update({"name": serializer.validated_data["slug"]})
        serializer.save(**extra_kwargs)

    class Meta:
        model = Membership
        fields = ("user",)


@extend_schema(tags=["memberships"])
@extend_schema_view(
    retrieve=extend_schema(
        summary="Get membership details",
        responses={
            "200": MembershipReadSerializer,
        },
    ),
    list=extend_schema(
        summary="List memberships",
        responses={
            "200": MembershipReadSerializer(many=True),
        },
    ),
    partial_update=extend_schema(
        summary="Update a membership",
        request=MembershipWriteSerializer(partial=True),
        responses={
            "200": MembershipReadSerializer,  # check MembershipWriteSerializer.to_representation
        },
    ),
    destroy=extend_schema(
        summary="Delete a membership",
        responses={
            "204": OpenApiResponse(description="The membership has been deleted"),
        },
    ),
)
class MembershipViewSet(
    mixins.RetrieveModelMixin,
    mixins.DestroyModelMixin,
    mixins.ListModelMixin,
    PartialUpdateModelMixin,
    viewsets.GenericViewSet,
):
    queryset = Membership.objects.select_related("invitation", "user").all()
    ordering = "-id"
    http_method_names = ["get", "patch", "delete", "head", "options"]
    search_fields = ("user", "role")
    filter_fields = list(search_fields) + ["id"]
    simple_filters = list(search_fields)
    ordering_fields = list(filter_fields)
    lookup_fields = {"user": "user__username"}
    iam_organization_field = "organization"

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return MembershipReadSerializer
        else:
            return MembershipWriteSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            permission = MembershipPermission.create_scope_list(self.request)
            queryset = permission.filter(queryset)

        return queryset


@extend_schema(tags=["invitations"])
@extend_schema_view(
    retrieve=extend_schema(
        summary="Get invitation details",
        responses={
            "200": InvitationReadSerializer,
        },
    ),
    list=extend_schema(
        summary="List invitations",
        responses={
            "200": InvitationReadSerializer(many=True),
        },
    ),
    partial_update=extend_schema(
        summary="Update an invitation",
        request=InvitationWriteSerializer(partial=True),
        responses={
            "200": InvitationReadSerializer,  # check InvitationWriteSerializer.to_representation
        },
    ),
    create=extend_schema(
        summary="Create an invitation",
        request=InvitationWriteSerializer,
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={
            "201": InvitationReadSerializer,  # check InvitationWriteSerializer.to_representation
        },
    ),
    destroy=extend_schema(
        summary="Delete an invitation",
        responses={
            "204": OpenApiResponse(description="The invitation has been deleted"),
        },
    ),
    accept=extend_schema(
        operation_id="invitations_accept",
        request=None,
        summary="Accept an invitation",
        responses={
            "200": OpenApiResponse(
                response=AcceptInvitationReadSerializer, description="The invitation is accepted"
            ),
            "400": OpenApiResponse(description="The invitation is expired or already accepted"),
        },
    ),
    decline=extend_schema(
        operation_id="invitations_decline",
        request=None,
        summary="Decline an invitation",
        responses={
            "204": OpenApiResponse(description="The invitation has been declined"),
        },
    ),
    resend=extend_schema(
        operation_id="invitations_resend",
        summary="Resend an invitation",
        request=None,
        responses={
            "204": OpenApiResponse(description="Invitation has been sent"),
            "400": OpenApiResponse(description="The invitation is already accepted"),
        },
    ),
)
class InvitationViewSet(
    viewsets.GenericViewSet,
    mixins.RetrieveModelMixin,
    mixins.ListModelMixin,
    PartialUpdateModelMixin,
    mixins.CreateModelMixin,
    mixins.DestroyModelMixin,
):
    queryset = Invitation.objects.all()
    http_method_names = ["get", "post", "patch", "delete", "head", "options"]
    iam_organization_field = "membership__organization"

    search_fields = ("owner",)
    filter_fields = list(search_fields) + ["user_id", "accepted"]
    simple_filters = list(search_fields)
    ordering_fields = list(simple_filters) + ["created_date"]
    ordering = "-created_date"
    lookup_fields = {
        "owner": "owner__username",
        "user_id": "membership__user__id",
        "accepted": "membership__is_active",
    }

    def get_serializer_class(self):
        if self.request.method in SAFE_METHODS:
            return InvitationReadSerializer
        else:
            return InvitationWriteSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        permission = InvitationPermission.create_scope_list(self.request)
        return permission.filter(queryset)

    def create(self, request):
        serializer = self.get_serializer(data=request.data)
        serializer.is_valid(raise_exception=True)
        try:
            self.perform_create(serializer)
        except ImproperlyConfigured:
            return Response(
                status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                data="Email backend is not configured.",
            )

        return Response(serializer.data, status=status.HTTP_201_CREATED)

    def perform_create(self, serializer):
        serializer.save(
            owner=self.request.user,
            key=get_random_string(length=64),
            organization=self.request.iam_context["organization"],
            request=self.request,
        )

    def perform_update(self, serializer):
        if "accepted" in self.request.query_params:
            serializer.instance.accept()
        else:
            super().perform_update(serializer)

    @transaction.atomic
    @action(detail=True, methods=["POST"], url_path="accept")
    def accept(self, request, pk):
        try:
            invitation = self.get_object()  # force to call check_object_permissions
            if invitation.expired:
                return Response(
                    status=status.HTTP_400_BAD_REQUEST,
                    data="Your invitation is expired. Please contact organization owner to renew it.",
                )
            if invitation.membership.is_active:
                return Response(
                    status=status.HTTP_400_BAD_REQUEST, data="Your invitation is already accepted."
                )
            invitation.accept()
            response_serializer = AcceptInvitationReadSerializer(
                data={"organization_slug": invitation.membership.organization.slug}
            )
            response_serializer.is_valid(raise_exception=True)
            return Response(status=status.HTTP_200_OK, data=response_serializer.data)
        except Invitation.DoesNotExist:
            return Response(
                status=status.HTTP_404_NOT_FOUND,
                data="This invitation does not exist. Please contact organization owner.",
            )

    @action(
        detail=True,
        methods=["POST"],
        url_path="resend",
        throttle_classes=[ResendOrganizationInvitationThrottle],
    )
    def resend(self, request, pk):
        try:
            invitation = self.get_object()  # force to call check_object_permissions
            if invitation.membership.is_active:
                return Response(
                    status=status.HTTP_400_BAD_REQUEST, data="This invitation is already accepted."
                )
            invitation.send(request)
            return Response(status=status.HTTP_204_NO_CONTENT)
        except Invitation.DoesNotExist:
            return Response(
                status=status.HTTP_404_NOT_FOUND, data="This invitation does not exist."
            )
        except ImproperlyConfigured:
            return Response(
                status=status.HTTP_500_INTERNAL_SERVER_ERROR,
                data="Email backend is not configured.",
            )

    @action(detail=True, methods=["POST"], url_path="decline")
    def decline(self, request, pk):
        try:
            invitation = self.get_object()  # force to call check_object_permissions
            membership = invitation.membership
            membership.delete()
            return Response(status=status.HTTP_204_NO_CONTENT)
        except Invitation.DoesNotExist:
            return Response(
                status=status.HTTP_404_NOT_FOUND, data="This invitation does not exist."
            )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\__init__.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\migrations\0001_initial.py =====
# Generated by Django 3.2.8 on 2021-10-26 14:52

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name="Organization",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("slug", models.SlugField(max_length=16, unique=True)),
                ("name", models.CharField(blank=True, max_length=64)),
                ("description", models.TextField(blank=True)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                ("contact", models.JSONField(blank=True, default=dict)),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="+",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="Membership",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("is_active", models.BooleanField(default=False)),
                ("joined_date", models.DateTimeField(null=True)),
                (
                    "role",
                    models.CharField(
                        choices=[
                            ("worker", "Worker"),
                            ("supervisor", "Supervisor"),
                            ("maintainer", "Maintainer"),
                            ("owner", "Owner"),
                        ],
                        max_length=16,
                    ),
                ),
                (
                    "organization",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="members",
                        to="organizations.organization",
                    ),
                ),
                (
                    "user",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="memberships",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
                "unique_together": {("user", "organization")},
            },
        ),
        migrations.CreateModel(
            name="Invitation",
            fields=[
                ("key", models.CharField(max_length=64, primary_key=True, serialize=False)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                (
                    "membership",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE, to="organizations.membership"
                    ),
                ),
                (
                    "owner",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\migrations\0002_invitation_sent_date.py =====
# Generated by Django 4.2.1 on 2023-09-27 06:52

from django.db import migrations, models


class Migration(migrations.Migration):
    dependencies = [
        ("organizations", "0001_initial"),
    ]

    operations = [
        migrations.AddField(
            model_name="invitation",
            name="sent_date",
            field=models.DateTimeField(null=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\migrations\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\rules\tests\generators\invitations_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "invitations"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["owner", "invitee", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [False, True]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "owner": {"id": random.randrange(300, 400)},
                "invitee": {"id": random.randrange(400, 500)},
                "role": role,
                "organization": {"id": random.randrange(500, 600)},
            }
            for role in ORG_ROLES
            if role is not None
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(
            lambda r: not r["limit"]
            or r["limit"].startswith("filter")
            or eval(r["limit"], {"resource": resource}),
            rules,
        )
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id
    elif ownership == "invitee":
        data["resource"]["invitee"]["id"] = user_id

    if scope == "create":
        data["resource"]["invitee"]["id"] = None

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" not in v:
                name += _get_name(prefix, **v)
        else:
            name += f"{prefix}_{str(v).upper()}"

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\rules\tests\generators\memberships_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "memberships"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["self", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [False, True]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    else:
        return [
            {
                "user": {"id": random.randrange(300, 400)},
                "is_active": active,
                "role": role,
                "organization": {"id": random.randrange(500, 600)},
            }
            for role in ORG_ROLES
            if role is not None
            for active in [False, True]
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    else:
        return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(
        filter(
            lambda r: r["membership"] == "na"
            or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"]),
            rules,
        )
    )
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False

    if scope != "create" and not data["resource"]["is_active"]:
        is_staff = membership == "owner" or membership == "maintainer"
        if is_staff:
            if scope != "view":
                if ORG_ROLES.index(membership) >= ORG_ROLES.index(resource["role"]):
                    return False
                if GROUPS.index(privilege) > GROUPS.index("user"):
                    return False
                if resource["user"]["id"] == data["auth"]["user"]["id"]:
                    return False
            return True
        return False

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]
    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id

        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "self":
        data["resource"]["user"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" not in v:
                name += _get_name(prefix, **v)
        else:
            name += f'{prefix}_{str(v).upper().replace(":", "_")}'

    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and same_org is False:
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):
                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\organizations\rules\tests\generators\organizations_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "organizations"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)

    return rules


simple_rules = read_rules(NAME)

SCOPES = {rule["scope"] for rule in simple_rules}
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["owner", "maintainer", "supervisor", "worker", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]


def RESOURCES(ownership):
    return [
        {"user": {"num_resources": n, "role": ownership if ownership != "none" else None}}
        for n in (0, 1, 10)
    ] + [None]


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True
    rules = list(filter(lambda r: scope == r["scope"], simple_rules))
    rules = list(filter(lambda r: r["context"] == "na" or context == r["context"], rules))
    rules = list(filter(lambda r: r["ownership"] == "na" or ownership == r["ownership"], rules))
    rules = list(filter(lambda r: r["membership"] == "na" or membership == r["membership"], rules))
    rules = list(filter(lambda r: GROUPS.index(privilege) <= GROUPS.index(r["privilege"]), rules))
    resource = data["resource"]
    rules = list(
        filter(
            lambda r: not r["limit"]
            or r["limit"].startswith("filter")
            or eval(r["limit"], {"resource": resource}),
            rules,
        )
    )

    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": {**resource, "owner": {"id": random.randrange(300, 400)}} if resource else None,
    }

    user_id = data["auth"]["user"]["id"]
    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        name += "_" + str(k)
        if isinstance(v, dict):
            name += _get_name("", **v)
        else:
            name += f"_{str(v).upper()}"

    return name


def get_name(scope, context, ownership, privilege, membership, resource):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and resource is not None:
        return False
    if resource is None and scope != "list":
        return False
    if scope == "list" and ownership != "None":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES
        ):
            for resource in RESOURCES(ownership):
                if not is_valid(scope, context, ownership, privilege, membership, resource):
                    continue

                test_name = get_name(scope, context, ownership, privilege, membership, resource)
                data = get_data(scope, context, ownership, privilege, membership, resource)
                result = eval_rule(scope, context, ownership, privilege, membership, data)
                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )

        # Write the script which is used to generate the file
        with open(sys.argv[0]) as this_file:
            f.write(f"\n\n# {os.path.split(sys.argv[0])[1]}\n")
            for line in this_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")

        # Write rules which are used to generate the file
        with open(os.path.join(sys.argv[1], f"{name}.csv")) as rego_file:
            f.write(f"\n\n# {name}.csv\n")
            for line in rego_file:
                if line.strip():
                    f.write(f"# {line}")
                else:
                    f.write(f"#\n")


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\apps.py =====
# Copyright (C) 2023 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class QualityControlConfig(AppConfig):
    name = "cvat.apps.quality_control"

    def ready(self) -> None:
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)

        # Required to define signals in the application
        from . import signals  # pylint: disable=unused-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\models.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from collections.abc import Sequence
from copy import deepcopy
from enum import Enum
from typing import TYPE_CHECKING, Any

from django.core.exceptions import ValidationError
from django.db import models
from django.forms.models import model_to_dict

from cvat.apps.engine.models import Job, ShapeType, Task, User

if TYPE_CHECKING:
    from cvat.apps.organizations.models import Organization


class AnnotationConflictType(str, Enum):
    MISSING_ANNOTATION = "missing_annotation"
    EXTRA_ANNOTATION = "extra_annotation"
    MISMATCHING_LABEL = "mismatching_label"
    LOW_OVERLAP = "low_overlap"
    MISMATCHING_DIRECTION = "mismatching_direction"
    MISMATCHING_ATTRIBUTES = "mismatching_attributes"
    MISMATCHING_GROUPS = "mismatching_groups"
    COVERED_ANNOTATION = "covered_annotation"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class AnnotationConflictSeverity(str, Enum):
    WARNING = "warning"
    ERROR = "error"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class MismatchingAnnotationKind(str, Enum):
    ATTRIBUTE = "attribute"
    LABEL = "label"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class QualityReportTarget(str, Enum):
    JOB = "job"
    TASK = "task"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class QualityTargetMetricType(str, Enum):
    ACCURACY = "accuracy"
    PRECISION = "precision"
    RECALL = "recall"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class QualityReport(models.Model):
    job = models.ForeignKey(
        Job, on_delete=models.CASCADE, related_name="quality_reports", null=True, blank=True
    )
    task = models.ForeignKey(
        Task, on_delete=models.CASCADE, related_name="quality_reports", null=True, blank=True
    )

    parent = models.ForeignKey(
        "self", on_delete=models.CASCADE, related_name="children", null=True, blank=True
    )
    children: Sequence[QualityReport]

    created_date = models.DateTimeField(auto_now_add=True)
    target_last_updated = models.DateTimeField()
    gt_last_updated = models.DateTimeField()

    assignee = models.ForeignKey(
        User, on_delete=models.SET_NULL, related_name="quality_reports", null=True, blank=True
    )
    assignee_last_updated = models.DateTimeField(null=True)

    data = models.JSONField()

    conflicts: Sequence[AnnotationConflict]

    @property
    def target(self) -> QualityReportTarget:
        if self.job:
            return QualityReportTarget.JOB
        elif self.task:
            return QualityReportTarget.TASK
        else:
            assert False

    def _parse_report(self):
        from cvat.apps.quality_control.quality_reports import ComparisonReport

        return ComparisonReport.from_json(self.data)

    @property
    def summary(self):
        report = self._parse_report()
        return report.comparison_summary

    def get_task(self) -> Task:
        if self.task is not None:
            return self.task
        else:
            return self.job.segment.task

    def get_json_report(self) -> str:
        return self.data

    def clean(self):
        if not (self.job is not None) ^ (self.task is not None):
            raise ValidationError("One of the 'job' and 'task' fields must be set")

    @property
    def organization_id(self) -> int | None:
        if task := self.get_task():
            return task.organization_id
        return None

    @property
    def organization(self) -> Organization | None:
        if task := self.get_task():
            return task.organization
        return None


class AnnotationConflict(models.Model):
    report = models.ForeignKey(QualityReport, on_delete=models.CASCADE, related_name="conflicts")
    frame = models.PositiveIntegerField()
    type = models.CharField(max_length=32, choices=AnnotationConflictType.choices())
    severity = models.CharField(max_length=32, choices=AnnotationConflictSeverity.choices())

    annotation_ids: Sequence[AnnotationId]

    @property
    def organization_id(self):
        return self.report.organization_id


class AnnotationType(str, Enum):
    TAG = "tag"
    SHAPE = "shape"
    TRACK = "track"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class AnnotationId(models.Model):
    conflict = models.ForeignKey(
        AnnotationConflict, on_delete=models.CASCADE, related_name="annotation_ids"
    )

    obj_id = models.PositiveIntegerField()
    job_id = models.PositiveIntegerField()
    type = models.CharField(max_length=32, choices=AnnotationType.choices())
    shape_type = models.CharField(
        max_length=32, choices=ShapeType.choices(), null=True, default=None
    )

    def clean(self) -> None:
        if self.type in [AnnotationType.SHAPE, AnnotationType.TRACK]:
            if not self.shape_type:
                raise ValidationError("Annotation kind must be specified")
        elif self.type == AnnotationType.TAG:
            if self.shape_type:
                raise ValidationError("Annotation kind must be empty")
        else:
            raise ValidationError(f"Unexpected type value '{self.type}'")


class PointSizeBase(str, Enum):
    IMAGE_SIZE = "image_size"
    GROUP_BBOX_SIZE = "group_bbox_size"

    def __str__(self) -> str:
        return self.value

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)


class QualitySettings(models.Model):
    task = models.OneToOneField(Task, on_delete=models.CASCADE, related_name="quality_settings")

    iou_threshold = models.FloatField()
    oks_sigma = models.FloatField()
    line_thickness = models.FloatField()

    low_overlap_threshold = models.FloatField()

    point_size_base = models.CharField(
        max_length=32, choices=PointSizeBase.choices(), default=PointSizeBase.GROUP_BBOX_SIZE
    )

    compare_line_orientation = models.BooleanField()
    line_orientation_threshold = models.FloatField()

    compare_groups = models.BooleanField()
    group_match_threshold = models.FloatField()

    check_covered_annotations = models.BooleanField()
    object_visibility_threshold = models.FloatField()

    panoptic_comparison = models.BooleanField()

    compare_attributes = models.BooleanField()

    empty_is_annotated = models.BooleanField(default=False)

    target_metric = models.CharField(
        max_length=32,
        choices=QualityTargetMetricType.choices(),
        default=QualityTargetMetricType.ACCURACY,
    )

    target_metric_threshold = models.FloatField(default=0.7)

    max_validations_per_job = models.PositiveIntegerField(default=0)

    def __init__(self, *args: Any, **kwargs: Any) -> None:
        defaults = deepcopy(self.get_defaults())
        for field in self._meta.fields:
            if field.name in defaults:
                field.default = defaults[field.name]

        super().__init__(*args, **kwargs)

    @classmethod
    def get_defaults(cls) -> dict:
        import cvat.apps.quality_control.quality_reports as qc

        default_settings = qc.DatasetComparator.DEFAULT_SETTINGS.to_dict()

        existing_fields = {f.name for f in cls._meta.fields}
        return {k: v for k, v in default_settings.items() if k in existing_fields}

    def to_dict(self):
        return model_to_dict(self)

    @property
    def organization_id(self):
        return self.task.organization_id


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import Optional, Union, cast

from django.conf import settings
from rest_framework.exceptions import PermissionDenied, ValidationError

from cvat.apps.engine.models import Project, Task
from cvat.apps.engine.permissions import TaskPermission
from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum, get_iam_context

from .models import AnnotationConflict, QualityReport, QualitySettings


class QualityReportPermission(OpenPolicyAgentPermission):
    obj: Optional[QualityReport]
    rq_job_owner_id: Optional[int]
    task_id: Optional[int]

    class Scopes(StrEnum):
        LIST = "list"
        CREATE = "create"
        VIEW = "view"
        VIEW_STATUS = "view:status"

    @classmethod
    def create_scope_check_status(cls, request, rq_job_owner_id: int, iam_context=None):
        if not iam_context and request:
            iam_context = get_iam_context(request, None)
        return cls(**iam_context, scope=cls.Scopes.VIEW_STATUS, rq_job_owner_id=rq_job_owner_id)

    @classmethod
    def create_scope_view(cls, request, report: Union[int, QualityReport], iam_context=None):
        if isinstance(report, int):
            try:
                report = QualityReport.objects.get(id=report)
            except QualityReport.DoesNotExist as ex:
                raise ValidationError(str(ex))

        # Access rights are the same as in the owning task
        # This component doesn't define its own rules in this case
        return TaskPermission.create_scope_view(
            request,
            task=report.get_task(),
            iam_context=iam_context,
        )

    @classmethod
    def create(cls, request, view, obj, iam_context):
        Scopes = __class__.Scopes

        permissions = []
        if view.basename == "quality_reports":
            for scope in cls.get_scopes(request, view, obj):
                if scope == Scopes.VIEW:
                    permissions.append(cls.create_scope_view(request, obj, iam_context=iam_context))
                elif scope == Scopes.LIST and isinstance(obj, Task):
                    permissions.append(TaskPermission.create_scope_view(request, task=obj))
                elif scope == Scopes.CREATE:
                    # Note: POST /api/quality/reports is used to initiate report creation and to check the process status
                    rq_id = request.query_params.get("rq_id")
                    task_id = request.data.get("task_id")

                    if not (task_id or rq_id):
                        raise PermissionDenied("Either task_id or rq_id must be specified")

                    if rq_id:
                        # There will be another check for this case during request processing
                        continue

                    if task_id is not None:
                        # The request may have a different org or org unset
                        # Here we need to retrieve iam_context for this user, based on the task_id
                        try:
                            task = Task.objects.get(id=task_id)
                        except Task.DoesNotExist:
                            raise ValidationError("The specified task does not exist")

                        iam_context = get_iam_context(request, task)

                        permissions.append(
                            TaskPermission.create_scope_view(request, task, iam_context=iam_context)
                        )

                    permissions.append(
                        cls.create_base_perm(
                            request,
                            view,
                            scope,
                            iam_context,
                            obj,
                            task_id=task_id,
                        )
                    )

                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        if "rq_job_owner_id" in kwargs:
            self.rq_job_owner_id = int(kwargs.pop("rq_job_owner_id"))

        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/quality_reports/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
                "create": Scopes.CREATE,
                "retrieve": Scopes.VIEW,
                "data": Scopes.VIEW,
            }[view.action]
        ]

    def get_resource(self):
        data = None

        if self.obj or self.scope == self.Scopes.CREATE:
            task: Optional[Task] = None
            project: Optional[Project] = None
            obj_id: Optional[int] = None

            if self.obj:
                obj_id = self.obj.id
                task = self.obj.get_task()
            elif self.scope == self.Scopes.CREATE and self.task_id:
                try:
                    task = Task.objects.get(id=self.task_id)
                except Task.DoesNotExist:
                    raise ValidationError("The specified task does not exist")

            if task and task.project:
                project = task.project
                organization_id = project.organization_id
            else:
                organization_id = task.organization_id

            data = {
                "id": obj_id,
                "organization": {"id": organization_id},
                "task": (
                    {
                        "owner": {"id": task.owner_id},
                        "assignee": {"id": task.assignee_id},
                    }
                    if task
                    else None
                ),
                "project": (
                    {
                        "owner": {"id": project.owner_id},
                        "assignee": {"id": project.assignee_id},
                    }
                    if project
                    else None
                ),
            }
        elif self.scope == self.Scopes.VIEW_STATUS:
            data = {"owner": {"id": self.rq_job_owner_id}}

        return data


class AnnotationConflictPermission(OpenPolicyAgentPermission):
    obj: Optional[AnnotationConflict]

    class Scopes(StrEnum):
        LIST = "list"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "annotation_conflicts":
            for scope in cls.get_scopes(request, view, obj):
                if scope == cls.Scopes.LIST and isinstance(obj, QualityReport):
                    permissions.append(
                        QualityReportPermission.create_scope_view(
                            request,
                            obj,
                            iam_context=iam_context,
                        )
                    )
                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/conflicts/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
            }[view.action]
        ]

    def get_resource(self):
        return None


class QualitySettingPermission(OpenPolicyAgentPermission):
    obj: Optional[QualitySettings]

    class Scopes(StrEnum):
        LIST = "list"
        VIEW = "view"
        UPDATE = "update"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        Scopes = __class__.Scopes

        permissions = []
        if view.basename == "quality_settings":
            for scope in cls.get_scopes(request, view, obj):
                if scope in [Scopes.VIEW, Scopes.UPDATE]:
                    obj = cast(QualitySettings, obj)

                    if scope == Scopes.VIEW:
                        task_scope = TaskPermission.Scopes.VIEW
                    elif scope == Scopes.UPDATE:
                        task_scope = TaskPermission.Scopes.UPDATE_DESC
                    else:
                        assert False

                    # Access rights are the same as in the owning task
                    # This component doesn't define its own rules in this case
                    permissions.append(
                        TaskPermission.create_base_perm(
                            request, view, iam_context=iam_context, scope=task_scope, obj=obj.task
                        )
                    )
                elif scope == cls.Scopes.LIST:
                    if task_id := request.query_params.get("task_id", None):
                        permissions.append(
                            TaskPermission.create_scope_view(
                                request,
                                int(task_id),
                                iam_context=iam_context,
                            )
                        )

                    permissions.append(cls.create_scope_list(request, iam_context))
                else:
                    permissions.append(cls.create_base_perm(request, view, scope, iam_context, obj))

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/quality_settings/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        return [
            {
                "list": Scopes.LIST,
                "retrieve": Scopes.VIEW,
                "partial_update": Scopes.UPDATE,
            }[view.action]
        ]

    def get_resource(self):
        data = None

        if self.obj:
            task = self.obj.task
            if task.project:
                organization_id = task.project.organization_id
            else:
                organization_id = task.organization_id

            data = {
                "id": self.obj.id,
                "organization": {"id": organization_id},
                "task": (
                    {
                        "owner": {"id": task.owner_id},
                        "assignee": {"id": task.assignee_id},
                    }
                    if task
                    else None
                ),
                "project": (
                    {
                        "owner": {"id": task.project.owner_id},
                        "assignee": {"id": task.project.assignee_id},
                    }
                    if task.project
                    else None
                ),
            }

        return data


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\quality_reports.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import itertools
import math
from collections import Counter
from collections.abc import Hashable, Sequence
from copy import deepcopy
from functools import cached_property, partial
from typing import Any, Callable, Optional, TypeVar, Union, cast

import datumaro as dm
import datumaro.components.annotations.matcher
import datumaro.components.comparator
import datumaro.util.annotation_util
import datumaro.util.mask_tools
import django_rq
import numpy as np
import rq
from attrs import asdict, define, fields_dict
from datumaro.util import dump_json, parse_json
from django.conf import settings
from django.db import transaction
from django_rq.queues import DjangoRQ as RqQueue
from rq.job import Job as RqJob
from scipy.optimize import linear_sum_assignment

from cvat.apps.dataset_manager.bindings import (
    CommonData,
    CvatToDmAnnotationConverter,
    GetCVATDataExtractor,
    JobData,
    match_dm_item,
)
from cvat.apps.dataset_manager.formats.registry import dm_env
from cvat.apps.dataset_manager.task import JobAnnotation
from cvat.apps.engine import serializers as engine_serializers
from cvat.apps.engine.frame_provider import TaskFrameProvider
from cvat.apps.engine.model_utils import bulk_create
from cvat.apps.engine.models import (
    DimensionType,
    Image,
    Job,
    JobType,
    ShapeType,
    StageChoice,
    StatusChoice,
    Task,
    User,
    ValidationMode,
)
from cvat.apps.engine.rq import BaseRQMeta, define_dependent_job
from cvat.apps.engine.types import ExtendedRequest
from cvat.apps.engine.utils import get_rq_lock_by_user, get_rq_lock_for_job
from cvat.apps.profiler import silk_profile
from cvat.apps.quality_control import models
from cvat.apps.quality_control.models import (
    AnnotationConflictSeverity,
    AnnotationConflictType,
    AnnotationType,
)


class Serializable:
    def _value_serializer(self, v):
        if isinstance(v, Serializable):
            return v.to_dict()
        elif isinstance(v, (list, tuple, set, frozenset)):
            return [self._value_serializer(vv) for vv in v]
        elif isinstance(v, dict):
            return {self._value_serializer(vk): self._value_serializer(vv) for vk, vv in v.items()}
        else:
            return v

    def to_dict(self) -> dict:
        return self._value_serializer(self._fields_dict())

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        d = asdict(self, recurse=False)

        for field_name in include_properties or []:
            d[field_name] = getattr(self, field_name)

        return d

    @classmethod
    def from_dict(cls, d: dict):
        raise NotImplementedError("Must be implemented in the subclass")


@define(kw_only=True)
class AnnotationId(Serializable):
    obj_id: int
    job_id: int
    type: AnnotationType
    shape_type: Optional[ShapeType]

    def _value_serializer(self, v):
        if isinstance(v, (AnnotationType, ShapeType)):
            return str(v)
        else:
            return super()._value_serializer(v)

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            obj_id=d["obj_id"],
            job_id=d["job_id"],
            type=AnnotationType(d["type"]),
            shape_type=ShapeType(d["shape_type"]) if d.get("shape_type") else None,
        )


@define(kw_only=True)
class AnnotationConflict(Serializable):
    frame_id: int
    type: AnnotationConflictType
    annotation_ids: list[AnnotationId]

    @property
    def severity(self) -> AnnotationConflictSeverity:
        if self.type in [
            AnnotationConflictType.MISSING_ANNOTATION,
            AnnotationConflictType.EXTRA_ANNOTATION,
            AnnotationConflictType.MISMATCHING_LABEL,
        ]:
            severity = AnnotationConflictSeverity.ERROR
        elif self.type in [
            AnnotationConflictType.LOW_OVERLAP,
            AnnotationConflictType.MISMATCHING_ATTRIBUTES,
            AnnotationConflictType.MISMATCHING_DIRECTION,
            AnnotationConflictType.MISMATCHING_GROUPS,
            AnnotationConflictType.COVERED_ANNOTATION,
        ]:
            severity = AnnotationConflictSeverity.WARNING
        else:
            assert False

        return severity

    def _value_serializer(self, v):
        if isinstance(v, (AnnotationConflictType, AnnotationConflictSeverity)):
            return str(v)
        else:
            return super()._value_serializer(v)

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(include_properties=include_properties or ["severity"])

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            frame_id=d["frame_id"],
            type=AnnotationConflictType(d["type"]),
            annotation_ids=list(AnnotationId.from_dict(v) for v in d["annotation_ids"]),
        )


@define(kw_only=True)
class ComparisonParameters(Serializable):
    included_annotation_types: list[dm.AnnotationType] = [
        dm.AnnotationType.bbox,
        dm.AnnotationType.points,
        dm.AnnotationType.mask,
        dm.AnnotationType.polygon,
        dm.AnnotationType.polyline,
        dm.AnnotationType.skeleton,
        dm.AnnotationType.label,
    ]

    non_groupable_ann_type = dm.AnnotationType.label
    "Annotation type that can't be grouped"

    compare_attributes: bool = True
    "Enables or disables attribute checks"

    ignored_attributes: list[str] = []

    iou_threshold: float = 0.4
    "Used for distinction between matched / unmatched shapes"

    low_overlap_threshold: float = 0.8
    "Used for distinction between strong / weak (low_overlap) matches"

    oks_sigma: float = 0.09
    "Like IoU threshold, but for points, % of the bbox area to match a pair of points"

    point_size_base: models.PointSizeBase = models.PointSizeBase.GROUP_BBOX_SIZE
    "Determines how to obtain the object size for point comparisons"

    line_thickness: float = 0.01
    "Thickness of polylines, relatively to the (image area) ^ 0.5"

    compare_line_orientation: bool = True
    "Indicates that polylines have direction"

    line_orientation_threshold: float = 0.1
    """
    The minimal gain in the IoU between the given and reversed line directions
    to count the line inverted
    """

    compare_groups: bool = True
    "Enables or disables group checks"

    group_match_threshold: float = 0.5
    "Minimal IoU for groups to be considered matching"

    check_covered_annotations: bool = True
    "Check for fully-covered annotations"

    object_visibility_threshold: float = 0.05
    "Minimal visible area % of the spatial annotations"

    panoptic_comparison: bool = True
    "Use only the visible part of the masks and polygons in comparisons"

    empty_is_annotated: bool = False
    """
    Consider unannotated (empty) frames virtually annotated as "nothing".
    If disabled, quality metrics, such as accuracy, will be 0 if both GT and DS frames
    have no annotations. When enabled, they will be 1 instead.
    This will also add virtual annotations to empty frames in the comparison results.
    """

    def _value_serializer(self, v):
        if isinstance(v, dm.AnnotationType):
            return str(v.name)
        else:
            return super()._value_serializer(v)

    @classmethod
    def from_dict(cls, d: dict):
        fields = fields_dict(cls)
        return cls(**{field_name: d[field_name] for field_name in fields if field_name in d})


@define(kw_only=True)
class ConfusionMatrix(Serializable):
    labels: list[str]
    rows: np.ndarray
    precision: np.ndarray
    recall: np.ndarray
    accuracy: np.ndarray
    jaccard_index: Optional[np.ndarray]

    @property
    def axes(self):
        return dict(cols="gt", rows="ds")

    def _value_serializer(self, v):
        if isinstance(v, np.ndarray):
            return v.tolist()
        else:
            return super()._value_serializer(v)

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(include_properties=include_properties or ["axes"])

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            labels=d["labels"],
            rows=np.asarray(d["rows"]),
            precision=np.asarray(d["precision"]),
            recall=np.asarray(d["recall"]),
            accuracy=np.asarray(d["accuracy"]),
            # This field didn't exist at first, so it might not be present
            # in old serialized instances.
            jaccard_index=np.asarray(d["jaccard_index"]) if "jaccard_index" in d else None,
        )


@define(kw_only=True)
class ComparisonReportAnnotationsSummary(Serializable):
    valid_count: int
    missing_count: int
    extra_count: int
    total_count: int
    ds_count: int
    gt_count: int
    confusion_matrix: ConfusionMatrix

    @property
    def accuracy(self) -> float:
        return self.valid_count / (self.total_count or 1)

    @property
    def precision(self) -> float:
        return self.valid_count / (self.ds_count or 1)

    @property
    def recall(self) -> float:
        return self.valid_count / (self.gt_count or 1)

    def accumulate(self, other: ComparisonReportAnnotationsSummary):
        for field in [
            "valid_count",
            "missing_count",
            "extra_count",
            "total_count",
            "ds_count",
            "gt_count",
        ]:
            setattr(self, field, getattr(self, field) + getattr(other, field))

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(
            include_properties=include_properties or ["accuracy", "precision", "recall"]
        )

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            valid_count=d["valid_count"],
            missing_count=d["missing_count"],
            extra_count=d["extra_count"],
            total_count=d["total_count"],
            ds_count=d["ds_count"],
            gt_count=d["gt_count"],
            confusion_matrix=ConfusionMatrix.from_dict(d["confusion_matrix"]),
        )


@define(kw_only=True)
class ComparisonReportAnnotationShapeSummary(Serializable):
    valid_count: int
    missing_count: int
    extra_count: int
    total_count: int
    ds_count: int
    gt_count: int
    mean_iou: float

    @property
    def accuracy(self) -> float:
        return self.valid_count / (self.total_count or 1)

    def accumulate(self, other: ComparisonReportAnnotationShapeSummary):
        for field in [
            "valid_count",
            "missing_count",
            "extra_count",
            "total_count",
            "ds_count",
            "gt_count",
        ]:
            setattr(self, field, getattr(self, field) + getattr(other, field))

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(include_properties=include_properties or ["accuracy"])

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            valid_count=d["valid_count"],
            missing_count=d["missing_count"],
            extra_count=d["extra_count"],
            total_count=d["total_count"],
            ds_count=d["ds_count"],
            gt_count=d["gt_count"],
            mean_iou=d["mean_iou"],
        )


@define(kw_only=True)
class ComparisonReportAnnotationLabelSummary(Serializable):
    valid_count: int
    invalid_count: int
    total_count: int

    @property
    def accuracy(self) -> float:
        return self.valid_count / (self.total_count or 1)

    def accumulate(self, other: ComparisonReportAnnotationLabelSummary):
        for field in ["valid_count", "total_count", "invalid_count"]:
            setattr(self, field, getattr(self, field) + getattr(other, field))

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(include_properties=include_properties or ["accuracy"])

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            valid_count=d["valid_count"],
            invalid_count=d["invalid_count"],
            total_count=d["total_count"],
        )


@define(kw_only=True)
class ComparisonReportAnnotationComponentsSummary(Serializable):
    shape: ComparisonReportAnnotationShapeSummary
    label: ComparisonReportAnnotationLabelSummary

    def accumulate(self, other: ComparisonReportAnnotationComponentsSummary):
        self.shape.accumulate(other.shape)
        self.label.accumulate(other.label)

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            shape=ComparisonReportAnnotationShapeSummary.from_dict(d["shape"]),
            label=ComparisonReportAnnotationLabelSummary.from_dict(d["label"]),
        )


@define(kw_only=True)
class ComparisonReportComparisonSummary(Serializable):
    frame_share: float
    frames: list[str]

    @property
    def mean_conflict_count(self) -> float:
        return self.conflict_count / (len(self.frames) or 1)

    conflict_count: int
    warning_count: int
    error_count: int
    conflicts_by_type: dict[AnnotationConflictType, int]

    annotations: ComparisonReportAnnotationsSummary
    annotation_components: ComparisonReportAnnotationComponentsSummary

    @property
    def frame_count(self) -> int:
        return len(self.frames)

    def _value_serializer(self, v):
        if isinstance(v, AnnotationConflictType):
            return str(v)
        else:
            return super()._value_serializer(v)

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(
            include_properties=include_properties
            or [
                "frame_count",
                "mean_conflict_count",
                "warning_count",
                "error_count",
                "conflicts_by_type",
            ]
        )

    @classmethod
    def from_dict(cls, d: dict):
        return cls(
            frame_share=d["frame_share"],
            frames=list(d["frames"]),
            conflict_count=d["conflict_count"],
            warning_count=d.get("warning_count", 0),
            error_count=d.get("error_count", 0),
            conflicts_by_type={
                AnnotationConflictType(k): v for k, v in d.get("conflicts_by_type", {}).items()
            },
            annotations=ComparisonReportAnnotationsSummary.from_dict(d["annotations"]),
            annotation_components=ComparisonReportAnnotationComponentsSummary.from_dict(
                d["annotation_components"]
            ),
        )


@define(kw_only=True, init=False)
class ComparisonReportFrameSummary(Serializable):
    conflicts: list[AnnotationConflict]

    @cached_property
    def conflict_count(self) -> int:
        return len(self.conflicts)

    @cached_property
    def warning_count(self) -> int:
        return len([c for c in self.conflicts if c.severity == AnnotationConflictSeverity.WARNING])

    @cached_property
    def error_count(self) -> int:
        return len([c for c in self.conflicts if c.severity == AnnotationConflictSeverity.ERROR])

    @cached_property
    def conflicts_by_type(self) -> dict[AnnotationConflictType, int]:
        return Counter(c.type for c in self.conflicts)

    annotations: ComparisonReportAnnotationsSummary
    annotation_components: ComparisonReportAnnotationComponentsSummary

    _CACHED_FIELDS = ["conflict_count", "warning_count", "error_count", "conflicts_by_type"]

    def _value_serializer(self, v):
        if isinstance(v, AnnotationConflictType):
            return str(v)
        else:
            return super()._value_serializer(v)

    def __init__(self, *args, **kwargs):
        # these fields are optional, but can be computed on access
        for field_name in self._CACHED_FIELDS:
            if field_name in kwargs:
                setattr(self, field_name, kwargs.pop(field_name))

        self.__attrs_init__(*args, **kwargs)

    def _fields_dict(self, *, include_properties: Optional[list[str]] = None) -> dict:
        return super()._fields_dict(include_properties=include_properties or self._CACHED_FIELDS)

    @classmethod
    def from_dict(cls, d: dict):
        optional_fields = set(cls._CACHED_FIELDS) - {
            "conflicts_by_type"  # requires extra conversion
        }
        return cls(
            **{field: d[field] for field in optional_fields if field in d},
            **(
                dict(
                    conflicts_by_type={
                        AnnotationConflictType(k): v for k, v in d["conflicts_by_type"].items()
                    }
                )
                if "conflicts_by_type" in d
                else {}
            ),
            conflicts=[AnnotationConflict.from_dict(v) for v in d["conflicts"]],
            annotations=ComparisonReportAnnotationsSummary.from_dict(d["annotations"]),
            annotation_components=ComparisonReportAnnotationComponentsSummary.from_dict(
                d["annotation_components"]
            ),
        )


@define(kw_only=True)
class ComparisonReport(Serializable):
    parameters: ComparisonParameters
    comparison_summary: ComparisonReportComparisonSummary
    frame_results: dict[int, ComparisonReportFrameSummary]

    @property
    def conflicts(self) -> list[AnnotationConflict]:
        return list(itertools.chain.from_iterable(r.conflicts for r in self.frame_results.values()))

    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> ComparisonReport:
        return cls(
            parameters=ComparisonParameters.from_dict(d["parameters"]),
            comparison_summary=ComparisonReportComparisonSummary.from_dict(d["comparison_summary"]),
            frame_results={
                int(k): ComparisonReportFrameSummary.from_dict(v)
                for k, v in d["frame_results"].items()
            },
        )

    def to_json(self) -> str:
        d = self.to_dict()

        # String keys are needed for json dumping
        d["frame_results"] = {str(k): v for k, v in d["frame_results"].items()}
        return dump_json(d).decode()

    @classmethod
    def from_json(cls, data: str) -> ComparisonReport:
        return cls.from_dict(parse_json(data))


class JobDataProvider:
    @classmethod
    def add_prefetch_info(cls, queryset):
        return JobAnnotation.add_prefetch_info(queryset)

    @transaction.atomic
    def __init__(self, job_id: int, *, queryset=None, included_frames=None) -> None:
        self.job_id = job_id
        self.job_annotation = JobAnnotation(job_id, queryset=queryset)
        self.job_annotation.init_from_db()
        self.job_data = JobData(
            annotation_ir=self.job_annotation.ir_data,
            db_job=self.job_annotation.db_job,
            use_server_track_ids=True,
            included_frames=included_frames,
        )

        self._annotation_memo = _MemoizingAnnotationConverterFactory()

    @cached_property
    def dm_dataset(self):
        extractor = GetCVATDataExtractor(self.job_data, convert_annotations=self._annotation_memo)
        return dm.Dataset.from_extractors(extractor, env=dm_env)

    def dm_item_id_to_frame_id(self, item: dm.DatasetItem) -> int:
        return match_dm_item(item, self.job_data)

    def dm_ann_to_ann_id(self, ann: dm.Annotation) -> AnnotationId:
        source_ann = self._annotation_memo.get_source_ann(ann)
        if "track_id" in ann.attributes:
            source_ann_id = source_ann.track_id
            ann_type = AnnotationType.TRACK
            shape_type = source_ann.type
        else:
            if isinstance(source_ann, CommonData.LabeledShape):
                ann_type = AnnotationType.SHAPE
                shape_type = source_ann.type
            elif isinstance(source_ann, CommonData.Tag):
                ann_type = AnnotationType.TAG
                shape_type = None
            else:
                assert False

            source_ann_id = source_ann.id

        return AnnotationId(
            obj_id=source_ann_id, type=ann_type, shape_type=shape_type, job_id=self.job_id
        )


class _MemoizingAnnotationConverterFactory:
    def __init__(self):
        self._annotation_mapping = {}  # dm annotation -> cvat annotation

    def remember_conversion(self, cvat_ann, dm_anns):
        for dm_ann in dm_anns:
            self._annotation_mapping[self._make_key(dm_ann)] = cvat_ann

    def _make_key(self, dm_ann: dm.Annotation) -> Hashable:
        return id(dm_ann)

    def get_source_ann(
        self, dm_ann: dm.Annotation
    ) -> Union[CommonData.Tag, CommonData.LabeledShape]:
        return self._annotation_mapping[self._make_key(dm_ann)]

    def clear(self):
        self._annotation_mapping.clear()

    def __call__(self, *args, **kwargs) -> list[dm.Annotation]:
        converter = _MemoizingAnnotationConverter(*args, factory=self, **kwargs)
        return converter.convert()


class _MemoizingAnnotationConverter(CvatToDmAnnotationConverter):
    def __init__(self, *args, factory: _MemoizingAnnotationConverterFactory, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self._factory = factory

    def _convert_tag(self, tag):
        converted = list(super()._convert_tag(tag))
        for dm_ann in converted:
            dm_ann.id = tag.id

        self._factory.remember_conversion(tag, converted)
        return converted

    def _convert_shape(self, shape, *, index):
        converted = list(super()._convert_shape(shape, index=index))
        for dm_ann in converted:
            dm_ann.id = shape.id

        self._factory.remember_conversion(shape, converted)
        return converted


_ShapeT1 = TypeVar("_ShapeT1")
_ShapeT2 = TypeVar("_ShapeT2")
ShapeSimilarityFunction = Callable[
    [_ShapeT1, _ShapeT2], float
]  # (shape1, shape2) -> [0; 1], returns 0 for mismatches, 1 for matches
LabelEqualityFunction = Callable[[_ShapeT1, _ShapeT2], bool]
SegmentMatchingResult = tuple[
    list[tuple[_ShapeT1, _ShapeT2]],  # matches
    list[tuple[_ShapeT1, _ShapeT2]],  # mismatches
    list[_ShapeT1],  # a unmatched
    list[_ShapeT2],  # b unmatched
]


def match_segments(
    a_segms: Sequence[_ShapeT1],
    b_segms: Sequence[_ShapeT2],
    *,
    distance: ShapeSimilarityFunction[_ShapeT1, _ShapeT2],
    dist_thresh: float = 1.0,
    label_matcher: LabelEqualityFunction[_ShapeT1, _ShapeT2] = lambda a, b: a.label == b.label,
) -> SegmentMatchingResult[_ShapeT1, _ShapeT2]:
    # Comparing to the dm version, this one changes the algorithm to match shapes first
    # label comparison is only used to distinguish between matches and mismatches

    assert callable(distance), distance
    assert callable(label_matcher), label_matcher

    max_anns = max(len(a_segms), len(b_segms))
    distances = np.array(
        [
            [
                1 - distance(a, b) if a is not None and b is not None else 1
                for b, _ in itertools.zip_longest(b_segms, range(max_anns), fillvalue=None)
            ]
            for a, _ in itertools.zip_longest(a_segms, range(max_anns), fillvalue=None)
        ]
    )
    distances[~np.isfinite(distances)] = 1
    distances[distances > 1 - dist_thresh] = 1

    if a_segms and b_segms:
        a_matches, b_matches = linear_sum_assignment(distances)
    else:
        a_matches = []
        b_matches = []

    # matches: segments we succeeded to match completely
    # mispred: segments we succeeded to match, having label mismatch
    matches = []
    mispred = []
    # *_umatched: segments of (*) we failed to match
    a_unmatched = []
    b_unmatched = []

    for a_idx, b_idx in zip(a_matches, b_matches):
        dist = distances[a_idx, b_idx]
        if dist > 1 - dist_thresh or dist == 1:
            if a_idx < len(a_segms):
                a_unmatched.append(a_segms[a_idx])
            if b_idx < len(b_segms):
                b_unmatched.append(b_segms[b_idx])
        else:
            a_ann = a_segms[a_idx]
            b_ann = b_segms[b_idx]
            if label_matcher(a_ann, b_ann):
                matches.append((a_ann, b_ann))
            else:
                mispred.append((a_ann, b_ann))

    if not len(a_matches) and not len(b_matches):
        a_unmatched = list(a_segms)
        b_unmatched = list(b_segms)

    return matches, mispred, a_unmatched, b_unmatched


def oks(a, b, sigma=0.1, bbox=None, scale=None, visibility_a=None, visibility_b=None):
    """
    Object Keypoint Similarity metric.
    https://cocodataset.org/#keypoints-eval
    """

    p1 = np.array(a.points).reshape((-1, 2))
    p2 = np.array(b.points).reshape((-1, 2))
    if len(p1) != len(p2):
        return 0

    if visibility_a is None:
        visibility_a = np.full(len(p1), True)
    else:
        visibility_a = np.asarray(visibility_a, dtype=bool)

    if visibility_b is None:
        visibility_b = np.full(len(p2), True)
    else:
        visibility_b = np.asarray(visibility_b, dtype=bool)

    if not scale:
        if bbox is None:
            bbox = datumaro.util.annotation_util.mean_bbox([a, b])
        scale = bbox[2] * bbox[3]

    dists = np.linalg.norm(p1 - p2, axis=1)
    return np.sum(
        visibility_a
        * visibility_b
        * np.exp((visibility_a == visibility_b) * (-(dists**2) / (2 * scale * (2 * sigma) ** 2)))
    ) / np.sum(visibility_a | visibility_b, dtype=float)


@define(kw_only=True)
class KeypointsMatcher(datumaro.components.annotations.matcher.PointsMatcher):
    def distance(self, a: dm.Points, b: dm.Points) -> float:
        a_bbox = self.instance_map[id(a)][1]
        b_bbox = self.instance_map[id(b)][1]
        if datumaro.util.annotation_util.bbox_iou(a_bbox, b_bbox) <= 0:
            return 0

        bbox = datumaro.util.annotation_util.mean_bbox([a_bbox, b_bbox])
        return oks(
            a,
            b,
            sigma=self.sigma,
            bbox=bbox,
            visibility_a=[v == dm.Points.Visibility.visible for v in a.visibility],
            visibility_b=[v == dm.Points.Visibility.visible for v in b.visibility],
        )


def _arr_div(a_arr: np.ndarray, b_arr: np.ndarray) -> np.ndarray:
    divisor = b_arr.copy()
    divisor[b_arr == 0] = 1
    return a_arr / divisor


def to_rle(ann: dm.Annotation, *, img_h: int, img_w: int):
    from pycocotools import mask as mask_utils

    if ann.type == dm.AnnotationType.polygon:
        return mask_utils.frPyObjects([ann.points], img_h, img_w)
    elif isinstance(ann, dm.RleMask):
        return [ann.rle]
    elif ann.type == dm.AnnotationType.mask:
        return [mask_utils.encode(ann.image)]
    else:
        assert False


def segment_iou(a: dm.Annotation, b: dm.Annotation, *, img_h: int, img_w: int) -> float:
    """
    Generic IoU computation with masks and polygons.
    Returns -1 if no intersection, [0; 1] otherwise
    """
    # Comparing to the dm version, this fixes the comparison for segments,
    # as the images size are required for correct decoding.
    # Boxes are not included, because they are not needed

    from pycocotools import mask as mask_utils

    a = to_rle(a, img_h=img_h, img_w=img_w)
    b = to_rle(b, img_h=img_h, img_w=img_w)

    # Note that mask_utils.iou expects (dt, gt). Check this if the 3rd param is True
    return float(mask_utils.iou(b, a, [0]))


@define(kw_only=True)
class LineMatcher(datumaro.components.annotations.matcher.LineMatcher):
    EPSILON = 1e-7

    torso_r: float = 0.25
    oriented: bool = False
    scale: float = None

    def distance(self, gt_ann: dm.PolyLine, ds_ann: dm.PolyLine) -> float:
        # Check distances of the very coarse estimates for the curves
        def _get_bbox_circle(ann: dm.PolyLine):
            xs = ann.points[0::2]
            ys = ann.points[1::2]
            x0 = min(xs)
            x1 = max(xs)
            y0 = min(ys)
            y1 = max(ys)
            return (x0 + x1) / 2, (y0 + y1) / 2, ((x1 - x0) ** 2 + (y1 - y0) ** 2) / 4

        gt_center_x, gt_center_y, gt_r2 = _get_bbox_circle(gt_ann)
        ds_center_x, ds_center_y, ds_r2 = _get_bbox_circle(ds_ann)
        sigma6_2 = self.scale * (6 * self.torso_r) ** 2
        if (
            (ds_center_x - gt_center_x) ** 2 + (ds_center_y - gt_center_y) ** 2
        ) > ds_r2 + gt_r2 + sigma6_2:
            return 0

        # Approximate lines to the same number of points for pointwise comparison
        a, b = self.approximate_points(
            np.array(gt_ann.points).reshape((-1, 2)), np.array(ds_ann.points).reshape((-1, 2))
        )

        # Compare the direct and, optionally, the reverse variants
        similarities = []
        candidates = [b]
        if not self.oriented:
            candidates.append(b[::-1])

        for candidate_b in candidates:
            similarities.append(self._compare_lines(a, candidate_b))

        return max(similarities)

    def _compare_lines(self, a: np.ndarray, b: np.ndarray) -> float:
        dists = np.linalg.norm(a - b, axis=1)

        scale = self.scale
        if scale is None:
            segment_dists = np.linalg.norm(a[1:] - a[:-1], axis=1)
            scale = np.sum(segment_dists) ** 2

        # Compute Gaussian for approximated lines similarly to OKS
        return sum(np.exp(-(dists**2) / (2 * scale * (2 * self.torso_r) ** 2))) / len(a)

    @classmethod
    def approximate_points(cls, a: np.ndarray, b: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
        """
        Creates 2 polylines with the same numbers of points,
        the points are placed on the original lines with the same step.
        The step for each point is determined as minimal to the next original
        point on one of the curves.
        A simpler, but slower version could be just approximate each curve to
        some big number of points. The advantage of this algo is that it keeps
        corners and original points untouched, while adding intermediate points.
        """
        a_segment_count = len(a) - 1
        b_segment_count = len(b) - 1

        a_segment_lengths = np.linalg.norm(a[1:] - a[:-1], axis=1)
        a_segment_end_dists = [0]
        for l in a_segment_lengths:
            a_segment_end_dists.append(a_segment_end_dists[-1] + l)
        a_segment_end_dists.pop(0)
        a_segment_end_dists.append(a_segment_end_dists[-1])  # duplicate for simpler code

        b_segment_lengths = np.linalg.norm(b[1:] - b[:-1], axis=1)
        b_segment_end_dists = [0]
        for l in b_segment_lengths:
            b_segment_end_dists.append(b_segment_end_dists[-1] + l)
        b_segment_end_dists.pop(0)
        b_segment_end_dists.append(b_segment_end_dists[-1])  # duplicate for simpler code

        a_length = a_segment_end_dists[-1]
        b_length = b_segment_end_dists[-1]

        # lines can have lesser number of points in some cases
        max_points_count = len(a) + len(b) - 1
        a_new_points = np.zeros((max_points_count, 2))
        b_new_points = np.zeros((max_points_count, 2))
        a_new_points[0] = a[0]
        b_new_points[0] = b[0]

        a_segment_idx = 0
        b_segment_idx = 0
        while a_segment_idx < a_segment_count or b_segment_idx < b_segment_count:
            next_point_idx = a_segment_idx + b_segment_idx + 1

            a_segment_end_pos = a_segment_end_dists[a_segment_idx] / (a_length or 1)
            b_segment_end_pos = b_segment_end_dists[b_segment_idx] / (b_length or 1)
            if a_segment_idx < a_segment_count and a_segment_end_pos <= b_segment_end_pos:
                if b_segment_idx < b_segment_count:
                    # advance b in the current segment to the relative position in a
                    q = (b_segment_end_pos - a_segment_end_pos) * (
                        b_length / (b_segment_lengths[b_segment_idx] or 1)
                    )
                    if abs(q) <= cls.EPSILON:
                        b_new_points[next_point_idx] = b[1 + b_segment_idx]
                    else:
                        b_new_points[next_point_idx] = b[b_segment_idx] * q + b[
                            1 + b_segment_idx
                        ] * (1 - q)
                elif b_segment_idx == b_segment_count:
                    b_new_points[next_point_idx] = b[b_segment_idx]

                # advance a to the end of the current segment
                a_segment_idx += 1
                a_new_points[next_point_idx] = a[a_segment_idx]

            elif b_segment_idx < b_segment_count:
                if a_segment_idx < a_segment_count:
                    # advance a in the current segment to the relative position in b
                    q = (a_segment_end_pos - b_segment_end_pos) * (
                        a_length / (a_segment_lengths[a_segment_idx] or 1)
                    )
                    if abs(q) <= cls.EPSILON:
                        a_new_points[next_point_idx] = a[1 + a_segment_idx]
                    else:
                        a_new_points[next_point_idx] = a[a_segment_idx] * q + a[
                            1 + a_segment_idx
                        ] * (1 - q)
                elif a_segment_idx == a_segment_count:
                    a_new_points[next_point_idx] = a[a_segment_idx]

                # advance b to the end of the current segment
                b_segment_idx += 1
                b_new_points[next_point_idx] = b[b_segment_idx]

            else:
                assert False

        # truncate the final values
        if next_point_idx < max_points_count:
            a_new_points = a_new_points[:next_point_idx]
            b_new_points = b_new_points[:next_point_idx]

        return a_new_points, b_new_points


class DistanceComparator(datumaro.components.comparator.DistanceComparator):
    def __init__(
        self,
        categories: dm.CategoriesInfo,
        *,
        included_ann_types: Optional[list[dm.AnnotationType]] = None,
        return_distances: bool = False,
        iou_threshold: float = 0.5,
        # https://cocodataset.org/#keypoints-eval
        # https://github.com/cocodataset/cocoapi/blob/8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9/PythonAPI/pycocotools/cocoeval.py#L523
        oks_sigma: float = 0.09,
        point_size_base: models.PointSizeBase = models.PointSizeBase.GROUP_BBOX_SIZE,
        compare_line_orientation: bool = False,
        line_torso_radius: float = 0.01,
        panoptic_comparison: bool = False,
        allow_groups: bool = True,
    ):
        super().__init__(iou_threshold=iou_threshold)
        self.categories = categories
        self._skeleton_info = {}
        self.included_ann_types = included_ann_types
        self.return_distances = return_distances

        self.oks_sigma = oks_sigma
        "% of the shape area"

        self.point_size_base = point_size_base
        "Compare point groups using the group bbox size or the image size"

        self.compare_line_orientation = compare_line_orientation
        "Whether lines are oriented or not"

        # Here we use a % of image size in pixels, using the image size as the scale
        self.line_torso_radius = line_torso_radius
        "% of the line length at the specified scale"

        self.panoptic_comparison = panoptic_comparison
        "Compare only the visible parts of polygons and masks"

        self.allow_groups = allow_groups
        """
        When comparing grouped annotations, consider all the group elements with the same label
        as the same annotation, if applicable. Affects polygons, masks, and points
        """

    def instance_bbox(
        self, instance_anns: Sequence[dm.Annotation]
    ) -> tuple[float, float, float, float]:
        return datumaro.util.annotation_util.max_bbox(
            a.get_bbox() if isinstance(a, dm.Skeleton) else a
            for a in instance_anns
            if hasattr(a, "get_bbox") and not a.attributes.get("outside", False)
        )

    @staticmethod
    def to_polygon(bbox_ann: dm.Bbox):
        points = bbox_ann.as_polygon()
        angle = bbox_ann.attributes.get("rotation", 0) / 180 * math.pi

        if angle:
            points = np.reshape(points, (-1, 2))
            center = (points[0] + points[2]) / 2
            rel_points = points - center
            cos = np.cos(angle)
            sin = np.sin(angle)
            rotation_matrix = ((cos, sin), (-sin, cos))
            points = np.matmul(rel_points, rotation_matrix) + center
            points = points.flatten()

        return dm.Polygon(points)

    @staticmethod
    def _get_ann_type(t: dm.AnnotationType, item: dm.DatasetItem) -> Sequence[dm.Annotation]:
        return [
            a for a in item.annotations if a.type == t and not a.attributes.get("outside", False)
        ]

    def _match_ann_type(self, t: dm.AnnotationType, *args):
        if t not in self.included_ann_types:
            return None

        # pylint: disable=no-value-for-parameter
        if t == dm.AnnotationType.label:
            return self.match_labels(*args)
        elif t == dm.AnnotationType.bbox:
            return self.match_boxes(*args)
        elif t == dm.AnnotationType.polygon:
            return self.match_segmentations(*args)
        elif t == dm.AnnotationType.points:
            return self.match_points(*args)
        elif t == dm.AnnotationType.skeleton:
            return self.match_skeletons(*args)
        elif t == dm.AnnotationType.polyline:
            return self.match_lines(*args)
        # pylint: enable=no-value-for-parameter
        else:
            return None

    def match_labels(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        def label_distance(a: dm.Label, b: dm.Label) -> float:
            if a is None or b is None:
                return 0
            return 0.5 + (a.label == b.label) / 2

        return self.match_segments(
            dm.AnnotationType.label,
            item_a,
            item_b,
            distance=label_distance,
            label_matcher=lambda a, b: a.label == b.label,
            dist_thresh=0.5,
        )

    def match_segments(
        self,
        t: dm.AnnotationType,
        item_a: dm.DatasetItem,
        item_b: dm.DatasetItem,
        *,
        distance: ShapeSimilarityFunction[_ShapeT1, _ShapeT2],
        label_matcher: LabelEqualityFunction[_ShapeT1, _ShapeT2] | None = None,
        a_objs: Sequence[_ShapeT1] | None = None,
        b_objs: Sequence[_ShapeT2] | None = None,
        dist_thresh: float | None = None,
    ):
        if a_objs is None:
            a_objs = self._get_ann_type(t, item_a)
        if b_objs is None:
            b_objs = self._get_ann_type(t, item_b)

        if self.return_distances:
            distance, distances = self._make_memoizing_distance(distance)

        if not a_objs and not b_objs:
            distances = {}
            returned_values = [], [], [], []
        else:
            extra_args = {}
            if label_matcher:
                extra_args["label_matcher"] = label_matcher

            returned_values = match_segments(
                a_objs,
                b_objs,
                distance=distance,
                dist_thresh=dist_thresh if dist_thresh is not None else self.iou_threshold,
                **extra_args,
            )

        if self.return_distances:
            returned_values = returned_values + (distances,)

        return returned_values

    def match_boxes(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        def _bbox_iou(a: dm.Bbox, b: dm.Bbox, *, img_w: int, img_h: int) -> float:
            if a.attributes.get("rotation", 0) == b.attributes.get("rotation", 0):
                return datumaro.util.annotation_util.bbox_iou(a, b)
            else:
                return segment_iou(self.to_polygon(a), self.to_polygon(b), img_h=img_h, img_w=img_w)

        img_h, img_w = item_a.media_as(dm.Image).size
        return self.match_segments(
            dm.AnnotationType.bbox,
            item_a,
            item_b,
            distance=partial(_bbox_iou, img_h=img_h, img_w=img_w),
        )

    def match_segmentations(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        def _get_segmentations(item):
            return self._get_ann_type(dm.AnnotationType.polygon, item) + self._get_ann_type(
                dm.AnnotationType.mask, item
            )

        img_h, img_w = item_a.media_as(dm.Image).size

        def _find_instances(annotations):
            instances = []
            instance_map = {}  # ann id -> instance id

            if self.allow_groups:
                # Group instance annotations by label.
                # Annotations with the same label and group will be merged,
                # and considered a single object in comparison
                groups = datumaro.util.annotation_util.find_instances(annotations)
            else:
                groups = [[a] for a in annotations]  # ignore groups

            for ann_group in groups:
                ann_group = sorted(ann_group, key=lambda a: a.label)
                for _, label_group in itertools.groupby(ann_group, key=lambda a: a.label):
                    label_group = list(label_group)
                    instance_id = len(instances)
                    instances.append(label_group)
                    for ann in label_group:
                        instance_map[id(ann)] = instance_id

            return instances, instance_map

        def _get_compiled_mask(
            anns: Sequence[dm.Annotation], *, instance_ids: dict[int, int]
        ) -> dm.CompiledMask:
            if not anns:
                return None

            from pycocotools import mask as mask_utils

            # Merge instance groups
            object_rle_groups = [to_rle(ann, img_h=img_h, img_w=img_w) for ann in anns]
            object_rles = [mask_utils.merge(g) for g in object_rle_groups]

            # Mask materialization can consume a lot of memory,
            # avoid storing all the masks simultaneously
            def _make_lazy_decode(i: int):
                def _lazy_decode() -> dm.BinaryMaskImage:
                    return mask_utils.decode([object_rles[i]])[:, :, 0]

                return _lazy_decode

            return dm.CompiledMask.from_instance_masks(
                # need to increment labels and instance ids by 1 to avoid confusion with background
                instance_masks=(
                    dm.Mask(image=_make_lazy_decode(i), z_order=ann.z_order, label=ann.label + 1)
                    for i, ann in enumerate(anns)
                ),
                instance_ids=(iid + 1 for iid in instance_ids),
            )

        a_instances, a_instance_map = _find_instances(_get_segmentations(item_a))
        b_instances, b_instance_map = _find_instances(_get_segmentations(item_b))

        if self.panoptic_comparison:
            a_compiled_mask = _get_compiled_mask(
                list(itertools.chain.from_iterable(a_instances)),
                instance_ids=[
                    a_instance_map[id(ann)] for ann in itertools.chain.from_iterable(a_instances)
                ],
            )
            b_compiled_mask = _get_compiled_mask(
                list(itertools.chain.from_iterable(b_instances)),
                instance_ids=[
                    b_instance_map[id(ann)] for ann in itertools.chain.from_iterable(b_instances)
                ],
            )
        else:
            a_compiled_mask = None
            b_compiled_mask = None

        segment_cache = {}

        def _get_segment(
            obj_id: int, *, compiled_mask: Optional[dm.CompiledMask] = None, instances
        ):
            key = (id(instances), obj_id)
            rle = segment_cache.get(key)

            if rle is None:
                from pycocotools import mask as mask_utils

                if compiled_mask is not None:
                    mask = compiled_mask.extract(obj_id + 1)

                    rle = mask_utils.encode(mask)
                else:
                    # Create merged RLE for the instance shapes
                    object_anns = instances[obj_id]
                    object_rle_groups = [
                        to_rle(ann, img_h=img_h, img_w=img_w) for ann in object_anns
                    ]
                    rle = mask_utils.merge(list(itertools.chain.from_iterable(object_rle_groups)))

                segment_cache[key] = rle

            return rle

        def _segment_comparator(a_inst_id: int, b_inst_id: int) -> float:
            a_segm = _get_segment(a_inst_id, compiled_mask=a_compiled_mask, instances=a_instances)
            b_segm = _get_segment(b_inst_id, compiled_mask=b_compiled_mask, instances=b_instances)

            from pycocotools import mask as mask_utils

            return float(mask_utils.iou([b_segm], [a_segm], [0])[0])

        def _label_matcher(a_inst_id: int, b_inst_id: int) -> bool:
            # labels are the same in the instance annotations
            # instances are required to have the same labels in all shapes
            a = a_instances[a_inst_id][0]
            b = b_instances[b_inst_id][0]
            return a.label == b.label

        results = self.match_segments(
            dm.AnnotationType.polygon,
            item_a,
            item_b,
            a_objs=range(len(a_instances)),
            b_objs=range(len(b_instances)),
            distance=_segment_comparator,
            label_matcher=_label_matcher,
        )

        # restore results for original annotations
        matched, mismatched, a_extra, b_extra = results[:4]
        if self.return_distances:
            distances = results[4]

        # i_x ~ instance idx in _x
        # ia_x ~ instance annotation in _x
        matched = [
            (ia_a, ia_b)
            for (i_a, i_b) in matched
            for (ia_a, ia_b) in itertools.product(a_instances[i_a], b_instances[i_b])
        ]
        mismatched = [
            (ia_a, ia_b)
            for (i_a, i_b) in mismatched
            for (ia_a, ia_b) in itertools.product(a_instances[i_a], b_instances[i_b])
        ]
        a_extra = [ia_a for i_a in a_extra for ia_a in a_instances[i_a]]
        b_extra = [ia_b for i_b in b_extra for ia_b in b_instances[i_b]]

        if self.return_distances:
            for i_a, i_b in list(distances.keys()):
                dist = distances.pop((i_a, i_b))

                for ia_a, ia_b in itertools.product(a_instances[i_a], b_instances[i_b]):
                    distances[(id(ia_a), id(ia_b))] = dist

        returned_values = (matched, mismatched, a_extra, b_extra)

        if self.return_distances:
            returned_values = returned_values + (distances,)

        return returned_values

    def match_lines(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        matcher = LineMatcher(
            oriented=self.compare_line_orientation,
            torso_r=self.line_torso_radius,
            scale=np.prod(item_a.media_as(dm.Image).size),
        )
        return self.match_segments(
            dm.AnnotationType.polyline, item_a, item_b, distance=matcher.distance
        )

    def match_points(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        a_points = self._get_ann_type(dm.AnnotationType.points, item_a)
        b_points = self._get_ann_type(dm.AnnotationType.points, item_b)

        if not a_points and not b_points:
            results = [[], [], [], []]

            if self.return_distances:
                results.append({})

            return tuple(results)

        instance_map = {}  # points id -> (instance group, instance bbox)
        for source_anns in [item_a.annotations, item_b.annotations]:
            if self.allow_groups:
                # Group instance annotations by label.
                # Annotations with the same label and group will be merged,
                # and considered a single object in comparison
                source_instances = datumaro.util.annotation_util.find_instances(source_anns)
            else:
                source_instances = [[a] for a in source_anns]  # ignore groups

            for instance_group in source_instances:
                instance_bbox = self.instance_bbox(instance_group)

                for ann in instance_group:
                    if ann.type == datumaro.AnnotationType.points:
                        instance_map[id(ann)] = [instance_group, instance_bbox]

        img_h, img_w = item_a.media_as(dm.Image).size

        def _distance(a: dm.Points, b: dm.Points) -> float:
            a_bbox = instance_map[id(a)][1]
            b_bbox = instance_map[id(b)][1]
            a_area = a_bbox[2] * a_bbox[3]
            b_area = b_bbox[2] * b_bbox[3]

            if a_area == 0 and b_area == 0:
                # Simple case: singular points without bbox
                # match them in the image space
                return oks(a, b, sigma=self.oks_sigma, scale=img_h * img_w)

            else:
                # Complex case: multiple points, grouped points, points with a bbox
                # Try to align points and then return the metric

                if self.point_size_base == models.PointSizeBase.IMAGE_SIZE:
                    scale = img_h * img_w
                elif self.point_size_base == models.PointSizeBase.GROUP_BBOX_SIZE:
                    # match points in their bbox space

                    if datumaro.util.annotation_util.bbox_iou(a_bbox, b_bbox) <= 0:
                        # this early exit may not work for points forming an axis-aligned line
                        return 0

                    bbox = datumaro.util.annotation_util.mean_bbox([a_bbox, b_bbox])
                    scale = bbox[2] * bbox[3]
                else:
                    assert False, f"Unknown point size base {self.point_size_base}"

                a_points = np.reshape(a.points, (-1, 2))
                b_points = np.reshape(b.points, (-1, 2))

                matches, mismatches, a_extra, b_extra = match_segments(
                    range(len(a_points)),
                    range(len(b_points)),
                    distance=lambda ai, bi: oks(
                        dm.Points(a_points[ai]),
                        dm.Points(b_points[bi]),
                        sigma=self.oks_sigma,
                        scale=scale,
                    ),
                    dist_thresh=self.iou_threshold,
                    label_matcher=lambda ai, bi: True,
                )

                # the exact array is determined by the label matcher
                # all the points will have the same match status,
                # because there is only 1 shared label for all the points
                matched_points = matches + mismatches

                a_sorting_indices = [ai for ai, _ in matched_points]
                a_points = a_points[a_sorting_indices]

                b_sorting_indices = [bi for _, bi in matched_points]
                b_points = b_points[b_sorting_indices]

                # Compute OKS for 2 groups of points, matching points aligned
                dists = np.linalg.norm(a_points - b_points, axis=1)
                return np.sum(np.exp(-(dists**2) / (2 * scale * (2 * self.oks_sigma) ** 2))) / (
                    len(matched_points) + len(a_extra) + len(b_extra)
                )

        return self.match_segments(
            dm.AnnotationType.points,
            item_a,
            item_b,
            a_objs=a_points,
            b_objs=b_points,
            distance=_distance,
        )

    def _get_skeleton_info(self, skeleton_label_id: int):
        label_cat = cast(dm.LabelCategories, self.categories[dm.AnnotationType.label])
        skeleton_info = self._skeleton_info.get(skeleton_label_id)

        if skeleton_info is None:
            skeleton_label_name = label_cat[skeleton_label_id].name

            # Build a sorted list of sublabels to arrange skeleton points during comparison
            skeleton_info = sorted(
                idx for idx, label in enumerate(label_cat) if label.parent == skeleton_label_name
            )
            self._skeleton_info[skeleton_label_id] = skeleton_info

        return skeleton_info

    def match_skeletons(self, item_a, item_b):
        a_skeletons = self._get_ann_type(dm.AnnotationType.skeleton, item_a)
        b_skeletons = self._get_ann_type(dm.AnnotationType.skeleton, item_b)
        if not a_skeletons and not b_skeletons:
            results = [[], [], [], []]

            if self.return_distances:
                results.append({})

            return tuple(results)

        # Convert skeletons to point lists for comparison
        # This is required to compute correct per-instance distance
        # It is assumed that labels are the same in the datasets
        skeleton_infos = {}
        points_map = {}
        skeleton_map = {}
        a_points = []
        b_points = []
        for source, source_points in [(a_skeletons, a_points), (b_skeletons, b_points)]:
            for skeleton in source:
                skeleton_info = skeleton_infos.setdefault(
                    skeleton.label, self._get_skeleton_info(skeleton.label)
                )

                # Merge skeleton points into a single list
                # The list is ordered by skeleton_info
                skeleton_points = [
                    next((p for p in skeleton.elements if p.label == sublabel), None)
                    for sublabel in skeleton_info
                ]

                # Build a single Points object for further comparisons
                merged_points = dm.Points()
                merged_points.points = np.ravel(
                    [p.points if p else [0, 0] for p in skeleton_points]
                )
                merged_points.visibility = np.ravel(
                    [p.visibility if p else [dm.Points.Visibility.absent] for p in skeleton_points]
                )
                merged_points.label = skeleton.label
                # no per-point attributes currently in CVAT

                if all(v == dm.Points.Visibility.absent for v in merged_points.visibility):
                    # The whole skeleton is outside, exclude it
                    skeleton_map[id(skeleton)] = None
                    continue

                points_map[id(merged_points)] = skeleton
                skeleton_map[id(skeleton)] = merged_points
                source_points.append(merged_points)

        instance_map = {}
        for source in [item_a.annotations, item_b.annotations]:
            for instance_group in datumaro.util.annotation_util.find_instances(source):
                instance_bbox = self.instance_bbox(instance_group)

                instance_group = [
                    skeleton_map[id(a)] if isinstance(a, dm.Skeleton) else a
                    for a in instance_group
                    if not isinstance(a, dm.Skeleton) or skeleton_map[id(a)] is not None
                ]
                for ann in instance_group:
                    instance_map[id(ann)] = [instance_group, instance_bbox]

        matcher = KeypointsMatcher(instance_map=instance_map, sigma=self.oks_sigma)

        results = self.match_segments(
            dm.AnnotationType.points,
            item_a,
            item_b,
            a_objs=a_points,
            b_objs=b_points,
            distance=matcher.distance,
        )

        matched, mismatched, a_extra, b_extra = results[:4]
        if self.return_distances:
            distances = results[4]

        matched = [(points_map[id(p_a)], points_map[id(p_b)]) for (p_a, p_b) in matched]
        mismatched = [(points_map[id(p_a)], points_map[id(p_b)]) for (p_a, p_b) in mismatched]
        a_extra = [points_map[id(p_a)] for p_a in a_extra]
        b_extra = [points_map[id(p_b)] for p_b in b_extra]

        # Map points back to skeletons
        if self.return_distances:
            for p_a_id, p_b_id in list(distances.keys()):
                dist = distances.pop((p_a_id, p_b_id))
                distances[(id(points_map[p_a_id]), id(points_map[p_b_id]))] = dist

        returned_values = (matched, mismatched, a_extra, b_extra)

        if self.return_distances:
            returned_values = returned_values + (distances,)

        return returned_values

    @classmethod
    def _make_memoizing_distance(cls, distance_function: Callable[[Any, Any], float]):
        distances = {}
        notfound = object()

        def memoizing_distance(a, b):
            if isinstance(a, int) and isinstance(b, int):
                key = (a, b)
            else:
                key = (id(a), id(b))

            dist = distances.get(key, notfound)

            if dist is notfound:
                dist = distance_function(a, b)
                distances[key] = dist

            return dist

        return memoizing_distance, distances

    def match_annotations(self, item_a, item_b):
        return {t: self._match_ann_type(t, item_a, item_b) for t in self.included_ann_types}


def _find_covered_segments(
    segments, *, img_w: int, img_h: int, visibility_threshold: float = 0.01
) -> Sequence[int]:
    from pycocotools import mask as mask_utils

    segments = [[s] for s in segments]
    input_rles = [mask_utils.frPyObjects(s, img_h, img_w) for s in segments]
    covered_ids = []
    for i, bottom_rles in enumerate(input_rles):
        top_rles = input_rles[i + 1 :]
        top_rle = mask_utils.merge(list(itertools.chain.from_iterable(top_rles)))
        intersection_rle = mask_utils.merge([top_rle] + bottom_rles, intersect=True)
        union_rle = mask_utils.merge([top_rle] + bottom_rles)

        bottom_area, intersection_area, union_area = mask_utils.area(
            bottom_rles + [intersection_rle, union_rle]
        )
        iou = intersection_area / (union_area or 1)

        if iou == 0:
            continue

        # Check if the bottom segment is fully covered by the top one
        if 1 - intersection_area / (bottom_area or 1) < visibility_threshold:
            covered_ids.append(i)

    return covered_ids


class _Comparator:
    def __init__(self, categories: dm.CategoriesInfo, *, settings: ComparisonParameters):
        self.ignored_attrs = set(settings.ignored_attributes) | {
            "track_id",  # changes from task to task, can't be defined manually with the same name
            "keyframe",  # indicates the way annotation obtained, meaningless to compare
            "z_order",  # changes from frame to frame, compared by other means
            "group",  # changes from job to job, compared by other means
            "rotation",  # handled by other means
            "outside",  # handled by other means
        }
        self.included_ann_types = settings.included_annotation_types
        self.non_groupable_ann_type = settings.non_groupable_ann_type
        self._annotation_comparator = DistanceComparator(
            categories,
            included_ann_types=set(self.included_ann_types)
            - {dm.AnnotationType.mask},  # masks are compared together with polygons
            return_distances=True,
            panoptic_comparison=settings.panoptic_comparison,
            iou_threshold=settings.iou_threshold,
            oks_sigma=settings.oks_sigma,
            point_size_base=settings.point_size_base,
            line_torso_radius=settings.line_thickness,
            compare_line_orientation=False,  # should not be taken from outside, handled differently
        )
        self.coverage_threshold = settings.object_visibility_threshold
        self.group_match_threshold = settings.group_match_threshold

    def match_attrs(self, ann_a: dm.Annotation, ann_b: dm.Annotation):
        a_attrs = ann_a.attributes
        b_attrs = ann_b.attributes

        keys_to_match = (a_attrs.keys() | b_attrs.keys()).difference(self.ignored_attrs)

        matches = []
        mismatches = []
        a_extra = []
        b_extra = []

        notfound = object()

        for k in keys_to_match:
            a_attr = a_attrs.get(k, notfound)
            b_attr = b_attrs.get(k, notfound)

            if a_attr is notfound:
                b_extra.append(k)
            elif b_attr is notfound:
                a_extra.append(k)
            elif a_attr == b_attr:
                matches.append(k)
            else:
                mismatches.append(k)

        return matches, mismatches, a_extra, b_extra

    def find_groups(
        self, item: dm.DatasetItem
    ) -> tuple[dict[int, list[dm.Annotation]], dict[int, int]]:
        ann_groups = datumaro.util.annotation_util.find_instances(
            [
                ann
                for ann in item.annotations
                if ann.type in self.included_ann_types and ann.type != self.non_groupable_ann_type
            ]
        )

        groups = {}
        group_map = {}  # ann id -> group id
        for group_id, group in enumerate(ann_groups):
            groups[group_id] = group
            for ann in group:
                group_map[id(ann)] = group_id

        return groups, group_map

    def match_groups(self, gt_groups, ds_groups, matched_anns):
        matched_ann_ids = dict((id(a), id(b)) for a, b in matched_anns)

        def _group_distance(gt_group_id, ds_group_id):
            intersection = sum(
                1
                for gt_ann in gt_groups[gt_group_id]
                for ds_ann in ds_groups[ds_group_id]
                if matched_ann_ids.get(id(gt_ann), None) == id(ds_ann)
            )
            union = len(gt_groups[gt_group_id]) + len(ds_groups[ds_group_id]) - intersection
            return intersection / (union or 1)

        matches, mismatches, gt_unmatched, ds_unmatched = match_segments(
            list(gt_groups),
            list(ds_groups),
            distance=_group_distance,
            label_matcher=lambda a, b: _group_distance(a, b) == 1,
            dist_thresh=self.group_match_threshold,
        )

        ds_to_gt_groups = {
            ds_group_id: gt_group_id
            for gt_group_id, ds_group_id in itertools.chain(
                matches, mismatches, zip(itertools.repeat(None), ds_unmatched)
            )
        }
        ds_to_gt_groups[None] = gt_unmatched

        return ds_to_gt_groups

    def find_covered(self, item: dm.DatasetItem) -> list[dm.Annotation]:
        # Get annotations that can cover or be covered
        spatial_types = {
            dm.AnnotationType.polygon,
            dm.AnnotationType.mask,
            dm.AnnotationType.bbox,
        }.intersection(self.included_ann_types)
        anns = sorted(
            [a for a in item.annotations if a.type in spatial_types], key=lambda a: a.z_order
        )

        segms = []
        for ann in anns:
            if ann.type == dm.AnnotationType.bbox:
                segms.append(ann.as_polygon())
            elif ann.type == dm.AnnotationType.polygon:
                segms.append(ann.points)
            elif ann.type == dm.AnnotationType.mask:
                segms.append(datumaro.util.mask_tools.mask_to_rle(ann.image))
            else:
                assert False

        img_h, img_w = item.media_as(dm.Image).size
        covered_ids = _find_covered_segments(
            segms, img_w=img_w, img_h=img_h, visibility_threshold=self.coverage_threshold
        )
        return [anns[i] for i in covered_ids]

    def match_annotations(self, item_a: dm.DatasetItem, item_b: dm.DatasetItem):
        per_type_results = self._annotation_comparator.match_annotations(item_a, item_b)

        merged_results = [[], [], [], [], {}]
        shape_merged_results = [[], [], [], [], {}]
        for shape_type in self.included_ann_types:
            shape_type_results = per_type_results.get(shape_type, None)
            if shape_type_results is None:
                continue

            for merged_field, field in zip(merged_results, shape_type_results[:-1]):
                merged_field.extend(field)

            if shape_type != dm.AnnotationType.label:
                for merged_field, field in zip(shape_merged_results, shape_type_results[:-1]):
                    merged_field.extend(field)
                shape_merged_results[-1].update(per_type_results[shape_type][-1])

            merged_results[-1].update(per_type_results[shape_type][-1])

        return {"all_ann_types": merged_results, "all_shape_ann_types": shape_merged_results}

    def get_distance(
        self, pairwise_distances, gt_ann: dm.Annotation, ds_ann: dm.Annotation
    ) -> Optional[float]:
        return pairwise_distances.get((id(gt_ann), id(ds_ann)))


class DatasetComparator:
    DEFAULT_SETTINGS = ComparisonParameters()

    def __init__(
        self,
        ds_data_provider: JobDataProvider,
        gt_data_provider: JobDataProvider,
        *,
        settings: Optional[ComparisonParameters] = None,
    ) -> None:
        if settings is None:
            settings = self.DEFAULT_SETTINGS
        self.settings = settings

        self._ds_data_provider = ds_data_provider
        self._gt_data_provider = gt_data_provider
        self._ds_dataset = self._ds_data_provider.dm_dataset
        self._gt_dataset = self._gt_data_provider.dm_dataset

        self._frame_results: dict[int, ComparisonReportFrameSummary] = {}

        self.comparator = _Comparator(self._gt_dataset.categories(), settings=settings)

    def _dm_item_to_frame_id(self, item: dm.DatasetItem, dataset: dm.Dataset) -> int:
        if dataset is self._ds_dataset:
            source_data_provider = self._ds_data_provider
        elif dataset is self._gt_dataset:
            source_data_provider = self._gt_data_provider
        else:
            assert False

        return source_data_provider.dm_item_id_to_frame_id(item)

    def _dm_ann_to_ann_id(self, ann: dm.Annotation, dataset: dm.Dataset):
        if dataset is self._ds_dataset:
            source_data_provider = self._ds_data_provider
        elif dataset is self._gt_dataset:
            source_data_provider = self._gt_data_provider
        else:
            assert False

        return source_data_provider.dm_ann_to_ann_id(ann)

    def _find_gt_conflicts(self):
        ds_job_dataset = self._ds_dataset
        gt_job_dataset = self._gt_dataset

        for gt_item in gt_job_dataset:
            ds_item = ds_job_dataset.get(id=gt_item.id, subset=gt_item.subset)
            if not ds_item:
                continue  # we need to compare only intersecting frames

            self._process_frame(ds_item, gt_item)

    def _process_frame(
        self, ds_item: dm.DatasetItem, gt_item: dm.DatasetItem
    ) -> list[AnnotationConflict]:
        frame_id = self._dm_item_to_frame_id(ds_item, self._ds_dataset)

        frame_results = self.comparator.match_annotations(gt_item, ds_item)
        self._frame_results.setdefault(frame_id, {})

        self._generate_frame_annotation_conflicts(
            frame_id, frame_results, gt_item=gt_item, ds_item=ds_item
        )

    def _generate_frame_annotation_conflicts(
        self, frame_id: str, frame_results, *, gt_item: dm.DatasetItem, ds_item: dm.DatasetItem
    ) -> list[AnnotationConflict]:
        conflicts = []

        matches, mismatches, gt_unmatched, ds_unmatched, _ = frame_results["all_ann_types"]
        (
            shape_matches,
            shape_mismatches,
            shape_gt_unmatched,
            shape_ds_unmatched,
            shape_pairwise_distances,
        ) = frame_results["all_shape_ann_types"]

        def _get_similarity(gt_ann: dm.Annotation, ds_ann: dm.Annotation) -> Optional[float]:
            return self.comparator.get_distance(shape_pairwise_distances, gt_ann, ds_ann)

        _matched_shapes = set(
            id(shape)
            for shape_pair in itertools.chain(shape_matches, shape_mismatches)
            for shape in shape_pair
        )

        def _find_closest_unmatched_shape(shape: dm.Annotation):
            this_shape_id = id(shape)

            this_shape_distances = []

            for (gt_shape_id, ds_shape_id), dist in shape_pairwise_distances.items():
                if gt_shape_id == this_shape_id:
                    other_shape_id = ds_shape_id
                elif ds_shape_id == this_shape_id:
                    other_shape_id = gt_shape_id
                else:
                    continue

                this_shape_distances.append((other_shape_id, dist))

            matched_ann, distance = max(this_shape_distances, key=lambda v: v[1], default=(None, 0))
            return matched_ann, distance

        for gt_ann, ds_ann in itertools.chain(matches, mismatches):
            similarity = _get_similarity(gt_ann, ds_ann)
            if similarity and similarity < self.settings.low_overlap_threshold:
                conflicts.append(
                    AnnotationConflict(
                        frame_id=frame_id,
                        type=AnnotationConflictType.LOW_OVERLAP,
                        annotation_ids=[
                            self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                            self._dm_ann_to_ann_id(gt_ann, self._gt_dataset),
                        ],
                    )
                )

        for unmatched_ann in gt_unmatched:
            conflicts.append(
                AnnotationConflict(
                    frame_id=frame_id,
                    type=AnnotationConflictType.MISSING_ANNOTATION,
                    annotation_ids=[self._dm_ann_to_ann_id(unmatched_ann, self._gt_dataset)],
                )
            )

        for unmatched_ann in ds_unmatched:
            conflicts.append(
                AnnotationConflict(
                    frame_id=frame_id,
                    type=AnnotationConflictType.EXTRA_ANNOTATION,
                    annotation_ids=[self._dm_ann_to_ann_id(unmatched_ann, self._ds_dataset)],
                )
            )

        for gt_ann, ds_ann in mismatches:
            conflicts.append(
                AnnotationConflict(
                    frame_id=frame_id,
                    type=AnnotationConflictType.MISMATCHING_LABEL,
                    annotation_ids=[
                        self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                        self._dm_ann_to_ann_id(gt_ann, self._gt_dataset),
                    ],
                )
            )

        resulting_distances = [
            _get_similarity(shape_gt_ann, shape_ds_ann)
            for shape_gt_ann, shape_ds_ann in itertools.chain(shape_matches, shape_mismatches)
        ]

        for shape_unmatched_ann in itertools.chain(shape_gt_unmatched, shape_ds_unmatched):
            shape_matched_ann_id, similarity = _find_closest_unmatched_shape(shape_unmatched_ann)
            if shape_matched_ann_id is not None:
                _matched_shapes.add(shape_matched_ann_id)
            resulting_distances.append(similarity)

        resulting_distances = [
            sim if sim is not None and (sim >= 0) else 0 for sim in resulting_distances
        ]

        mean_iou = np.mean(resulting_distances) if resulting_distances else 0

        if (
            self.settings.compare_line_orientation
            and dm.AnnotationType.polyline in self.comparator.included_ann_types
        ):
            # Check line directions
            line_matcher = LineMatcher(
                torso_r=self.settings.line_thickness,
                oriented=True,
                scale=np.prod(gt_item.media_as(dm.Image).size),
            )

            for gt_ann, ds_ann in itertools.chain(matches, mismatches):
                if gt_ann.type != ds_ann.type or gt_ann.type != dm.AnnotationType.polyline:
                    continue

                non_oriented_distance = _get_similarity(gt_ann, ds_ann)
                oriented_distance = line_matcher.distance(gt_ann, ds_ann)

                # need to filter computation errors from line approximation
                # and (almost) orientation-independent cases
                if (
                    non_oriented_distance - oriented_distance
                    > self.settings.line_orientation_threshold
                ):
                    conflicts.append(
                        AnnotationConflict(
                            frame_id=frame_id,
                            type=AnnotationConflictType.MISMATCHING_DIRECTION,
                            annotation_ids=[
                                self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                                self._dm_ann_to_ann_id(gt_ann, self._gt_dataset),
                            ],
                        )
                    )

        if self.settings.check_covered_annotations:
            ds_covered_anns = self.comparator.find_covered(ds_item)

            for ds_ann in ds_covered_anns:
                conflicts.append(
                    AnnotationConflict(
                        frame_id=frame_id,
                        type=AnnotationConflictType.COVERED_ANNOTATION,
                        annotation_ids=[
                            self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                        ],
                    )
                )

        if self.settings.compare_attributes:
            for gt_ann, ds_ann in matches:
                attribute_results = self.comparator.match_attrs(gt_ann, ds_ann)
                if any(attribute_results[1:]):
                    conflicts.append(
                        AnnotationConflict(
                            frame_id=frame_id,
                            type=AnnotationConflictType.MISMATCHING_ATTRIBUTES,
                            annotation_ids=[
                                self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                                self._dm_ann_to_ann_id(gt_ann, self._gt_dataset),
                            ],
                        )
                    )

        if self.settings.compare_groups:
            gt_groups, gt_group_map = self.comparator.find_groups(gt_item)
            ds_groups, ds_group_map = self.comparator.find_groups(ds_item)
            shape_matched_objects = shape_matches + shape_mismatches
            ds_to_gt_groups = self.comparator.match_groups(
                gt_groups, ds_groups, shape_matched_objects
            )

            for gt_ann, ds_ann in shape_matched_objects:
                gt_group = gt_groups.get(gt_group_map[id(gt_ann)], [gt_ann])
                ds_group = ds_groups.get(ds_group_map[id(ds_ann)], [ds_ann])
                ds_gt_group = ds_to_gt_groups.get(ds_group_map[id(ds_ann)], None)

                if (
                    # Check ungrouped objects
                    (len(gt_group) == 1 and len(ds_group) != 1)
                    or
                    # Check grouped objects
                    ds_gt_group != gt_group_map[id(gt_ann)]
                ):
                    conflicts.append(
                        AnnotationConflict(
                            frame_id=frame_id,
                            type=AnnotationConflictType.MISMATCHING_GROUPS,
                            annotation_ids=[
                                self._dm_ann_to_ann_id(ds_ann, self._ds_dataset),
                                self._dm_ann_to_ann_id(gt_ann, self._gt_dataset),
                            ],
                        )
                    )

        valid_shapes_count = len(shape_matches) + len(shape_mismatches)
        missing_shapes_count = len(shape_gt_unmatched)
        extra_shapes_count = len(shape_ds_unmatched)
        total_shapes_count = (
            len(shape_matches)
            + len(shape_mismatches)
            + len(shape_gt_unmatched)
            + len(shape_ds_unmatched)
        )
        ds_shapes_count = len(shape_matches) + len(shape_mismatches) + len(shape_ds_unmatched)
        gt_shapes_count = len(shape_matches) + len(shape_mismatches) + len(shape_gt_unmatched)

        valid_labels_count = len(matches)
        invalid_labels_count = len(mismatches)
        total_labels_count = valid_labels_count + invalid_labels_count

        confusion_matrix_labels, confusion_matrix, label_id_map = self._make_zero_confusion_matrix()
        for gt_ann, ds_ann in itertools.chain(
            # fully matched annotations - shape, label, attributes
            matches,
            mismatches,
            zip(itertools.repeat(None), ds_unmatched),
            zip(gt_unmatched, itertools.repeat(None)),
        ):
            ds_label_idx = label_id_map[ds_ann.label] if ds_ann else self._UNMATCHED_IDX
            gt_label_idx = label_id_map[gt_ann.label] if gt_ann else self._UNMATCHED_IDX
            confusion_matrix[ds_label_idx, gt_label_idx] += 1

        if self.settings.empty_is_annotated:
            # Add virtual annotations for empty frames
            if not gt_item.annotations and not ds_item.annotations:
                valid_labels_count = 1
                total_labels_count = 1

                valid_shapes_count = 1
                total_shapes_count = 1

            if not ds_item.annotations:
                ds_shapes_count = 1

            if not gt_item.annotations:
                gt_shapes_count = 1

        self._frame_results[frame_id] = ComparisonReportFrameSummary(
            annotations=self._generate_frame_annotations_summary(
                confusion_matrix, confusion_matrix_labels
            ),
            annotation_components=ComparisonReportAnnotationComponentsSummary(
                shape=ComparisonReportAnnotationShapeSummary(
                    valid_count=valid_shapes_count,
                    missing_count=missing_shapes_count,
                    extra_count=extra_shapes_count,
                    total_count=total_shapes_count,
                    ds_count=ds_shapes_count,
                    gt_count=gt_shapes_count,
                    mean_iou=mean_iou,
                ),
                label=ComparisonReportAnnotationLabelSummary(
                    valid_count=valid_labels_count,
                    invalid_count=invalid_labels_count,
                    total_count=total_labels_count,
                ),
            ),
            conflicts=conflicts,
        )

        return conflicts

    # row/column index in the confusion matrix corresponding to unmatched annotations
    _UNMATCHED_IDX = -1

    def _make_zero_confusion_matrix(self) -> tuple[list[str], np.ndarray, dict[int, int]]:
        label_id_idx_map = {}
        label_names = []
        for label_id, label in enumerate(self._gt_dataset.categories()[dm.AnnotationType.label]):
            if not label.parent:
                label_id_idx_map[label_id] = len(label_names)
                label_names.append(label.name)

        label_names.append("unmatched")

        num_labels = len(label_names)
        confusion_matrix = np.zeros((num_labels, num_labels), dtype=int)

        return label_names, confusion_matrix, label_id_idx_map

    def _compute_annotations_summary(
        self, confusion_matrix: np.ndarray, confusion_matrix_labels: list[str]
    ) -> ComparisonReportAnnotationsSummary:
        matched_ann_counts = np.diag(confusion_matrix)
        ds_ann_counts = np.sum(confusion_matrix, axis=1)
        gt_ann_counts = np.sum(confusion_matrix, axis=0)
        total_annotations_count = np.sum(confusion_matrix)

        label_jaccard_indices = _arr_div(
            matched_ann_counts, ds_ann_counts + gt_ann_counts - matched_ann_counts
        )
        label_precisions = _arr_div(matched_ann_counts, ds_ann_counts)
        label_recalls = _arr_div(matched_ann_counts, gt_ann_counts)
        label_accuracies = (
            total_annotations_count  # TP + TN + FP + FN
            - (ds_ann_counts - matched_ann_counts)  # - FP
            - (gt_ann_counts - matched_ann_counts)  # - FN
            # ... = TP + TN
        ) / (total_annotations_count or 1)

        valid_annotations_count = np.sum(matched_ann_counts)
        missing_annotations_count = np.sum(confusion_matrix[self._UNMATCHED_IDX, :])
        extra_annotations_count = np.sum(confusion_matrix[:, self._UNMATCHED_IDX])
        ds_annotations_count = np.sum(ds_ann_counts[: self._UNMATCHED_IDX])
        gt_annotations_count = np.sum(gt_ann_counts[: self._UNMATCHED_IDX])

        return ComparisonReportAnnotationsSummary(
            valid_count=valid_annotations_count,
            missing_count=missing_annotations_count,
            extra_count=extra_annotations_count,
            total_count=total_annotations_count,
            ds_count=ds_annotations_count,
            gt_count=gt_annotations_count,
            confusion_matrix=ConfusionMatrix(
                labels=confusion_matrix_labels,
                rows=confusion_matrix,
                precision=label_precisions,
                recall=label_recalls,
                accuracy=label_accuracies,
                jaccard_index=label_jaccard_indices,
            ),
        )

    def _generate_frame_annotations_summary(
        self, confusion_matrix: np.ndarray, confusion_matrix_labels: list[str]
    ) -> ComparisonReportAnnotationsSummary:
        summary = self._compute_annotations_summary(confusion_matrix, confusion_matrix_labels)

        if self.settings.empty_is_annotated:
            # Add virtual annotations for empty frames
            if not summary.total_count:
                summary.valid_count = 1
                summary.total_count = 1

            if not summary.ds_count:
                summary.ds_count = 1

            if not summary.gt_count:
                summary.gt_count = 1

        return summary

    def _generate_dataset_annotations_summary(
        self, frame_summaries: dict[int, ComparisonReportFrameSummary]
    ) -> tuple[ComparisonReportAnnotationsSummary, ComparisonReportAnnotationComponentsSummary]:
        # accumulate stats
        annotation_components = ComparisonReportAnnotationComponentsSummary(
            shape=ComparisonReportAnnotationShapeSummary(
                valid_count=0,
                missing_count=0,
                extra_count=0,
                total_count=0,
                ds_count=0,
                gt_count=0,
                mean_iou=0,
            ),
            label=ComparisonReportAnnotationLabelSummary(
                valid_count=0,
                invalid_count=0,
                total_count=0,
            ),
        )
        mean_ious = []
        empty_gt_frames = set()
        empty_ds_frames = set()
        confusion_matrix_labels, confusion_matrix, _ = self._make_zero_confusion_matrix()

        for frame_id, frame_result in frame_summaries.items():
            confusion_matrix += frame_result.annotations.confusion_matrix.rows

            if self.settings.empty_is_annotated and not np.any(
                frame_result.annotations.confusion_matrix.rows[
                    np.triu_indices_from(frame_result.annotations.confusion_matrix.rows)
                ]
            ):
                empty_ds_frames.add(frame_id)

            if self.settings.empty_is_annotated and not np.any(
                frame_result.annotations.confusion_matrix.rows[
                    np.tril_indices_from(frame_result.annotations.confusion_matrix.rows)
                ]
            ):
                empty_gt_frames.add(frame_id)

            if annotation_components is None:
                annotation_components = deepcopy(frame_result.annotation_components)
            else:
                annotation_components.accumulate(frame_result.annotation_components)

            mean_ious.append(frame_result.annotation_components.shape.mean_iou)

        annotation_summary = self._compute_annotations_summary(
            confusion_matrix, confusion_matrix_labels
        )

        if self.settings.empty_is_annotated:
            # Add virtual annotations for empty frames,
            # they are not included in the confusion matrix
            annotation_summary.valid_count += len(empty_ds_frames & empty_gt_frames)
            annotation_summary.total_count += len(empty_ds_frames | empty_gt_frames)
            annotation_summary.ds_count += len(empty_ds_frames)
            annotation_summary.gt_count += len(empty_gt_frames)

        # Cannot be computed in accumulate()
        annotation_components.shape.mean_iou = np.mean(mean_ious)

        return annotation_summary, annotation_components

    def generate_report(self) -> ComparisonReport:
        self._find_gt_conflicts()

        intersection_frames = []
        conflicts = []
        for frame_id, frame_result in self._frame_results.items():
            intersection_frames.append(frame_id)
            conflicts += frame_result.conflicts

        annotation_summary, annotations_component_summary = (
            self._generate_dataset_annotations_summary(self._frame_results)
        )

        return ComparisonReport(
            parameters=self.settings,
            comparison_summary=ComparisonReportComparisonSummary(
                frame_share=(
                    len(intersection_frames) / (len(self._ds_data_provider.job_data.rel_range) or 1)
                ),
                frames=intersection_frames,
                conflict_count=len(conflicts),
                warning_count=len(
                    [c for c in conflicts if c.severity == AnnotationConflictSeverity.WARNING]
                ),
                error_count=len(
                    [c for c in conflicts if c.severity == AnnotationConflictSeverity.ERROR]
                ),
                conflicts_by_type=Counter(c.type for c in conflicts),
                annotations=annotation_summary,
                annotation_components=annotations_component_summary,
            ),
            frame_results=self._frame_results,
        )


class QualityReportUpdateManager:
    _QUEUE_CUSTOM_JOB_PREFIX = "quality-check-"
    _RQ_CUSTOM_QUALITY_CHECK_JOB_TYPE = "custom_quality_check"
    _JOB_RESULT_TTL = 120

    def _get_queue(self) -> RqQueue:
        return django_rq.get_queue(settings.CVAT_QUEUES.QUALITY_REPORTS.value)

    def _make_custom_quality_check_job_id(self, task_id: int, user_id: int) -> str:
        # FUTURE-TODO: it looks like job ID template should not include user_id because:
        # 1. There is no need to compute quality reports several times for different users
        # 2. Each user (not only rq job owner) that has permission to access a task should
        # be able to check the status of the computation process
        return f"{self._QUEUE_CUSTOM_JOB_PREFIX}task-{task_id}-user-{user_id}"

    class QualityReportsNotAvailable(Exception):
        pass

    def _check_quality_reporting_available(self, task: Task):
        if task.dimension != DimensionType.DIM_2D:
            raise self.QualityReportsNotAvailable("Quality reports are only supported in 2d tasks")

        gt_job = task.gt_job
        if gt_job is None or not (
            gt_job.stage == StageChoice.ACCEPTANCE and gt_job.state == StatusChoice.COMPLETED
        ):
            raise self.QualityReportsNotAvailable(
                "Quality reports require a Ground Truth job in the task "
                f"at the {StageChoice.ACCEPTANCE} stage "
                f"and in the {StatusChoice.COMPLETED} state"
            )

    class JobAlreadyExists(QualityReportsNotAvailable):
        def __str__(self):
            return "Quality computation job for this task already enqueued"

    def schedule_custom_quality_check_job(
        self, request: ExtendedRequest, task: Task, *, user_id: int
    ) -> str:
        """
        Schedules a quality report computation job, supposed for updates by a request.
        """

        self._check_quality_reporting_available(task)

        queue = self._get_queue()
        rq_id = self._make_custom_quality_check_job_id(task_id=task.id, user_id=user_id)

        # ensure that there is no race condition when processing parallel requests
        with get_rq_lock_for_job(queue, rq_id):
            if rq_job := queue.fetch_job(rq_id):
                if rq_job.get_status(refresh=False) in (
                    rq.job.JobStatus.QUEUED,
                    rq.job.JobStatus.STARTED,
                    rq.job.JobStatus.SCHEDULED,
                    rq.job.JobStatus.DEFERRED,
                ):
                    raise self.JobAlreadyExists()

                rq_job.delete()

            with get_rq_lock_by_user(queue, user_id=user_id):
                dependency = define_dependent_job(
                    queue, user_id=user_id, rq_id=rq_id, should_be_dependent=True
                )

                queue.enqueue(
                    self._check_task_quality,
                    task_id=task.id,
                    job_id=rq_id,
                    meta=BaseRQMeta.build(request=request, db_obj=task),
                    result_ttl=self._JOB_RESULT_TTL,
                    failure_ttl=self._JOB_RESULT_TTL,
                    depends_on=dependency,
                )

        return rq_id

    def get_quality_check_job(self, rq_id: str) -> Optional[RqJob]:
        queue = self._get_queue()
        rq_job = queue.fetch_job(rq_id)

        if rq_job and not self.is_custom_quality_check_job(rq_job):
            rq_job = None

        return rq_job

    def is_custom_quality_check_job(self, rq_job: RqJob) -> bool:
        return isinstance(rq_job.id, str) and rq_job.id.startswith(self._QUEUE_CUSTOM_JOB_PREFIX)

    @classmethod
    @silk_profile()
    def _check_task_quality(cls, *, task_id: int) -> int:
        return cls()._compute_reports(task_id=task_id)

    def _compute_reports(self, task_id: int) -> int:
        with transaction.atomic():
            try:
                # Preload all the data for the computations.
                # It must be done atomically and before all the computations,
                # because the task and jobs can be changed after the beginning,
                # which will lead to inconsistent results
                # TODO: check performance of select_for_update(),
                # maybe make several fetching attempts if received data is outdated
                task = Task.objects.select_related("data").get(id=task_id)
            except Task.DoesNotExist:
                # The task could have been deleted during scheduling
                return

            # Try to use a shared queryset to minimize DB requests
            job_queryset = Job.objects.select_related("segment")
            job_queryset = job_queryset.filter(segment__task_id=task_id)

            # The GT job could have been removed during scheduling, so we need to check it exists
            gt_job: Job = next(
                (job for job in job_queryset if job.type == JobType.GROUND_TRUTH), None
            )
            if gt_job is None:
                return

            # TODO: Probably, can be optimized to this:
            # - task updated (the gt job, frame set or labels changed) -> everything is computed
            # - job updated -> job report is computed
            #   old reports can be reused in this case (need to add M-1 relationship in reports)

            # Add prefetch data to the shared queryset
            # All the jobs / segments share the same task, so we can load it just once.
            # We reuse the same object for better memory use (OOM is possible otherwise).
            # Perform manual "join", since django can't do this.
            gt_job = JobDataProvider.add_prefetch_info(job_queryset).get(id=gt_job.id)
            for job in job_queryset:
                job.segment.task = gt_job.segment.task

            gt_job_data_provider = JobDataProvider(gt_job.id, queryset=job_queryset)
            active_validation_frames = gt_job_data_provider.job_data.get_included_frames()

            validation_layout = task.data.validation_layout
            if validation_layout.mode == ValidationMode.GT_POOL:
                task_frame_provider = TaskFrameProvider(task)
                active_validation_frames = set(
                    task_frame_provider.get_rel_frame_number(abs_frame)
                    for abs_frame, abs_real_frame in (
                        Image.objects.filter(data=task.data, is_placeholder=True)
                        .values_list("frame", "real_frame")
                        .iterator(chunk_size=10000)
                    )
                    if task_frame_provider.get_rel_frame_number(abs_real_frame)
                    in active_validation_frames
                )

            jobs: list[Job] = [j for j in job_queryset if j.type == JobType.ANNOTATION]
            job_data_providers = {
                job.id: JobDataProvider(
                    job.id,
                    queryset=job_queryset,
                    included_frames=active_validation_frames,
                )
                for job in jobs
            }

            quality_params = self._get_task_quality_params(task)

        job_comparison_reports: dict[int, ComparisonReport] = {}
        for job in jobs:
            job_data_provider = job_data_providers[job.id]
            comparator = DatasetComparator(
                job_data_provider, gt_job_data_provider, settings=quality_params
            )
            job_comparison_reports[job.id] = comparator.generate_report()

            # Release resources
            del job_data_provider.dm_dataset

        task_comparison_report = self._compute_task_report(task, job_comparison_reports)

        with transaction.atomic():
            # The task could have been deleted during processing
            try:
                Task.objects.get(id=task_id)
            except Task.DoesNotExist:
                return

            job_quality_reports = {}
            for job in jobs:
                job_comparison_report = job_comparison_reports[job.id]
                job_report = dict(
                    job=job,
                    target_last_updated=job.updated_date,
                    gt_last_updated=gt_job.updated_date,
                    assignee_id=job.assignee_id,
                    assignee_last_updated=job.assignee_updated_date,
                    data=job_comparison_report.to_json(),
                    conflicts=[c.to_dict() for c in job_comparison_report.conflicts],
                )

                job_quality_reports[job.id] = job_report

            task_report = self._save_reports(
                task_report=dict(
                    task=task,
                    target_last_updated=task.updated_date,
                    gt_last_updated=gt_job.updated_date,
                    assignee_id=task.assignee_id,
                    assignee_last_updated=task.assignee_updated_date,
                    data=task_comparison_report.to_json(),
                    conflicts=[],  # the task doesn't have own conflicts
                ),
                job_reports=list(job_quality_reports.values()),
            )

        return task_report.id

    def _compute_task_report(
        self, task: Task, job_reports: dict[int, ComparisonReport]
    ) -> ComparisonReport:
        # The task dataset can be different from any jobs' dataset because of frame overlaps
        # between jobs, from which annotations are merged to get the task annotations.
        # Thus, a separate report could be computed for the task. Instead, here we only
        # compute the combined summary of the job reports.
        task_intersection_frames = set()
        task_conflicts: list[AnnotationConflict] = []
        task_annotations_summary = None
        task_ann_components_summary = None
        task_mean_shape_ious = []
        task_frame_results = {}
        task_frame_results_counts = {}
        confusion_matrix = None
        for r in job_reports.values():
            task_intersection_frames.update(r.comparison_summary.frames)
            task_conflicts.extend(r.conflicts)

            if task_annotations_summary:
                task_annotations_summary.accumulate(r.comparison_summary.annotations)
            else:
                task_annotations_summary = deepcopy(r.comparison_summary.annotations)

            if confusion_matrix is None:
                num_labels = len(r.comparison_summary.annotations.confusion_matrix.labels)
                confusion_matrix = np.zeros((num_labels, num_labels), dtype=int)

            confusion_matrix += r.comparison_summary.annotations.confusion_matrix.rows

            if task_ann_components_summary:
                task_ann_components_summary.accumulate(r.comparison_summary.annotation_components)
            else:
                task_ann_components_summary = deepcopy(r.comparison_summary.annotation_components)
            task_mean_shape_ious.append(task_ann_components_summary.shape.mean_iou)

            for frame_id, job_frame_result in r.frame_results.items():
                task_frame_result = cast(
                    Optional[ComparisonReportFrameSummary], task_frame_results.get(frame_id)
                )
                frame_results_count = task_frame_results_counts.get(frame_id, 0)

                if task_frame_result is None:
                    task_frame_result = deepcopy(job_frame_result)
                else:
                    task_frame_result.conflicts += job_frame_result.conflicts

                    task_frame_result.annotations.accumulate(job_frame_result.annotations)
                    task_frame_result.annotation_components.accumulate(
                        job_frame_result.annotation_components
                    )

                    task_frame_result.annotation_components.shape.mean_iou = (
                        task_frame_result.annotation_components.shape.mean_iou * frame_results_count
                        + job_frame_result.annotation_components.shape.mean_iou
                    ) / (frame_results_count + 1)

                task_frame_results_counts[frame_id] = 1 + frame_results_count
                task_frame_results[frame_id] = task_frame_result

        task_annotations_summary.confusion_matrix.rows = confusion_matrix

        task_ann_components_summary.shape.mean_iou = np.mean(task_mean_shape_ious)

        task_report_data = ComparisonReport(
            parameters=next(iter(job_reports.values())).parameters,
            comparison_summary=ComparisonReportComparisonSummary(
                frame_share=len(task_intersection_frames) / (task.data.size or 1),
                frames=sorted(task_intersection_frames),
                conflict_count=len(task_conflicts),
                warning_count=len(
                    [c for c in task_conflicts if c.severity == AnnotationConflictSeverity.WARNING]
                ),
                error_count=len(
                    [c for c in task_conflicts if c.severity == AnnotationConflictSeverity.ERROR]
                ),
                conflicts_by_type=Counter(c.type for c in task_conflicts),
                annotations=task_annotations_summary,
                annotation_components=task_ann_components_summary,
            ),
            frame_results=task_frame_results,
        )

        return task_report_data

    def _save_reports(self, *, task_report: dict, job_reports: list[dict]) -> models.QualityReport:
        # TODO: add validation (e.g. ann id count for different types of conflicts)

        db_task_report = models.QualityReport(
            task=task_report["task"],
            target_last_updated=task_report["target_last_updated"],
            gt_last_updated=task_report["gt_last_updated"],
            assignee_id=task_report["assignee_id"],
            assignee_last_updated=task_report["assignee_last_updated"],
            data=task_report["data"],
        )
        db_task_report.save()

        db_job_reports = []
        for job_report in job_reports:
            db_job_report = models.QualityReport(
                parent=db_task_report,
                job=job_report["job"],
                target_last_updated=job_report["target_last_updated"],
                gt_last_updated=job_report["gt_last_updated"],
                assignee_id=job_report["assignee_id"],
                assignee_last_updated=job_report["assignee_last_updated"],
                data=job_report["data"],
            )
            db_job_reports.append(db_job_report)

        db_job_reports = bulk_create(db_model=models.QualityReport, objs=db_job_reports)

        db_conflicts = []
        db_report_iter = itertools.chain([db_task_report], db_job_reports)
        report_iter = itertools.chain([task_report], job_reports)
        for report, db_report in zip(report_iter, db_report_iter):
            for conflict in report["conflicts"]:
                db_conflict = models.AnnotationConflict(
                    report=db_report,
                    type=conflict["type"],
                    frame=conflict["frame_id"],
                    severity=conflict["severity"],
                )
                db_conflicts.append(db_conflict)

        db_conflicts = bulk_create(db_model=models.AnnotationConflict, objs=db_conflicts)

        db_ann_ids = []
        db_conflicts_iter = iter(db_conflicts)
        for report in itertools.chain([task_report], job_reports):
            for conflict, db_conflict in zip(report["conflicts"], db_conflicts_iter):
                for ann_id in conflict["annotation_ids"]:
                    db_ann_id = models.AnnotationId(
                        conflict=db_conflict,
                        job_id=ann_id["job_id"],
                        obj_id=ann_id["obj_id"],
                        type=ann_id["type"],
                        shape_type=ann_id["shape_type"],
                    )
                    db_ann_ids.append(db_ann_id)

        db_ann_ids = bulk_create(db_model=models.AnnotationId, objs=db_ann_ids)

        return db_task_report

    def _get_task_quality_params(self, task: Task) -> Optional[ComparisonParameters]:
        quality_params, _ = models.QualitySettings.objects.get_or_create(task=task)
        return ComparisonParameters.from_dict(quality_params.to_dict())


def prepare_report_for_downloading(db_report: models.QualityReport, *, host: str) -> str:
    # Decorate the report for better usability and readability:
    # - add conflicting annotation links like:
    # <host>/tasks/62/jobs/82?frame=250&type=shape&serverID=33741
    # - convert some fractions to percents
    # - add common report info

    def _serialize_assignee(assignee: Optional[User]) -> Optional[dict]:
        if not db_report.assignee:
            return None

        reported_keys = ["id", "username", "first_name", "last_name"]
        assert set(reported_keys).issubset(engine_serializers.BasicUserSerializer.Meta.fields)
        # check that only safe fields are reported

        return {k: getattr(assignee, k) for k in reported_keys}

    task_id = db_report.get_task().id
    serialized_data = dict(
        job_id=db_report.job.id if db_report.job is not None else None,
        task_id=task_id,
        parent_id=db_report.parent.id if db_report.parent is not None else None,
        created_date=str(db_report.created_date),
        target_last_updated=str(db_report.target_last_updated),
        gt_last_updated=str(db_report.gt_last_updated),
        assignee=_serialize_assignee(db_report.assignee),
    )

    comparison_report = ComparisonReport.from_json(db_report.get_json_report())
    serialized_data.update(comparison_report.to_dict())

    for frame_result in serialized_data["frame_results"].values():
        for conflict in frame_result["conflicts"]:
            for ann_id in conflict["annotation_ids"]:
                ann_id["url"] = (
                    f"{host}tasks/{task_id}/jobs/{ann_id['job_id']}"
                    f"?frame={conflict['frame_id']}"
                    f"&type={ann_id['type']}"
                    f"&serverID={ann_id['obj_id']}"
                )

    # Add the percent representation for better human readability
    serialized_data["comparison_summary"]["frame_share_percent"] = (
        serialized_data["comparison_summary"]["frame_share"] * 100
    )

    # String keys are needed for json dumping
    serialized_data["frame_results"] = {
        str(k): v for k, v in serialized_data["frame_results"].items()
    }
    return dump_json(serialized_data, indent=True, append_newline=True).decode()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\serializers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap

from rest_framework import serializers

from cvat.apps.engine import serializers as engine_serializers
from cvat.apps.quality_control import models


class AnnotationIdSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.AnnotationId
        fields = ("obj_id", "job_id", "type", "shape_type")
        read_only_fields = fields


class AnnotationConflictSerializer(serializers.ModelSerializer):
    annotation_ids = AnnotationIdSerializer(many=True)

    class Meta:
        model = models.AnnotationConflict
        fields = ("id", "frame", "type", "annotation_ids", "report_id", "severity")
        read_only_fields = fields


class QualityReportSummarySerializer(serializers.Serializer):
    frame_count = serializers.IntegerField()
    frame_share = serializers.FloatField()
    conflict_count = serializers.IntegerField()
    warning_count = serializers.IntegerField()
    error_count = serializers.IntegerField()
    conflicts_by_type = serializers.DictField(child=serializers.IntegerField())

    valid_count = serializers.IntegerField(source="annotations.valid_count")
    ds_count = serializers.IntegerField(source="annotations.ds_count")
    gt_count = serializers.IntegerField(source="annotations.gt_count")
    total_count = serializers.IntegerField(source="annotations.total_count")

    accuracy = serializers.FloatField(source="annotations.accuracy")
    precision = serializers.FloatField(source="annotations.precision")
    recall = serializers.FloatField(source="annotations.recall")


class QualityReportSerializer(serializers.ModelSerializer):
    target = serializers.ChoiceField(models.QualityReportTarget.choices())
    assignee = engine_serializers.BasicUserSerializer(allow_null=True, read_only=True)
    summary = QualityReportSummarySerializer()

    class Meta:
        model = models.QualityReport
        fields = (
            "id",
            "job_id",
            "task_id",
            "parent_id",
            "target",
            "summary",
            "created_date",
            "target_last_updated",
            "gt_last_updated",
            "assignee",
        )
        read_only_fields = fields


class QualityReportCreateSerializer(serializers.Serializer):
    task_id = serializers.IntegerField(write_only=True, required=False)


class QualitySettingsSerializer(serializers.ModelSerializer):
    class Meta:
        model = models.QualitySettings
        fields = (
            "id",
            "task_id",
            "target_metric",
            "target_metric_threshold",
            "max_validations_per_job",
            "iou_threshold",
            "oks_sigma",
            "point_size_base",
            "line_thickness",
            "low_overlap_threshold",
            "compare_line_orientation",
            "line_orientation_threshold",
            "compare_groups",
            "group_match_threshold",
            "check_covered_annotations",
            "object_visibility_threshold",
            "panoptic_comparison",
            "compare_attributes",
            "empty_is_annotated",
        )
        read_only_fields = (
            "id",
            "task_id",
        )

        extra_kwargs = {k: {"required": False} for k in fields}
        extra_kwargs.setdefault("empty_is_annotated", {}).setdefault("default", False)

        for field_name, help_text in {
            "target_metric": "The primary metric used for quality estimation",
            "target_metric_threshold": """
                Defines the minimal quality requirements in terms of the selected target metric.
            """,
            "max_validations_per_job": """
                The maximum number of job validation attempts for the job assignee.
                The job can be automatically accepted if the job quality is above the required
                threshold, defined by the target threshold parameter.
            """,
            "iou_threshold": "Used for distinction between matched / unmatched shapes",
            "low_overlap_threshold": """
                Used for distinction between strong / weak (low_overlap) matches
            """,
            "oks_sigma": """
                Like IoU threshold, but for points.
                The percent of the bbox side, used as the radius of the circle around the GT point,
                where the checked point is expected to be. For boxes with different width and
                height, the "side" is computed as a geometric mean of the width and height.
                Read more: https://cocodataset.org/#keypoints-eval
            """,
            "point_size_base": """
                When comparing point annotations (including both separate points and point groups),
                the OKS sigma parameter defines matching area for each GT point based to the
                object size. The point size base parameter allows to configure how to determine
                the object size.
                If {image_size}, the image size is used. Useful if each point
                annotation represents a separate object or boxes grouped with points do not
                represent object boundaries.
                If {group_bbox_size}, the object size is based on
                the point group bbox size. Useful if each point group represents an object
                or there is a bbox grouped with points, representing the object size.
            """.format(
                image_size=models.PointSizeBase.IMAGE_SIZE,
                group_bbox_size=models.PointSizeBase.GROUP_BBOX_SIZE,
            ),
            "line_thickness": """
                Thickness of polylines, relatively to the (image area) ^ 0.5.
                The distance to the boundary around the GT line,
                inside of which the checked line points should be
            """,
            "compare_line_orientation": "Enables or disables polyline orientation comparison",
            "line_orientation_threshold": """
                The minimal gain in the GT IoU between the given and reversed line directions
                to consider the line inverted.
                Only used when the 'compare_line_orientation' parameter is true
            """,
            "compare_groups": "Enables or disables annotation group checks",
            "group_match_threshold": """
                Minimal IoU for groups to be considered matching.
                Only used when the 'compare_groups' parameter is true
            """,
            "check_covered_annotations": """
                Check for partially-covered annotations, useful in segmentation tasks
            """,
            "object_visibility_threshold": """
                Minimal visible area percent of the spatial annotations (polygons, masks)
                for reporting covered annotations.
                Only used when the 'object_visibility_threshold' parameter is true
            """,
            "panoptic_comparison": """
                Use only the visible part of the masks and polygons in comparisons
            """,
            "compare_attributes": "Enables or disables annotation attribute comparison",
            "empty_is_annotated": """
                Consider empty frames annotated as "empty". This affects target metrics like
                accuracy in cases there are no annotations. If disabled, frames without annotations
                are counted as not matching (accuracy is 0). If enabled, accuracy will be 1 instead.
                This will also add virtual annotations to empty frames in the comparison results.
            """,
        }.items():
            extra_kwargs.setdefault(field_name, {}).setdefault(
                "help_text", textwrap.dedent(help_text.lstrip("\n"))
            )

        for field_name in fields:
            if field_name.endswith("_threshold") or field_name in ["oks_sigma", "line_thickness"]:
                extra_kwargs.setdefault(field_name, {}).setdefault("min_value", 0)
                extra_kwargs.setdefault(field_name, {}).setdefault("max_value", 1)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\signals.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.db.models.signals import post_save
from django.dispatch import receiver

from cvat.apps.engine.models import Job, Task
from cvat.apps.quality_control.models import QualitySettings


@receiver(post_save, sender=Task, dispatch_uid=__name__ + ".save_task-initialize_quality_settings")
@receiver(post_save, sender=Job, dispatch_uid=__name__ + ".save_job-initialize_quality_settings")
def __save_task__initialize_quality_settings(
    instance: Task | Job, created: bool, raw: bool, **kwargs
):
    # Initializes default quality settings for the task
    # this is done in a signal to decouple this component from the engine app
    if created and not raw:
        if isinstance(instance, Task):
            task = instance
        elif isinstance(instance, Job):
            task = instance.segment.task
        else:
            assert False

        QualitySettings.objects.get_or_create(task=task)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\urls.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.urls import include, path
from rest_framework import routers

from cvat.apps.quality_control import views

router = routers.DefaultRouter(trailing_slash=False)
router.register("reports", views.QualityReportViewSet, basename="quality_reports")
router.register("conflicts", views.QualityConflictsViewSet, basename="annotation_conflicts")
router.register("settings", views.QualitySettingsViewSet, basename="quality_settings")

urlpatterns = [
    # entry point for API
    path("quality/", include(router.urls)),
]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\views.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap

from django.db.models import Q
from django.http import HttpResponse
from drf_spectacular.types import OpenApiTypes
from drf_spectacular.utils import (
    OpenApiParameter,
    OpenApiResponse,
    extend_schema,
    extend_schema_view,
)
from rest_framework import mixins, status, viewsets
from rest_framework.decorators import action
from rest_framework.exceptions import NotFound, ValidationError
from rest_framework.response import Response
from rq.job import JobStatus as RqJobStatus

from cvat.apps.engine.mixins import PartialUpdateModelMixin
from cvat.apps.engine.models import Task
from cvat.apps.engine.rq import BaseRQMeta
from cvat.apps.engine.serializers import RqIdSerializer
from cvat.apps.engine.utils import get_server_url
from cvat.apps.quality_control import quality_reports as qc
from cvat.apps.quality_control.models import (
    AnnotationConflict,
    QualityReport,
    QualityReportTarget,
    QualitySettings,
)
from cvat.apps.quality_control.permissions import (
    AnnotationConflictPermission,
    QualityReportPermission,
    QualitySettingPermission,
)
from cvat.apps.quality_control.serializers import (
    AnnotationConflictSerializer,
    QualityReportCreateSerializer,
    QualityReportSerializer,
    QualitySettingsSerializer,
)


@extend_schema(tags=["quality"])
@extend_schema_view(
    list=extend_schema(
        summary="List annotation conflicts in a quality report",
        parameters=[
            # These filters are implemented differently from others
            OpenApiParameter(
                "report_id",
                type=OpenApiTypes.INT,
                description="A simple equality filter for report id",
            ),
        ],
        responses={
            "200": AnnotationConflictSerializer(many=True),
        },
    ),
)
class QualityConflictsViewSet(viewsets.GenericViewSet, mixins.ListModelMixin):
    queryset = (
        AnnotationConflict.objects.select_related(
            "report",
            "report__parent",
            "report__job",
            "report__job__segment",
            "report__job__segment__task",
            "report__job__segment__task__organization",
            "report__task",
            "report__task__organization",
        )
        .prefetch_related(
            "annotation_ids",
        )
        .all()
    )

    iam_organization_field = [
        "report__job__segment__task__organization",
        "report__task__organization",
    ]

    search_fields = []
    filter_fields = list(search_fields) + ["id", "frame", "type", "job_id", "task_id", "severity"]
    simple_filters = set(filter_fields) - {"id"}
    lookup_fields = {
        "job_id": "report__job__id",
        "task_id": "report__job__segment__task__id",  # task reports do not contain own conflicts
    }
    ordering_fields = list(filter_fields)
    ordering = "-id"
    serializer_class = AnnotationConflictSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            if report_id := self.request.query_params.get("report_id", None):
                # NOTE: This filter is too complex to be implemented by other means,
                # it has a dependency on the report type
                try:
                    report = QualityReport.objects.get(id=report_id)
                except QualityReport.DoesNotExist as ex:
                    raise NotFound(f"Report {report_id} does not exist") from ex

                self.check_object_permissions(self.request, report)

                if report.target == QualityReportTarget.TASK:
                    queryset = queryset.filter(
                        Q(report=report) | Q(report__parent=report)
                    ).distinct()
                elif report.target == QualityReportTarget.JOB:
                    queryset = queryset.filter(report=report)
                else:
                    assert False
            else:
                perm = AnnotationConflictPermission.create_scope_list(self.request)
                queryset = perm.filter(queryset)

        return queryset


@extend_schema(tags=["quality"])
@extend_schema_view(
    retrieve=extend_schema(
        operation_id="quality_retrieve_report",  # the default produces the plural
        summary="Get quality report details",
        responses={
            "200": QualityReportSerializer,
        },
    ),
    list=extend_schema(
        summary="List quality reports",
        parameters=[
            # These filters are implemented differently from others
            OpenApiParameter(
                "task_id", type=OpenApiTypes.INT, description="A simple equality filter for task id"
            ),
            OpenApiParameter(
                "target", type=OpenApiTypes.STR, description="A simple equality filter for target"
            ),
        ],
        responses={
            "200": QualityReportSerializer(many=True),
        },
    ),
)
class QualityReportViewSet(
    viewsets.GenericViewSet,
    mixins.ListModelMixin,
    mixins.RetrieveModelMixin,
    mixins.CreateModelMixin,
):
    queryset = QualityReport.objects.prefetch_related(
        "job",
        "job__segment",
        "job__segment__task",
        "job__segment__task__organization",
        "task",
        "task__organization",
    ).all()

    iam_organization_field = ["job__segment__task__organization", "task__organization"]

    search_fields = []
    filter_fields = list(search_fields) + [
        "id",
        "job_id",
        "created_date",
        "gt_last_updated",
        "target_last_updated",
        "parent_id",
    ]
    simple_filters = list(
        set(filter_fields) - {"id", "created_date", "gt_last_updated", "target_last_updated"}
    )
    ordering_fields = list(filter_fields)
    ordering = "id"

    def get_serializer_class(self):
        # a separate method is required for drf-spectacular to work
        return QualityReportSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            if task_id := self.request.query_params.get("task_id", None):
                # NOTE: This filter is too complex to be implemented by other means
                try:
                    task = Task.objects.get(id=task_id)
                except Task.DoesNotExist as ex:
                    raise NotFound(f"Task {task_id} does not exist") from ex

                self.check_object_permissions(self.request, task)

                queryset = queryset.filter(
                    Q(job__segment__task__id=task_id) | Q(task__id=task_id)
                ).distinct()
            else:
                perm = QualityReportPermission.create_scope_list(self.request)
                queryset = perm.filter(queryset)

            if target := self.request.query_params.get("target", None):
                if target == QualityReportTarget.JOB:
                    queryset = queryset.filter(job__isnull=False)
                elif target == QualityReportTarget.TASK:
                    queryset = queryset.filter(task__isnull=False)
                else:
                    raise ValidationError(
                        "Unexpected 'target' filter value '{}'. Valid values are: {}".format(
                            target, ", ".join(m[0] for m in QualityReportTarget.choices())
                        )
                    )

        return queryset

    CREATE_REPORT_RQ_ID_PARAMETER = "rq_id"

    @extend_schema(
        operation_id="quality_create_report",
        summary="Create a quality report",
        parameters=[
            OpenApiParameter(
                CREATE_REPORT_RQ_ID_PARAMETER,
                type=str,
                description=textwrap.dedent(
                    """\
                    The report creation request id. Can be specified to check the report
                    creation status.
                """
                ),
            )
        ],
        request=QualityReportCreateSerializer(required=False),
        responses={
            "201": QualityReportSerializer,
            "202": OpenApiResponse(
                RqIdSerializer,
                description=textwrap.dedent(
                    """\
                    A quality report request has been enqueued, the request id is returned.
                    The request status can be checked at this endpoint by passing the {}
                    as the query parameter. If the request id is specified, this response
                    means the quality report request is queued or is being processed.
                """.format(
                        CREATE_REPORT_RQ_ID_PARAMETER
                    )
                ),
            ),
            "400": OpenApiResponse(
                description="Invalid or failed request, check the response data for details"
            ),
        },
    )
    def create(self, request, *args, **kwargs):
        self.check_permissions(request)

        rq_id = request.query_params.get(self.CREATE_REPORT_RQ_ID_PARAMETER, None)

        if rq_id is None:
            input_serializer = QualityReportCreateSerializer(data=request.data)
            input_serializer.is_valid(raise_exception=True)

            task_id = input_serializer.validated_data["task_id"]

            try:
                task = Task.objects.get(pk=task_id)
            except Task.DoesNotExist as ex:
                raise NotFound(f"Task {task_id} does not exist") from ex

            try:
                rq_id = qc.QualityReportUpdateManager().schedule_custom_quality_check_job(
                    request=request, task=task, user_id=request.user.id
                )
                serializer = RqIdSerializer({"rq_id": rq_id})
                return Response(serializer.data, status=status.HTTP_202_ACCEPTED)
            except qc.QualityReportUpdateManager.QualityReportsNotAvailable as ex:
                raise ValidationError(str(ex))

        else:
            serializer = RqIdSerializer(data={"rq_id": rq_id})
            serializer.is_valid(raise_exception=True)
            rq_id = serializer.validated_data["rq_id"]

            report_manager = qc.QualityReportUpdateManager()
            rq_job = report_manager.get_quality_check_job(rq_id)
            # FUTURE-TODO: move into permissions
            # and allow not only rq job owner to check the status
            if (
                not rq_job
                or not QualityReportPermission.create_scope_check_status(
                    request, rq_job_owner_id=BaseRQMeta.for_job(rq_job).user.id
                )
                .check_access()
                .allow
            ):
                # We should not provide job existence information to unauthorized users
                raise NotFound("Unknown request id")

            rq_job_status = rq_job.get_status(refresh=False)

            if rq_job_status == RqJobStatus.FAILED:
                message = str(rq_job.exc_info)
                rq_job.delete()
                raise ValidationError(message)
            elif rq_job_status in (
                RqJobStatus.QUEUED,
                RqJobStatus.STARTED,
                RqJobStatus.SCHEDULED,
                RqJobStatus.DEFERRED,
            ):
                return Response(serializer.data, status=status.HTTP_202_ACCEPTED)
            elif rq_job_status == RqJobStatus.FINISHED:
                return_value = rq_job.return_value()
                rq_job.delete()
                if not return_value:
                    raise ValidationError("No report has been computed")

                report = self.get_queryset().get(pk=return_value)
                report_serializer = QualityReportSerializer(
                    instance=report, context={"request": request}
                )
                return Response(
                    data=report_serializer.data,
                    status=status.HTTP_201_CREATED,
                    headers=self.get_success_headers(report_serializer.data),
                )

            raise AssertionError(f"Unexpected rq job '{rq_id}' status '{rq_job_status}'")

    @extend_schema(
        operation_id="quality_retrieve_report_data",
        summary="Get quality report contents",
        responses={"200": OpenApiTypes.OBJECT},
    )
    @action(detail=True, methods=["GET"], url_path="data", serializer_class=None)
    def data(self, request, pk):
        report = self.get_object()  # check permissions
        json_report = qc.prepare_report_for_downloading(report, host=get_server_url(request))
        return HttpResponse(json_report.encode(), content_type="application/json")


@extend_schema(tags=["quality"])
@extend_schema_view(
    list=extend_schema(
        summary="List quality settings instances",
        responses={
            "200": QualitySettingsSerializer(many=True),
        },
    ),
    retrieve=extend_schema(
        summary="Get quality settings instance details",
        parameters=[
            OpenApiParameter(
                "id",
                type=OpenApiTypes.INT,
                location="path",
                description="An id of a quality settings instance",
            )
        ],
        responses={
            "200": QualitySettingsSerializer,
        },
    ),
    partial_update=extend_schema(
        summary="Update a quality settings instance",
        parameters=[
            OpenApiParameter(
                "id",
                type=OpenApiTypes.INT,
                location="path",
                description="An id of a quality settings instance",
            )
        ],
        request=QualitySettingsSerializer(partial=True),
        responses={
            "200": QualitySettingsSerializer,
        },
    ),
)
class QualitySettingsViewSet(
    viewsets.GenericViewSet,
    mixins.ListModelMixin,
    mixins.RetrieveModelMixin,
    PartialUpdateModelMixin,
):
    queryset = QualitySettings.objects.select_related("task", "task__organization").all()

    iam_organization_field = "task__organization"

    search_fields = []
    filter_fields = ["id", "task_id"]
    simple_filters = ["task_id"]
    ordering_fields = ["id"]
    ordering = "id"

    serializer_class = QualitySettingsSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            permissions = QualitySettingPermission.create_scope_list(self.request)
            queryset = permissions.filter(queryset)

        return queryset


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0001_initial.py =====
# Generated by Django 4.2.1 on 2023-06-08 12:31

import django.db.models.deletion
from django.db import migrations, models


class Migration(migrations.Migration):
    initial = True

    dependencies = [
        ("engine", "0070_add_job_type_created_date"),
    ]

    operations = [
        migrations.CreateModel(
            name="AnnotationConflict",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("frame", models.PositiveIntegerField()),
                (
                    "type",
                    models.CharField(
                        choices=[
                            ("missing_annotation", "MISSING_ANNOTATION"),
                            ("extra_annotation", "EXTRA_ANNOTATION"),
                            ("mismatching_label", "MISMATCHING_LABEL"),
                            ("low_overlap", "LOW_OVERLAP"),
                            ("mismatching_direction", "MISMATCHING_DIRECTION"),
                            ("mismatching_attributes", "MISMATCHING_ATTRIBUTES"),
                            ("mismatching_groups", "MISMATCHING_GROUPS"),
                            ("covered_annotation", "COVERED_ANNOTATION"),
                        ],
                        max_length=32,
                    ),
                ),
                (
                    "severity",
                    models.CharField(
                        choices=[("warning", "WARNING"), ("error", "ERROR")], max_length=32
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="QualitySettings",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("iou_threshold", models.FloatField()),
                ("oks_sigma", models.FloatField()),
                ("line_thickness", models.FloatField()),
                ("low_overlap_threshold", models.FloatField()),
                ("compare_line_orientation", models.BooleanField()),
                ("line_orientation_threshold", models.FloatField()),
                ("compare_groups", models.BooleanField()),
                ("group_match_threshold", models.FloatField()),
                ("check_covered_annotations", models.BooleanField()),
                ("object_visibility_threshold", models.FloatField()),
                ("panoptic_comparison", models.BooleanField()),
                ("compare_attributes", models.BooleanField()),
                (
                    "task",
                    models.OneToOneField(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="quality_settings",
                        to="engine.task",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="QualityReport",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("target_last_updated", models.DateTimeField()),
                ("gt_last_updated", models.DateTimeField()),
                ("data", models.JSONField()),
                (
                    "job",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="quality_reports",
                        to="engine.job",
                    ),
                ),
                (
                    "parent",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="children",
                        to="quality_control.qualityreport",
                    ),
                ),
                (
                    "task",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="quality_reports",
                        to="engine.task",
                    ),
                ),
            ],
        ),
        migrations.CreateModel(
            name="AnnotationId",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("obj_id", models.PositiveIntegerField()),
                ("job_id", models.PositiveIntegerField()),
                (
                    "type",
                    models.CharField(
                        choices=[("tag", "TAG"), ("shape", "SHAPE"), ("track", "TRACK")],
                        max_length=32,
                    ),
                ),
                (
                    "shape_type",
                    models.CharField(
                        choices=[
                            ("rectangle", "RECTANGLE"),
                            ("polygon", "POLYGON"),
                            ("polyline", "POLYLINE"),
                            ("points", "POINTS"),
                            ("ellipse", "ELLIPSE"),
                            ("cuboid", "CUBOID"),
                            ("mask", "MASK"),
                            ("skeleton", "SKELETON"),
                        ],
                        default=None,
                        max_length=32,
                        null=True,
                    ),
                ),
                (
                    "conflict",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="annotation_ids",
                        to="quality_control.annotationconflict",
                    ),
                ),
            ],
        ),
        migrations.AddField(
            model_name="annotationconflict",
            name="report",
            field=models.ForeignKey(
                on_delete=django.db.models.deletion.CASCADE,
                related_name="conflicts",
                to="quality_control.qualityreport",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0002_qualityreport_assignee.py =====
# Generated by Django 4.2.13 on 2024-07-12 19:06

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("quality_control", "0001_initial"),
    ]

    operations = [
        migrations.AddField(
            model_name="qualityreport",
            name="assignee",
            field=models.ForeignKey(
                blank=True,
                null=True,
                on_delete=django.db.models.deletion.SET_NULL,
                related_name="quality_reports",
                to=settings.AUTH_USER_MODEL,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0003_qualityreport_assignee_last_updated_and_more.py =====
# Generated by Django 4.2.15 on 2024-08-21 13:56

from django.db import migrations, models

import cvat.apps.quality_control.models


class Migration(migrations.Migration):

    dependencies = [
        ("quality_control", "0002_qualityreport_assignee"),
    ]

    operations = [
        migrations.AddField(
            model_name="qualityreport",
            name="assignee_last_updated",
            field=models.DateTimeField(null=True),
        ),
        migrations.AddField(
            model_name="qualitysettings",
            name="max_validations_per_job",
            field=models.PositiveIntegerField(default=0),
        ),
        migrations.AddField(
            model_name="qualitysettings",
            name="target_metric",
            field=models.CharField(
                choices=[
                    ("accuracy", "ACCURACY"),
                    ("precision", "PRECISION"),
                    ("recall", "RECALL"),
                ],
                default=cvat.apps.quality_control.models.QualityTargetMetricType["ACCURACY"],
                max_length=32,
            ),
        ),
        migrations.AddField(
            model_name="qualitysettings",
            name="target_metric_threshold",
            field=models.FloatField(default=0.7),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0004_qualitysettings_point_size_base.py =====
# Generated by Django 4.2.15 on 2024-11-06 15:39

from django.db import migrations, models

import cvat.apps.quality_control.models


class Migration(migrations.Migration):

    dependencies = [
        ("quality_control", "0003_qualityreport_assignee_last_updated_and_more"),
    ]

    operations = [
        migrations.AddField(
            model_name="qualitysettings",
            name="point_size_base",
            field=models.CharField(
                choices=[("image_size", "IMAGE_SIZE"), ("group_bbox_size", "GROUP_BBOX_SIZE")],
                default=cvat.apps.quality_control.models.PointSizeBase["GROUP_BBOX_SIZE"],
                max_length=32,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0005_qualitysettings_match_empty.py =====
# Generated by Django 4.2.15 on 2024-11-05 14:22

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("quality_control", "0004_qualitysettings_point_size_base"),
    ]

    operations = [
        migrations.AddField(
            model_name="qualitysettings",
            name="match_empty_frames",
            field=models.BooleanField(default=False),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\0006_rename_match_empty_frames_qualitysettings_empty_is_annotated.py =====
# Generated by Django 4.2.15 on 2024-12-29 19:08

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ("quality_control", "0005_qualitysettings_match_empty"),
    ]

    operations = [
        migrations.RenameField(
            model_name="qualitysettings",
            old_name="match_empty_frames",
            new_name="empty_is_annotated",
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\quality_control\migrations\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


from django.apps import AppConfig


class RedisHandlerConfig(AppConfig):
    name = "cvat.apps.redis_handler"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\migration_loader.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from datetime import datetime
from pathlib import Path
from typing import Any, ClassVar

from attrs import field, frozen, validators
from django.apps import AppConfig, apps
from django.utils import timezone
from redis import Redis

from cvat.apps.redis_handler.redis_migrations import BaseMigration
from cvat.apps.redis_handler.utils import get_class_from_module


def to_datetime(value: float | str | datetime) -> datetime:
    if isinstance(value, datetime):
        return value
    elif isinstance(value, str):
        value = float(value)

    return datetime.fromtimestamp(value)


@frozen
class AppliedMigration:
    SET_KEY: ClassVar[str] = "cvat:applied_migrations"
    KEY_PREFIX: ClassVar[str] = "cvat:applied_migration:"

    name: str = field(validator=[validators.instance_of(str), validators.max_len(128)])
    app_label: str = field(validator=[validators.instance_of(str), validators.max_len(128)])
    applied_date: datetime = field(
        validator=[validators.instance_of(datetime)], converter=to_datetime, factory=timezone.now
    )

    def get_key(self) -> str:
        return f"{self.app_label}.{self.name}"

    def get_key_with_prefix(self) -> str:
        return self.KEY_PREFIX + self.get_key()

    def to_dict(self) -> dict[str, Any]:
        return {
            "applied_date": self.applied_date.timestamp(),
        }

    def save(self, *, connection: Redis) -> None:
        with connection.pipeline() as pipe:
            pipe.hset(self.get_key_with_prefix(), mapping=self.to_dict())
            pipe.sadd(self.SET_KEY, self.get_key())
            pipe.execute()


class LoaderError(Exception):
    pass


class MigrationLoader:
    REDIS_MIGRATIONS_DIR_NAME = "redis_migrations"
    REDIS_MIGRATION_CLASS_NAME = "Migration"

    def __init__(self, *, connection: Redis) -> None:
        self._connection = connection
        self._app_config_mapping = {
            app_config.label: app_config for app_config in self._find_app_configs()
        }
        self._disk_migrations_per_app: dict[str, list[str]] = {}
        self._applied_migrations: dict[str, set[str]] = {}
        self._unapplied_migrations: list[BaseMigration] = []

        self._load_from_disk()
        self._init_applied_migrations()
        self._init_unapplied_migrations()

    @classmethod
    def _find_app_configs(cls) -> list[AppConfig]:
        return [
            app_config
            for app_config in apps.get_app_configs()
            if app_config.name.startswith("cvat")
            and (Path(app_config.path) / cls.REDIS_MIGRATIONS_DIR_NAME).exists()
        ]

    def _load_from_disk(self):
        for app_label, app_config in self._app_config_mapping.items():
            migrations_dir = Path(app_config.path) / self.REDIS_MIGRATIONS_DIR_NAME
            for migration_file in sorted(migrations_dir.glob("[0-9]*.py")):
                migration_name = migration_file.stem
                (self._disk_migrations_per_app.setdefault(app_label, [])).append(migration_name)

    def _init_applied_migrations(self):
        applied_migration_keys: list[str] = [
            i.decode("utf-8") for i in self._connection.smembers(AppliedMigration.SET_KEY)
        ]
        for key in applied_migration_keys:
            app_label, migration_name = key.split(".")
            self._applied_migrations.setdefault(app_label, set()).add(migration_name)

    def _init_unapplied_migrations(self):
        for app_label, migration_names in self._disk_migrations_per_app.items():
            app_config = self._app_config_mapping[app_label]
            app_unapplied_migrations = sorted(
                set(migration_names) - self._applied_migrations.get(app_label, set())
            )
            for migration_name in app_unapplied_migrations:
                MigrationClass = self.get_migration_class(app_config.name, migration_name)
                self._unapplied_migrations.append(
                    MigrationClass(migration_name, app_config.label, connection=self._connection)
                )

    def get_migration_class(self, app_name: str, migration_name: str) -> BaseMigration:
        migration_module_path = ".".join([app_name, self.REDIS_MIGRATIONS_DIR_NAME, migration_name])
        MigrationClass = get_class_from_module(
            migration_module_path, self.REDIS_MIGRATION_CLASS_NAME
        )

        if not MigrationClass or not issubclass(MigrationClass, BaseMigration):
            raise LoaderError(f"Invalid migration: {migration_module_path}")

        return MigrationClass

    def __iter__(self):
        yield from self._unapplied_migrations

    def __len__(self):
        return len(self._unapplied_migrations)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import importlib
from pathlib import Path


def get_class_from_module(module_path: str | Path, class_name: str) -> type | None:
    module = importlib.import_module(module_path)
    klass = getattr(module, class_name, None)
    return klass


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\management\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\management\commands\migrateredis.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import sys
import traceback
from argparse import ArgumentParser

from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from redis import Redis

from cvat.apps.redis_handler.migration_loader import AppliedMigration, MigrationLoader


class Command(BaseCommand):
    help = "Applies Redis migrations and records them in the database"

    def add_arguments(self, parser: ArgumentParser) -> None:
        parser.add_argument(
            "--check",
            action="store_true",
            help="Checks whether Redis migrations have been applied; exits with non-zero status if not",
        )

    def handle(self, *args, **options) -> None:
        conn = Redis(
            host=settings.REDIS_INMEM_SETTINGS["HOST"],
            port=settings.REDIS_INMEM_SETTINGS["PORT"],
            db=settings.REDIS_INMEM_SETTINGS["DB"],
            password=settings.REDIS_INMEM_SETTINGS["PASSWORD"],
        )
        loader = MigrationLoader(connection=conn)

        if options["check"]:
            if not loader:
                return

            sys.exit(1)

        if not loader:
            self.stdout.write("No migrations to apply")
            return

        for migration in loader:
            try:
                migration.run()

                # add migration to applied ones
                applied_migration = AppliedMigration(
                    name=migration.name,
                    app_label=migration.app_label,
                )
                applied_migration.save(connection=conn)

            except Exception as ex:
                self.stderr.write(
                    self.style.ERROR(
                        f"[{migration.app_label}] Failed to apply migration: {migration.name}"
                    )
                )
                self.stderr.write(self.style.ERROR(f"\n{traceback.format_exc()}"))
                raise CommandError(str(ex))

            self.stdout.write(
                self.style.SUCCESS(
                    f"[{migration.app_label}] Successfully applied migration: {migration.name}"
                )
            )


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\management\commands\runperiodicjob.py =====
from argparse import ArgumentParser

from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from django.utils.module_loading import import_string


class Command(BaseCommand):
    help = "Run a configured periodic job immediately"

    def add_arguments(self, parser: ArgumentParser) -> None:
        parser.add_argument("job_id", help="ID of the job to run")

    def handle(self, *args, **options):
        job_id = options["job_id"]

        for job_definition in settings.PERIODIC_RQ_JOBS:
            if job_definition["id"] == job_id:
                job_func = import_string(job_definition["func"])
                job_func(
                    *(job_definition.get("args", [])),
                    **(job_definition.get("kwargs", {})),
                )
                return

        raise CommandError(f"Job with ID {job_id} not found")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\management\commands\syncperiodicjobs.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from argparse import ArgumentParser
from collections import defaultdict

import django_rq
from django.conf import settings
from django.core.management.base import BaseCommand
from rq.job import Job as RQJob


class Command(BaseCommand):
    help = "Synchronize periodic jobs in Redis with the project configuration"

    _PERIODIC_JOBS_KEY_PREFIX = "cvat:utils:periodic-jobs:"

    def add_arguments(self, parser: ArgumentParser) -> None:
        parser.add_argument(
            "--clear", action="store_true", help="Remove jobs from Redis instead of updating them"
        )

    def handle(self, *args, **options):
        configured_jobs = defaultdict(dict)

        if not options["clear"]:
            for job in settings.PERIODIC_RQ_JOBS:
                configured_jobs[job["queue"]][job["id"]] = job

        for queue_name in settings.RQ_QUEUES:
            self.stdout.write(f"Processing queue {queue_name}...")

            periodic_jobs_key = self._PERIODIC_JOBS_KEY_PREFIX + queue_name

            queue = django_rq.get_queue(queue_name)
            scheduler = django_rq.get_scheduler(queue_name, queue=queue)

            stored_jobs_for_queue = {
                member.decode("UTF-8") for member in queue.connection.smembers(periodic_jobs_key)
            }
            configured_jobs_for_queue = configured_jobs[queue_name]

            # Delete jobs that are no longer in the configuration
            jobs_to_delete = stored_jobs_for_queue.difference(configured_jobs_for_queue.keys())

            for job_id in jobs_to_delete:
                self.stdout.write(f"Deleting job {job_id}...")
                scheduler.cancel(job_id)
                if job := queue.fetch_job(job_id):
                    job.delete()

                queue.connection.srem(periodic_jobs_key, job_id)

            def is_job_actual(job: RQJob, job_definition: dict):
                return (
                    job.func_name == job_definition["func"]
                    and job.meta.get("cron_string") == job_definition["cron_string"]
                    and (
                        not (job.args or job_definition.get("args"))
                        or job.args == job_definition.get("args")
                    )
                    and (
                        not (job.kwargs or job_definition.get("kwargs"))
                        or job.kwargs == job_definition.get("kwargs")
                    )
                )

            # Add/update jobs from the configuration
            for job_definition in configured_jobs_for_queue.values():
                job_id = job_definition["id"]

                if job := queue.fetch_job(job_id):
                    if is_job_actual(job, job_definition):
                        self.stdout.write(f"Job {job_id} is unchanged")
                        queue.connection.sadd(periodic_jobs_key, job_id)
                        continue

                    self.stdout.write(f"Recreating job {job_id}...")
                    job.delete()
                else:
                    self.stdout.write(f"Creating job {job_id}...")

                scheduler.cron(
                    cron_string=job_definition["cron_string"],
                    func=job_definition["func"],
                    id=job_id,
                    args=job_definition.get("args"),
                    kwargs=job_definition.get("kwargs"),
                )

                queue.connection.sadd(periodic_jobs_key, job_id)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\management\commands\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\redis_migrations\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from abc import ABCMeta, abstractmethod

from attrs import define, field, validators
from redis import Redis


@define
class BaseMigration(metaclass=ABCMeta):
    name: str = field(validator=[validators.instance_of(str)])
    app_label: str = field(validator=[validators.instance_of(str)])
    connection: Redis = field(validator=[validators.instance_of(Redis)], kw_only=True)

    @abstractmethod
    def run(self) -> None: ...


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\tests\test_redis_migrations.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
from pathlib import Path
from unittest import TestCase
from unittest.mock import patch

import fakeredis
from django.core.management import call_command

from cvat.apps.redis_handler.migration_loader import AppliedMigration, LoaderError, MigrationLoader
from cvat.apps.redis_handler.utils import get_class_from_module

from .utils import path_to_module

WORKDIR = Path("cvat/apps")

MIGRATION_DIR = MigrationLoader.REDIS_MIGRATIONS_DIR_NAME
MIGRATION_CLASS_NAME = MigrationLoader.REDIS_MIGRATION_CLASS_NAME
MIGRATION_NAME_FORMAT = "{:03}_{}.py"
BAD_MIGRATION_FILE = """\
class Migration:
    @classmethod
    def run(cls): ...

"""


@patch(
    f"cvat.apps.redis_handler.management.commands.migrateredis.Redis",
    return_value=fakeredis.FakeRedis(),
)
class TestRedisMigrations(TestCase):
    class BadMigration:

        def __init__(self, app_name: str, migration_name: str):
            self.app_name = app_name
            self.app_path = WORKDIR / app_name
            assert self.app_path.exists()
            self.migration_name = migration_name
            self.number = 0

            self.migration_file_path = self.generate_migration_file()
            mock_migration_module_path = path_to_module(self.migration_file_path)

            self.test_class = get_class_from_module(
                mock_migration_module_path, MIGRATION_CLASS_NAME
            )
            assert self.test_class is not None

        def make_migration_name(self):
            return MIGRATION_NAME_FORMAT.format(self.number, self.migration_name)

        def generate_migration_file(self) -> Path:
            migration_dir = self.app_path / MIGRATION_DIR
            if not os.path.exists(migration_dir):
                os.mkdir(migration_dir)
            filename = self.make_migration_name()
            migration_file_path = migration_dir / filename
            with open(migration_file_path, "w") as file:
                file.write(BAD_MIGRATION_FILE)
            return migration_file_path

        def cleanup(self):
            os.remove(self.migration_file_path)

    def test_migration_added_and_applied(self, redis):

        # Keys are not added yet
        with self.assertRaises(SystemExit):
            call_command("migrateredis", check=True)

        # Add keys
        call_command("migrateredis")

        # Keys are added
        self.assertIsNone(call_command("migrateredis", check=True))

        # Check keys added
        expected_migrations = {
            f"{app_config.label}.{f.stem}".encode()
            for app_config in MigrationLoader._find_app_configs()
            for f in (Path(app_config.path) / MigrationLoader.REDIS_MIGRATIONS_DIR_NAME).glob(
                "[0-9]*.py"
            )
        }
        assert len(expected_migrations)

        with redis() as conn:
            applied_migrations = conn.smembers(AppliedMigration.SET_KEY)
            self.assertEqual(expected_migrations, applied_migrations)

    def test_migration_bad(self, _):
        self.test_migration = self.BadMigration("redis_handler", "bad")
        self.addCleanup(self.test_migration.cleanup)

        with patch.object(self.test_migration.test_class, "run") as mock_run:
            with self.assertRaises(LoaderError):
                call_command("migrateredis")
            mock_run.assert_not_called()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\tests\utils.py =====
from pathlib import Path


def path_to_module(path: Path) -> str:
    return str(path).removesuffix(".py").replace("/", ".")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\redis_handler\tests\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\apps.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from django.apps import AppConfig


class WebhooksConfig(AppConfig):
    name = "cvat.apps.webhooks"

    def ready(self):
        from cvat.apps.iam.permissions import load_app_permissions

        load_app_permissions(self)

        from . import signals  # pylint: disable=unused-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\event_type.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from .models import WebhookTypeChoice


def event_name(action, resource):
    return f"{action}:{resource}"


class Events:
    RESOURCES = {
        "project": ["create", "update", "delete"],
        "task": ["create", "update", "delete"],
        "job": ["create", "update", "delete"],
        "issue": ["create", "update", "delete"],
        "comment": ["create", "update", "delete"],
        "organization": ["update", "delete"],
        "invitation": ["create", "delete"],
        "membership": ["create", "update", "delete"],
    }

    @classmethod
    def select(cls, resources):
        return [
            f"{event_name(action, resource)}"
            for resource in resources
            for action in cls.RESOURCES.get(resource, [])
        ]


class EventTypeChoice:
    @classmethod
    def choices(cls):
        return sorted((val, val.upper()) for val in AllEvents.events)


class AllEvents:
    webhook_type = "all"
    events = list(
        event_name(action, resource)
        for resource, actions in Events.RESOURCES.items()
        for action in actions
    )


class ProjectEvents:
    webhook_type = WebhookTypeChoice.PROJECT
    events = [
        *Events.select(["task", "job", "label", "issue", "comment"]),
        event_name("update", "project"),
        event_name("delete", "project"),
    ]


class OrganizationEvents:
    webhook_type = WebhookTypeChoice.ORGANIZATION
    events = AllEvents.events


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\models.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from enum import Enum

from django.contrib.auth.models import User
from django.db import models

from cvat.apps.engine.models import Project, TimestampedModel
from cvat.apps.organizations.models import Organization


class WebhookTypeChoice(str, Enum):
    ORGANIZATION = "organization"
    PROJECT = "project"

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value


class WebhookContentTypeChoice(str, Enum):
    JSON = "application/json"

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value


class Webhook(TimestampedModel):
    target_url = models.URLField(max_length=8192)
    description = models.CharField(max_length=128, default="", blank=True)

    events = models.CharField(max_length=4096, default="")
    type = models.CharField(max_length=16, choices=WebhookTypeChoice.choices())
    content_type = models.CharField(
        max_length=64,
        choices=WebhookContentTypeChoice.choices(),
        default=WebhookContentTypeChoice.JSON,
    )
    secret = models.CharField(max_length=64, blank=True, default="")

    is_active = models.BooleanField(default=True)
    enable_ssl = models.BooleanField(default=True)

    owner = models.ForeignKey(
        User, null=True, blank=True, on_delete=models.SET_NULL, related_name="+"
    )
    project = models.ForeignKey(Project, null=True, on_delete=models.CASCADE, related_name="+")
    organization = models.ForeignKey(
        Organization, null=True, on_delete=models.CASCADE, related_name="+"
    )

    class Meta:
        default_permissions = ()
        constraints = [
            models.CheckConstraint(
                name="webhooks_project_or_organization",
                check=(
                    models.Q(type=WebhookTypeChoice.PROJECT.value, project_id__isnull=False)
                    | models.Q(
                        type=WebhookTypeChoice.ORGANIZATION.value,
                        project_id__isnull=True,
                        organization_id__isnull=False,
                    )
                ),
            )
        ]


class WebhookDelivery(TimestampedModel):
    webhook = models.ForeignKey(Webhook, on_delete=models.CASCADE, related_name="deliveries")
    event = models.CharField(max_length=64)

    status_code = models.PositiveIntegerField(null=True, default=None)
    redelivery = models.BooleanField(default=False)

    changed_fields = models.CharField(max_length=4096, default="")

    request = models.JSONField(default=dict)
    response = models.JSONField(default=dict)

    class Meta:
        default_permissions = ()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\permissions.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import Optional

from django.conf import settings
from rest_framework.exceptions import ValidationError

from cvat.apps.engine.models import Project
from cvat.apps.engine.permissions import ProjectPermission, UserPermission
from cvat.apps.iam.permissions import OpenPolicyAgentPermission, StrEnum

from .models import Webhook, WebhookTypeChoice


class WebhookPermission(OpenPolicyAgentPermission):
    obj: Optional[Webhook]

    class Scopes(StrEnum):
        CREATE = "create"
        CREATE_IN_PROJECT = "create@project"
        CREATE_IN_ORG = "create@organization"
        DELETE = "delete"
        UPDATE = "update"
        LIST = "list"
        VIEW = "view"

    @classmethod
    def create(cls, request, view, obj, iam_context):
        permissions = []
        if view.basename == "webhook":
            project_id = request.data.get("project_id")
            for scope in cls.get_scopes(request, view, obj):
                self = cls.create_base_perm(
                    request, view, scope, iam_context, obj, project_id=project_id
                )
                permissions.append(self)

            owner = request.data.get("owner_id") or request.data.get("owner")
            if owner:
                perm = UserPermission.create_scope_view(iam_context, owner)
                permissions.append(perm)

            if project_id:
                perm = ProjectPermission.create_scope_view(request, project_id, iam_context)
                permissions.append(perm)

        return permissions

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.url = settings.IAM_OPA_DATA_URL + "/webhooks/allow"

    @staticmethod
    def get_scopes(request, view, obj):
        Scopes = __class__.Scopes
        scope = {
            ("create", "POST"): Scopes.CREATE,
            ("destroy", "DELETE"): Scopes.DELETE,
            ("partial_update", "PATCH"): Scopes.UPDATE,
            ("update", "PUT"): Scopes.UPDATE,
            ("list", "GET"): Scopes.LIST,
            ("retrieve", "GET"): Scopes.VIEW,
            ("ping", "POST"): Scopes.UPDATE,
            ("deliveries", "GET"): Scopes.VIEW,
            ("retrieve_delivery", "GET"): Scopes.VIEW,
            ("redelivery", "POST"): Scopes.UPDATE,
        }[(view.action, request.method)]

        scopes = []
        if scope == Scopes.CREATE:
            webhook_type = request.data.get("type")
            if webhook_type in [m.value for m in WebhookTypeChoice]:
                scope = Scopes(str(scope) + f"@{webhook_type}")
            scopes.append(scope)
        else:
            scopes.append(scope)

        return scopes

    def get_resource(self):
        data = None
        if self.obj:
            data = {
                "id": self.obj.id,
                "owner": {"id": self.obj.owner_id},
                "organization": {"id": self.obj.organization_id},
                "project": None,
            }
            if self.obj.type == "project" and self.obj.project_id:
                data["project"] = {"owner": {"id": self.obj.project.owner_id}}
        elif self.scope in [
            __class__.Scopes.CREATE,
            __class__.Scopes.CREATE_IN_PROJECT,
            __class__.Scopes.CREATE_IN_ORG,
        ]:
            project = None
            if self.project_id:
                try:
                    project = Project.objects.get(id=self.project_id)
                except Project.DoesNotExist:
                    raise ValidationError(
                        f"Could not find project with provided id: {self.project_id}"
                    )

            data = {
                "id": None,
                "owner": self.user_id,
                "project": (
                    {
                        "owner": (
                            {
                                "id": project.owner_id,
                            }
                            if project.owner_id
                            else None
                        ),
                    }
                    if project
                    else None
                ),
                "organization": (
                    {
                        "id": self.org_id,
                    }
                    if self.org_id is not None
                    else None
                ),
                "user": {
                    "id": self.user_id,
                },
            }

        return data


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\serializers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework import serializers

from cvat.apps.engine.models import Project
from cvat.apps.engine.serializers import BasicUserSerializer, WriteOnceMixin

from .event_type import EventTypeChoice, OrganizationEvents, ProjectEvents
from .models import Webhook, WebhookContentTypeChoice, WebhookDelivery, WebhookTypeChoice


class EventTypeValidator:
    requires_context = True

    def get_webhook_type(self, attrs, serializer):
        if serializer.instance is not None:
            return serializer.instance.type
        return attrs.get("type")

    def __call__(self, attrs, serializer):
        if attrs.get("events") is not None:
            webhook_type = self.get_webhook_type(attrs, serializer)
            events = set(EventTypesSerializer().to_representation(attrs["events"]))
            if (
                webhook_type == WebhookTypeChoice.PROJECT
                and not events.issubset(set(ProjectEvents.events))
            ) or (
                webhook_type == WebhookTypeChoice.ORGANIZATION
                and not events.issubset(set(OrganizationEvents.events))
            ):
                raise serializers.ValidationError(f"Invalid events list for {webhook_type} webhook")


class EventTypesSerializer(serializers.MultipleChoiceField):
    def __init__(self, *args, **kwargs):
        super().__init__(choices=EventTypeChoice.choices(), *args, **kwargs)

    def to_representation(self, value):
        if isinstance(value, list):
            return sorted(super().to_representation(value))
        return sorted(list(super().to_representation(value.split(","))))

    def to_internal_value(self, data):
        return ",".join(super().to_internal_value(data))


class EventsSerializer(serializers.Serializer):
    webhook_type = serializers.ChoiceField(choices=WebhookTypeChoice.choices())
    events = EventTypesSerializer()


class WebhookReadSerializer(serializers.ModelSerializer):
    owner = BasicUserSerializer(read_only=True, required=False, allow_null=True)

    events = EventTypesSerializer(read_only=True)

    project_id = serializers.IntegerField(required=False, allow_null=True)
    type = serializers.ChoiceField(choices=WebhookTypeChoice.choices())
    content_type = serializers.ChoiceField(choices=WebhookContentTypeChoice.choices())

    last_status = serializers.IntegerField(source="deliveries.last.status_code", read_only=True)

    last_delivery_date = serializers.DateTimeField(
        source="deliveries.last.updated_date", read_only=True
    )

    class Meta:
        model = Webhook
        fields = (
            "id",
            "url",
            "target_url",
            "description",
            "type",
            "content_type",
            "is_active",
            "enable_ssl",
            "created_date",
            "updated_date",
            "owner",
            "project_id",
            "organization",
            "events",
            "last_status",
            "last_delivery_date",
        )
        read_only_fields = fields
        extra_kwargs = {
            "organization": {"allow_null": True},
        }


class WebhookWriteSerializer(WriteOnceMixin, serializers.ModelSerializer):
    events = EventTypesSerializer(write_only=True)

    project_id = serializers.IntegerField(write_only=True, allow_null=True, required=False)

    def to_representation(self, instance):
        serializer = WebhookReadSerializer(instance, context=self.context)
        return serializer.data

    class Meta:
        model = Webhook
        fields = (
            "target_url",
            "description",
            "type",
            "content_type",
            "secret",
            "is_active",
            "enable_ssl",
            "project_id",
            "events",
        )
        write_once_fields = ("type", "project_id")
        validators = [EventTypeValidator()]

    def create(self, validated_data):
        if (project_id := validated_data.get("project_id")) is not None:
            validated_data["organization"] = Project.objects.get(pk=project_id).organization

        db_webhook = Webhook.objects.create(**validated_data)
        return db_webhook


class WebhookDeliveryReadSerializer(serializers.ModelSerializer):
    webhook_id = serializers.IntegerField(read_only=True)

    class Meta:
        model = WebhookDelivery
        fields = (
            "id",
            "webhook_id",
            "event",
            "status_code",
            "redelivery",
            "created_date",
            "updated_date",
            "changed_fields",
            "request",
            "response",
        )
        read_only_fields = fields


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\signals.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import hashlib
import hmac
import json
from copy import deepcopy
from http import HTTPStatus

import django_rq
import requests
from django.conf import settings
from django.core.exceptions import ObjectDoesNotExist
from django.db import transaction
from django.db.models.signals import post_delete, post_save, pre_delete, pre_save
from django.dispatch import Signal, receiver

from cvat.apps.engine.models import Comment, Issue, Job, Project, Task
from cvat.apps.engine.serializers import BasicUserSerializer
from cvat.apps.events.handlers import (
    get_instance_diff,
    get_request,
    get_serializer,
    get_user,
    organization_id,
    project_id,
)
from cvat.apps.organizations.models import Invitation, Membership, Organization
from cvat.utils.http import PROXIES_FOR_UNTRUSTED_URLS, make_requests_session

from .event_type import EventTypeChoice, event_name
from .models import Webhook, WebhookDelivery, WebhookTypeChoice

WEBHOOK_TIMEOUT = 10
RESPONSE_SIZE_LIMIT = 1 * 1024 * 1024  # 1 MB

signal_redelivery = Signal()
signal_ping = Signal()


def send_webhook(webhook, payload, redelivery=False):
    headers = {}
    if webhook.secret:
        headers["X-Signature-256"] = (
            "sha256="
            + hmac.new(
                webhook.secret.encode("utf-8"),
                json.dumps(payload).encode("utf-8"),
                digestmod=hashlib.sha256,
            ).hexdigest()
        )

    response_body = None
    try:
        with make_requests_session() as session:
            response = session.post(
                webhook.target_url,
                json=payload,
                verify=webhook.enable_ssl,
                headers=headers,
                timeout=WEBHOOK_TIMEOUT,
                stream=True,
                proxies=PROXIES_FOR_UNTRUSTED_URLS,
            )
            status_code = response.status_code
            response_body = response.raw.read(RESPONSE_SIZE_LIMIT + 1, decode_content=True)
    except requests.ConnectionError:
        status_code = HTTPStatus.BAD_GATEWAY
    except requests.Timeout:
        status_code = HTTPStatus.GATEWAY_TIMEOUT

    response = ""
    if response_body is not None and len(response_body) < RESPONSE_SIZE_LIMIT + 1:
        response = response_body.decode("utf-8")

    delivery = WebhookDelivery.objects.create(
        webhook_id=webhook.id,
        event=payload["event"],
        status_code=status_code,
        changed_fields=",".join(list(payload.get("before_update", {}).keys())),
        redelivery=redelivery,
        request=payload,
        response=response,
    )

    return delivery


def add_to_queue(webhook, payload, redelivery=False):
    queue = django_rq.get_queue(settings.CVAT_QUEUES.WEBHOOKS.value)
    queue.enqueue_call(func=send_webhook, args=(webhook, payload, redelivery))


def batch_add_to_queue(webhooks, data):
    payload = deepcopy(data)
    for webhook in webhooks:
        payload["webhook_id"] = webhook.id
        add_to_queue(webhook, payload)


def select_webhooks(instance, event):
    selected_webhooks = []
    pid = project_id(instance)
    oid = organization_id(instance)
    if oid is not None:
        webhooks = Webhook.objects.filter(
            is_active=True,
            events__contains=event,
            type=WebhookTypeChoice.ORGANIZATION,
            organization=oid,
        )
        selected_webhooks += list(webhooks)

    if pid is not None:
        webhooks = Webhook.objects.filter(
            is_active=True,
            events__contains=event,
            type=WebhookTypeChoice.PROJECT,
            project=pid,
        )
        selected_webhooks += list(webhooks)

    return selected_webhooks


def get_sender(instance):
    user = get_user(instance)
    if isinstance(user, dict):
        return user
    return BasicUserSerializer(user, context={"request": get_request(instance)}).data


@receiver(pre_save, sender=Project, dispatch_uid=__name__ + ":project:pre_save")
@receiver(pre_save, sender=Task, dispatch_uid=__name__ + ":task:pre_save")
@receiver(pre_save, sender=Job, dispatch_uid=__name__ + ":job:pre_save")
@receiver(pre_save, sender=Issue, dispatch_uid=__name__ + ":issue:pre_save")
@receiver(pre_save, sender=Comment, dispatch_uid=__name__ + ":comment:pre_save")
@receiver(pre_save, sender=Organization, dispatch_uid=__name__ + ":organization:pre_save")
@receiver(pre_save, sender=Invitation, dispatch_uid=__name__ + ":invitation:pre_save")
@receiver(pre_save, sender=Membership, dispatch_uid=__name__ + ":membership:pre_save")
def pre_save_resource_event(sender, instance, **kwargs):
    instance._webhooks_selected_webhooks = []

    if instance.pk is None:
        created = True
    else:
        try:
            old_instance = sender.objects.get(pk=instance.pk)
            created = False
        except ObjectDoesNotExist:
            created = True

    resource_name = instance.__class__.__name__.lower()

    event_type = event_name("create" if created else "update", resource_name)
    if event_type not in map(lambda a: a[0], EventTypeChoice.choices()):
        return

    instance._webhooks_selected_webhooks = select_webhooks(instance, event_type)
    if not instance._webhooks_selected_webhooks:
        return

    if created:
        instance._webhooks_old_data = None
    else:
        old_serializer = get_serializer(instance=old_instance)
        instance._webhooks_old_data = old_serializer.data


@receiver(post_save, sender=Project, dispatch_uid=__name__ + ":project:post_save")
@receiver(post_save, sender=Task, dispatch_uid=__name__ + ":task:post_save")
@receiver(post_save, sender=Job, dispatch_uid=__name__ + ":job:post_save")
@receiver(post_save, sender=Issue, dispatch_uid=__name__ + ":issue:post_save")
@receiver(post_save, sender=Comment, dispatch_uid=__name__ + ":comment:post_save")
@receiver(post_save, sender=Organization, dispatch_uid=__name__ + ":organization:post_save")
@receiver(post_save, sender=Invitation, dispatch_uid=__name__ + ":invitation:post_save")
@receiver(post_save, sender=Membership, dispatch_uid=__name__ + ":membership:post_save")
def post_save_resource_event(sender, instance, created: bool, raw: bool, **kwargs):
    if created and raw:
        return

    selected_webhooks = instance._webhooks_selected_webhooks
    del instance._webhooks_selected_webhooks

    if not selected_webhooks:
        return

    old_data = instance._webhooks_old_data
    del instance._webhooks_old_data

    created = old_data is None

    resource_name = instance.__class__.__name__.lower()
    event_type = event_name("create" if created else "update", resource_name)

    serializer = get_serializer(instance=instance)

    data = {
        "event": event_type,
        resource_name: serializer.data,
        "sender": get_sender(instance),
    }

    if not created:
        if diff := get_instance_diff(old_data=old_data, data=serializer.data):
            data["before_update"] = {attr: value["old_value"] for attr, value in diff.items()}

    transaction.on_commit(
        lambda: batch_add_to_queue(selected_webhooks, data),
        robust=True,
    )


@receiver(pre_delete, sender=Project, dispatch_uid=__name__ + ":project:pre_delete")
@receiver(pre_delete, sender=Task, dispatch_uid=__name__ + ":task:pre_delete")
@receiver(pre_delete, sender=Job, dispatch_uid=__name__ + ":job:pre_delete")
@receiver(pre_delete, sender=Issue, dispatch_uid=__name__ + ":issue:pre_delete")
@receiver(pre_delete, sender=Comment, dispatch_uid=__name__ + ":comment:pre_delete")
@receiver(pre_delete, sender=Organization, dispatch_uid=__name__ + ":organization:pre_delete")
@receiver(pre_delete, sender=Invitation, dispatch_uid=__name__ + ":invitation:pre_delete")
@receiver(pre_delete, sender=Membership, dispatch_uid=__name__ + ":membership:pre_delete")
def pre_delete_resource_event(sender, instance, **kwargs):
    resource_name = instance.__class__.__name__.lower()

    related_webhooks = []
    if resource_name in ["project", "organization"]:
        related_webhooks = select_webhooks(instance, event_name("delete", resource_name))

    serializer = get_serializer(instance=deepcopy(instance))
    instance._deleted_object = dict(serializer.data)
    instance._related_webhooks = related_webhooks


@receiver(post_delete, sender=Project, dispatch_uid=__name__ + ":project:post_delete")
@receiver(post_delete, sender=Task, dispatch_uid=__name__ + ":task:post_delete")
@receiver(post_delete, sender=Job, dispatch_uid=__name__ + ":job:post_delete")
@receiver(post_delete, sender=Issue, dispatch_uid=__name__ + ":issue:post_delete")
@receiver(post_delete, sender=Comment, dispatch_uid=__name__ + ":comment:post_delete")
@receiver(post_delete, sender=Organization, dispatch_uid=__name__ + ":organization:post_delete")
@receiver(post_delete, sender=Invitation, dispatch_uid=__name__ + ":invitation:post_delete")
@receiver(post_delete, sender=Membership, dispatch_uid=__name__ + ":membership:post_delete")
def post_delete_resource_event(sender, instance, **kwargs):
    resource_name = instance.__class__.__name__.lower()

    event_type = event_name("delete", resource_name)
    if event_type not in map(lambda a: a[0], EventTypeChoice.choices()):
        return

    filtered_webhooks = select_webhooks(instance, event_type)

    data = {
        "event": event_type,
        resource_name: getattr(instance, "_deleted_object"),
        "sender": get_sender(instance),
    }

    related_webhooks = [
        webhook
        for webhook in getattr(instance, "_related_webhooks", [])
        if webhook.id not in map(lambda a: a.id, filtered_webhooks)
    ]

    transaction.on_commit(
        lambda: batch_add_to_queue(filtered_webhooks + related_webhooks, data),
        robust=True,
    )


@receiver(signal_redelivery)
def redelivery(sender, data=None, **kwargs):
    add_to_queue(sender.get_object(), data, redelivery=True)


@receiver(signal_ping)
def ping(sender, serializer, **kwargs):
    data = {"event": "ping", "webhook": serializer.data, "sender": get_sender(serializer.instance)}
    delivery = send_webhook(serializer.instance, data, redelivery=False)
    return delivery


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\urls.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from rest_framework.routers import DefaultRouter

from .views import WebhookViewSet

router = DefaultRouter(trailing_slash=False)
router.register("webhooks", WebhookViewSet)

urlpatterns = router.urls


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\views.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from drf_spectacular.utils import (
    OpenApiParameter,
    OpenApiResponse,
    OpenApiTypes,
    extend_schema,
    extend_schema_view,
)
from rest_framework import status, viewsets
from rest_framework.decorators import action
from rest_framework.permissions import SAFE_METHODS
from rest_framework.response import Response

from cvat.apps.engine.view_utils import list_action, make_paginated_response
from cvat.apps.iam.filters import ORGANIZATION_OPEN_API_PARAMETERS

from .event_type import AllEvents, OrganizationEvents, ProjectEvents
from .models import Webhook, WebhookDelivery, WebhookTypeChoice
from .permissions import WebhookPermission
from .serializers import (
    EventsSerializer,
    WebhookDeliveryReadSerializer,
    WebhookReadSerializer,
    WebhookWriteSerializer,
)
from .signals import signal_ping, signal_redelivery


@extend_schema(tags=["webhooks"])
@extend_schema_view(
    retrieve=extend_schema(
        summary="Get webhook details",
        responses={"200": WebhookReadSerializer},
    ),
    list=extend_schema(
        summary="List webhooks",
        responses={"200": WebhookReadSerializer(many=True)},
    ),
    update=extend_schema(
        summary="Replace a webhook",
        request=WebhookWriteSerializer,
        responses={"200": WebhookReadSerializer},  # check WebhookWriteSerializer.to_representation
    ),
    partial_update=extend_schema(
        summary="Update a webhook",
        request=WebhookWriteSerializer,
        responses={"200": WebhookReadSerializer},  # check WebhookWriteSerializer.to_representation
    ),
    create=extend_schema(
        request=WebhookWriteSerializer,
        summary="Create a webhook",
        parameters=ORGANIZATION_OPEN_API_PARAMETERS,
        responses={"201": WebhookReadSerializer},  # check WebhookWriteSerializer.to_representation
    ),
    destroy=extend_schema(
        summary="Delete a webhook",
        responses={"204": OpenApiResponse(description="The webhook has been deleted")},
    ),
)
class WebhookViewSet(viewsets.ModelViewSet):
    queryset = Webhook.objects.prefetch_related("owner").all()
    ordering = "-id"
    http_method_names = ["get", "post", "delete", "patch", "put"]

    search_fields = ("target_url", "owner", "type", "description")
    filter_fields = list(search_fields) + ["id", "project_id", "updated_date"]
    simple_filters = list(set(search_fields) - {"description"} | {"project_id"})
    ordering_fields = list(filter_fields)
    lookup_fields = {"owner": "owner__username"}
    iam_organization_field = "organization"

    def get_serializer_class(self):
        if self.request.path.endswith("redelivery") or self.request.path.endswith("ping"):
            return None
        else:
            if self.request.method in SAFE_METHODS:
                return WebhookReadSerializer
            else:
                return WebhookWriteSerializer

    def get_queryset(self):
        queryset = super().get_queryset()

        if self.action == "list":
            perm = WebhookPermission.create_scope_list(self.request)
            queryset = perm.filter(queryset)

        return queryset

    def perform_create(self, serializer):
        serializer.save(
            owner=self.request.user,
            organization=self.request.iam_context["organization"],
        )

    @extend_schema(
        summary="List available webhook events",
        parameters=[
            OpenApiParameter(
                "type",
                description="Type of webhook",
                location=OpenApiParameter.QUERY,
                type=OpenApiTypes.STR,
                required=False,
            )
        ],
        responses={"200": OpenApiResponse(EventsSerializer)},
    )
    @action(
        detail=False,
        methods=["GET"],
        serializer_class=EventsSerializer,
        permission_classes=[],
    )
    def events(self, request):
        webhook_type = request.query_params.get("type", "all")
        events = None
        if webhook_type == "all":
            events = AllEvents
        elif webhook_type == WebhookTypeChoice.PROJECT:
            events = ProjectEvents
        elif webhook_type == WebhookTypeChoice.ORGANIZATION:
            events = OrganizationEvents

        if events is None:
            return Response("Incorrect value of type parameter", status=status.HTTP_400_BAD_REQUEST)

        return Response(EventsSerializer().to_representation(events))

    @extend_schema(
        summary="List deliveries for a webhook",
        responses=WebhookDeliveryReadSerializer(
            many=True
        ),  # Duplicate to still get 'list' op. name
    )
    @list_action(serializer_class=WebhookDeliveryReadSerializer)
    def deliveries(self, request, pk):
        self.get_object()  # force call of check_object_permissions()
        queryset = WebhookDelivery.objects.filter(webhook_id=pk).order_by("-updated_date")
        return make_paginated_response(
            queryset, viewset=self, serializer_type=self.serializer_class
        )  # from @action

    @extend_schema(
        summary="Get details of a webhook delivery",
        responses={"200": WebhookDeliveryReadSerializer},
    )
    @action(
        detail=True,
        methods=["GET"],
        url_path=r"deliveries/(?P<delivery_id>\d+)",
        serializer_class=WebhookDeliveryReadSerializer,
    )
    def retrieve_delivery(self, request, pk, delivery_id):
        self.get_object()  # force call of check_object_permissions()
        queryset = WebhookDelivery.objects.get(webhook_id=pk, id=delivery_id)
        serializer = WebhookDeliveryReadSerializer(queryset, context={"request": request})
        return Response(serializer.data)

    @extend_schema(
        summary="Redeliver a webhook delivery",
        request=None,
        responses={200: None},
    )
    @action(
        detail=True,
        methods=["POST"],
        url_path=r"deliveries/(?P<delivery_id>\d+)/redelivery",
        serializer_class=None,
    )
    def redelivery(self, request, pk, delivery_id):
        delivery = WebhookDelivery.objects.get(webhook_id=pk, id=delivery_id)
        signal_redelivery.send(sender=self, data=delivery.request)
        return Response({}, status=status.HTTP_200_OK)

    @extend_schema(
        summary="Send a ping webhook",
        request=None,
        responses={"200": WebhookDeliveryReadSerializer},
    )
    @action(detail=True, methods=["POST"], serializer_class=WebhookDeliveryReadSerializer)
    def ping(self, request, pk):
        instance = self.get_object()  # force call of check_object_permissions()
        serializer = WebhookReadSerializer(instance, context={"request": request})

        delivery = signal_ping.send(sender=self, serializer=serializer)[0][1]
        serializer = WebhookDeliveryReadSerializer(delivery, context={"request": request})
        return Response(serializer.data)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\migrations\0001_initial.py =====
# Generated by Django 3.2.15 on 2022-09-19 08:26

import django.db.models.deletion
from django.conf import settings
from django.db import migrations, models

import cvat.apps.webhooks.models


class Migration(migrations.Migration):

    initial = True

    dependencies = [
        ("engine", "0060_alter_label_parent"),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ("organizations", "0001_initial"),
    ]

    operations = [
        migrations.CreateModel(
            name="Webhook",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("target_url", models.URLField()),
                ("description", models.CharField(blank=True, default="", max_length=128)),
                ("events", models.CharField(default="", max_length=4096)),
                (
                    "type",
                    models.CharField(
                        choices=[("organization", "ORGANIZATION"), ("project", "PROJECT")],
                        max_length=16,
                    ),
                ),
                (
                    "content_type",
                    models.CharField(
                        choices=[("application/json", "JSON")],
                        default=cvat.apps.webhooks.models.WebhookContentTypeChoice["JSON"],
                        max_length=64,
                    ),
                ),
                ("secret", models.CharField(blank=True, default="", max_length=64)),
                ("is_active", models.BooleanField(default=True)),
                ("enable_ssl", models.BooleanField(default=True)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                (
                    "organization",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="+",
                        to="organizations.organization",
                    ),
                ),
                (
                    "owner",
                    models.ForeignKey(
                        blank=True,
                        null=True,
                        on_delete=django.db.models.deletion.SET_NULL,
                        related_name="+",
                        to=settings.AUTH_USER_MODEL,
                    ),
                ),
                (
                    "project",
                    models.ForeignKey(
                        null=True,
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="+",
                        to="engine.project",
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.CreateModel(
            name="WebhookDelivery",
            fields=[
                (
                    "id",
                    models.AutoField(
                        auto_created=True, primary_key=True, serialize=False, verbose_name="ID"
                    ),
                ),
                ("event", models.CharField(max_length=64)),
                ("status_code", models.CharField(max_length=128, null=True)),
                ("redelivery", models.BooleanField(default=False)),
                ("created_date", models.DateTimeField(auto_now_add=True)),
                ("updated_date", models.DateTimeField(auto_now=True)),
                ("changed_fields", models.CharField(default="", max_length=4096)),
                ("request", models.JSONField(default=dict)),
                ("response", models.JSONField(default=dict)),
                (
                    "webhook",
                    models.ForeignKey(
                        on_delete=django.db.models.deletion.CASCADE,
                        related_name="deliveries",
                        to="webhooks.webhook",
                    ),
                ),
            ],
            options={
                "default_permissions": (),
            },
        ),
        migrations.AddConstraint(
            model_name="webhook",
            constraint=models.CheckConstraint(
                check=models.Q(
                    models.Q(("project_id__isnull", False), ("type", "project")),
                    models.Q(
                        ("organization_id__isnull", False),
                        ("project_id__isnull", True),
                        ("type", "organization"),
                    ),
                    _connector="OR",
                ),
                name="webhooks_project_or_organization",
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\migrations\0002_alter_webhookdelivery_status_code.py =====
# Generated by Django 3.2.15 on 2022-09-27 12:28

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("webhooks", "0001_initial"),
    ]

    operations = [
        migrations.AlterField(
            model_name="webhookdelivery",
            name="status_code",
            field=models.IntegerField(
                choices=[
                    ("CONTINUE", 100),
                    ("SWITCHING_PROTOCOLS", 101),
                    ("PROCESSING", 102),
                    ("OK", 200),
                    ("CREATED", 201),
                    ("ACCEPTED", 202),
                    ("NON_AUTHORITATIVE_INFORMATION", 203),
                    ("NO_CONTENT", 204),
                    ("RESET_CONTENT", 205),
                    ("PARTIAL_CONTENT", 206),
                    ("MULTI_STATUS", 207),
                    ("ALREADY_REPORTED", 208),
                    ("IM_USED", 226),
                    ("MULTIPLE_CHOICES", 300),
                    ("MOVED_PERMANENTLY", 301),
                    ("FOUND", 302),
                    ("SEE_OTHER", 303),
                    ("NOT_MODIFIED", 304),
                    ("USE_PROXY", 305),
                    ("TEMPORARY_REDIRECT", 307),
                    ("PERMANENT_REDIRECT", 308),
                    ("BAD_REQUEST", 400),
                    ("UNAUTHORIZED", 401),
                    ("PAYMENT_REQUIRED", 402),
                    ("FORBIDDEN", 403),
                    ("NOT_FOUND", 404),
                    ("METHOD_NOT_ALLOWED", 405),
                    ("NOT_ACCEPTABLE", 406),
                    ("PROXY_AUTHENTICATION_REQUIRED", 407),
                    ("REQUEST_TIMEOUT", 408),
                    ("CONFLICT", 409),
                    ("GONE", 410),
                    ("LENGTH_REQUIRED", 411),
                    ("PRECONDITION_FAILED", 412),
                    ("REQUEST_ENTITY_TOO_LARGE", 413),
                    ("REQUEST_URI_TOO_LONG", 414),
                    ("UNSUPPORTED_MEDIA_TYPE", 415),
                    ("REQUESTED_RANGE_NOT_SATISFIABLE", 416),
                    ("EXPECTATION_FAILED", 417),
                    ("MISDIRECTED_REQUEST", 421),
                    ("UNPROCESSABLE_ENTITY", 422),
                    ("LOCKED", 423),
                    ("FAILED_DEPENDENCY", 424),
                    ("UPGRADE_REQUIRED", 426),
                    ("PRECONDITION_REQUIRED", 428),
                    ("TOO_MANY_REQUESTS", 429),
                    ("REQUEST_HEADER_FIELDS_TOO_LARGE", 431),
                    ("UNAVAILABLE_FOR_LEGAL_REASONS", 451),
                    ("INTERNAL_SERVER_ERROR", 500),
                    ("NOT_IMPLEMENTED", 501),
                    ("BAD_GATEWAY", 502),
                    ("SERVICE_UNAVAILABLE", 503),
                    ("GATEWAY_TIMEOUT", 504),
                    ("HTTP_VERSION_NOT_SUPPORTED", 505),
                    ("VARIANT_ALSO_NEGOTIATES", 506),
                    ("INSUFFICIENT_STORAGE", 507),
                    ("LOOP_DETECTED", 508),
                    ("NOT_EXTENDED", 510),
                    ("NETWORK_AUTHENTICATION_REQUIRED", 511),
                ],
                default=None,
                null=True,
            ),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\migrations\0003_alter_webhookdelivery_status_code.py =====
# Generated by Django 3.2.15 on 2022-10-11 09:02

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("webhooks", "0002_alter_webhookdelivery_status_code"),
    ]

    operations = [
        migrations.AlterField(
            model_name="webhookdelivery",
            name="status_code",
            field=models.PositiveIntegerField(default=None, null=True),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\migrations\0004_alter_webhook_target_url.py =====
# Generated by Django 3.2.18 on 2023-03-20 17:43

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ("webhooks", "0003_alter_webhookdelivery_status_code"),
    ]

    operations = [
        migrations.AlterField(
            model_name="webhook",
            name="target_url",
            field=models.URLField(max_length=8192),
        ),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\migrations\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat\apps\webhooks\rules\tests\generators\webhooks_test.gen.rego.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import os
import random
import sys
from itertools import product

NAME = "webhooks"


def read_rules(name):
    rules = []
    with open(os.path.join(sys.argv[1], f"{name}.csv")) as f:
        reader = csv.DictReader(f)
        for row in reader:
            row = {k.lower(): v.lower().replace("n/a", "na") for k, v in row.items()}
            row["limit"] = row["limit"].replace("none", "None")
            found = False
            for col, val in row.items():
                if col in ["limit", "method", "url", "resource"]:
                    continue
                complex_val = [v.strip() for v in val.split(",")]
                if len(complex_val) > 1:
                    found = True
                    for item in complex_val:
                        new_row = row.copy()
                        new_row[col] = item
                        rules.append(new_row)
            if not found:
                rules.append(row)
    return rules


random.seed(42)
simple_rules = read_rules(NAME)
SCOPES = list({rule["scope"] for rule in simple_rules})
CONTEXTS = ["sandbox", "organization"]
OWNERSHIPS = ["project:owner", "owner", "none"]
GROUPS = ["admin", "user", "worker", "none"]
ORG_ROLES = ["owner", "maintainer", "supervisor", "worker", None]
SAME_ORG = [True, False]


def RESOURCES(scope):
    if scope == "list":
        return [None]
    elif scope == "create@project":
        return [
            {
                "owner": {"id": random.randrange(100, 200)},
                "assignee": {"id": random.randrange(200, 300)},
                "organization": {"id": random.randrange(300, 400)},
                "project": {"owner": {"id": random.randrange(400, 500)}},
                "num_resources": count,
            }
            for count in (0, 3, 10)
        ]
    elif scope == "create@organization":
        return [
            {
                "owner": {"id": random.randrange(100, 200)},
                "assignee": {"id": random.randrange(200, 300)},
                "organization": {"id": random.randrange(300, 400)},
                "project": None,
                "num_resources": count,
            }
            for count in (0, 3, 10)
        ]
    else:
        return [
            {
                "id": random.randrange(100, 200),
                "owner": {"id": random.randrange(200, 300)},
                "organization": {"id": random.randrange(300, 400)},
                "project": {"owner": {"id": random.randrange(400, 500)}},
            }
        ]


def is_same_org(org1, org2):
    if org1 is not None and org2 is not None:
        return org1["id"] == org2["id"]
    elif org1 is None and org2 is None:
        return True
    return False


def eval_rule(scope, context, ownership, privilege, membership, data):
    if privilege == "admin":
        return True

    rules = list(
        filter(
            lambda r: scope == r["scope"]
            and (r["context"] == "na" or context == r["context"])
            and (r["ownership"] == "na" or ownership == r["ownership"])
            and (
                r["membership"] == "na"
                or ORG_ROLES.index(membership) <= ORG_ROLES.index(r["membership"])
            )
            and GROUPS.index(privilege) <= GROUPS.index(r["privilege"]),
            simple_rules,
        )
    )

    resource = data["resource"]

    rules = list(
        filter(lambda r: not r["limit"] or eval(r["limit"], {"resource": resource}), rules)
    )
    if (
        not is_same_org(data["auth"]["organization"], data["resource"]["organization"])
        and context != "sandbox"
    ):
        return False
    return bool(rules)


def get_data(scope, context, ownership, privilege, membership, resource, same_org):
    data = {
        "scope": scope,
        "auth": {
            "user": {"id": random.randrange(0, 100), "privilege": privilege},
            "organization": (
                {
                    "id": random.randrange(100, 200),
                    "owner": {"id": random.randrange(200, 300)},
                    "user": {"role": membership},
                }
                if context == "organization"
                else None
            ),
        },
        "resource": resource,
    }

    user_id = data["auth"]["user"]["id"]

    if context == "organization":
        org_id = data["auth"]["organization"]["id"]
        if data["auth"]["organization"]["user"]["role"] == "owner":
            data["auth"]["organization"]["owner"]["id"] = user_id
        if same_org:
            data["resource"]["organization"]["id"] = org_id

    if ownership == "owner":
        data["resource"]["owner"]["id"] = user_id

    if ownership == "project:owner":
        data["resource"]["project"]["owner"]["id"] = user_id

    return data


def _get_name(prefix, **kwargs):
    name = prefix
    for k, v in kwargs.items():
        prefix = "_" + str(k)
        if isinstance(v, dict):
            if "id" in v:
                v = v.copy()
                v.pop("id")
            if v:
                name += _get_name(prefix, **v)
        else:
            name += "".join(
                map(
                    lambda c: c if c.isalnum() else {"@": "_IN_"}.get(c, "_"),
                    f"{prefix}_{str(v).upper()}",
                )
            )
    return name


def get_name(scope, context, ownership, privilege, membership, resource, same_org):
    return _get_name("test", **locals())


def is_valid(scope, context, ownership, privilege, membership, resource, same_org):
    if context == "sandbox" and membership:
        return False
    if scope == "list" and ownership != "None":
        return False
    if context == "sandbox" and not same_org:
        return False
    if scope.startswith("create") and ownership != "None":
        return False

    return True


def gen_test_rego(name):
    with open(f"{name}_test.gen.rego", "wt") as f:
        f.write(f"package {name}\nimport rego.v1\n\n")
        for scope, context, ownership, privilege, membership, same_org in product(
            SCOPES, CONTEXTS, OWNERSHIPS, GROUPS, ORG_ROLES, SAME_ORG
        ):
            for resource in RESOURCES(scope):

                if not is_valid(
                    scope, context, ownership, privilege, membership, resource, same_org
                ):
                    continue

                data = get_data(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                test_name = get_name(
                    scope, context, ownership, privilege, membership, resource, same_org
                )
                result = eval_rule(scope, context, ownership, privilege, membership, data)

                f.write(
                    "{test_name} if {{\n    {allow} with input as {data}\n}}\n\n".format(
                        test_name=test_name,
                        allow="allow" if result else "not allow",
                        data=json.dumps(data),
                    )
                )


gen_test_rego(NAME)


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\base.py =====
# Copyright (C) 2018-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

"""
Django settings for CVAT project.

Generated by 'django-admin startproject' using Django 2.0.1.

For more information on this file, see
https://docs.djangoproject.com/en/2.0/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/2.0/ref/settings/
"""

import mimetypes
import os
import sys
import tempfile
import urllib
from datetime import timedelta
from enum import Enum

from attr.converters import to_bool
from corsheaders.defaults import default_headers
from logstash_async.constants import constants as logstash_async_constants

from cvat import __version__

mimetypes.add_type("application/wasm", ".wasm", True)

from pathlib import Path

from django.core.exceptions import ImproperlyConfigured

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = str(Path(__file__).parents[2])

ALLOWED_HOSTS = os.environ.get("ALLOWED_HOSTS", "localhost,127.0.0.1").split(",")
INTERNAL_IPS = ["127.0.0.1"]
SECRET_KEY = os.environ.get("DJANGO_SECRET_KEY", "")


def generate_secret_key():
    """
    Creates secret_key.py in such a way that multiple processes calling
    this will all end up with the same key (assuming that they share the
    same "keys" directory).
    """

    from django.utils.crypto import get_random_string

    keys_dir = os.path.join(BASE_DIR, "keys")
    if not os.path.isdir(keys_dir):
        os.mkdir(keys_dir)

    secret_key_fname = "secret_key.py"  # nosec

    with tempfile.NamedTemporaryFile(mode="wt", dir=keys_dir, prefix=secret_key_fname + ".") as f:
        chars = "abcdefghijklmnopqrstuvwxyz0123456789!@#$%^&*(-_=+)"
        f.write("SECRET_KEY = '{}'\n".format(get_random_string(50, chars)))

        # Make sure the file contents are written before we link to it
        # from the final location.
        f.flush()

        try:
            os.link(f.name, os.path.join(keys_dir, secret_key_fname))
        except FileExistsError:
            # Somebody else created the secret key first.
            # Discard ours and use theirs.
            pass


if not SECRET_KEY:
    try:
        sys.path.append(BASE_DIR)
        from keys.secret_key import SECRET_KEY  # pylint: disable=unused-import
    except ModuleNotFoundError:
        generate_secret_key()
        from keys.secret_key import SECRET_KEY

DEFAULT_AUTO_FIELD = "django.db.models.AutoField"
INSTALLED_APPS = [
    "django.contrib.admin",
    "django.contrib.auth",
    "django.contrib.contenttypes",
    "django.contrib.sessions",
    "django.contrib.messages",
    "django.contrib.staticfiles",
    "django_rq",
    "compressor",
    "django_sendfile",
    "dj_rest_auth",
    "dj_rest_auth.registration",
    "dj_pagination",
    "django_filters",
    "rest_framework",
    "rest_framework.authtoken",
    "drf_spectacular",
    "django.contrib.sites",
    "allauth",
    "allauth.account",
    "corsheaders",
    "allauth.socialaccount",
    "health_check",
    "health_check.cache",
    "health_check.db",
    "health_check.contrib.psutil",
    "cvat.apps.iam",
    "cvat.apps.dataset_manager",
    "cvat.apps.organizations",
    "cvat.apps.engine",
    "cvat.apps.dataset_repo",
    "cvat.apps.lambda_manager",
    "cvat.apps.webhooks",
    "cvat.apps.health",
    "cvat.apps.events",
    "cvat.apps.quality_control",
    "cvat.apps.redis_handler",
    "cvat.apps.consensus",
]

SITE_ID = 1

REST_FRAMEWORK = {
    "DEFAULT_PARSER_CLASSES": [
        "rest_framework.parsers.JSONParser",
    ],
    "DEFAULT_RENDERER_CLASSES": [
        "cvat.apps.engine.renderers.CVATAPIRenderer",
        "rest_framework.renderers.BrowsableAPIRenderer",
    ],
    "DEFAULT_PERMISSION_CLASSES": [
        "rest_framework.permissions.IsAuthenticated",
        "cvat.apps.iam.permissions.PolicyEnforcer",
    ],
    "DEFAULT_AUTHENTICATION_CLASSES": [
        "rest_framework.authentication.TokenAuthentication",
        "cvat.apps.iam.authentication.SignatureAuthentication",
        "rest_framework.authentication.SessionAuthentication",
        "rest_framework.authentication.BasicAuthentication",
    ],
    "DEFAULT_VERSIONING_CLASS": "rest_framework.versioning.AcceptHeaderVersioning",
    "ALLOWED_VERSIONS": ("2.0"),
    "DEFAULT_VERSION": "2.0",
    "VERSION_PARAM": "version",
    "DEFAULT_PAGINATION_CLASS": "cvat.apps.engine.pagination.CustomPagination",
    "PAGE_SIZE": 10,
    "DEFAULT_FILTER_BACKENDS": (
        "cvat.apps.engine.filters.SimpleFilter",
        "cvat.apps.engine.filters.SearchFilter",
        "cvat.apps.engine.filters.OrderingFilter",
        "cvat.apps.engine.filters.JsonLogicFilter",
        "cvat.apps.iam.filters.OrganizationFilterBackend",
    ),
    "SEARCH_PARAM": "search",
    # Disable default handling of the 'format' query parameter by REST framework
    "URL_FORMAT_OVERRIDE": "scheme",
    "DEFAULT_THROTTLE_CLASSES": [
        "rest_framework.throttling.AnonRateThrottle",
    ],
    "DEFAULT_THROTTLE_RATES": {
        "anon": "100/minute",
    },
    "DEFAULT_METADATA_CLASS": "rest_framework.metadata.SimpleMetadata",
    "DEFAULT_SCHEMA_CLASS": "cvat.apps.iam.schema.CustomAutoSchema",
    "EXCEPTION_HANDLER": "cvat.apps.events.handlers.handle_viewset_exception",
}


REST_AUTH = {
    "REGISTER_SERIALIZER": "cvat.apps.iam.serializers.RegisterSerializerEx",
    "LOGIN_SERIALIZER": "cvat.apps.iam.serializers.LoginSerializerEx",
    "PASSWORD_RESET_SERIALIZER": "cvat.apps.iam.serializers.PasswordResetSerializerEx",
    "OLD_PASSWORD_FIELD_ENABLED": True,
}

ANALYTICS_ENABLED = to_bool(os.getenv("CVAT_ANALYTICS", False))

if ANALYTICS_ENABLED:
    INSTALLED_APPS += ["cvat.apps.log_viewer"]

MIDDLEWARE = [
    "django.middleware.security.SecurityMiddleware",
    "django.contrib.sessions.middleware.SessionMiddleware",
    "cvat.apps.iam.middleware.SessionRefreshMiddleware",
    "corsheaders.middleware.CorsMiddleware",
    "django.middleware.common.CommonMiddleware",
    "django.middleware.csrf.CsrfViewMiddleware",
    # FIXME
    # 'corsheaders.middleware.CorsPostCsrfMiddleware',
    "django.contrib.auth.middleware.AuthenticationMiddleware",
    "django.middleware.gzip.GZipMiddleware",
    "cvat.apps.engine.middleware.RequestTrackingMiddleware",
    "crum.CurrentRequestUserMiddleware",
    "django.contrib.messages.middleware.MessageMiddleware",
    "django.middleware.clickjacking.XFrameOptionsMiddleware",
    "dj_pagination.middleware.PaginationMiddleware",
    "cvat.apps.iam.middleware.ContextMiddleware",
    "allauth.account.middleware.AccountMiddleware",
]

UI_URL = ""

STATICFILES_FINDERS = [
    "django.contrib.staticfiles.finders.FileSystemFinder",
    "django.contrib.staticfiles.finders.AppDirectoriesFinder",
    "compressor.finders.CompressorFinder",
]

ROOT_URLCONF = "cvat.urls"

TEMPLATES = [
    {
        "BACKEND": "django.template.backends.django.DjangoTemplates",
        "DIRS": [],
        "APP_DIRS": True,
        "OPTIONS": {
            "context_processors": [
                "django.template.context_processors.debug",
                "django.template.context_processors.request",
                "django.contrib.auth.context_processors.auth",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

# IAM settings
IAM_TYPE = "BASIC"
IAM_BASE_EXCEPTION = None  # a class which will be used by IAM to report errors
IAM_DEFAULT_ROLE = "user"

IAM_ADMIN_ROLE = "admin"
# Index in the list below corresponds to the priority (0 has highest priority)
IAM_ROLES = [IAM_ADMIN_ROLE, "user", "worker"]
IAM_OPA_HOST = "http://opa:8181"
IAM_OPA_DATA_URL = f"{IAM_OPA_HOST}/v1/data"
LOGIN_URL = "rest_login"
LOGIN_REDIRECT_URL = "/"

OBJECTS_NOT_RELATED_WITH_ORG = ["user", "lambda_function", "lambda_request", "server", "request"]

# ORG settings
ORG_INVITATION_CONFIRM = "No"
ORG_INVITATION_EXPIRY_DAYS = 7


AUTHENTICATION_BACKENDS = [
    "django.contrib.auth.backends.ModelBackend",
    "allauth.account.auth_backends.AuthenticationBackend",
]

# https://github.com/pennersr/django-allauth
ACCOUNT_EMAIL_VERIFICATION = "none"
ACCOUNT_AUTHENTICATION_METHOD = "username_email"

# set UI url to redirect after a successful e-mail confirmation
# changed from '/auth/login' to '/auth/email-confirmation' for email confirmation message
ACCOUNT_EMAIL_CONFIRMATION_ANONYMOUS_REDIRECT_URL = "/auth/email-confirmation"
ACCOUNT_EMAIL_VERIFICATION_SENT_REDIRECT_URL = "/auth/email-verification-sent"
INCORRECT_EMAIL_CONFIRMATION_URL = "/auth/incorrect-email-confirmation"

# Django-RQ
# https://github.com/rq/django-rq


class CVAT_QUEUES(Enum):
    IMPORT_DATA = "import"
    EXPORT_DATA = "export"
    AUTO_ANNOTATION = "annotation"
    WEBHOOKS = "webhooks"
    NOTIFICATIONS = "notifications"
    QUALITY_REPORTS = "quality_reports"
    CLEANING = "cleaning"
    CHUNKS = "chunks"
    CONSENSUS = "consensus"


redis_inmem_host = os.getenv("CVAT_REDIS_INMEM_HOST", "localhost")
redis_inmem_port = os.getenv("CVAT_REDIS_INMEM_PORT", 6379)
redis_inmem_password = os.getenv("CVAT_REDIS_INMEM_PASSWORD", "")

REDIS_INMEM_SETTINGS = {
    "HOST": redis_inmem_host,
    "PORT": redis_inmem_port,
    "DB": 0,
    "PASSWORD": redis_inmem_password,
}

RQ_QUEUES = {
    CVAT_QUEUES.IMPORT_DATA.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "4h",
    },
    CVAT_QUEUES.EXPORT_DATA.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "4h",
    },
    CVAT_QUEUES.AUTO_ANNOTATION.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "24h",
    },
    CVAT_QUEUES.WEBHOOKS.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "1h",
    },
    CVAT_QUEUES.NOTIFICATIONS.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "1h",
    },
    CVAT_QUEUES.QUALITY_REPORTS.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "1h",
    },
    CVAT_QUEUES.CLEANING.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "2h",
    },
    CVAT_QUEUES.CHUNKS.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "5m",
    },
    CVAT_QUEUES.CONSENSUS.value: {
        **REDIS_INMEM_SETTINGS,
        "DEFAULT_TIMEOUT": "1h",
    },
}

NUCLIO = {
    "SCHEME": os.getenv("CVAT_NUCLIO_SCHEME", "http"),
    "HOST": os.getenv("CVAT_NUCLIO_HOST", "localhost"),
    "PORT": int(os.getenv("CVAT_NUCLIO_PORT", 8070)),
    "DEFAULT_TIMEOUT": int(os.getenv("CVAT_NUCLIO_DEFAULT_TIMEOUT", 120)),
    "FUNCTION_NAMESPACE": os.getenv("CVAT_NUCLIO_FUNCTION_NAMESPACE", "nuclio"),
    "INVOKE_METHOD": os.getenv(
        "CVAT_NUCLIO_INVOKE_METHOD",
        default="dashboard" if "KUBERNETES_SERVICE_HOST" in os.environ else "direct",
    ),
}

assert NUCLIO["INVOKE_METHOD"] in {"dashboard", "direct"}

RQ_SHOW_ADMIN_LINK = True
RQ_EXCEPTION_HANDLERS = [
    "cvat.apps.engine.views.rq_exception_handler",
    "cvat.apps.events.handlers.handle_rq_exception",
]

PERIODIC_RQ_JOBS = [
    {
        "queue": CVAT_QUEUES.CLEANING.value,
        "id": "clean_up_sessions",
        "func": "cvat.apps.iam.utils.clean_up_sessions",
        "cron_string": "0 0 * * *",
    },
    {
        "queue": CVAT_QUEUES.CLEANING.value,
        "id": "cron_export_cache_directory_cleanup",
        "func": "cvat.apps.dataset_manager.cron.cleanup_export_cache_directory",
        # Run twice a day (at midnight and at noon)
        "cron_string": "0 0,12 * * *",
    },
    {
        "queue": CVAT_QUEUES.CLEANING.value,
        "id": "cron_tmp_directory_cleanup",
        "func": "cvat.apps.dataset_manager.cron.cleanup_tmp_directory",
        # Run once a day
        "cron_string": "0 18 * * *",
    },
]

# JavaScript and CSS compression
# https://django-compressor.readthedocs.io

COMPRESS_CSS_FILTERS = [
    "compressor.filters.css_default.CssAbsoluteFilter",
    "compressor.filters.cssmin.rCSSMinFilter",
]
COMPRESS_JS_FILTERS = []  # No compression for js files (template literals were compressed bad)

# Password validation
# https://docs.djangoproject.com/en/2.0/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        "NAME": "django.contrib.auth.password_validation.UserAttributeSimilarityValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.MinimumLengthValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.CommonPasswordValidator",
    },
    {
        "NAME": "django.contrib.auth.password_validation.NumericPasswordValidator",
    },
]

# Internationalization
# https://docs.djangoproject.com/en/2.0/topics/i18n/

LANGUAGE_CODE = "en-us"

TIME_ZONE = os.getenv("TZ", "Etc/UTC")

USE_I18N = True

USE_L10N = True

USE_TZ = True

CSRF_COOKIE_NAME = "csrftoken"

# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/2.0/howto/static-files/

STATIC_URL = "/static/"
STATIC_ROOT = os.path.join(BASE_DIR, "static")
os.makedirs(STATIC_ROOT, exist_ok=True)

# Make sure to update other config files when updating these directories
DATA_ROOT = os.path.join(BASE_DIR, "data")

MEDIA_DATA_ROOT = os.path.join(DATA_ROOT, "data")
os.makedirs(MEDIA_DATA_ROOT, exist_ok=True)

CACHE_ROOT = os.path.join(DATA_ROOT, "cache")
os.makedirs(CACHE_ROOT, exist_ok=True)

EXPORT_CACHE_ROOT = os.path.join(CACHE_ROOT, "export")
os.makedirs(EXPORT_CACHE_ROOT, exist_ok=True)

EVENTS_LOCAL_DB_ROOT = os.path.join(BASE_DIR, "events")
os.makedirs(EVENTS_LOCAL_DB_ROOT, exist_ok=True)
EVENTS_LOCAL_DB_FILE = os.path.join(
    EVENTS_LOCAL_DB_ROOT,
    os.getenv("CVAT_EVENTS_LOCAL_DB_FILENAME", "events.db"),
)
if not os.path.exists(EVENTS_LOCAL_DB_FILE):
    open(EVENTS_LOCAL_DB_FILE, "w").close()

JOBS_ROOT = os.path.join(DATA_ROOT, "jobs")
os.makedirs(JOBS_ROOT, exist_ok=True)

TASKS_ROOT = os.path.join(DATA_ROOT, "tasks")
os.makedirs(TASKS_ROOT, exist_ok=True)

PROJECTS_ROOT = os.path.join(DATA_ROOT, "projects")
os.makedirs(PROJECTS_ROOT, exist_ok=True)

ASSETS_ROOT = os.path.join(DATA_ROOT, "assets")
os.makedirs(ASSETS_ROOT, exist_ok=True)

SHARE_ROOT = os.path.join(BASE_DIR, "share")
os.makedirs(SHARE_ROOT, exist_ok=True)

MODELS_ROOT = os.path.join(DATA_ROOT, "models")
os.makedirs(MODELS_ROOT, exist_ok=True)

LOGS_ROOT = os.path.join(BASE_DIR, "logs")
os.makedirs(LOGS_ROOT, exist_ok=True)

MIGRATIONS_LOGS_ROOT = os.path.join(LOGS_ROOT, "migrations")
os.makedirs(MIGRATIONS_LOGS_ROOT, exist_ok=True)

CLOUD_STORAGE_ROOT = os.path.join(DATA_ROOT, "storages")
os.makedirs(CLOUD_STORAGE_ROOT, exist_ok=True)

TMP_FILES_ROOT = os.path.join(DATA_ROOT, "tmp")
os.makedirs(TMP_FILES_ROOT, exist_ok=True)

# logging is known to be unreliable with RQ when using async transports
vector_log_handler = os.getenv("VECTOR_EVENT_HANDLER", "AsynchronousLogstashHandler")

logstash_async_constants.QUEUED_EVENTS_FLUSH_INTERVAL = 2.0
LOGGING = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "vector": {"format": "%(message)s"},
        "standard": {"format": "[%(asctime)s] %(levelname)s %(name)s: %(message)s"},
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "filters": [],
            "formatter": "standard",
        },
        "server_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "filename": os.path.join(BASE_DIR, "logs", "cvat_server.log"),
            "formatter": "standard",
            "maxBytes": 1024 * 1024 * 50,  # 50 MB
            "backupCount": 5,
        },
        "dataset_handler": {
            "class": "logging.handlers.RotatingFileHandler",
            "level": "DEBUG",
            "filename": os.path.join(BASE_DIR, "logs", "cvat_server_dataset.log"),
            "formatter": "standard",
            "maxBytes": 1024 * 1024 * 50,  # 50 MB
            "backupCount": 3,
        },
        "vector": {
            "level": "INFO",
            "class": f"logstash_async.handler.{vector_log_handler}",
            "formatter": "vector",
            "transport": "logstash_async.transport.HttpTransport",
            "ssl_enable": False,
            "ssl_verify": False,
            "host": os.getenv("DJANGO_LOG_SERVER_HOST", "localhost"),
            "port": os.getenv("DJANGO_LOG_SERVER_PORT", 8282),
            "version": 1,
            "message_type": "django",
            "database_path": EVENTS_LOCAL_DB_FILE,
        },
    },
    "root": {
        "handlers": ["console", "server_file"],
    },
    "loggers": {
        "cvat": {
            "level": os.getenv("DJANGO_LOG_LEVEL", "DEBUG"),
        },
        "dataset_logger": {
            "handlers": ["dataset_handler"],
        },
        "django": {
            "level": "INFO",
        },
        "vector": {
            "handlers": [],
            "level": "INFO",
            # set True for debug
            "propagate": False,
        },
    },
}

CVAT_LOG_IMPORT_ERRORS = to_bool(os.getenv("CVAT_LOG_IMPORT_ERRORS", False))

if os.getenv("DJANGO_LOG_SERVER_HOST"):
    LOGGING["loggers"]["vector"]["handlers"] += ["vector"]

DATA_UPLOAD_MAX_MEMORY_SIZE = 100 * 1024 * 1024  # 100 MB
DATA_UPLOAD_MAX_NUMBER_FIELDS = None  # this django check disabled
DATA_UPLOAD_MAX_NUMBER_FILES = None

redis_ondisk_host = os.getenv("CVAT_REDIS_ONDISK_HOST", "localhost")
# The default port is not Redis's default port (6379).
# This is so that a developer can run both in-mem Redis and on-disk Kvrocks on their machine
# without running into a port conflict.
redis_ondisk_port = os.getenv("CVAT_REDIS_ONDISK_PORT", 6666)
redis_ondisk_password = os.getenv("CVAT_REDIS_ONDISK_PASSWORD", "")

# Sets the timeout for the expiration of data chunk in redis_ondisk
CVAT_CHUNK_CACHE_TTL = 3600 * 24  # 1 day

# Sets the timeout for the expiration of preview image in redis_ondisk
CVAT_PREVIEW_CACHE_TTL = 3600 * 24 * 7  # 7 days

CACHES = {
    "default": {
        "BACKEND": "django.core.cache.backends.locmem.LocMemCache",
    },
    "media": {
        "BACKEND": "django.core.cache.backends.redis.RedisCache",
        "LOCATION": f"redis://:{urllib.parse.quote(redis_ondisk_password)}@{redis_ondisk_host}:{redis_ondisk_port}",
        "TIMEOUT": CVAT_CHUNK_CACHE_TTL,
    },
}

USE_CACHE = True

CORS_ALLOW_HEADERS = list(default_headers) + [
    # tus upload protocol headers
    "upload-offset",
    "upload-length",
    "tus-version",
    "tus-resumable",
    # extended upload protocol headers
    "upload-start",
    "upload-finish",
    "upload-multiple",
    "x-organization",
]

TUS_MAX_FILE_SIZE = 26843545600  # 25gb
TUS_DEFAULT_CHUNK_SIZE = 104857600  # 100 mb

# This setting makes request secure if X-Forwarded-Proto: 'https' header is specified by our proxy
# More about forwarded headers - https://doc.traefik.io/traefik/getting-started/faq/#what-are-the-forwarded-headers-when-proxying-http-requests
# How django uses X-Forwarded-Proto - https://docs.djangoproject.com/en/2.2/ref/settings/#secure-proxy-ssl-header
SECURE_PROXY_SSL_HEADER = ("HTTP_X_FORWARDED_PROTO", "https")

SECURE_REFERRER_POLICY = "strict-origin-when-cross-origin"

# Forwarded host - https://docs.djangoproject.com/en/4.0/ref/settings/#std:setting-USE_X_FORWARDED_HOST
# Is used in TUS uploads to provide correct upload endpoint
USE_X_FORWARDED_HOST = True

# Django-sendfile requires to set SENDFILE_ROOT
# https://github.com/moggers87/django-sendfile2
SENDFILE_ROOT = BASE_DIR

CVAT_DOCS_URL = "https://docs.cvat.ai/docs/"

SPECTACULAR_SETTINGS = {
    "TITLE": "CVAT REST API",
    "DESCRIPTION": "REST API for Computer Vision Annotation Tool (CVAT)",
    # Statically set schema version. May also be an empty string. When used together with
    # view versioning, will become '0.0.0 (v2)' for 'v2' versioned requests.
    # Set VERSION to None if only the request version should be rendered.
    "VERSION": __version__,
    "CONTACT": {
        "name": "CVAT.ai team",
        "url": "https://github.com/cvat-ai/cvat",
        "email": "support@cvat.ai",
    },
    "LICENSE": {
        "name": "MIT License",
        "url": "https://en.wikipedia.org/wiki/MIT_License",
    },
    "SERVE_PUBLIC": True,
    "SERVE_PERMISSIONS": ["rest_framework.permissions.IsAuthenticated"],
    # https://swagger.io/docs/open-source-tools/swagger-ui/usage/configuration/
    "SWAGGER_UI_SETTINGS": {
        "deepLinking": True,
        "displayOperationId": True,
        "displayRequestDuration": True,
        "filter": True,
        "showExtensions": True,
    },
    "TOS": "https://www.google.com/policies/terms/",
    "EXTERNAL_DOCS": {
        "description": "CVAT documentation",
        "url": CVAT_DOCS_URL,
    },
    # OTHER SETTINGS
    # https://drf-spectacular.readthedocs.io/en/latest/settings.html
    #
    # TODO: Our current implementation does not suppose this.
    # Need to reconsider this later. It happens, for example,
    # in TaskSerializer for data-originated fields - they can be empty.
    # https://github.com/tfranzel/drf-spectacular/issues/54
    "COMPONENT_NO_READ_ONLY_REQUIRED": True,
    # Required for correct file upload type (bytes)
    "COMPONENT_SPLIT_REQUEST": True,
    "ENUM_NAME_OVERRIDES": {
        "LabelType": "cvat.apps.engine.models.LabelType",
        "ShapeType": "cvat.apps.engine.models.ShapeType",
        "OperationStatus": "cvat.apps.engine.models.StateChoice",
        "ChunkType": "cvat.apps.engine.models.DataChoice",
        "StorageMethod": "cvat.apps.engine.models.StorageMethodChoice",
        "JobStatus": "cvat.apps.engine.models.StatusChoice",
        "JobStage": "cvat.apps.engine.models.StageChoice",
        "JobType": "cvat.apps.engine.models.JobType",
        "QualityReportTarget": "cvat.apps.quality_control.models.QualityReportTarget",
        "StorageType": "cvat.apps.engine.models.StorageChoice",
        "SortingMethod": "cvat.apps.engine.models.SortingMethod",
        "WebhookType": "cvat.apps.webhooks.models.WebhookTypeChoice",
        "WebhookContentType": "cvat.apps.webhooks.models.WebhookContentTypeChoice",
        "RequestStatus": "cvat.apps.engine.models.RequestStatus",
        "ValidationMode": "cvat.apps.engine.models.ValidationMode",
        "FrameSelectionMethod": "cvat.apps.engine.models.JobFrameSelectionMethod",
    },
    # Coercion of {pk} to {id} is controlled by SCHEMA_COERCE_PATH_PK. Additionally,
    # some libraries (e.g. drf-nested-routers) use "_pk" suffixed path variables.
    # This setting globally coerces path variables like "{user_pk}" to "{user_id}".
    "SCHEMA_COERCE_PATH_PK_SUFFIX": True,
    "SCHEMA_PATH_PREFIX": "/api",
    "SCHEMA_PATH_PREFIX_TRIM": False,
    "GENERIC_ADDITIONAL_PROPERTIES": None,
}

# set similar UI restrictions
# https://github.com/cvat-ai/cvat/blob/bad1dc2799afbb22222faaecc7336d999f4cc3fe/cvat-ui/src/utils/validation-patterns.ts#L26
ACCOUNT_USERNAME_MIN_LENGTH = 5
ACCOUNT_LOGOUT_ON_PASSWORD_CHANGE = True

ACCOUNT_ADAPTER = "cvat.apps.iam.adapters.DefaultAccountAdapterEx"

CVAT_HOST = os.getenv("CVAT_HOST", "localhost")
CVAT_BASE_URL = os.getenv("CVAT_BASE_URL", f"http://{CVAT_HOST}:8080").rstrip("/")

CLICKHOUSE = {
    "events": {
        "NAME": os.getenv("CLICKHOUSE_DB", "cvat"),
        "HOST": os.getenv("CLICKHOUSE_HOST", "localhost"),
        "PORT": os.getenv("CLICKHOUSE_PORT", 8123),
        "USER": os.getenv("CLICKHOUSE_USER", "user"),
        "PASSWORD": os.getenv("CLICKHOUSE_PASSWORD", "user"),
    }
}

if (postgres_password_file := os.getenv("CVAT_POSTGRES_PASSWORD_FILE")) is not None:
    if "CVAT_POSTGRES_PASSWORD" in os.environ:
        raise ImproperlyConfigured(
            "The CVAT_POSTGRES_PASSWORD and CVAT_POSTGRES_PASSWORD_FILE"
            " environment variables must not be set at the same time"
        )

    postgres_password = Path(postgres_password_file).read_text(encoding="UTF-8").rstrip("\n")
else:
    postgres_password = os.getenv("CVAT_POSTGRES_PASSWORD", "")

# Database
# https://docs.djangoproject.com/en/3.2/ref/settings/#databases
DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.postgresql",
        "HOST": os.getenv("CVAT_POSTGRES_HOST", "cvat_db"),
        "NAME": os.getenv("CVAT_POSTGRES_DBNAME", "cvat"),
        "USER": os.getenv("CVAT_POSTGRES_USER", "root"),
        "PASSWORD": postgres_password,
        "PORT": os.getenv("CVAT_POSTGRES_PORT", 5432),
        "OPTIONS": {
            "application_name": os.getenv("CVAT_POSTGRES_APPLICATION_NAME", "cvat"),
        },
    }
}

BUCKET_CONTENT_MAX_PAGE_SIZE = 500

IMPORT_CACHE_FAILED_TTL = timedelta(days=30)
IMPORT_CACHE_SUCCESS_TTL = timedelta(hours=1)
IMPORT_CACHE_CLEAN_DELAY = timedelta(hours=12)

ASSET_MAX_SIZE_MB = 10
ASSET_SUPPORTED_TYPES = ("image/jpeg", "image/png", "image/webp", "image/gif", "application/pdf")
ASSET_MAX_IMAGE_SIZE = 1920
ASSET_MAX_COUNT_PER_GUIDE = 30

SMOKESCREEN_ENABLED = True

# By default, email backend is django.core.mail.backends.smtp.EmailBackend
# But it won't work without additional configuration, so we set it to None
# to check configuration and throw ImproperlyConfigured if thats a case
EMAIL_BACKEND = None

ONE_RUNNING_JOB_IN_QUEUE_PER_USER = to_bool(os.getenv("ONE_RUNNING_JOB_IN_QUEUE_PER_USER", False))

# How many chunks can be prepared simultaneously during task creation in case the cache is not used
CVAT_CONCURRENT_CHUNK_PROCESSING = int(os.getenv("CVAT_CONCURRENT_CHUNK_PROCESSING", 1))

from cvat.rq_patching import update_started_job_registry_cleanup

update_started_job_registry_cleanup()

CLOUD_DATA_DOWNLOADING_MAX_THREADS_NUMBER = 4
CLOUD_DATA_DOWNLOADING_NUMBER_OF_FILES_PER_THREAD = 1000

# Indicates the maximum number of days a file or directory is retained in the temporary directory
TMP_FILE_OR_DIR_RETENTION_DAYS = 3

LOGO_FILENAME = "logo.svg"
ABOUT_INFO = {
    "subtitle": "Open Data Annotation Platform",
}


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\development.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Inherit parent config
from .base import *  # pylint: disable=wildcard-import

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

INSTALLED_APPS += [
    "django_extensions",
]

ALLOWED_HOSTS.append("testserver")

# Django-sendfile:
# https://github.com/moggers87/django-sendfile2
SENDFILE_BACKEND = "django_sendfile.backends.development"

# Cross-Origin Resource Sharing settings for CVAT UI
UI_SCHEME = os.environ.get("CVAT_UI_SCHEME", "http")
UI_HOST = os.environ.get("CVAT_UI_HOST", "localhost")
UI_PORT = os.environ.get("CVAT_UI_PORT", 3000)
CORS_ALLOW_CREDENTIALS = True
UI_URL = "{}://{}".format(UI_SCHEME, UI_HOST)

if UI_PORT and UI_PORT != "80":
    UI_URL += ":{}".format(UI_PORT)

CSRF_TRUSTED_ORIGINS = [UI_URL]

# set UI url to redirect to after successful e-mail confirmation
ACCOUNT_EMAIL_CONFIRMATION_ANONYMOUS_REDIRECT_URL = "{}/auth/email-confirmation".format(UI_URL)
ACCOUNT_EMAIL_VERIFICATION_SENT_REDIRECT_URL = "{}/auth/email-verification-sent".format(UI_URL)
INCORRECT_EMAIL_CONFIRMATION_URL = "{}/auth/incorrect-email-confirmation".format(UI_URL)

CORS_ORIGIN_WHITELIST = [UI_URL]
CORS_REPLACE_HTTPS_REFERER = True
IAM_OPA_HOST = "http://localhost:8181"
IAM_OPA_DATA_URL = f"{IAM_OPA_HOST}/v1/data"

INSTALLED_APPS += ["silk"]

MIDDLEWARE += [
    "silk.middleware.SilkyMiddleware",
]

# Django profiler
# https://github.com/jazzband/django-silk
SILKY_PYTHON_PROFILER = True
SILKY_PYTHON_PROFILER_BINARY = True
SILKY_PYTHON_PROFILER_RESULT_PATH = os.path.join(BASE_DIR, "profiles/")
os.makedirs(SILKY_PYTHON_PROFILER_RESULT_PATH, exist_ok=True)
SILKY_AUTHENTICATION = True
SILKY_AUTHORISATION = True
SILKY_MAX_REQUEST_BODY_SIZE = 1024
SILKY_MAX_RESPONSE_BODY_SIZE = 1024
SILKY_IGNORE_PATHS = ["/admin", "/documentation", "/django-rq", "/auth"]
SILKY_MAX_RECORDED_REQUESTS = 10**4

# Database
# https://docs.djangoproject.com/en/3.2/ref/settings/#databases
DATABASES["default"]["HOST"] = os.getenv("CVAT_POSTGRES_HOST", "localhost")

SMOKESCREEN_ENABLED = False


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\email_settings.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Inherit parent config
from cvat.settings.production import *  # pylint: disable=wildcard-import

# https://github.com/pennersr/django-allauth
ACCOUNT_AUTHENTICATION_METHOD = "username_email"
ACCOUNT_CONFIRM_EMAIL_ON_GET = True
ACCOUNT_EMAIL_REQUIRED = True
ACCOUNT_EMAIL_VERIFICATION = "mandatory"

# Email backend settings for Django
EMAIL_BACKEND = "django.core.mail.backends.console.EmailBackend"


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\production.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

# Inherit parent config
from .base import *  # pylint: disable=wildcard-import

DEBUG = False

NUCLIO["HOST"] = os.getenv("CVAT_NUCLIO_HOST", "nuclio")

# Django-sendfile:
# https://github.com/moggers87/django-sendfile2
SENDFILE_BACKEND = "django_sendfile.backends.nginx"
SENDFILE_URL = "/"

LOGGING["formatters"]["verbose_uvicorn_access"] = {
    "()": "uvicorn.logging.AccessFormatter",
    "format": '[{asctime}] {levelprefix} {client_addr} - "{request_line}" {status_code}',
    "style": "{",
}
LOGGING["handlers"]["verbose_uvicorn_access"] = {
    "formatter": "verbose_uvicorn_access",
    "class": "logging.StreamHandler",
    "stream": "ext://sys.stdout",
}
LOGGING["loggers"]["uvicorn.access"] = {
    "handlers": ["verbose_uvicorn_access"],
    "level": "INFO",
    "propagate": False,
}


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\testing.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import tempfile

# Inherit parent config
from .development import *  # pylint: disable=wildcard-import

DATABASES = {
    "default": {
        "ENGINE": "django.db.backends.sqlite3",
        "NAME": os.path.join(BASE_DIR, "db.sqlite3"),
    },
}

_temp_dir = tempfile.TemporaryDirectory(dir=BASE_DIR, suffix="cvat")
BASE_DIR = _temp_dir.name

DATA_ROOT = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_ROOT, exist_ok=True)

MEDIA_DATA_ROOT = os.path.join(DATA_ROOT, "data")
os.makedirs(MEDIA_DATA_ROOT, exist_ok=True)

CACHE_ROOT = os.path.join(DATA_ROOT, "cache")
os.makedirs(CACHE_ROOT, exist_ok=True)

EXPORT_CACHE_ROOT = os.path.join(CACHE_ROOT, "export")
os.makedirs(EXPORT_CACHE_ROOT, exist_ok=True)

JOBS_ROOT = os.path.join(DATA_ROOT, "jobs")
os.makedirs(JOBS_ROOT, exist_ok=True)

TASKS_ROOT = os.path.join(DATA_ROOT, "tasks")
os.makedirs(TASKS_ROOT, exist_ok=True)

PROJECTS_ROOT = os.path.join(DATA_ROOT, "projects")
os.makedirs(PROJECTS_ROOT, exist_ok=True)

SHARE_ROOT = os.path.join(BASE_DIR, "share")
os.makedirs(SHARE_ROOT, exist_ok=True)

MODELS_ROOT = os.path.join(DATA_ROOT, "models")
os.makedirs(MODELS_ROOT, exist_ok=True)

LOGS_ROOT = os.path.join(BASE_DIR, "logs")
os.makedirs(LOGS_ROOT, exist_ok=True)

MIGRATIONS_LOGS_ROOT = os.path.join(LOGS_ROOT, "migrations")
os.makedirs(MIGRATIONS_LOGS_ROOT, exist_ok=True)

CLOUD_STORAGE_ROOT = os.path.join(DATA_ROOT, "storages")
os.makedirs(CLOUD_STORAGE_ROOT, exist_ok=True)

TMP_FILES_ROOT = os.path.join(DATA_ROOT, "tmp")
os.makedirs(TMP_FILES_ROOT, exist_ok=True)

# To avoid ERROR django.security.SuspiciousFileOperation:
# The joined path (...) is located outside of the base path component
MEDIA_ROOT = BASE_DIR

# Suppress all logs by default
for logger in LOGGING["loggers"].values():
    if isinstance(logger, dict) and "level" in logger:
        logger["level"] = "ERROR"

LOGGING["handlers"]["server_file"] = LOGGING["handlers"]["console"]

PASSWORD_HASHERS = ("django.contrib.auth.hashers.MD5PasswordHasher",)

# When you run ./manage.py test, Django looks at the TEST_RUNNER setting to
# determine what to do. By default, TEST_RUNNER points to
# 'django.test.runner.DiscoverRunner'. This class defines the default Django
# testing behavior.
TEST_RUNNER = "cvat.settings.testing.PatchedDiscoverRunner"

from django.test.runner import DiscoverRunner


class PatchedDiscoverRunner(DiscoverRunner):
    def __init__(self, *args, **kwargs):
        # Used fakeredis for testing (don't affect production redis)
        import django_rq.queues
        from fakeredis import FakeRedis, FakeStrictRedis

        simple_redis = FakeRedis()
        strict_redis = FakeStrictRedis()
        django_rq.queues.get_redis_connection = lambda _, strict: (
            strict_redis if strict else simple_redis
        )

        # Run all RQ requests syncroniously
        for config in RQ_QUEUES.values():
            config["ASYNC"] = False

        super().__init__(*args, **kwargs)


# No need to profile unit tests
INSTALLED_APPS.remove("silk")
MIDDLEWARE.remove("silk.middleware.SilkyMiddleware")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\testing_rest.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Inherit parent config
from cvat.settings.production import *  # pylint: disable=wildcard-import

# We use MD5 password hasher instead of default PBKDF2 here to speed up REST API tests,
# because the current implementation of the tests requires authentication in each test case
# so using the PBKDF2 hasher slows them.
PASSWORD_HASHERS = [
    "django.contrib.auth.hashers.MD5PasswordHasher",
]

IMPORT_CACHE_CLEAN_DELAY = timedelta(seconds=30)

# The tests should not fail due to high disk utilization of CI infrastructure that we have no control over
# But let's keep this check enabled
HEALTH_CHECK = {
    "DISK_USAGE_MAX": 100,  # percent
}


# ===== 文件: D:\wow_ai\docker\cvat\cvat\settings\__init__.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat\utils\http.py =====
# Copyright (C) 2023 Intel Corporation
#
# SPDX-License-Identifier: MIT

import requests
import requests.utils
from django.conf import settings

from cvat import __version__

_CVAT_USER_AGENT = f"CVAT/{__version__} {requests.utils.default_user_agent()}"

# Note that setting Session.proxies doesn't work correctly if an upstream proxy
# is specified via environment variables: <https://github.com/psf/requests/issues/2018>.
# Therefore, for robust operation, these need to be passed with every request via the
# proxies= parameter.
PROXIES_FOR_UNTRUSTED_URLS = None

if settings.SMOKESCREEN_ENABLED:
    PROXIES_FOR_UNTRUSTED_URLS = {
        "http": "http://localhost:4750",
        "https": "http://localhost:4750",
    }


def make_requests_session() -> requests.Session:
    session = requests.Session()
    session.headers["User-Agent"] = _CVAT_USER_AGENT
    return session


# ===== 文件: D:\wow_ai\docker\cvat\cvat\utils\remote_debugger.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os


def is_debugging_enabled() -> bool:
    return os.environ.get("CVAT_DEBUG_ENABLED") == "yes"


if is_debugging_enabled():
    import debugpy

    class RemoteDebugger:
        """
        Support for VS code debugger.

        Supports both single- and multi-thread scenarios.

        Read docs: https://github.com/microsoft/debugpy
        Read more: https://modwsgi.readthedocs.io/en/develop/user-guides/debugging-techniques.html
        """

        ENV_VAR_PORT = "CVAT_DEBUG_PORT"
        ENV_VAR_WAIT = "CVAT_DEBUG_WAIT"
        __debugger_initialized = False

        @classmethod
        def _singleton_init(cls):
            if cls.__debugger_initialized:
                return

            try:
                port = int(os.environ[cls.ENV_VAR_PORT])

                # The only intended use is in Docker.
                # Using 127.0.0.1 will not allow host connections
                addr = ("0.0.0.0", port)  # nosec - B104:hardcoded_bind_all_interfaces

                # Debugpy is a singleton
                # We put it in the main thread of the process and then report new threads
                debugpy.listen(addr)

                # In most cases it makes no sense to debug subprocesses
                # Feel free to enable if needed.
                debugpy.configure({"subProcess": False})

                if os.environ.get(cls.ENV_VAR_WAIT) == "yes":
                    debugpy.wait_for_client()
            except Exception as ex:
                raise Exception("failed to set debugger") from ex

            cls.__debugger_initialized = True

        def __init__(self) -> None:
            self._singleton_init()

        def attach_current_thread(self) -> None:
            debugpy.debug_this_thread()


# ===== 文件: D:\wow_ai\docker\cvat\cvat\utils\version.py =====
# Copyright (C) 2019-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT
#
# Note: It is slightly re-implemented Django version of code. We cannot use
# get_version from django.utils.version module because get_git_changeset will
# always return empty value (cwd=repo_dir isn't correct). Also it gives us a
# way to define the version as we like.

import datetime
import os
import subprocess


def get_version(version):
    """Return a PEP 440-compliant version number from VERSION."""
    # Now build the two parts of the version number:
    # main = X.Y.Z
    # sub = .devN - for pre-alpha releases
    #     | {a|b|rc}N - for alpha, beta, and rc releases

    main = get_main_version(version)

    sub = ""
    if version[3] == "alpha" and version[4] == 0:
        git_changeset = get_git_changeset()
        if git_changeset:
            sub = ".dev%s" % git_changeset

    elif version[3] != "final":
        mapping = {"alpha": "a", "beta": "b", "rc": "rc"}
        sub = mapping[version[3]] + str(version[4])

    return main + sub


def get_main_version(version):
    """Return main version (X.Y.Z) from VERSION."""
    return ".".join(str(x) for x in version[:3])


def get_git_changeset():
    """Return a numeric identifier of the latest git changeset.

    The result is the UTC timestamp of the changeset in YYYYMMDDHHMMSS format.
    This value isn't guaranteed to be unique, but collisions are very unlikely,
    so it's sufficient for generating the development version numbers.
    """
    repo_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    git_log = subprocess.Popen(  # nosec: B603, B607
        ["git", "log", "--pretty=format:%ct", "--quiet", "-1", "HEAD"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        cwd=repo_dir,
        universal_newlines=True,
    )
    timestamp = git_log.communicate()[0]
    try:
        timestamp = datetime.datetime.fromtimestamp(int(timestamp), tz=datetime.timezone.utc)
    except ValueError:
        return None
    return timestamp.strftime("%Y%m%d%H%M%S")


# ===== 文件: D:\wow_ai\docker\cvat\cvat\utils\__init__.py =====
# Copyright (C) 2023 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\setup.py =====
# Copyright (C) 2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os.path as osp
import re

from setuptools import find_packages, setup


def find_version(project_dir=None):
    if not project_dir:
        project_dir = osp.dirname(osp.abspath(__file__))

    file_path = osp.join(project_dir, "version.py")

    with open(file_path, "r") as version_file:
        version_text = version_file.read()

    # PEP440:
    # https://www.python.org/dev/peps/pep-0440/#appendix-b-parsing-version-strings-with-regular-expressions
    pep_regex = r"([1-9]\d*!)?(0|[1-9]\d*)(\.(0|[1-9]\d*))*((a|b|rc)(0|[1-9]\d*))?(\.post(0|[1-9]\d*))?(\.dev(0|[1-9]\d*))?"
    version_regex = r"VERSION\s*=\s*.(" + pep_regex + ")."
    match = re.match(version_regex, version_text)
    if not match:
        raise RuntimeError("Failed to find version string in '%s'" % file_path)

    version = version_text[match.start(1) : match.end(1)]
    return version


BASE_REQUIREMENTS_FILE = "requirements/base.txt"


def parse_requirements(filename=BASE_REQUIREMENTS_FILE):
    with open(filename) as fh:
        return fh.readlines()


BASE_REQUIREMENTS = parse_requirements(BASE_REQUIREMENTS_FILE)

with open("README.md", "r") as fh:
    long_description = fh.read()

setup(
    name="cvat-cli",
    version=find_version(project_dir="src/cvat_cli"),
    description="Command-line client for CVAT",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/cvat-ai/cvat/",
    package_dir={"": "src"},
    packages=find_packages(where="src"),
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.9",
    install_requires=BASE_REQUIREMENTS,
    entry_points={
        "console_scripts": [
            "cvat-cli=cvat_cli.__main__:main",
        ],
    },
    include_package_data=True,
)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\version.py =====
VERSION = "2.35.1"


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\__main__.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import logging
import sys

import urllib3.exceptions
from cvat_sdk import exceptions

from ._internal.commands_all import COMMANDS
from ._internal.common import (
    CriticalError,
    build_client,
    configure_common_arguments,
    configure_logger,
)
from ._internal.utils import popattr

logger = logging.getLogger(__name__)


def main(args: list[str] = None):
    parser = argparse.ArgumentParser(description=COMMANDS.description)
    configure_common_arguments(parser)
    COMMANDS.configure_parser(parser)

    parsed_args = parser.parse_args(args)

    configure_logger(logger, parsed_args)

    try:
        with build_client(parsed_args, logger=logger) as client:
            popattr(parsed_args, "_executor")(client, **vars(parsed_args))
    except (exceptions.ApiException, urllib3.exceptions.HTTPError, CriticalError) as e:
        logger.critical(e)
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(main())


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\agent.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import concurrent.futures
import contextlib
import json
import multiprocessing
import random
import secrets
import shutil
import tempfile
import threading
from collections.abc import Iterator
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import TYPE_CHECKING, Optional, Union

import attrs
import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.datasets as cvatds
import urllib3.exceptions
from cvat_sdk import Client, models
from cvat_sdk.auto_annotation.driver import (
    _AnnotationMapper,
    _DetectionFunctionContextImpl,
    _SpecNameMapping,
)
from cvat_sdk.exceptions import ApiException

from .common import CriticalError, FunctionLoader

if TYPE_CHECKING:
    from _typeshed import SupportsReadline

FUNCTION_PROVIDER_NATIVE = "native"
FUNCTION_KIND_DETECTOR = "detector"
REQUEST_CATEGORY_BATCH = "batch"

_POLLING_INTERVAL_MEAN_FREQUENT = timedelta(seconds=60)
_POLLING_INTERVAL_MEAN_RARE = timedelta(minutes=10)
_JITTER_AMOUNT = 0.15

_UPDATE_INTERVAL = timedelta(seconds=30)


class _RecoverableExecutor:
    # A wrapper around ProcessPoolExecutor that recreates the underlying
    # executor when a worker crashes.
    def __init__(self, initializer, initargs):
        self._mp_context = multiprocessing.get_context("spawn")
        self._initializer = initializer
        self._initargs = initargs
        self._executor = self._new_executor()

    def _new_executor(self):
        return concurrent.futures.ProcessPoolExecutor(
            max_workers=1,
            mp_context=self._mp_context,
            initializer=self._initializer,
            initargs=self._initargs,
        )

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        self._executor.shutdown()

    def submit(self, func, /, *args, **kwargs):
        return self._executor.submit(func, *args, **kwargs)

    def result(self, future: concurrent.futures.Future):
        try:
            return future.result()
        except concurrent.futures.BrokenExecutor:
            self._executor.shutdown()
            self._executor = self._new_executor()
            raise


_current_function: cvataa.DetectionFunction


def _worker_init(function_loader: FunctionLoader):
    global _current_function
    _current_function = function_loader.load()


def _worker_job_get_function_spec():
    return _current_function.spec


def _worker_job_detect(context, image):
    return _current_function.detect(context, image)


@attrs.frozen
class _Event:
    type: str
    data: str


@attrs.frozen
class _NewReconnectionDelay:
    delay: timedelta


def _parse_event_stream(
    stream: SupportsReadline[bytes],
) -> Iterator[Union[_Event, _NewReconnectionDelay]]:
    # https://html.spec.whatwg.org/multipage/server-sent-events.html#event-stream-interpretation

    event_type = event_data = ""

    while True:
        line_bytes = stream.readline()
        if not line_bytes:
            return

        line = line_bytes.decode("UTF-8").removesuffix("\n").removesuffix("\r")

        # Technically, a standalone \r is supposed to be treated as a line terminator,
        # but it's annoying to implement, and there's no reason for CVAT to use that.
        if "\r" in line:
            raise ValueError("CR found in event stream")

        if not line:
            yield _Event(event_type, event_data.removesuffix("\n"))
            event_type = event_data = ""
            continue

        if line.startswith(":"):
            # it's a comment/keepalive
            continue

        if ":" in line:
            field_name, field_value = line.split(":", maxsplit=1)
            field_value = field_value.removeprefix(" ")
        else:
            field_name = line
            field_value = ""

        if field_name == "event":
            event_type = field_value
        elif field_name == "data":
            event_data += field_value + "\n"
        elif field_name == "retry":
            if field_value.isascii() and field_value.isdecimal():
                yield _NewReconnectionDelay(timedelta(milliseconds=int(field_value)))


class _Agent:
    def __init__(self, client: Client, executor: _RecoverableExecutor, function_id: int):
        self._rng = random.Random()  # nosec

        self._client = client
        self._executor = executor
        self._function_id = function_id
        self._function_spec = self._executor.result(
            self._executor.submit(_worker_job_get_function_spec)
        )

        _, response = self._client.api_client.call_api(
            "/api/functions/{function_id}",
            "GET",
            path_params={"function_id": self._function_id},
        )

        remote_function = json.loads(response.data)

        self._validate_function_compatibility(remote_function)

        self._agent_id = secrets.token_hex(16)
        self._client.logger.info("Agent starting with ID %r", self._agent_id)

        self._cached_task_id = None

        self._queue_watch_response = None
        self._queue_watch_response_lock = threading.Lock()
        self._queue_watcher_should_stop = threading.Event()

        self._potential_work_condition = threading.Condition(threading.Lock())
        self._batch_request_might_be_available = False

        self._polling_interval = _POLLING_INTERVAL_MEAN_FREQUENT

        # If we fail to connect to the queue event stream, it might be because
        # the server is too old and doesn't support the watch endpoint.
        # In this case, it doesn't make sense to continue trying to connect frequently,
        # although we should still be trying occasionally in case the error is transient.
        # Once we're successful, we'll rely on the server to set a new reconnection delay.
        self._queue_reconnection_delay = _POLLING_INTERVAL_MEAN_RARE

    def _validate_function_compatibility(self, remote_function: dict) -> None:
        function_id = remote_function["id"]

        if remote_function["provider"] != FUNCTION_PROVIDER_NATIVE:
            raise CriticalError(
                f"Function #{function_id} has provider {remote_function['provider']!r}. "
                f"Agents can only be run for functions with provider {FUNCTION_PROVIDER_NATIVE!r}."
            )

        if isinstance(self._function_spec, cvataa.DetectionFunctionSpec):
            self._validate_detection_function_compatibility(remote_function)
            self._calculate_result_for_ar = self._calculate_result_for_detection_ar
        else:
            raise CriticalError(
                f"Unsupported function spec type: {type(self._function_spec).__name__}"
            )

    def _validate_detection_function_compatibility(self, remote_function: dict) -> None:
        incompatible_msg = (
            f"Function #{remote_function['id']} is incompatible with function object: "
        )

        if remote_function["kind"] != FUNCTION_KIND_DETECTOR:
            raise CriticalError(
                incompatible_msg
                + f"kind is {remote_function['kind']!r} (expected {FUNCTION_KIND_DETECTOR!r})."
            )

        labels_by_name = {label.name: label for label in self._function_spec.labels}

        for remote_label in remote_function["labels_v2"]:
            label_desc = f"label {remote_label['name']!r}"
            label = labels_by_name.get(remote_label["name"])

            self._validate_sublabel_compatibility(remote_label, label, incompatible_msg, label_desc)

            sublabels_by_name = {sl.name: sl for sl in getattr(label, "sublabels", [])}

            for remote_sl in remote_label.get("sublabels", []):
                sl_desc = f"sublabel {remote_sl['name']!r} of {label_desc}"
                sl = sublabels_by_name.get(remote_sl["name"])

                self._validate_sublabel_compatibility(remote_sl, sl, incompatible_msg, sl_desc)

    def _validate_sublabel_compatibility(
        self, remote_sl: dict, sl: Optional[models.Sublabel], incompatible_msg: str, sl_desc: str
    ):
        if not sl:
            raise CriticalError(incompatible_msg + f"{sl_desc} is not supported.")

        if remote_sl["type"] not in {"any", "unknown"} and remote_sl["type"] != sl.type:
            raise CriticalError(
                incompatible_msg + f"{sl_desc} has type {remote_sl['type']!r}, "
                f"but the function object declares type {sl.type!r}."
            )

        attrs_by_name = {attr.name: attr for attr in getattr(sl, "attributes", [])}

        for remote_attr in remote_sl["attributes"]:
            attr_desc = f"attribute {remote_attr['name']!r} of {sl_desc}"
            attr = attrs_by_name.get(remote_attr["name"])

            if not attr:
                raise CriticalError(incompatible_msg + f"{attr_desc} is not supported.")

            if remote_attr["input_type"] != attr.input_type.value:
                raise CriticalError(
                    incompatible_msg + f"{attr_desc} has input type {remote_attr['input_type']!r},"
                    f" but the function object declares input type {attr.input_type.value!r}."
                )

            if remote_attr["values"] != attr.values:
                raise CriticalError(
                    incompatible_msg + f"{attr_desc} has values {remote_attr['values']!r},"
                    f" but the function object declares values {attr.values!r}."
                )

    def _wait_between_polls(self):
        # offset the interval randomly to avoid synchronization between workers
        timeout_multiplier = self._rng.uniform(1 - _JITTER_AMOUNT, 1 + _JITTER_AMOUNT)

        with self._potential_work_condition:
            self._potential_work_condition.wait_for(
                lambda: self._batch_request_might_be_available,
                timeout=self._polling_interval.total_seconds() * timeout_multiplier,
            )

            self._batch_request_might_be_available = False

    def _dispatch_queue_event(self, event: _Event) -> None:
        if event.type == "newrequest":
            event_data_object = json.loads(event.data)
            request_category = event_data_object["request_category"]

            with self._potential_work_condition:
                if request_category == REQUEST_CATEGORY_BATCH:
                    self._client.logger.info("Received notification about a new batch request")
                    self._batch_request_might_be_available = True
                    self._potential_work_condition.notify()
                else:
                    self._client.logger.warning(
                        "Received notification about a new request of unknown category: %r",
                        request_category,
                    )
        else:
            self._client.logger.warning("Received event of unknown type: %r", event.type)

    def _wait_before_reconnecting_to_queue(self):
        delay_multiplier = self._rng.uniform(1, 1 + _JITTER_AMOUNT)
        self._queue_watcher_should_stop.wait(
            timeout=self._queue_reconnection_delay.total_seconds() * delay_multiplier
        )

        # Apply exponential backoff.
        self._queue_reconnection_delay = min(
            self._queue_reconnection_delay * 2, _POLLING_INTERVAL_MEAN_RARE
        )

    def _watch_queue(self) -> None:
        while not self._queue_watcher_should_stop.is_set():
            # Until we can (re)connect to the event stream, poll more frequently.
            self._polling_interval = _POLLING_INTERVAL_MEAN_FREQUENT

            with self._queue_watch_response_lock:
                self._client.logger.info("Attempting to watch the function's queue...")

                try:
                    _, self._queue_watch_response = self._client.api_client.call_api(
                        "/api/functions/queues/{queue_id}/watch",
                        "GET",
                        path_params={"queue_id": f"function:{self._function_id}"},
                        _parse_response=False,
                    )
                except Exception:
                    self._client.logger.error(
                        "Failed to connect to the queue event stream; will retry",
                        exc_info=True,
                    )
                    self._wait_before_reconnecting_to_queue()
                    continue
                else:
                    self._client.logger.info("Connected to the queue event stream")

                    # Now we can rely on notifications, so slow down polling.
                    self._polling_interval = _POLLING_INTERVAL_MEAN_RARE

            try:
                for message in _parse_event_stream(self._queue_watch_response):
                    if isinstance(message, _Event):
                        self._dispatch_queue_event(message)
                    elif isinstance(message, _NewReconnectionDelay):
                        self._queue_reconnection_delay = message.delay
                        self._client.logger.info(
                            "New queue event stream reconnection delay is %fs",
                            self._queue_reconnection_delay.total_seconds(),
                        )
                    else:
                        assert False, f"unexpected message type {type(message)}"

                self._queue_watch_response.release_conn()

                # We should normally not get here unless the function is deleted on the server.
                # However, we don't know that for sure, so instead of quitting immediately,
                # we'll ask the main thread to poll for an AR.
                # If the function did get deleted, the main thread will get a 404 and quit.
                # Otherwise, we'll just reconnect again.
                with self._potential_work_condition:
                    self._batch_request_might_be_available = True
                    self._potential_work_condition.notify()

                self._client.logger.warning("Event stream ended; will reconnect")
            except Exception:
                # This is an extra check to prevent useless messages.
                # If we crashed, but the main thread wants us to stop anyway,
                # we should just stop and not spam the log.
                if self._queue_watcher_should_stop.is_set():
                    break

                self._client.logger.error(
                    "Event stream interrupted or other error; will reconnect", exc_info=True
                )
            finally:
                self._queue_watch_response.close()

            self._wait_before_reconnecting_to_queue()

    def run(self, *, burst: bool) -> None:
        if burst:
            while ar_assignment := self._poll_for_ar():
                self._process_ar(ar_assignment)
            self._client.logger.info("No annotation requests left in queue; exiting.")
        else:
            watcher = threading.Thread(name="Queue Watcher", target=self._watch_queue)
            watcher.start()

            try:
                while True:
                    if ar_assignment := self._poll_for_ar():
                        self._process_ar(ar_assignment)
                    else:
                        self._wait_between_polls()
            finally:
                self._queue_watcher_should_stop.set()

                with self._queue_watch_response_lock:
                    if self._queue_watch_response:
                        with contextlib.suppress(Exception):
                            # shutdown() requires urllib3 2.3.0, whereas we only require 1.25
                            # (via the SDK). The reason we can't bump the requirement is that
                            # the testsuite depends on botocore, which is incompatible with urllib3
                            # 2.x on Python 3.9 and earlier.
                            # Since pip will, by default, install the latest dependency versions,
                            # most users should not be affected. For the ones that are, shutdown
                            # will be broken, but everything else should still work fine.
                            # This should be revisited once we drop Python 3.9 support.
                            self._queue_watch_response.shutdown()

                watcher.join()

    def _process_ar(self, ar_assignment: dict) -> None:
        self._client.logger.info("Got annotation request assignment: %r", ar_assignment)

        ar_id = ar_assignment["ar_id"]

        try:
            result = self._calculate_result_for_ar(ar_id, ar_assignment["ar_params"])

            self._client.logger.info("Submitting result for AR %r...", ar_id)
            self._client.api_client.call_api(
                "/api/functions/queues/{queue_id}/requests/{request_id}/complete",
                "POST",
                path_params={"queue_id": f"function:{self._function_id}", "request_id": ar_id},
                body={"agent_id": self._agent_id, "annotations": result},
            )
            self._client.logger.info("AR %r completed", ar_id)
        except Exception as ex:
            self._client.logger.error("Failed to process AR %r", ar_id, exc_info=True)

            # Arbitrary exceptions may contain details of the client's system or code, which
            # shouldn't be exposed to the server (and to users of the function).
            # Therefore, we only produce a limited amount of detail, and only in known failure cases.
            error_message = "Unknown error"

            if isinstance(ex, ApiException):
                if ex.status:
                    error_message = f"Received HTTP status {ex.status}"
                else:
                    error_message = "Failed an API call"
            elif isinstance(ex, urllib3.exceptions.RequestError):
                if isinstance(ex, urllib3.exceptions.MaxRetryError):
                    ex_type = type(ex.reason)
                else:
                    ex_type = type(ex)

                error_message = f"Failed to make an HTTP request to {ex.url} ({ex_type.__name__})"
            elif isinstance(ex, urllib3.exceptions.HTTPError):
                error_message = "Failed to make an HTTP request"
            elif isinstance(ex, cvataa.BadFunctionError):
                error_message = "Underlying function returned incorrect result: " + str(ex)
            elif isinstance(ex, concurrent.futures.BrokenExecutor):
                error_message = "Worker process crashed"

            try:
                self._client.api_client.call_api(
                    "/api/functions/queues/{queue_id}/requests/{request_id}/fail",
                    "POST",
                    path_params={
                        "queue_id": f"function:{self._function_id}",
                        "request_id": ar_id,
                    },
                    body={"agent_id": self._agent_id, "exc_info": error_message},
                )
            except Exception:
                self._client.logger.error("Couldn't fail AR %r", ar_id, exc_info=True)
            else:
                self._client.logger.info("AR %r failed", ar_id)

    def _poll_for_ar(self) -> Optional[dict]:
        while True:
            self._client.logger.info("Trying to acquire an annotation request...")
            try:
                _, response = self._client.api_client.call_api(
                    "/api/functions/queues/{queue_id}/requests/acquire",
                    "POST",
                    path_params={"queue_id": f"function:{self._function_id}"},
                    body={"agent_id": self._agent_id, "request_category": REQUEST_CATEGORY_BATCH},
                )
                break
            except (urllib3.exceptions.HTTPError, ApiException) as ex:
                if isinstance(ex, ApiException) and ex.status and 400 <= ex.status < 500:
                    # We did something wrong; no point in retrying.
                    raise

                self._client.logger.error("Acquire request failed; will retry", exc_info=True)
                self._wait_between_polls()

        response_data = json.loads(response.data)
        return response_data["ar_assignment"]

    def _calculate_result_for_detection_ar(
        self, ar_id: str, ar_params
    ) -> models.PatchedLabeledDataRequest:
        if ar_params["type"] != "annotate_task":
            raise RuntimeError(f"Unsupported AR type: {ar_params['type']!r}")

        if ar_params["task"] != self._cached_task_id:
            # To avoid uncontrolled disk usage,
            # we'll only keep one task in the cache at a time.
            self._client.logger.info("Switched to a new task; clearing the cache...")
            if self._client.config.cache_dir.exists():
                shutil.rmtree(self._client.config.cache_dir)

        ds = cvatds.TaskDataset(self._client, ar_params["task"], load_annotations=False)

        self._cached_task_id = ar_params["task"]

        # Fetching the dataset might take a while, so do a progress update to let the server
        # know we're still alive.
        self._update_ar(ar_id, 0)
        last_update_timestamp = datetime.now(tz=timezone.utc)

        conv_mask_to_poly = ar_params["conv_mask_to_poly"]

        spec_nm = _SpecNameMapping.from_api(
            {
                k: models.LabelMappingEntryRequest._from_openapi_data(**v)
                for k, v in ar_params["mapping"].items()
            }
        )

        mapper = _AnnotationMapper(
            self._client.logger,
            self._function_spec.labels,
            ds.labels,
            allow_unmatched_labels=False,
            spec_nm=spec_nm,
            conv_mask_to_poly=conv_mask_to_poly,
        )

        all_annotations = models.PatchedLabeledDataRequest(shapes=[])

        for sample_index, sample in enumerate(ds.samples):
            context = _DetectionFunctionContextImpl(
                frame_name=sample.frame_name,
                conf_threshold=ar_params["threshold"],
                conv_mask_to_poly=conv_mask_to_poly,
            )
            shapes = self._executor.result(
                self._executor.submit(_worker_job_detect, context, sample.media.load_image())
            )

            mapper.validate_and_remap(shapes, sample.frame_index)
            all_annotations.shapes.extend(shapes)

            current_timestamp = datetime.now(tz=timezone.utc)

            if current_timestamp >= last_update_timestamp + _UPDATE_INTERVAL:
                self._update_ar(ar_id, (sample_index + 1) / len(ds.samples))
                last_update_timestamp = current_timestamp

        return all_annotations

    def _update_ar(self, ar_id: str, progress: float) -> None:
        self._client.logger.info("Updating AR %r progress to %.2f%%", ar_id, progress * 100)
        self._client.api_client.call_api(
            "/api/functions/queues/{queue_id}/requests/{request_id}/update",
            "POST",
            path_params={"queue_id": f"function:{self._function_id}", "request_id": ar_id},
            body={"agent_id": self._agent_id, "progress": progress},
        )


def run_agent(
    client: Client, function_loader: FunctionLoader, function_id: int, *, burst: bool
) -> None:
    with (
        _RecoverableExecutor(initializer=_worker_init, initargs=[function_loader]) as executor,
        tempfile.TemporaryDirectory() as cache_dir,
    ):
        client.config.cache_dir = Path(cache_dir, "cache")
        client.logger.info("Will store cache at %s", client.config.cache_dir)

        agent = _Agent(client, executor, function_id)
        agent.run(burst=burst)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\commands_all.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from .command_base import CommandGroup, DeprecatedAlias
from .commands_functions import COMMANDS as COMMANDS_FUNCTIONS
from .commands_projects import COMMANDS as COMMANDS_PROJECTS
from .commands_tasks import COMMANDS as COMMANDS_TASKS

COMMANDS = CommandGroup(description="Perform operations on CVAT resources.")

COMMANDS.add_command("function", COMMANDS_FUNCTIONS)
COMMANDS.add_command("project", COMMANDS_PROJECTS)
COMMANDS.add_command("task", COMMANDS_TASKS)

_legacy_mapping = {
    "create": "create",
    "ls": "ls",
    "delete": "delete",
    "frames": "frames",
    "dump": "export-dataset",
    "upload": "import-dataset",
    "export": "backup",
    "import": "create-from-backup",
    "auto-annotate": "auto-annotate",
}

for _legacy, _new in _legacy_mapping.items():
    COMMANDS.add_command(_legacy, DeprecatedAlias(COMMANDS_TASKS.commands[_new], f"task {_new}"))


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\commands_functions.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import json
import textwrap
from collections.abc import Sequence
from typing import Union

import cvat_sdk.auto_annotation as cvataa
from cvat_sdk import Client, models

from .agent import FUNCTION_KIND_DETECTOR, FUNCTION_PROVIDER_NATIVE, run_agent
from .command_base import CommandGroup
from .common import FunctionLoader, configure_function_implementation_arguments

COMMANDS = CommandGroup(description="Perform operations on CVAT lambda functions.")


@COMMANDS.command_class("create-native")
class FunctionCreateNative:
    description = textwrap.dedent(
        """\
        Create a CVAT function that can be powered by an agent running the given local function.
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "name",
            help="a human-readable name for the function",
        )

        configure_function_implementation_arguments(parser)

    @staticmethod
    def _dump_sublabel_spec(
        sl_spec: Union[models.SublabelRequest, models.PatchedLabelRequest],
    ) -> dict:
        result = {
            "name": sl_spec.name,
            "attributes": [
                {
                    "name": attribute_spec.name,
                    "input_type": attribute_spec.input_type,
                    "values": attribute_spec.values,
                }
                for attribute_spec in getattr(sl_spec, "attributes", [])
            ],
        }

        if getattr(sl_spec, "type", "any") != "any":
            # Add the type conditionally, to stay compatible with older
            # CVAT versions when the function doesn't define label types.
            result["type"] = sl_spec.type

        return result

    def execute(
        self,
        client: Client,
        *,
        name: str,
        function_loader: FunctionLoader,
    ) -> None:
        function = function_loader.load()

        remote_function = {
            "provider": FUNCTION_PROVIDER_NATIVE,
            "name": name,
        }

        if isinstance(function.spec, cvataa.DetectionFunctionSpec):
            remote_function["kind"] = FUNCTION_KIND_DETECTOR
            remote_function["labels_v2"] = []

            for label_spec in function.spec.labels:
                remote_function["labels_v2"].append(self._dump_sublabel_spec(label_spec))

                if sublabels := getattr(label_spec, "sublabels", None):
                    remote_function["labels_v2"][-1]["sublabels"] = [
                        self._dump_sublabel_spec(sublabel) for sublabel in sublabels
                    ]
        else:
            raise cvataa.BadFunctionError(
                f"Unsupported function spec type: {type(function.spec).__name__}"
            )

        _, response = client.api_client.call_api(
            "/api/functions",
            "POST",
            body=remote_function,
        )

        remote_function = json.loads(response.data)

        client.logger.info(
            "Created function #%d: %s", remote_function["id"], remote_function["name"]
        )
        print(remote_function["id"])


@COMMANDS.command_class("delete")
class FunctionDelete:
    description = "Delete a list of functions, ignoring those which don't exist."

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("function_ids", type=int, help="IDs of functions to delete", nargs="+")

    def execute(self, client: Client, *, function_ids: Sequence[int]) -> None:
        for function_id in function_ids:
            _, response = client.api_client.call_api(
                "/api/functions/{function_id}",
                "DELETE",
                path_params={"function_id": function_id},
                _check_status=False,
            )

            if 200 <= response.status <= 299:
                client.logger.info(f"Function #{function_id} deleted")
            elif response.status == 404:
                client.logger.warning(f"Function #{function_id} not found")
            else:
                client.logger.error(
                    f"Failed to delete function #{function_id}: "
                    f"{response.msg} (status {response.status})"
                )


@COMMANDS.command_class("run-agent")
class FunctionRunAgent:
    description = "Process requests for a given native function, indefinitely."

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "function_id",
            type=int,
            help="ID of the function to process requests for",
        )

        configure_function_implementation_arguments(parser)

        parser.add_argument(
            "--burst",
            action="store_true",
            help="process all pending requests and then exit",
        )

    def execute(
        self,
        client: Client,
        *,
        function_id: int,
        function_loader: FunctionLoader,
        burst: bool,
    ) -> None:
        run_agent(client, function_loader, function_id, burst=burst)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\commands_projects.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import textwrap

from cvat_sdk import Client, models

from .command_base import CommandGroup, GenericCommand, GenericDeleteCommand, GenericListCommand
from .parsers import parse_label_arg

COMMANDS = CommandGroup(description="Perform operations on CVAT projects.")


class GenericProjectCommand(GenericCommand):
    resource_type_str = "project"

    def repo(self, client: Client):
        return client.projects


@COMMANDS.command_class("ls")
class ProjectList(GenericListCommand, GenericProjectCommand):
    pass


@COMMANDS.command_class("create")
class ProjectCreate:
    description = textwrap.dedent(
        """\
        Create a new CVAT project, optionally importing a dataset.
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("name", type=str, help="name of the project")
        parser.add_argument(
            "--bug_tracker", "--bug", default=argparse.SUPPRESS, type=str, help="bug tracker URL"
        )
        parser.add_argument(
            "--labels",
            default=[],
            type=parse_label_arg,
            help="string or file containing JSON labels specification (default: %(default)s)",
        )
        parser.add_argument(
            "--dataset_path",
            default="",
            type=str,
            help="path to the dataset file to import",
        )
        parser.add_argument(
            "--dataset_format",
            default="CVAT 1.1",
            type=str,
            help="format of the dataset file being uploaded"
            " (only applies when --dataset_path is specified; default: %(default)s)",
        )
        parser.add_argument(
            "--completion_verification_period",
            dest="status_check_period",
            default=2,
            type=float,
            help="period between status checks"
            " (only applies when --dataset_path is specified; default: %(default)s)",
        )

    def execute(
        self,
        client: Client,
        *,
        name: str,
        labels: dict,
        dataset_path: str,
        dataset_format: str,
        status_check_period: int,
        **kwargs,
    ) -> None:
        project = client.projects.create_from_dataset(
            spec=models.ProjectWriteRequest(name=name, labels=labels, **kwargs),
            dataset_path=dataset_path,
            dataset_format=dataset_format,
            status_check_period=status_check_period,
        )
        print(project.id)


@COMMANDS.command_class("delete")
class ProjectDelete(GenericDeleteCommand, GenericProjectCommand):
    pass


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\commands_tasks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import argparse
import textwrap
from collections.abc import Sequence
from typing import Optional

import cvat_sdk.auto_annotation as cvataa
from attr.converters import to_bool
from cvat_sdk import Client, models
from cvat_sdk.core.helpers import DeferredTqdmProgressReporter
from cvat_sdk.core.proxies.tasks import ResourceType

from .command_base import CommandGroup, GenericCommand, GenericDeleteCommand, GenericListCommand
from .common import FunctionLoader, configure_function_implementation_arguments
from .parsers import parse_label_arg, parse_resource_type, parse_threshold

COMMANDS = CommandGroup(description="Perform operations on CVAT tasks.")


class GenericTaskCommand(GenericCommand):
    resource_type_str = "task"

    def repo(self, client: Client):
        return client.tasks


@COMMANDS.command_class("ls")
class TaskList(GenericListCommand, GenericTaskCommand):
    pass


@COMMANDS.command_class("create")
class TaskCreate:
    description = textwrap.dedent(
        """\
        Create a new CVAT task. To create a task, you need
        to specify labels using the --labels argument or
        attach the task to an existing project using the
        --project_id argument.
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("name", type=str, help="name of the task")
        parser.add_argument(
            "resource_type",
            default="local",
            choices=list(ResourceType),
            type=parse_resource_type,
            help="type of files specified",
        )
        parser.add_argument("resources", type=str, help="list of paths or URLs", nargs="+")
        parser.add_argument(
            "--annotation_path", default="", type=str, help="path to annotation file"
        )
        parser.add_argument(
            "--annotation_format",
            default="CVAT 1.1",
            type=str,
            help="format of the annotation file being uploaded, e.g. CVAT 1.1",
        )
        parser.add_argument(
            "--bug_tracker", "--bug", default=argparse.SUPPRESS, type=str, help="bug tracker URL"
        )
        parser.add_argument(
            "--chunk_size",
            default=argparse.SUPPRESS,
            type=int,
            help="the number of frames per chunk",
        )
        parser.add_argument(
            "--completion_verification_period",
            dest="status_check_period",
            default=2,
            type=float,
            help=textwrap.dedent(
                """\
                number of seconds to wait until checking
                if data compression finished (necessary before uploading annotations)
                """
            ),
        )
        parser.add_argument(
            "--copy_data",
            default=False,
            action="store_true",
            help=textwrap.dedent(
                """\
                set the option to copy the data, only used when resource type is
                share (default: %(default)s)
                """
            ),
        )
        parser.add_argument(
            "--frame_step",
            default=argparse.SUPPRESS,
            type=int,
            help=textwrap.dedent(
                """\
                set the frame step option in the advanced configuration
                when uploading image series or videos
                """
            ),
        )
        parser.add_argument(
            "--image_quality",
            default=70,
            type=int,
            help=textwrap.dedent(
                """\
                set the image quality option in the advanced configuration
                when creating tasks.(default: %(default)s)
                """
            ),
        )
        parser.add_argument(
            "--labels",
            default="[]",
            type=parse_label_arg,
            help="string or file containing JSON labels specification",
        )
        parser.add_argument(
            "--project_id", default=argparse.SUPPRESS, type=int, help="project ID if project exists"
        )
        parser.add_argument(
            "--overlap",
            default=argparse.SUPPRESS,
            type=int,
            help="the number of intersected frames between different segments",
        )
        parser.add_argument(
            "--segment_size",
            default=argparse.SUPPRESS,
            type=int,
            help="the number of frames in a segment",
        )
        parser.add_argument(
            "--sorting-method",
            default="lexicographical",
            choices=["lexicographical", "natural", "predefined", "random"],
            help="""data soring method (default: %(default)s)""",
        )
        parser.add_argument(
            "--start_frame",
            default=argparse.SUPPRESS,
            type=int,
            help="the start frame of the video",
        )
        parser.add_argument(
            "--stop_frame", default=argparse.SUPPRESS, type=int, help="the stop frame of the video"
        )
        parser.add_argument(
            "--use_cache",
            action="store_true",
            help="""use cache""",  # automatically sets default=False
        )
        parser.add_argument(
            "--use_zip_chunks",
            action="store_true",  # automatically sets default=False
            help="""zip chunks before sending them to the server""",
        )
        parser.add_argument(
            "--cloud_storage_id",
            default=argparse.SUPPRESS,
            type=int,
            help="cloud storage ID if you would like to use data from cloud storage",
        )
        parser.add_argument(
            "--filename_pattern",
            default=argparse.SUPPRESS,
            type=str,
            help=textwrap.dedent(
                """\
                pattern for filtering data from the manifest file for the upload.
                Only shell-style wildcards are supported:
                * - matches everything;
                ? - matches any single character;
                [seq] - matches any character in 'seq';
                [!seq] - matches any character not in seq
                """
            ),
        )

    def execute(
        self,
        client,
        *,
        name: str,
        labels: list[dict[str, str]],
        resources: Sequence[str],
        resource_type: ResourceType,
        annotation_path: str,
        annotation_format: str,
        status_check_period: int,
        **kwargs,
    ) -> None:
        task_params = {}
        data_params = {}

        for k, v in kwargs.items():
            if k in models.DataRequest.attribute_map or k == "frame_step":
                data_params[k] = v
            else:
                task_params[k] = v

        task = client.tasks.create_from_data(
            spec=models.TaskWriteRequest(name=name, labels=labels, **task_params),
            resource_type=resource_type,
            resources=resources,
            data_params=data_params,
            annotation_path=annotation_path,
            annotation_format=annotation_format,
            status_check_period=status_check_period,
            pbar=DeferredTqdmProgressReporter(),
        )
        print(task.id)


@COMMANDS.command_class("delete")
class TaskDelete(GenericDeleteCommand, GenericTaskCommand):
    pass


@COMMANDS.command_class("frames")
class TaskFrames:
    description = textwrap.dedent(
        """\
        Download the requested frame numbers for a task and save images as
        task_<ID>_frame_<FRAME>.jpg.
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("task_id", type=int, help="task ID")
        parser.add_argument("frame_ids", type=int, help="list of frame IDs to download", nargs="+")
        parser.add_argument(
            "--outdir", type=str, default="", help="directory to save images (default: CWD)"
        )
        parser.add_argument(
            "--quality",
            type=str,
            choices=("original", "compressed"),
            default="original",
            help="choose quality of images (default: %(default)s)",
        )

    def execute(
        self,
        client: Client,
        *,
        task_id: int,
        frame_ids: Sequence[int],
        outdir: str,
        quality: str,
    ) -> None:
        client.tasks.retrieve(obj_id=task_id).download_frames(
            frame_ids=frame_ids,
            outdir=outdir,
            quality=quality,
            filename_pattern=f"task_{task_id}" + "_frame_{frame_id:06d}{frame_ext}",
        )


@COMMANDS.command_class("export-dataset")
class TaskExportDataset:
    description = textwrap.dedent(
        """\
        Export a task as a dataset in the specified format (e.g. 'YOLO 1.1').
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("task_id", type=int, help="task ID")
        parser.add_argument("filename", type=str, help="output file")
        parser.add_argument(
            "--format",
            dest="fileformat",
            type=str,
            default="CVAT for images 1.1",
            help="annotation format (default: %(default)s)",
        )
        parser.add_argument(
            "--completion_verification_period",
            dest="status_check_period",
            default=2,
            type=float,
            help="number of seconds to wait until checking if dataset building finished",
        )
        parser.add_argument(
            "--with-images",
            type=to_bool,
            default=False,
            dest="include_images",
            help="Whether to include images or not (default: %(default)s)",
        )

    def execute(
        self,
        client: Client,
        *,
        task_id: int,
        fileformat: str,
        filename: str,
        status_check_period: int,
        include_images: bool,
    ) -> None:
        client.tasks.retrieve(obj_id=task_id).export_dataset(
            format_name=fileformat,
            filename=filename,
            pbar=DeferredTqdmProgressReporter(),
            status_check_period=status_check_period,
            include_images=include_images,
        )


@COMMANDS.command_class("import-dataset")
class TaskImportDataset:
    description = textwrap.dedent(
        """\
        Import annotations into a task from a dataset in the specified format
        (e.g. 'YOLO 1.1').
        """
    )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("task_id", type=int, help="task ID")
        parser.add_argument("filename", type=str, help="upload file")
        parser.add_argument(
            "--format",
            dest="fileformat",
            type=str,
            default="CVAT 1.1",
            help="annotation format (default: %(default)s)",
        )

    def execute(
        self,
        client: Client,
        *,
        task_id: int,
        fileformat: str,
        filename: str,
    ) -> None:
        client.tasks.retrieve(obj_id=task_id).import_annotations(
            format_name=fileformat,
            filename=filename,
            pbar=DeferredTqdmProgressReporter(),
        )


@COMMANDS.command_class("backup")
class TaskBackup:
    description = """Download a task backup."""

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("task_id", type=int, help="task ID")
        parser.add_argument("filename", type=str, help="output file")
        parser.add_argument(
            "--completion_verification_period",
            dest="status_check_period",
            default=2,
            type=float,
            help="time interval between checks if archive building has been finished, in seconds",
        )

    def execute(
        self, client: Client, *, task_id: int, filename: str, status_check_period: int
    ) -> None:
        client.tasks.retrieve(obj_id=task_id).download_backup(
            filename=filename,
            status_check_period=status_check_period,
            pbar=DeferredTqdmProgressReporter(),
        )


@COMMANDS.command_class("create-from-backup")
class TaskCreateFromBackup:
    description = """Create a task from a backup file."""

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("filename", type=str, help="upload file")
        parser.add_argument(
            "--completion_verification_period",
            dest="status_check_period",
            default=2,
            type=float,
            help="time interval between checks if archive processing was finished, in seconds",
        )

    def execute(self, client: Client, *, filename: str, status_check_period: int) -> None:
        task = client.tasks.create_from_backup(
            filename=filename,
            status_check_period=status_check_period,
            pbar=DeferredTqdmProgressReporter(),
        )
        print(task.id)


@COMMANDS.command_class("auto-annotate")
class TaskAutoAnnotate:
    description = "Automatically annotate a CVAT task by running a function on the local machine."

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument("task_id", type=int, help="task ID")

        configure_function_implementation_arguments(parser)

        parser.add_argument(
            "--clear-existing",
            action="store_true",
            help="Remove existing annotations from the task",
        )

        parser.add_argument(
            "--allow-unmatched-labels",
            action="store_true",
            help="Allow the function to declare labels/sublabels/attributes not configured in the task",
        )

        parser.add_argument(
            "--conf-threshold",
            type=parse_threshold,
            help="Confidence threshold for filtering detections",
            default=None,
        )

        parser.add_argument(
            "--conv-mask-to-poly",
            action="store_true",
            help="Convert mask shapes to polygon shapes",
        )

    def execute(
        self,
        client: Client,
        *,
        task_id: int,
        function_loader: FunctionLoader,
        clear_existing: bool = False,
        allow_unmatched_labels: bool = False,
        conf_threshold: Optional[float],
        conv_mask_to_poly: bool,
    ) -> None:
        function = function_loader.load()

        cvataa.annotate_task(
            client,
            task_id,
            function,
            pbar=DeferredTqdmProgressReporter(),
            clear_existing=clear_existing,
            allow_unmatched_labels=allow_unmatched_labels,
            conf_threshold=conf_threshold,
            conv_mask_to_poly=conv_mask_to_poly,
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\command_base.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import json
import textwrap
import types
from abc import ABCMeta, abstractmethod
from collections.abc import Mapping, Sequence
from typing import Callable, Protocol

from cvat_sdk import Client


class Command(Protocol):
    @property
    def description(self) -> str: ...

    def configure_parser(self, parser: argparse.ArgumentParser) -> None: ...

    # The exact parameters accepted by `execute` vary between commands,
    # so we're forced to declare it like this instead of as a method.
    @property
    def execute(self) -> Callable[..., None]: ...


class CommandGroup:
    def __init__(self, *, description: str) -> None:
        self._commands: dict[str, Command] = {}
        self.description = description

    def command_class(self, name: str):
        def decorator(cls: type):
            self._commands[name] = cls()
            return cls

        return decorator

    def add_command(self, name: str, command: Command) -> None:
        self._commands[name] = command

    @property
    def commands(self) -> Mapping[str, Command]:
        return types.MappingProxyType(self._commands)

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        subparsers = parser.add_subparsers(required=True)

        for name, command in self._commands.items():
            subparser = subparsers.add_parser(name, description=command.description)
            subparser.set_defaults(_executor=command.execute)
            command.configure_parser(subparser)

    def execute(self) -> None:
        # It should be impossible for a command group to be executed,
        # because configure_parser requires that a subcommand is specified.
        assert False, "unreachable code"


class DeprecatedAlias:
    def __init__(self, command: Command, replacement: str) -> None:
        self._command = command
        self._replacement = replacement

    @property
    def description(self) -> str:
        return textwrap.dedent(
            f"""\
            {self._command.description}
            (Deprecated; use "{self._replacement}" instead.)
            """
        )

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        self._command.configure_parser(parser)

    def execute(self, client: Client, **kwargs) -> None:
        client.logger.warning('This command is deprecated. Use "%s" instead.', self._replacement)
        self._command.execute(client, **kwargs)


class GenericCommand(metaclass=ABCMeta):
    @abstractmethod
    def repo(self, client: Client): ...

    @property
    @abstractmethod
    def resource_type_str(self) -> str: ...


class GenericListCommand(GenericCommand):
    @property
    def description(self) -> str:
        return f"List all CVAT {self.resource_type_str}s in either basic or JSON format."

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "--json",
            dest="use_json_output",
            default=False,
            action="store_true",
            help="output JSON data",
        )

    def execute(self, client: Client, *, use_json_output: bool = False):
        results = self.repo(client).list(return_json=use_json_output)
        if use_json_output:
            print(json.dumps(json.loads(results), indent=2))
        else:
            for r in results:
                print(r.id)


class GenericDeleteCommand(GenericCommand):
    @property
    def description(self):
        return f"Delete a list of {self.resource_type_str}s, ignoring those which don't exist."

    def configure_parser(self, parser: argparse.ArgumentParser) -> None:
        parser.add_argument(
            "ids", type=int, help=f"list of {self.resource_type_str} IDs", nargs="+"
        )

    def execute(self, client: Client, *, ids: Sequence[int]) -> None:
        self.repo(client).remove_by_ids(ids)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\common.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import getpass
import importlib
import importlib.util
import logging
import os
import sys
from http.client import HTTPConnection
from pathlib import Path
from typing import Any, Optional

import attrs
import cvat_sdk.auto_annotation as cvataa
from cvat_sdk.core.client import Client, Config

from ..version import VERSION
from .parsers import BuildDictAction, parse_function_parameter
from .utils import popattr


class CriticalError(Exception):
    pass


def get_auth(s):
    """Parse USER[:PASS] strings and prompt for password if none was
    supplied."""
    user, _, password = s.partition(":")
    password = password or os.environ.get("PASS") or getpass.getpass()
    return user, password


def configure_common_arguments(parser: argparse.ArgumentParser) -> None:
    parser.add_argument("--version", action="version", version=VERSION)
    parser.add_argument(
        "--insecure",
        action="store_true",
        help="Allows to disable SSL certificate check",
    )

    parser.add_argument(
        "--auth",
        type=get_auth,
        metavar="USER:[PASS]",
        default=getpass.getuser(),
        help="""defaults to the current user and supports the PASS
                environment variable or password prompt
                (default user: %(default)s).""",
    )
    parser.add_argument(
        "--server-host", type=str, default="localhost", help="host (default: %(default)s)"
    )
    parser.add_argument(
        "--server-port",
        type=int,
        default=None,
        help="port (default: 80 for http and 443 for https connections)",
    )
    parser.add_argument(
        "--organization",
        "--org",
        metavar="SLUG",
        help="""short name (slug) of the organization
                to use when listing or creating resources;
                set to blank string to use the personal workspace
                (default: list all accessible objects, create in personal workspace)""",
    )
    parser.add_argument(
        "--debug",
        action="store_const",
        dest="loglevel",
        const=logging.DEBUG,
        default=logging.INFO,
        help="show debug output",
    )


def configure_logger(logger: logging.Logger, parsed_args: argparse.Namespace) -> None:
    level = popattr(parsed_args, "loglevel")
    formatter = logging.Formatter(
        "[%(asctime)s] %(levelname)s: %(message)s", datefmt="%Y-%m-%d %H:%M:%S", style="%"
    )
    handler = logging.StreamHandler(sys.stderr)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(level)
    if level <= logging.DEBUG:
        HTTPConnection.debuglevel = 1


def build_client(parsed_args: argparse.Namespace, logger: logging.Logger) -> Client:
    config = Config(verify_ssl=not popattr(parsed_args, "insecure"))

    url = popattr(parsed_args, "server_host")
    if server_port := popattr(parsed_args, "server_port"):
        url += f":{server_port}"

    client = Client(
        url=url,
        logger=logger,
        config=config,
        check_server_version=False,  # version is checked after auth to support versions < 2.3
    )

    client.login(popattr(parsed_args, "auth"))
    client.check_server_version(fail_if_unsupported=False)

    client.organization_slug = popattr(parsed_args, "organization")

    return client


def configure_function_implementation_arguments(parser: argparse.ArgumentParser) -> None:
    function_group = parser.add_mutually_exclusive_group(required=True)

    function_group.add_argument(
        "--function-module",
        metavar="MODULE",
        help="qualified name of a module to use as the function",
    )

    function_group.add_argument(
        "--function-file",
        metavar="PATH",
        type=Path,
        help="path to a Python source file to use as the function",
    )

    parser.add_argument(
        "--function-parameter",
        "-p",
        metavar="NAME=TYPE:VALUE",
        type=parse_function_parameter,
        action=BuildDictAction,
        dest="function_parameters",
        help="parameter for the function",
    )

    original_executor = parser.get_default("_executor")

    def execute_with_function_loader(
        client,
        *,
        function_module: Optional[str],
        function_file: Optional[Path],
        function_parameters: dict[str, Any],
        **kwargs,
    ):
        original_executor(
            client,
            function_loader=FunctionLoader(function_module, function_file, function_parameters),
            **kwargs,
        )

    parser.set_defaults(_executor=execute_with_function_loader)


@attrs.frozen
class FunctionLoader:
    function_module: Optional[str]
    function_file: Optional[Path]
    function_parameters: dict[str, Any]

    def __attrs_post_init__(self):
        assert self.function_module is not None or self.function_file is not None

    def load(self) -> cvataa.DetectionFunction:
        if self.function_module is not None:
            function = importlib.import_module(self.function_module)
        else:
            module_spec = importlib.util.spec_from_file_location(
                "__cvat_function__", self.function_file
            )
            function = importlib.util.module_from_spec(module_spec)
            module_spec.loader.exec_module(function)

        if hasattr(function, "create"):
            # this is actually a function factory
            function = function.create(**self.function_parameters)
        else:
            if self.function_parameters:
                raise TypeError("function takes no parameters")

        return function


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\parsers.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import json
import os.path
from typing import Any

from attr.converters import to_bool
from cvat_sdk.core.proxies.tasks import ResourceType


def parse_resource_type(s: str) -> ResourceType:
    try:
        return ResourceType[s.upper()]
    except KeyError:
        return s


def parse_label_arg(s):
    """If s is a file load it as JSON, otherwise parse s as JSON."""
    if os.path.exists(s):
        with open(s, "r") as fp:
            return json.load(fp)
    else:
        return json.loads(s)


def parse_function_parameter(s: str) -> tuple[str, Any]:
    key, sep, type_and_value = s.partition("=")

    if not sep:
        raise argparse.ArgumentTypeError("parameter value not specified")

    type_, sep, value = type_and_value.partition(":")

    if not sep:
        raise argparse.ArgumentTypeError("parameter type not specified")

    if type_ == "int":
        value = int(value)
    elif type_ == "float":
        value = float(value)
    elif type_ == "str":
        pass
    elif type_ == "bool":
        value = to_bool(value)
    else:
        raise argparse.ArgumentTypeError(f"unsupported parameter type {type_!r}")

    return (key, value)


def parse_threshold(s: str) -> float:
    try:
        value = float(s)
    except ValueError as e:
        raise argparse.ArgumentTypeError("must be a number") from e

    if not 0 <= value <= 1:
        raise argparse.ArgumentTypeError("must be between 0 and 1")
    return value


class BuildDictAction(argparse.Action):
    def __init__(self, option_strings, dest, default=None, **kwargs):
        super().__init__(option_strings, dest, default=default or {}, **kwargs)

    def __call__(self, parser, namespace, values, option_string=None):
        key, value = values
        getattr(namespace, self.dest)[key] = value


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


def popattr(obj, name):
    value = getattr(obj, name)
    delattr(obj, name)
    return value


# ===== 文件: D:\wow_ai\docker\cvat\cvat-cli\src\cvat_cli\_internal\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\attributes.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from collections.abc import Mapping
from typing import Callable, Union

from . import models


class _CheckboxAttributeValueValidator:
    def __init__(self, values: list[str]) -> None:
        pass

    def __call__(self, value: str) -> bool:
        return value in {"true", "false"}


class _NumberAttributeValueValidator:
    def __init__(self, values: list[str]) -> None:
        if len(values) != 3:
            raise ValueError(f"wrong number of values: expected 3, got {len(values)}")

        try:
            (self._min_value, self._max_value, self._step) = map(int, values)
        except ValueError as ex:
            raise ValueError(f"values could not be converted to integers") from ex

        try:
            number_attribute_values(self._min_value, self._max_value, self._step)
        except ValueError as ex:
            raise ValueError(f"invalid values: {ex}") from ex

    def __call__(self, value: str) -> bool:
        try:
            value = int(value)
        except ValueError:
            return False

        return (
            self._min_value <= value <= self._max_value
            and (value - self._min_value) % self._step == 0
        )


class _SelectAttributeValueValidator:
    def __init__(self, values: list[str]) -> None:
        if len(values) == 0:
            raise ValueError("empty list of allowed values")

        self._values = frozenset(values)

    def __call__(self, value: str) -> bool:
        return value in self._values


class _TextAttributeValueValidator:
    def __init__(self, values: list[str]) -> None:
        pass

    def __call__(self, value: str) -> bool:
        return True


_VALIDATOR_CLASSES = {
    "checkbox": _CheckboxAttributeValueValidator,
    "number": _NumberAttributeValueValidator,
    "radio": _SelectAttributeValueValidator,
    "select": _SelectAttributeValueValidator,
    "text": _TextAttributeValueValidator,
}

# make sure all possible types are covered
assert set(models.InputTypeEnum.allowed_values[("value",)].values()) == _VALIDATOR_CLASSES.keys()


def attribute_value_validator(spec: models.IAttributeRequest) -> Callable[[str], bool]:
    """
    Returns a callable that can be used to verify
    whether an attribute value is suitable for an attribute with the given spec.
    The resulting callable takes a single argument (the attribute value as a string)
    and returns True if and only if the value is suitable.

    The spec's `values` attribute must be consistent with its `input_type` attribute,
    otherwise ValueError will be raised.
    """
    return _VALIDATOR_CLASSES[spec.input_type.value](spec.values)


def number_attribute_values(min_value: int, max_value: int, /, step: int = 1) -> list[str]:
    """
    Returns a list suitable as the value of the "values" field of an `AttributeRequest`
    with `input_type="number"`.
    """

    if min_value > max_value:
        raise ValueError("min_value must be less than or equal to max_value")

    if step <= 0:
        raise ValueError("step must be positive")

    if (max_value - min_value) % step != 0:
        raise ValueError("step must be a divisor of max_value - min_value")

    return [str(min_value), str(max_value), str(step)]


def attribute_vals_from_dict(
    id_to_value: Mapping[int, Union[str, int, bool]], /
) -> list[models.AttributeValRequest]:
    """
    Returns a list of AttributeValRequest objects with given IDs and values.

    The input value must be a mapping from attribute spec IDs to corresponding values.
    A value may be specified as a string, an integer, or a boolean.
    Integers and booleans will be converted to strings according to the format CVAT expects
    for attributes with input type "number" and "checkbox", respectively.
    """

    def val_as_string(v: Union[str, int, bool]) -> str:
        if v is True:
            return "true"
        if v is False:
            return "false"
        if isinstance(v, int):
            return str(v)
        if isinstance(v, str):
            return v
        assert False, f"unexpected value {v!r} of type {type(v)}"

    return [
        models.AttributeValRequest(spec_id=k, value=val_as_string(v))
        for k, v in id_to_value.items()
    ]


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\exceptions.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# pylint: disable=unused-import
from cvat_sdk.api_client.exceptions import (
    ApiAttributeError,
    ApiException,
    ApiKeyError,
    ApiTypeError,
    ApiValueError,
    OpenApiException,
)
from cvat_sdk.core.exceptions import CvatSdkException


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\masks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
from collections.abc import Sequence

import numpy as np
from numpy.typing import ArrayLike


def encode_mask(bitmap: ArrayLike, /, bbox: Sequence[float]) -> list[float]:
    """
    Encodes an image mask into an array of numbers suitable for the "points"
    attribute of a LabeledShapeRequest object of type "mask".

    bitmap must be a boolean array of shape (H, W), where H is the height and
    W is the width of the image that the mask applies to.

    bbox must have the form [x1, y1, x2, y2], where (0, 0) <= (x1, y1) < (x2, y2) <= (W, H).
    The mask will be limited to points between (x1, y1) and (x2, y2).
    """

    bitmap = np.asanyarray(bitmap)
    if bitmap.ndim != 2:
        raise ValueError("bitmap must have 2 dimensions")
    if bitmap.dtype != np.bool_:
        raise ValueError("bitmap must have boolean items")

    x1, y1 = map(math.floor, bbox[0:2])
    x2, y2 = map(math.ceil, bbox[2:4])

    if not (0 <= x1 < x2 <= bitmap.shape[1] and 0 <= y1 < y2 <= bitmap.shape[0]):
        raise ValueError("bbox has invalid coordinates")

    flat = bitmap[y1:y2, x1:x2].ravel()

    (run_indices,) = np.diff(flat, prepend=[not flat[0]], append=[not flat[-1]]).nonzero()
    if flat[0]:
        run_lengths = np.diff(run_indices, prepend=[0])
    else:
        run_lengths = np.diff(run_indices)

    return run_lengths.tolist() + [x1, y1, x2 - 1, y2 - 1]


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\models.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Reexport symbols for public SDK API
from cvat_sdk.api_client.models import *  # pylint: disable=wildcard-import


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from cvat_sdk.core.client import Client, Config, make_client
from cvat_sdk.version import VERSION as __version__


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\driver.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import logging
from collections.abc import Mapping, Sequence
from typing import Callable, Optional, Union

import attrs
from typing_extensions import TypeAlias

import cvat_sdk.models as models
from cvat_sdk.core import Client
from cvat_sdk.core.progress import NullProgressReporter, ProgressReporter
from cvat_sdk.datasets.task_dataset import TaskDataset

from ..attributes import attribute_value_validator
from .exceptions import BadFunctionError
from .interface import DetectionFunction, DetectionFunctionContext, DetectionFunctionSpec


@attrs.frozen
class _AttributeNameMapping:
    name: str


@attrs.frozen
class _SublabelNameMapping:
    name: str
    attributes: Optional[Mapping[str, _AttributeNameMapping]] = attrs.field(
        kw_only=True, default=None
    )

    def map_attribute(self, name: str) -> Optional[_AttributeNameMapping]:
        if self.attributes is None:
            return _AttributeNameMapping(name)

        return self.attributes.get(name)

    @classmethod
    def from_api(cls, raw: models.SublabelMappingEntryRequest, /) -> _SublabelNameMapping:
        return _SublabelNameMapping(
            name=raw.name,
            attributes=(
                {k: _AttributeNameMapping(v) for k, v in raw.attributes.items()}
                if hasattr(raw, "attributes")
                else None
            ),
        )


@attrs.frozen
class _LabelNameMapping(_SublabelNameMapping):
    sublabels: Optional[Mapping[str, _SublabelNameMapping]] = attrs.field(
        kw_only=True, default=None
    )

    def map_sublabel(self, name: str) -> Optional[_SublabelNameMapping]:
        if self.sublabels is None:
            return _SublabelNameMapping(name)

        return self.sublabels.get(name)

    @classmethod
    def from_api(cls, raw: models.LabelMappingEntryRequest, /) -> _LabelNameMapping:
        return _LabelNameMapping(
            **attrs.asdict(_SublabelNameMapping.from_api(raw), recurse=False),
            sublabels=(
                {k: _SublabelNameMapping.from_api(v) for k, v in raw.sublabels.items()}
                if hasattr(raw, "sublabels")
                else None
            ),
        )


@attrs.frozen
class _SpecNameMapping:
    labels: Optional[Mapping[str, _LabelNameMapping]] = attrs.field(kw_only=True, default=None)

    def map_label(self, name: str) -> Optional[_LabelNameMapping]:
        if self.labels is None:
            return _LabelNameMapping(name)

        return self.labels.get(name)

    @classmethod
    def from_api(cls, raw: dict[str, models.LabelMappingEntryRequest], /) -> _SpecNameMapping:
        return cls(labels={k: _LabelNameMapping.from_api(v) for k, v in raw.items()})


class _AnnotationMapper:
    @attrs.frozen
    class _AttributeIdMapping:
        id: int
        value_validator: Callable[[str], bool]

    @attrs.frozen
    class _SublabelIdMapping:
        id: int
        attributes: Mapping[int, Optional[_AnnotationMapper._AttributeIdMapping]]

    @attrs.frozen
    class _LabelIdMapping(_SublabelIdMapping):
        sublabels: Mapping[int, Optional[_AnnotationMapper._SublabelIdMapping]]
        expected_num_elements: int
        expected_type: str

    _SpecIdMapping: TypeAlias = Mapping[int, Optional[_LabelIdMapping]]

    _spec_id_mapping: _SpecIdMapping

    def _get_expected_function_output_type(self, fun_label, ds_label):
        fun_output_type = getattr(fun_label, "type", "any")
        if fun_output_type == "any":
            return ds_label.type

        if self._conv_mask_to_poly and fun_output_type == "mask":
            fun_output_type = "polygon"

        if not self._are_label_types_compatible(fun_output_type, ds_label.type):
            raise BadFunctionError(
                f"label {fun_label.name!r} has type {fun_output_type!r} in the function,"
                f" but {ds_label.type!r} in the dataset"
            )
        return fun_output_type

    def _build_attribute_id_mapping(
        self, fun_attr: models.IAttribute, ds_attr: models.IAttribute, attr_desc: str
    ) -> _AttributeIdMapping:
        # We could potentially be more lax with these checks. For example, we could permit
        # fun_attr.values to be a subset of ds_attr.values. For simplicity though,
        # we'll just use exact comparisons for now.
        if ds_attr.input_type != fun_attr.input_type:
            raise BadFunctionError(
                f"{attr_desc} has input type {fun_attr.input_type!r} in the function,"
                f" but {ds_attr.input_type!r} in the dataset"
            )

        if ds_attr.input_type.value in {"text", "checkbox"}:
            values_match = True
        elif ds_attr.input_type.value in {"select", "radio"}:
            values_match = sorted(ds_attr.values) == sorted(fun_attr.values)
        else:
            values_match = ds_attr.values == fun_attr.values

        if not values_match:
            raise BadFunctionError(
                f"{attr_desc} has values {fun_attr.values!r} in the function,"
                f" but {ds_attr.values!r} in the dataset"
            )

        return self._AttributeIdMapping(
            id=ds_attr.id,
            value_validator=attribute_value_validator(fun_attr),
        )

    def _build_sublabel_id_mapping(
        self,
        fun_sl: models.ISublabel,
        ds_sl: models.ISublabel,
        sl_desc: str,
        *,
        sl_nm: _SublabelNameMapping,
        allow_unmatched_labels: bool,
    ) -> _SublabelIdMapping:
        ds_attrs_by_name = {ds_attr.name: ds_attr for ds_attr in ds_sl.attributes}

        def attribute_mapping(
            fun_attr: models.IAttribute,
        ) -> Optional[_AnnotationMapper._AttributeIdMapping]:
            attr_desc = f"attribute {fun_attr.name!r} of {sl_desc}"

            attr_nm = sl_nm.map_attribute(fun_attr.name)
            if attr_nm is None:
                return None

            ds_attr = ds_attrs_by_name.get(attr_nm.name)
            if not ds_attr:
                if not allow_unmatched_labels:
                    raise BadFunctionError(f"{attr_desc} is not in dataset")

                self._logger.info(
                    "%s is not in dataset; any annotations using it will be ignored", attr_desc
                )
                return None

            return self._build_attribute_id_mapping(fun_attr, ds_attr, attr_desc)

        return self._SublabelIdMapping(
            ds_sl.id,
            attributes={
                attr.id: attribute_mapping(attr) for attr in getattr(fun_sl, "attributes", [])
            },
        )

    def _build_label_id_mapping(
        self,
        fun_label: models.ILabel,
        ds_label: models.ILabel,
        label_desc: str,
        *,
        label_nm: _LabelNameMapping,
        allow_unmatched_labels: bool,
    ) -> _LabelIdMapping:
        base_mapping = self._build_sublabel_id_mapping(
            fun_label,
            ds_label,
            label_desc,
            sl_nm=label_nm,
            allow_unmatched_labels=allow_unmatched_labels,
        )

        ds_sublabels_by_name = {ds_sl.name: ds_sl for ds_sl in ds_label.sublabels}

        def sublabel_mapping(
            fun_sl: models.ISublabel,
        ) -> Optional[_AnnotationMapper._SublabelIdMapping]:
            sl_desc = f"sublabel {fun_sl.name!r} of {label_desc}"

            sublabel_nm = label_nm.map_sublabel(fun_sl.name)
            if sublabel_nm is None:
                return None

            ds_sl = ds_sublabels_by_name.get(sublabel_nm.name)
            if not ds_sl:
                if not allow_unmatched_labels:
                    raise BadFunctionError(f"{sl_desc} is not in dataset")

                self._logger.info(
                    "%s is not in dataset; any annotations using it will be ignored", sl_desc
                )
                return None

            return self._build_sublabel_id_mapping(
                fun_sl,
                ds_sl,
                sl_desc,
                sl_nm=sublabel_nm,
                allow_unmatched_labels=allow_unmatched_labels,
            )

        return self._LabelIdMapping(
            **attrs.asdict(base_mapping, recurse=False),
            sublabels={
                fun_sl.id: sublabel_mapping(fun_sl)
                for fun_sl in getattr(fun_label, "sublabels", [])
            },
            expected_num_elements=len(ds_label.sublabels),
            expected_type=self._get_expected_function_output_type(fun_label, ds_label),
        )

    def _build_spec_id_mapping(
        self,
        fun_labels: Sequence[models.ILabel],
        ds_labels: Sequence[models.ILabel],
        *,
        spec_nm: _SpecNameMapping,
        allow_unmatched_labels: bool,
    ) -> _SpecIdMapping:
        ds_labels_by_name = {ds_label.name: ds_label for ds_label in ds_labels}

        def label_id_mapping(
            fun_label: models.ILabel,
        ) -> Optional[_AnnotationMapper._LabelIdMapping]:
            label_desc = f"label {fun_label.name!r}"

            label_nm = spec_nm.map_label(fun_label.name)
            if label_nm is None:
                return None

            ds_label = ds_labels_by_name.get(label_nm.name)
            if ds_label is None:
                if not allow_unmatched_labels:
                    raise BadFunctionError(f"{label_desc} is not in dataset")

                self._logger.info(
                    "%s is not in dataset; any annotations using it will be ignored", label_desc
                )
                return None

            return self._build_label_id_mapping(
                fun_label,
                ds_label,
                label_desc,
                label_nm=label_nm,
                allow_unmatched_labels=allow_unmatched_labels,
            )

        return {fun_label.id: label_id_mapping(fun_label) for fun_label in fun_labels}

    def __init__(
        self,
        logger: logging.Logger,
        fun_labels: Sequence[models.ILabel],
        ds_labels: Sequence[models.ILabel],
        *,
        allow_unmatched_labels: bool,
        conv_mask_to_poly: bool,
        spec_nm: _SpecNameMapping = _SpecNameMapping(),
    ) -> None:
        self._logger = logger
        self._conv_mask_to_poly = conv_mask_to_poly

        self._spec_id_mapping = self._build_spec_id_mapping(
            fun_labels, ds_labels, spec_nm=spec_nm, allow_unmatched_labels=allow_unmatched_labels
        )

    def _remap_attribute(
        self,
        attribute: models.AttributeValRequest,
        label_id_mapping: _SublabelIdMapping,
        seen_attr_ids: set[int],
    ) -> bool:
        try:
            attr_id_mapping = label_id_mapping.attributes[attribute.spec_id]
        except KeyError:
            raise BadFunctionError(
                f"function output attribute with unknown ID ({attribute.spec_id})"
            )

        if not attr_id_mapping:
            return False

        if attr_id_mapping.id in seen_attr_ids:
            raise BadFunctionError("function output shape with multiple attributes with same ID")

        if not attr_id_mapping.value_validator(attribute.value):
            raise BadFunctionError(
                f"function output attribute value ({attribute.value!r})"
                f" that is unsuitable for its attribute ({attribute.spec_id})"
            )

        attribute.spec_id = attr_id_mapping.id

        seen_attr_ids.add(attr_id_mapping.id)

        return True

    def _remap_attributes(
        self,
        shape: Union[models.LabeledShapeRequest, models.SubLabeledShapeRequest],
        label_id_mapping: _SublabelIdMapping,
    ) -> None:
        seen_attr_ids = set()

        if hasattr(shape, "attributes"):
            shape.attributes[:] = [
                attribute
                for attribute in shape.attributes
                if self._remap_attribute(attribute, label_id_mapping, seen_attr_ids)
            ]

    def _remap_element(
        self,
        element: models.SubLabeledShapeRequest,
        ds_frame: int,
        label_id_mapping: _LabelIdMapping,
        seen_sl_ids: set[int],
    ) -> bool:
        if hasattr(element, "id"):
            raise BadFunctionError("function output shape element with preset id")

        if hasattr(element, "source"):
            raise BadFunctionError("function output shape element with preset source")
        element.source = "auto"

        if element.frame != 0:
            raise BadFunctionError(
                f"function output shape element with unexpected frame number ({element.frame})"
            )

        element.frame = ds_frame

        if element.type.value != "points":
            raise BadFunctionError(
                f"function output skeleton with element type other than 'points' ({element.type.value})"
            )

        try:
            sl_id_mapping = label_id_mapping.sublabels[element.label_id]
        except KeyError:
            raise BadFunctionError(
                f"function output shape with unknown sublabel ID ({element.label_id})"
            )

        if not sl_id_mapping:
            return False

        if sl_id_mapping.id in seen_sl_ids:
            raise BadFunctionError(
                "function output skeleton with multiple elements with same sublabel"
            )

        element.label_id = sl_id_mapping.id

        seen_sl_ids.add(sl_id_mapping.id)

        self._remap_attributes(element, sl_id_mapping)

        return True

    def _remap_elements(
        self, shape: models.LabeledShapeRequest, ds_frame: int, label_id_mapping: _LabelIdMapping
    ) -> None:
        if shape.type.value == "skeleton":
            seen_sl_ids = set()

            shape.elements[:] = [
                element
                for element in shape.elements
                if self._remap_element(element, ds_frame, label_id_mapping, seen_sl_ids)
            ]

            if len(shape.elements) != label_id_mapping.expected_num_elements:
                # There could only be fewer elements than expected,
                # because the reverse would imply that there are more distinct sublabel IDs
                # than are actually defined in the dataset.
                assert len(shape.elements) < label_id_mapping.expected_num_elements

                raise BadFunctionError(
                    "function output skeleton with fewer elements than expected"
                    f" ({len(shape.elements)} vs {label_id_mapping.expected_num_elements})"
                )
        else:
            if getattr(shape, "elements", None):
                raise BadFunctionError("function output non-skeleton shape with elements")

    def _remap_shape(self, shape: models.LabeledShapeRequest, ds_frame: int) -> bool:
        if hasattr(shape, "id"):
            raise BadFunctionError("function output shape with preset id")

        if hasattr(shape, "source"):
            raise BadFunctionError("function output shape with preset source")
        shape.source = "auto"

        if shape.frame != 0:
            raise BadFunctionError(
                f"function output shape with unexpected frame number ({shape.frame})"
            )

        shape.frame = ds_frame

        try:
            label_id_mapping = self._spec_id_mapping[shape.label_id]
        except KeyError:
            raise BadFunctionError(
                f"function output shape with unknown label ID ({shape.label_id})"
            )

        if not label_id_mapping:
            return False

        shape.label_id = label_id_mapping.id

        if not self._are_label_types_compatible(shape.type.value, label_id_mapping.expected_type):
            raise BadFunctionError(
                f"function output shape of type {shape.type.value!r}"
                f" (expected {label_id_mapping.expected_type!r})"
            )

        if shape.type.value == "mask" and self._conv_mask_to_poly:
            raise BadFunctionError("function output mask shape despite conv_mask_to_poly=True")

        self._remap_attributes(shape, label_id_mapping)

        self._remap_elements(shape, ds_frame, label_id_mapping)

        return True

    def validate_and_remap(self, shapes: list[models.LabeledShapeRequest], ds_frame: int) -> None:
        shapes[:] = [shape for shape in shapes if self._remap_shape(shape, ds_frame)]

    @staticmethod
    def _are_label_types_compatible(source_type: str, destination_type: str) -> bool:
        assert source_type != "any"
        return destination_type == "any" or destination_type == source_type


@attrs.frozen(kw_only=True)
class _DetectionFunctionContextImpl(DetectionFunctionContext):
    frame_name: str
    conf_threshold: Optional[float] = None
    conv_mask_to_poly: bool = False


def annotate_task(
    client: Client,
    task_id: int,
    function: DetectionFunction,
    *,
    pbar: Optional[ProgressReporter] = None,
    clear_existing: bool = False,
    allow_unmatched_labels: bool = False,
    conf_threshold: Optional[float] = None,
    conv_mask_to_poly: bool = False,
) -> None:
    """
    Downloads data for the task with the given ID, applies the given function to it
    and uploads the resulting annotations back to the task.

    Only tasks with 2D image (not video) data are supported at the moment.

    client is used to make all requests to the CVAT server.

    Currently, the only type of auto-annotation function supported is the detection function.
    A function of this type is applied independently to each image in the task.
    The resulting annotations are then combined and modified as follows:

    * The label IDs are replaced with the IDs of the corresponding labels in the task.
    * The frame numbers are replaced with the frame number of the image.
    * The sources are set to "auto".

    See the documentation for DetectionFunction for more details.

    If the function is found to violate any constraints set in its interface, BadFunctionError
    is raised.

    pbar, if supplied, is used to report progress information.

    If clear_existing is true, any annotations already existing in the tesk are removed.
    Otherwise, they are kept, and the new annotations are added to them.

    The allow_unmatched_labels parameter controls the behavior in the case when a detection
    function declares a label/sublabel/attribute in its spec
    that has no corresponding label/sublabel/attribute in the task.
    If it's set to True, any annotations/keypoints/attribute values
    returned by the function that refer to such labels/sublabels/attributes are dropped.
    If it's set to False, BadFunctionError is raised.

    The conf_threshold parameter must be None or a number between 0 and 1. It will be passed
    to the AA function as the conf_threshold attribute of the context object.

    The conv_mask_to_poly parameter will be passed to the AA function as the conv_mask_to_poly
    attribute of the context object. If it's true, and the AA function returns any mask shapes,
    BadFunctionError will be raised.
    """

    if pbar is None:
        pbar = NullProgressReporter()

    if conf_threshold is not None and not 0 <= conf_threshold <= 1:
        raise ValueError("conf_threshold must be None or a number between 0 and 1")

    dataset = TaskDataset(client, task_id, load_annotations=False)

    assert isinstance(function.spec, DetectionFunctionSpec)

    mapper = _AnnotationMapper(
        client.logger,
        function.spec.labels,
        dataset.labels,
        allow_unmatched_labels=allow_unmatched_labels,
        conv_mask_to_poly=conv_mask_to_poly,
    )

    shapes = []

    with pbar.task(total=len(dataset.samples), unit="samples"):
        for sample in pbar.iter(dataset.samples):
            frame_shapes = function.detect(
                _DetectionFunctionContextImpl(
                    frame_name=sample.frame_name,
                    conf_threshold=conf_threshold,
                    conv_mask_to_poly=conv_mask_to_poly,
                ),
                sample.media.load_image(),
            )
            mapper.validate_and_remap(frame_shapes, sample.frame_index)
            shapes.extend(frame_shapes)

    client.logger.info("Uploading annotations to task %d...", task_id)

    if clear_existing:
        client.tasks.api.update_annotations(
            task_id, task_annotations_update_request=models.LabeledDataRequest(shapes=shapes)
        )
    else:
        client.tasks.api.partial_update_annotations(
            "create",
            task_id,
            patched_labeled_data_request=models.PatchedLabeledDataRequest(shapes=shapes),
        )

    client.logger.info("Upload complete")


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\exceptions.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


class BadFunctionError(Exception):
    """
    An exception that signifies that an auto-detection function has violated some constraint
    set by its interface.
    """


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\interface.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import abc
from collections.abc import Sequence
from typing import Optional, Protocol

import attrs
import PIL.Image

import cvat_sdk.models as models

from ..attributes import attribute_value_validator
from .exceptions import BadFunctionError


@attrs.frozen(kw_only=True)
class DetectionFunctionSpec:
    """
    Static information about an auto-annotation detection function.

    Objects of this class should be treated as immutable;
    do not modify them or any nested objects after they are created.
    """

    labels: Sequence[models.PatchedLabelRequest] = attrs.field()
    """
    Information about labels that the function supports.

    The members of the sequence must follow the same constraints as if they were being
    used to create a CVAT project, and the following additional constraints:

    * The id attribute must be set to a distinct integer.

    * The id attribute of any sublabels must be set to an integer, distinct between all
      sublabels of the same parent label.

    * The id attribute of any attributes must be set to an integer, distinct between all
      attributes of the same label or sublabel.

    `BadFunctionError` will be raised if any constraint violations are detected.

    It's recommented to use the helper factory functions (label_spec, skeleton_label_spec,
    keypoint_spec) to create the label objects, as they are more concise than the model
    constructors and help to follow some of the constraints.
    """

    @classmethod
    def _validate_attributes(
        cls, attributes: Sequence[models.AttributeRequest], label_desc: str
    ) -> None:
        seen_attr_ids = set()

        for attr in attributes:
            attr_desc = f"attribute {attr.name!r} of {label_desc}"

            if not hasattr(attr, "id"):
                raise BadFunctionError(f"{attr_desc} has no ID")

            if attr.id in seen_attr_ids:
                raise BadFunctionError(f"{attr_desc} has same ID as another attribute ({attr.id})")

            seen_attr_ids.add(attr.id)

            try:
                attribute_value_validator(attr)
            except ValueError as ex:
                raise BadFunctionError(f"{attr_desc} has invalid values: {ex}") from ex

    @classmethod
    def _validate_label_spec(cls, label: models.PatchedLabelRequest) -> None:
        label_desc = f"label {label.name!r}"

        cls._validate_attributes(getattr(label, "attributes", []), label_desc)

        if getattr(label, "sublabels", []):
            label_type = getattr(label, "type", "any")
            if label_type != "skeleton":
                raise BadFunctionError(
                    f"{label_desc} with sublabels has type {label_type!r} (should be 'skeleton')"
                )

            seen_sl_ids = set()

            for sl in label.sublabels:
                sl_desc = f"sublabel {sl.name!r} of {label_desc}"

                if not hasattr(sl, "id"):
                    raise BadFunctionError(f"{sl_desc} has no ID")

                if sl.id in seen_sl_ids:
                    raise BadFunctionError(f"{sl_desc} has same ID as another sublabel ({sl.id})")

                seen_sl_ids.add(sl.id)

                if sl.type != "points":
                    raise BadFunctionError(f"{sl_desc} has type {sl.type!r} (should be 'points')")

                cls._validate_attributes(getattr(sl, "attributes", []), sl_desc)

    @labels.validator
    def _validate_labels(self, attribute, value: Sequence[models.PatchedLabelRequest]) -> None:
        seen_label_ids = set()

        for label in value:
            if not hasattr(label, "id"):
                raise BadFunctionError(f"label {label.name!r} has no ID")

            if label.id in seen_label_ids:
                raise BadFunctionError(
                    f"label {label.name} has same ID as another label ({label.id})"
                )
            seen_label_ids.add(label.id)

            self._validate_label_spec(label)


class DetectionFunctionContext(metaclass=abc.ABCMeta):
    """
    Information that is supplied to an auto-annotation detection function.
    """

    @property
    @abc.abstractmethod
    def frame_name(self) -> str:
        """
        The file name of the frame that the current image corresponds to in
        the dataset.
        """

    @property
    @abc.abstractmethod
    def conf_threshold(self) -> Optional[float]:
        """
        The confidence threshold that the function should use for filtering
        detections.

        If the function is able to estimate confidence levels, then:

        * If this value is None, the function may apply a default threshold at its discretion.

        * Otherwise, it will be a number between 0 and 1. The function must only return
          objects with confidence levels greater than or equal to this value.

        If the function is not able to estimate confidence levels, it can ignore this value.
        """

    @property
    @abc.abstractmethod
    def conv_mask_to_poly(self) -> bool:
        """
        If this is true, the function must convert any mask shapes to polygon shapes
        before returning them.

        If the function does not return any mask shapes, then it can ignore this value.
        """


class DetectionFunction(Protocol):
    """
    The interface that an auto-annotation detection function must implement.

    A detection function is supposed to accept an image and return a list of shapes
    describing objects in that image.

    Since the same function could be used with multiple datasets, it needs some way
    to refer to labels without using dataset-specific label IDs. The way this is
    accomplished is that the function declares its own labels via the spec attribute,
    and then refers to those labels in the returned annotations. The caller then matches
    up the labels from the function's spec with the labels in the actual dataset, and
    replaces the label IDs in the returned annotations with IDs of the corresponding
    labels in the dataset.

    The matching of labels between the function and the dataset is done by name.
    Therefore, a function can be used with a dataset if they have (at least some) labels
    that have the same name.
    """

    @property
    def spec(self) -> DetectionFunctionSpec:
        """Returns the function's spec."""
        ...

    def detect(
        self, context: DetectionFunctionContext, image: PIL.Image.Image
    ) -> list[models.LabeledShapeRequest]:
        """
        Detects objects on the supplied image and returns the results.

        The supplied context will contain information about the current image.

        The returned LabeledShapeRequest objects must follow general constraints
        imposed by the data model (such as the number of points in a shape),
        as well as the following additional constraints:

        * The id attribute must not be set.

        * The source attribute must not be set.

        * The frame_id attribute must be set to 0.

        * The label_id attribute must equal one of the label IDs
          in the function spec.

        * There must not be any attributes (attribute support may be added in a
          future version).

        * The above constraints also apply to each sub-shape (element of a shape),
          except that the label_id of a sub-shape must equal one of the sublabel IDs
          of the label of its parent shape.

        It's recommented to use the helper factory functions (shape, rectangle, skeleton,
        keypoint) to create the shape objects, as they are more concise than the model
        constructors and help to follow some of the constraints.

        The function must not retain any references to the returned objects,
        so that the caller may freely modify them.
        """
        ...


# spec factories


# pylint: disable-next=redefined-builtin
def label_spec(name: str, id: int, **kwargs) -> models.PatchedLabelRequest:
    """Helper factory function for PatchedLabelRequest."""
    return models.PatchedLabelRequest(name=name, id=id, **kwargs)


# pylint: disable-next=redefined-builtin
def skeleton_label_spec(
    name: str, id: int, sublabels: Sequence[models.SublabelRequest], **kwargs
) -> models.PatchedLabelRequest:
    """Helper factory function for PatchedLabelRequest with type="skeleton"."""
    return label_spec(name, id, type="skeleton", sublabels=sublabels, **kwargs)


# pylint: disable-next=redefined-builtin
def keypoint_spec(name: str, id: int, **kwargs) -> models.SublabelRequest:
    """Helper factory function for SublabelRequest with type="points"."""
    return models.SublabelRequest(name=name, id=id, type="points", **kwargs)


def attribute_spec(
    name: str,
    # pylint: disable-next=redefined-builtin
    id: int,
    input_type: str,
    values: list[str],
    **kwargs,
) -> models.AttributeRequest:
    """Helper factory function for AttributeRequest with mutable=False."""
    return models.AttributeRequest(
        name=name, id=id, input_type=input_type, values=values, mutable=False, **kwargs
    )


def number_attribute_spec(
    name: str,
    # pylint: disable-next=redefined-builtin
    id: int,
    values: list[str],
    **kwargs,
) -> models.AttributeRequest:
    """
    Helper factory function for AttributeRequest with input_type="number".

    It's recommended to use the `cvat_sdk.attributes.number_attribute_values` function
    to create the `values` argument.
    """
    return attribute_spec(name, id, "number", values, **kwargs)


# pylint: disable-next=redefined-builtin
def checkbox_attribute_spec(name: str, id: int, **kwargs) -> models.AttributeRequest:
    """Helper factory function for AttributeRequest with input_type="checkbox"."""
    return attribute_spec(name, id, "checkbox", [], **kwargs)


def radio_attribute_spec(
    name: str,
    # pylint: disable-next=redefined-builtin
    id: int,
    values: list[str],
    **kwargs,
) -> models.AttributeRequest:
    """Helper factory function for AttributeRequest with input_type="radio"."""
    return attribute_spec(name, id, "radio", values, **kwargs)


def select_attribute_spec(
    name: str,
    # pylint: disable-next=redefined-builtin
    id: int,
    values: list[str],
    **kwargs,
) -> models.AttributeRequest:
    """Helper factory function for AttributeRequest with input_type="select"."""
    return attribute_spec(name, id, "select", values, **kwargs)


# pylint: disable-next=redefined-builtin
def text_attribute_spec(name: str, id: int, **kwargs) -> models.AttributeRequest:
    """Helper factory function for AttributeRequest with input_type="text"."""
    return attribute_spec(name, id, "text", [], **kwargs)


# annotation factories


def shape(label_id: int, **kwargs) -> models.LabeledShapeRequest:
    """Helper factory function for LabeledShapeRequest with frame=0."""
    return models.LabeledShapeRequest(label_id=label_id, frame=0, **kwargs)


def rectangle(label_id: int, points: Sequence[float], **kwargs) -> models.LabeledShapeRequest:
    """Helper factory function for LabeledShapeRequest with frame=0 and type="rectangle"."""
    return shape(label_id, type="rectangle", points=points, **kwargs)


def polygon(label_id: int, points: Sequence[float], **kwargs) -> models.LabeledShapeRequest:
    """Helper factory function for LabeledShapeRequest with frame=0 and type="polygon"."""
    return shape(label_id, type="polygon", points=points, **kwargs)


def mask(label_id: int, points: Sequence[float], **kwargs) -> models.LabeledShapeRequest:
    """
    Helper factory function for LabeledShapeRequest with frame=0 and type="mask".

    It's recommended to use the cvat_sdk.masks.encode_mask function to build the
    points argument.
    """
    return shape(label_id, type="mask", points=points, **kwargs)


def skeleton(
    label_id: int, elements: Sequence[models.SubLabeledShapeRequest], **kwargs
) -> models.LabeledShapeRequest:
    """Helper factory function for LabeledShapeRequest with frame=0 and type="skeleton"."""
    return shape(label_id, type="skeleton", elements=elements, **kwargs)


def keypoint(label_id: int, points: Sequence[float], **kwargs) -> models.SubLabeledShapeRequest:
    """Helper factory function for SubLabeledShapeRequest with frame=0 and type="points"."""
    return models.SubLabeledShapeRequest(
        label_id=label_id, frame=0, type="points", points=points, **kwargs
    )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from .driver import annotate_task
from .exceptions import BadFunctionError
from .interface import (
    DetectionFunction,
    DetectionFunctionContext,
    DetectionFunctionSpec,
    attribute_spec,
    checkbox_attribute_spec,
    keypoint,
    keypoint_spec,
    label_spec,
    mask,
    number_attribute_spec,
    polygon,
    radio_attribute_spec,
    rectangle,
    select_attribute_spec,
    shape,
    skeleton,
    skeleton_label_spec,
    text_attribute_spec,
)

__all__ = [
    "annotate_task",
    "attribute_spec",
    "BadFunctionError",
    "checkbox_attribute_spec",
    "DetectionFunction",
    "DetectionFunctionContext",
    "DetectionFunctionSpec",
    "keypoint_spec",
    "keypoint",
    "label_spec",
    "mask",
    "number_attribute_spec",
    "polygon",
    "radio_attribute_spec",
    "rectangle",
    "select_attribute_spec",
    "shape",
    "skeleton_label_spec",
    "skeleton",
    "text_attribute_spec",
]


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\functions\torchvision_detection.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import PIL.Image

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models

from ._torchvision import TorchvisionFunction


class _TorchvisionDetectionFunction(TorchvisionFunction):
    _label_type = "rectangle"

    def detect(
        self, context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
    ) -> list[models.LabeledShapeRequest]:
        conf_threshold = context.conf_threshold or 0
        results = self._model([self._transforms(image)])

        return [
            cvataa.rectangle(label.item(), [x.item() for x in box])
            for result in results
            for box, label, score in zip(result["boxes"], result["labels"], result["scores"])
            if score >= conf_threshold
        ]


create = _TorchvisionDetectionFunction


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\functions\torchvision_instance_segmentation.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
from collections.abc import Iterator

import numpy as np
import PIL.Image
from skimage import measure
from torch import Tensor

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models
from cvat_sdk.masks import encode_mask

from ._torchvision import TorchvisionFunction


def _is_positively_oriented(contour: np.ndarray) -> bool:
    ys, xs = contour.T

    # This is the shoelace formula, except we only need the sign of the result,
    # so we compare instead of subtracting. Compared to the typical formula,
    # the sign is inverted, because the Y axis points downwards.
    return np.sum(xs * np.roll(ys, -1)) < np.sum(ys * np.roll(xs, -1))


def _generate_shapes(
    context: cvataa.DetectionFunctionContext, box: Tensor, mask: Tensor, label: Tensor
) -> Iterator[models.LabeledShapeRequest]:
    LEVEL = 0.5

    if context.conv_mask_to_poly:
        # Since we treat mask values of exactly LEVEL as true, we'd like them
        # to also be considered high by find_contours. And for that, the level
        # parameter must be slightly less than LEVEL.
        contours = measure.find_contours(mask[0].detach().numpy(), level=math.nextafter(LEVEL, 0))

        for contour in contours:
            if len(contour) < 3 or _is_positively_oriented(contour):
                continue

            contour = measure.approximate_polygon(contour, tolerance=2.5)

            yield cvataa.polygon(label.item(), contour[:, ::-1].ravel().tolist())

    else:
        yield cvataa.mask(label.item(), encode_mask(mask[0] >= LEVEL, box.tolist()))


class _TorchvisionInstanceSegmentationFunction(TorchvisionFunction):
    _label_type = "mask"

    def detect(
        self, context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
    ) -> list[models.LabeledShapeRequest]:
        conf_threshold = context.conf_threshold or 0
        results = self._model([self._transforms(image)])

        return [
            shape
            for result in results
            for box, mask, label, score in zip(
                result["boxes"], result["masks"], result["labels"], result["scores"]
            )
            if score >= conf_threshold
            for shape in _generate_shapes(context, box, mask, label)
        ]


create = _TorchvisionInstanceSegmentationFunction


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\functions\torchvision_keypoint_detection.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from functools import cached_property

import PIL.Image

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models

from ._torchvision import TorchvisionFunction


class _TorchvisionKeypointDetectionFunction(TorchvisionFunction):
    @cached_property
    def spec(self) -> cvataa.DetectionFunctionSpec:
        return cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.skeleton_label_spec(
                    cat,
                    i,
                    [
                        cvataa.keypoint_spec(name, j)
                        for j, name in enumerate(self._weights.meta["keypoint_names"])
                    ],
                )
                for i, cat in enumerate(self._weights.meta["categories"])
            ]
        )

    def detect(
        self, context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
    ) -> list[models.LabeledShapeRequest]:
        conf_threshold = context.conf_threshold or 0
        results = self._model([self._transforms(image)])

        return [
            cvataa.skeleton(
                label.item(),
                elements=[
                    cvataa.keypoint(
                        keypoint_id,
                        [keypoint[0].item(), keypoint[1].item()],
                        occluded=not keypoint[2].item(),
                    )
                    for keypoint_id, keypoint in enumerate(keypoints)
                ],
            )
            for result in results
            for keypoints, label, score in zip(
                result["keypoints"], result["labels"], result["scores"]
            )
            if score >= conf_threshold
        ]


create = _TorchvisionKeypointDetectionFunction


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\functions\_torchvision.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from functools import cached_property

import torchvision.models

import cvat_sdk.auto_annotation as cvataa


class TorchvisionFunction:
    _label_type = "any"

    def __init__(self, model_name: str, weights_name: str = "DEFAULT", **kwargs) -> None:
        weights_enum = torchvision.models.get_model_weights(model_name)
        self._weights = weights_enum[weights_name]
        self._transforms = self._weights.transforms()
        self._model = torchvision.models.get_model(model_name, weights=self._weights, **kwargs)
        self._model.eval()

    @cached_property
    def spec(self) -> cvataa.DetectionFunctionSpec:
        return cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec(cat, i, type=self._label_type)
                for i, cat in enumerate(self._weights.meta["categories"])
                if cat != "N/A"
            ]
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\auto_annotation\functions\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\client.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


from __future__ import annotations

import logging
import urllib.parse
from collections.abc import Generator, Sequence
from contextlib import contextmanager, suppress
from pathlib import Path
from time import sleep
from typing import Any, Optional, TypeVar

import attrs
import packaging.specifiers as specifiers
import packaging.version as pv
import platformdirs
import urllib3
import urllib3.exceptions

from cvat_sdk.api_client import ApiClient, Configuration, exceptions, models
from cvat_sdk.core.exceptions import IncompatibleVersionException, InvalidHostException
from cvat_sdk.core.proxies.issues import CommentsRepo, IssuesRepo
from cvat_sdk.core.proxies.jobs import JobsRepo
from cvat_sdk.core.proxies.model_proxy import Repo
from cvat_sdk.core.proxies.organizations import OrganizationsRepo
from cvat_sdk.core.proxies.projects import ProjectsRepo
from cvat_sdk.core.proxies.tasks import TasksRepo
from cvat_sdk.core.proxies.users import UsersRepo
from cvat_sdk.version import VERSION

_DEFAULT_CACHE_DIR = platformdirs.user_cache_path("cvat-sdk", "CVAT.ai")

_RepoType = TypeVar("_RepoType", bound=Repo)


@attrs.define
class Config:
    """
    Allows to tweak behavior of Client instances.
    """

    status_check_period: float = 5
    """Operation status check period, in seconds"""

    allow_unsupported_server: bool = True
    """Allow to use SDK with an unsupported server version. If disabled, raise an exception"""

    verify_ssl: Optional[bool] = None
    """Whether to verify host SSL certificate or not"""

    cache_dir: Path = attrs.field(converter=Path, default=_DEFAULT_CACHE_DIR)
    """Directory in which to store cached server data"""


_VERSION_OBJ = pv.Version(VERSION)


class Client:
    """
    Provides session management, implements authentication operations
    and simplifies access to server APIs.
    """

    SUPPORTED_SERVER_VERSIONS = (
        pv.Version(f"{_VERSION_OBJ.epoch}!{_VERSION_OBJ.major}.{_VERSION_OBJ.minor}"),
        pv.Version(f"{_VERSION_OBJ.epoch}!{_VERSION_OBJ.major}.{_VERSION_OBJ.minor+1}"),
    )

    def __init__(
        self,
        url: str,
        *,
        logger: Optional[logging.Logger] = None,
        config: Optional[Config] = None,
        check_server_version: bool = True,
    ) -> None:
        url = self._validate_and_prepare_url(url)

        self.logger = logger or logging.getLogger(__name__)
        """The root logger"""

        self.config = config or Config()
        """Configuration for this object"""

        self.api_map = CVAT_API_V2(url)
        """Handles server API URL interaction logic"""

        self.api_client = ApiClient(
            Configuration(host=self.api_map.host, verify_ssl=self.config.verify_ssl)
        )
        """Provides low-level access to the CVAT server"""

        if check_server_version:
            self.check_server_version()

        self._repos: dict[str, Repo] = {}
        """A cache for created Repository instances"""

    _ORG_SLUG_HEADER = "X-Organization"

    @property
    def organization_slug(self) -> Optional[str]:
        """
        If this is set to a slug for an organization,
        all requests will be made in the context of that organization.

        If it's set to an empty string, requests will be made in the context
        of the user's personal workspace.

        If set to None (the default), no organization context will be used.
        """
        return self.api_client.default_headers.get(self._ORG_SLUG_HEADER)

    @organization_slug.setter
    def organization_slug(self, org_slug: Optional[str]):
        if org_slug is None:
            self.api_client.default_headers.pop(self._ORG_SLUG_HEADER, None)
        else:
            self.api_client.default_headers[self._ORG_SLUG_HEADER] = org_slug

    @contextmanager
    def organization_context(self, slug: str) -> Generator[None, None, None]:
        prev_slug = self.organization_slug
        self.organization_slug = slug
        try:
            yield
        finally:
            self.organization_slug = prev_slug

    ALLOWED_SCHEMAS = ("https", "http")

    @classmethod
    def _validate_and_prepare_url(cls, url: str) -> str:
        url_parts = url.split("://", maxsplit=1)
        if len(url_parts) == 2:
            schema, base_url = url_parts
        else:
            schema = ""
            base_url = url

        base_url = base_url.rstrip("/")

        if schema and schema not in cls.ALLOWED_SCHEMAS:
            raise InvalidHostException(
                f"Invalid url schema '{schema}', expected "
                f"one of <none>, {', '.join(cls.ALLOWED_SCHEMAS)}"
            )

        if not schema:
            schema = cls._detect_schema(base_url)
            url = f"{schema}://{base_url}"

        return url

    @classmethod
    def _detect_schema(cls, base_url: str) -> str:
        for schema in cls.ALLOWED_SCHEMAS:
            with ApiClient(Configuration(host=f"{schema}://{base_url}")) as api_client:
                with suppress(urllib3.exceptions.RequestError):
                    (_, response) = api_client.server_api.retrieve_about(
                        _request_timeout=5, _parse_response=False, _check_status=False
                    )

                    if response.status in [200, 401]:
                        # Server versions prior to 2.3.0 respond with unauthorized
                        # 2.3.0 allows unauthorized access
                        return schema

        raise InvalidHostException(
            "Failed to detect host schema automatically, please check "
            "the server url and try to specify 'https://' or 'http://' explicitly"
        )

    def __enter__(self):
        self.api_client.__enter__()
        return self

    def __exit__(self, exc_type, exc_value, traceback) -> None:
        return self.api_client.__exit__(exc_type, exc_value, traceback)

    def close(self) -> None:
        return self.__exit__(None, None, None)

    def login(self, credentials: tuple[str, str]) -> None:
        (auth, _) = self.api_client.auth_api.create_login(
            models.LoginSerializerExRequest(username=credentials[0], password=credentials[1])
        )

        assert "sessionid" in self.api_client.cookies
        assert "csrftoken" in self.api_client.cookies
        self.api_client.set_default_header("Authorization", "Token " + auth.key)

    def has_credentials(self) -> bool:
        return (
            ("sessionid" in self.api_client.cookies)
            or ("csrftoken" in self.api_client.cookies)
            or bool(self.api_client.get_common_headers().get("Authorization", ""))
        )

    def logout(self) -> None:
        if self.has_credentials():
            self.api_client.auth_api.create_logout()
            self.api_client.cookies.pop("sessionid", None)
            self.api_client.cookies.pop("csrftoken", None)
            self.api_client.default_headers.pop("Authorization", None)

    def wait_for_completion(
        self: Client,
        rq_id: str,
        *,
        status_check_period: Optional[int] = None,
    ) -> tuple[models.Request, urllib3.HTTPResponse]:
        if status_check_period is None:
            status_check_period = self.config.status_check_period

        while True:
            sleep(status_check_period)

            request, response = self.api_client.requests_api.retrieve(rq_id)

            if request.status.value == models.RequestStatus.allowed_values[("value",)]["FINISHED"]:
                break
            elif request.status.value == models.RequestStatus.allowed_values[("value",)]["FAILED"]:
                raise exceptions.ApiException(
                    status=request.status, reason=request.message, http_resp=response
                )

        return request, response

    def check_server_version(self, fail_if_unsupported: Optional[bool] = None) -> None:
        if fail_if_unsupported is None:
            fail_if_unsupported = not self.config.allow_unsupported_server

        try:
            server_version = self.get_server_version()
        except exceptions.ApiException as e:
            msg = (
                "Failed to retrieve server API version: %s. "
                "Some SDK functions may not work properly with this server."
            ) % (e,)
            self.logger.warning(msg)
            if fail_if_unsupported:
                raise IncompatibleVersionException(msg)
            return

        if not any(
            self._is_version_compatible(server_version, supported_version)
            for supported_version in self.SUPPORTED_SERVER_VERSIONS
        ):
            msg = (
                "Server version '%s' is not compatible with SDK version '%s'. "
                "Some SDK functions may not work properly with this server. "
                "You can continue using this SDK, or you can "
                "try to update with 'pip install cvat-sdk'."
            ) % (server_version, pv.Version(VERSION))
            self.logger.warning(msg)
            if fail_if_unsupported:
                raise IncompatibleVersionException(msg)

    def _is_version_compatible(self, current: pv.Version, target: pv.Version) -> bool:
        # Check for (major, minor) compatibility.
        # Micro releases and fixes do not affect API compatibility in general.
        epoch = f"{target.epoch}!" if target.epoch else ""  # 1.0 ~= 0!1.0 is false
        return current in specifiers.Specifier(
            f"~= {epoch}{target.major}.{target.minor}.{target.micro}"
        )

    def get_server_version(self) -> pv.Version:
        (about, _) = self.api_client.server_api.retrieve_about()
        return pv.Version(about.version)

    def _get_repo(self, repo_type: _RepoType) -> _RepoType:
        repo = self._repos.get(repo_type, None)
        if repo is None:
            repo = repo_type(self)
            self._repos[repo_type] = repo
        return repo

    @property
    def tasks(self) -> TasksRepo:
        return self._get_repo(TasksRepo)

    @property
    def projects(self) -> ProjectsRepo:
        return self._get_repo(ProjectsRepo)

    @property
    def jobs(self) -> JobsRepo:
        return self._get_repo(JobsRepo)

    @property
    def users(self) -> UsersRepo:
        return self._get_repo(UsersRepo)

    @property
    def organizations(self) -> OrganizationsRepo:
        return self._get_repo(OrganizationsRepo)

    @property
    def issues(self) -> IssuesRepo:
        return self._get_repo(IssuesRepo)

    @property
    def comments(self) -> CommentsRepo:
        return self._get_repo(CommentsRepo)


class CVAT_API_V2:
    """Build parameterized API URLs"""

    def __init__(self, host: str):
        self.host = host.rstrip("/")
        self.base = self.host + "/api/"

    def make_endpoint_url(
        self,
        path: str,
        *,
        psub: Optional[Sequence[Any]] = None,
        kwsub: Optional[dict[str, Any]] = None,
        query_params: Optional[dict[str, Any]] = None,
    ) -> str:
        url = self.host + path
        if psub or kwsub:
            url = url.format(*(psub or []), **(kwsub or {}))
        if query_params:
            url += "?" + urllib.parse.urlencode(query_params)
        return url


def make_client(
    host: str, *, port: Optional[int] = None, credentials: Optional[tuple[str, str]] = None
) -> Client:
    url = host.rstrip("/")
    if port:
        url = f"{url}:{port}"

    client = Client(url=url)
    if credentials is not None:
        client.login(credentials)
    return client


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\downloading.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import json
from contextlib import closing
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from cvat_sdk.api_client.api_client import Endpoint
from cvat_sdk.core.helpers import expect_status
from cvat_sdk.core.progress import NullProgressReporter, ProgressReporter
from cvat_sdk.core.utils import atomic_writer

if TYPE_CHECKING:
    from cvat_sdk.core.client import Client


class Downloader:
    """
    Implements common downloading protocols
    """

    def __init__(self, client: Client):
        self._client = client

    def download_file(
        self,
        url: str,
        output_path: Path,
        *,
        timeout: int = 60,
        pbar: Optional[ProgressReporter] = None,
    ) -> None:
        """
        Downloads the file from url into a temporary file, then renames it to the requested name.
        """

        CHUNK_SIZE = 10 * 2**20

        assert not output_path.exists()

        if pbar is None:
            pbar = NullProgressReporter()

        response = self._client.api_client.rest_client.GET(
            url,
            _request_timeout=timeout,
            headers=self._client.api_client.get_common_headers(),
            _parse_response=False,
        )
        with closing(response):
            try:
                file_size = int(response.headers.get("Content-Length", 0))
            except ValueError:
                file_size = None

            with (
                atomic_writer(output_path, "wb") as fd,
                pbar.task(
                    total=file_size,
                    desc="Downloading",
                    unit_scale=True,
                    unit="B",
                    unit_divisor=1024,
                ),
            ):
                while True:
                    chunk = response.read(amt=CHUNK_SIZE, decode_content=False)
                    if not chunk:
                        break

                    pbar.advance(len(chunk))
                    fd.write(chunk)

    def prepare_file(
        self,
        endpoint: Endpoint,
        *,
        url_params: Optional[dict[str, Any]] = None,
        query_params: Optional[dict[str, Any]] = None,
        status_check_period: Optional[int] = None,
    ):
        client = self._client
        if status_check_period is None:
            status_check_period = client.config.status_check_period

        client.logger.info("Waiting for the server to prepare the file...")

        url = client.api_map.make_endpoint_url(
            endpoint.path, kwsub=url_params, query_params=query_params
        )

        # initialize background process
        response = client.api_client.rest_client.request(
            method=endpoint.settings["http_method"],
            url=url,
            headers=client.api_client.get_common_headers(),
        )

        client.logger.debug("STATUS %s", response.status)
        expect_status(202, response)
        rq_id = json.loads(response.data).get("rq_id")
        assert rq_id, "Request identifier was not found in server response"

        # wait until background process will be finished or failed
        request, response = client.wait_for_completion(
            rq_id, status_check_period=status_check_period
        )

        return request

    def prepare_and_download_file_from_endpoint(
        self,
        endpoint: Endpoint,
        filename: Path,
        *,
        url_params: Optional[dict[str, Any]] = None,
        query_params: Optional[dict[str, Any]] = None,
        pbar: Optional[ProgressReporter] = None,
        status_check_period: Optional[int] = None,
    ):
        client = self._client

        if status_check_period is None:
            status_check_period = client.config.status_check_period

        export_request = self.prepare_file(
            endpoint,
            url_params=url_params,
            query_params=query_params,
            status_check_period=status_check_period,
        )

        assert export_request.result_url, "Result url was not found in server response"
        self.download_file(export_request.result_url, output_path=filename, pbar=pbar)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\exceptions.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


class CvatSdkException(Exception):
    """Base class for SDK exceptions"""


class InvalidHostException(CvatSdkException):
    """Indicates an invalid hostname error"""


class IncompatibleVersionException(CvatSdkException):
    """Indicates server and SDK version mismatch"""


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\helpers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import json
import warnings
from collections.abc import Iterable
from typing import Any, Optional, Union

import tqdm
import urllib3

from cvat_sdk import exceptions
from cvat_sdk.api_client.api_client import Endpoint
from cvat_sdk.core.progress import BaseProgressReporter, ProgressReporter


def get_paginated_collection(
    endpoint: Endpoint, *, return_json: bool = False, **kwargs
) -> Union[list, list[dict[str, Any]]]:
    """
    Accumulates results from all the pages
    """

    results = []
    page = 1
    while True:
        (page_contents, response) = endpoint.call_with_http_info(**kwargs, page=page)
        expect_status(200, response)

        if return_json:
            results.extend(json.loads(response.data).get("results", []))
        else:
            results.extend(page_contents.results)

        if (
            page_contents is not None
            and not page_contents.next
            or page_contents is None
            and not json.loads(response.data).get("next")
        ):
            break
        page += 1

    return results


class _BaseTqdmProgressReporter(BaseProgressReporter):
    tqdm: Optional[tqdm.tqdm]

    def report_status(self, progress: int):
        super().report_status(progress)
        self.tqdm.update(progress - self.tqdm.n)

    def advance(self, delta: int):
        super().advance(delta)
        self.tqdm.update(delta)


class TqdmProgressReporter(_BaseTqdmProgressReporter):
    def __init__(self, instance: tqdm.tqdm) -> None:
        super().__init__()
        warnings.warn(f"use {DeferredTqdmProgressReporter.__name__} instead", DeprecationWarning)

        self.tqdm = instance

    def start2(self, total: int, *, desc: Optional[str] = None, **kwargs) -> None:
        super().start2(total=total, desc=desc, **kwargs)

        self.tqdm.reset(total)
        self.tqdm.set_description_str(desc)

    def finish(self):
        self.tqdm.refresh()
        super().finish()


class DeferredTqdmProgressReporter(_BaseTqdmProgressReporter):
    def __init__(self, tqdm_args: Optional[dict] = None) -> None:
        super().__init__()
        self.tqdm_args = tqdm_args or {}
        self.tqdm = None

    def start2(
        self,
        total: int,
        *,
        desc: Optional[str] = None,
        unit: str = "it",
        unit_scale: bool = False,
        unit_divisor: int = 1000,
        **kwargs,
    ) -> None:
        super().start2(
            total=total,
            desc=desc,
            unit=unit,
            unit_scale=unit_scale,
            unit_divisor=unit_divisor,
            **kwargs,
        )
        assert not self.tqdm

        self.tqdm = tqdm.tqdm(
            **self.tqdm_args,
            total=total,
            desc=desc,
            unit=unit,
            unit_scale=unit_scale,
            unit_divisor=unit_divisor,
        )

    def finish(self):
        self.tqdm.close()
        self.tqdm = None
        super().finish()


class StreamWithProgress:
    def __init__(self, stream: io.RawIOBase, pbar: ProgressReporter):
        self.stream = stream
        self.pbar = pbar

        assert self.stream.tell() == 0

    def read(self, size=-1):
        chunk = self.stream.read(size)
        if chunk is not None:
            self.pbar.advance(len(chunk))
        return chunk

    def seek(self, pos: int, whence: int = io.SEEK_SET) -> None:
        old_pos = self.stream.tell()
        new_pos = self.stream.seek(pos, whence)
        self.pbar.advance(new_pos - old_pos)
        return new_pos

    def tell(self) -> int:
        return self.stream.tell()


def expect_status(codes: Union[int, Iterable[int]], response: urllib3.HTTPResponse) -> None:
    if not hasattr(codes, "__iter__"):
        codes = [codes]

    if response.status in codes:
        return

    if 300 <= response.status <= 500:
        raise exceptions.ApiException(response.status, reason=response.msg, http_resp=response)
    else:
        raise exceptions.ApiException(
            response.status, reason="Unexpected status code received", http_resp=response
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\progress.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import contextlib
from collections.abc import Generator, Iterable
from typing import Optional, TypeVar

T = TypeVar("T")


class ProgressReporter:
    """
    Use as follows:

    with r.task(...):
        r.report_status(...)
        r.advance(...)

        for x in r.iter(...):
            ...

    Implementations must override start2, finish, report_status and advance.
    """

    @contextlib.contextmanager
    def task(self, **kwargs) -> Generator[None, None, None]:
        """
        Returns a context manager that represents a long-running task
        for which progress can be reported.

        Entering it creates a progress bar, and exiting it destroys it.

        kwargs will be passed to `start()`.
        """
        self.start2(**kwargs)

        try:
            yield None
        finally:
            self.finish()

    def start(self, total: int, *, desc: Optional[str] = None) -> None:
        """
        This is a compatibility method. Override start2 instead.
        """
        raise NotImplementedError

    def start2(
        self,
        total: int,
        *,
        desc: Optional[str] = None,
        unit: str = "it",
        unit_scale: bool = False,
        unit_divisor: int = 1000,
        **kwargs,
    ) -> None:
        """
        Initializes the progress bar.

        total, desc, unit, unit_scale, unit_divisor have the same meaning as in tqdm.

        kwargs is included for future extension; implementations of this method
        must ignore it.
        """
        self.start(total=total, desc=desc)

    def report_status(self, progress: int):
        """Updates the progress bar"""
        raise NotImplementedError

    def advance(self, delta: int):
        """Updates the progress bar"""
        raise NotImplementedError

    def finish(self):
        """Finishes the progress bar"""
        pass  # pylint: disable=unnecessary-pass

    def iter(
        self,
        iterable: Iterable[T],
    ) -> Iterable[T]:
        """
        Traverses the iterable and reports progress simultaneously.

        Args:
            iterable: An iterable to be traversed

        Returns:
            An iterable over elements of the input sequence
        """

        for elem in iterable:
            yield elem
            self.advance(1)


class BaseProgressReporter(ProgressReporter):
    def __init__(self) -> None:
        self._in_progress = False

    def start2(
        self,
        total: int,
        *,
        desc: Optional[str] = None,
        unit: str = "it",
        unit_scale: bool = False,
        unit_divisor: int = 1000,
        **kwargs,
    ) -> None:
        assert not self._in_progress
        self._in_progress = True

    def report_status(self, progress: int):
        assert self._in_progress

    def advance(self, delta: int):
        assert self._in_progress

    def finish(self) -> None:
        assert self._in_progress
        self._in_progress = False

    def __del__(self):
        assert not self._in_progress, "Unfinished task!"


class NullProgressReporter(BaseProgressReporter):
    pass


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\uploading.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import json
import os
from contextlib import AbstractContextManager
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

import requests
import urllib3

from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.api_client.exceptions import ApiException
from cvat_sdk.api_client.rest import RESTClientObject
from cvat_sdk.core.helpers import StreamWithProgress, expect_status
from cvat_sdk.core.progress import NullProgressReporter, ProgressReporter

if TYPE_CHECKING:
    from cvat_sdk.core.client import Client

import tusclient.uploader as tus_uploader
from tusclient.client import TusClient as _TusClient
from tusclient.client import Uploader as _TusUploader
from tusclient.request import TusRequest as _TusRequest
from tusclient.request import TusUploadFailed as _TusUploadFailed

MAX_REQUEST_SIZE = 100 * 2**20


class _RestClientAdapter:
    # Provides requests.Session-like interface for REST client
    # only patch is called in the tus client

    def __init__(self, rest_client: RESTClientObject):
        self.rest_client = rest_client

    def _request(self, method, url, data=None, json=None, **kwargs):
        raw = self.rest_client.request(
            method=method,
            url=url,
            headers=kwargs.get("headers"),
            query_params=kwargs.get("params"),
            post_params=json,
            body=data,
            _parse_response=False,
            _request_timeout=kwargs.get("timeout"),
            _check_status=False,
        )

        result = requests.Response()
        result._content = raw.data
        result.raw = raw
        result.headers.update(raw.headers)
        result.status_code = raw.status
        result.reason = raw.msg
        return result

    def patch(self, *args, **kwargs):
        return self._request("PATCH", *args, **kwargs)


class _MyTusUploader(_TusUploader):
    # Adjusts the library code for CVAT server
    # Allows to reuse session

    def __init__(self, *_args, api_client: ApiClient, **_kwargs):
        self._api_client = api_client
        super().__init__(*_args, **_kwargs)

    def _do_request(self):
        self.request = _TusRequest(self)
        self.request.handle = _RestClientAdapter(self._api_client.rest_client)
        try:
            self.request.perform()
            self.verify_upload()
        except _TusUploadFailed as error:
            self._retry_or_cry(error)

    @tus_uploader._catch_requests_error
    def create_url(self):
        """
        Return upload url.

        Makes request to tus server to create a new upload url for the required file upload.
        """
        headers = self.headers
        headers["upload-length"] = str(self.file_size)
        headers["upload-metadata"] = ",".join(self.encode_metadata())
        resp = self._api_client.rest_client.POST(self.client.url, headers=headers)
        self.real_filename = resp.headers.get("Upload-Filename")
        url = resp.headers.get("location")
        if url is None:
            msg = "Attempt to retrieve create file url with status {}".format(resp.status_code)
            raise tus_uploader.TusCommunicationError(msg, resp.status_code, resp.content)
        return tus_uploader.urljoin(self.client.url, url)

    @tus_uploader._catch_requests_error
    def get_offset(self):
        """
        Return offset from tus server.

        This is different from the instance attribute 'offset' because this makes an
        http request to the tus server to retrieve the offset.
        """
        try:
            resp = self._api_client.rest_client.HEAD(self.url, headers=self.headers)
        except ApiException as ex:
            if ex.status == 405:  # Method Not Allowed
                # In CVAT up to version 2.2.0, HEAD requests were internally
                # converted to GET by mod_wsgi, and subsequently rejected by the server.
                # For compatibility with old servers, we'll handle such rejections by
                # restarting the upload from the beginning.
                return 0

            raise tus_uploader.TusCommunicationError(
                f"Attempt to retrieve offset failed with status {ex.status}",
                ex.status,
                ex.body,
            ) from ex

        offset = resp.headers.get("upload-offset")
        if offset is None:
            raise tus_uploader.TusCommunicationError(
                f"Attempt to retrieve offset failed with status {resp.status}",
                resp.status,
                resp.data,
            )

        return int(offset)


class Uploader:
    """
    Implements common uploading protocols
    """

    _CHUNK_SIZE = 10 * 2**20

    def __init__(self, client: Client):
        self._client = client

    def upload_file(
        self,
        url: str,
        filename: Path,
        *,
        meta: dict[str, Any],
        query_params: dict[str, Any] = None,
        fields: Optional[dict[str, Any]] = None,
        pbar: Optional[ProgressReporter] = None,
        logger=None,
    ) -> urllib3.HTTPResponse:
        """
        Annotation uploads:
        - have "filename" meta field in chunks
        - have "filename" and "format" query params in the "Upload-Finished" request


        Data (image, video, ...) uploads:
        - have "filename" meta field in chunks
        - have a number of fields in the "Upload-Finished" request


        Backup uploads:
        - have "filename" meta field in chunks
        - have "filename" query params in the "Upload-Finished" request

        OR
        - have "task_file" field in the POST request data (a file)

        meta['filename'] is always required. It must be set to the "visible" file name or path

        Returns:
            response of the last request (the "Upload-Finished" one)
        """
        # "CVAT-TUS" protocol has 2 extra messages
        # query params are used only in the extra messages
        assert meta["filename"]

        if pbar is None:
            pbar = NullProgressReporter()

        file_size = filename.stat().st_size

        self._tus_start_upload(url, query_params=query_params)
        with self._uploading_task(pbar, file_size):
            real_filename = self._upload_file_data_with_tus(
                url=url, filename=filename, meta=meta, pbar=pbar, logger=logger
            )
        query_params["filename"] = real_filename
        return self._tus_finish_upload(url, query_params=query_params, fields=fields)

    @staticmethod
    def _uploading_task(pbar: ProgressReporter, total_size: int) -> AbstractContextManager[None]:
        return pbar.task(
            total=total_size, desc="Uploading data", unit_scale=True, unit="B", unit_divisor=1024
        )

    @staticmethod
    def _make_tus_uploader(api_client: ApiClient, url: str, **kwargs):
        # Add headers required by CVAT server
        headers = {}
        headers["Origin"] = api_client.configuration.host
        headers.update(api_client.get_common_headers())

        client = _TusClient(url, headers=headers)

        return _MyTusUploader(client=client, api_client=api_client, **kwargs)

    def _upload_file_data_with_tus(self, url, filename, *, meta=None, pbar, logger=None) -> str:
        with open(filename, "rb") as input_file:
            tus_uploader = self._make_tus_uploader(
                self._client.api_client,
                url=url.rstrip("/") + "/",
                metadata=meta,
                file_stream=StreamWithProgress(input_file, pbar),
                chunk_size=Uploader._CHUNK_SIZE,
                log_func=logger,
            )
            tus_uploader.upload()
            return tus_uploader.real_filename

    def _tus_start_upload(self, url, *, query_params=None):
        response = self._client.api_client.rest_client.POST(
            url,
            query_params=query_params,
            headers={
                "Upload-Start": "",
                **self._client.api_client.get_common_headers(),
            },
        )
        expect_status(202, response)
        return response

    def _tus_finish_upload(self, url, *, query_params=None, fields=None):
        response = self._client.api_client.rest_client.POST(
            url,
            headers={
                "Upload-Finish": "",
                **self._client.api_client.get_common_headers(),
            },
            query_params=query_params,
            post_params=fields,
        )
        expect_status(202, response)
        return response


class AnnotationUploader(Uploader):
    def upload_file_and_wait(
        self,
        endpoint: Endpoint,
        filename: Path,
        format_name: str,
        *,
        conv_mask_to_poly: Optional[bool] = None,
        url_params: Optional[dict[str, Any]] = None,
        pbar: Optional[ProgressReporter] = None,
        status_check_period: Optional[int] = None,
    ):
        url = self._client.api_map.make_endpoint_url(endpoint.path, kwsub=url_params)
        params = {"format": format_name, "filename": filename.name}
        response = self.upload_file(
            url, filename, pbar=pbar, query_params=params, meta={"filename": params["filename"]}
        )

        if conv_mask_to_poly is not None:
            params["conv_mask_to_poly"] = "true" if conv_mask_to_poly else "false"

        rq_id = json.loads(response.data).get("rq_id")
        assert rq_id, "The rq_id was not found in the response"

        self._client.wait_for_completion(rq_id, status_check_period=status_check_period)


class DatasetUploader(Uploader):
    def upload_file_and_wait(
        self,
        upload_endpoint: Endpoint,
        filename: Path,
        format_name: str,
        *,
        url_params: Optional[dict[str, Any]] = None,
        conv_mask_to_poly: Optional[bool] = None,
        pbar: Optional[ProgressReporter] = None,
        status_check_period: Optional[int] = None,
    ):
        url = self._client.api_map.make_endpoint_url(upload_endpoint.path, kwsub=url_params)
        params = {"format": format_name, "filename": filename.name}

        if conv_mask_to_poly is not None:
            params["conv_mask_to_poly"] = "true" if conv_mask_to_poly else "false"

        response = self.upload_file(
            url, filename, pbar=pbar, query_params=params, meta={"filename": params["filename"]}
        )
        rq_id = json.loads(response.data).get("rq_id")
        assert rq_id, "The rq_id was not found in the response"

        self._client.wait_for_completion(rq_id, status_check_period=status_check_period)


class DataUploader(Uploader):
    def __init__(self, client: Client, *, max_request_size: int = MAX_REQUEST_SIZE):
        super().__init__(client)
        self.max_request_size = max_request_size

    def upload_files(
        self,
        url: str,
        resources: list[Path],
        *,
        pbar: Optional[ProgressReporter] = None,
        **kwargs,
    ):
        bulk_file_groups, separate_files, total_size = self._split_files_by_requests(resources)

        if pbar is None:
            pbar = NullProgressReporter()

        if str(kwargs.get("sorting_method")).lower() == "predefined":
            # Request file ordering, because we reorder files to send more efficiently
            kwargs.setdefault("upload_file_order", [p.name for p in resources])

        with self._uploading_task(pbar, total_size):
            self._tus_start_upload(url)

            for group, group_size in bulk_file_groups:
                files = {}
                for i, filename in enumerate(group):
                    files[f"client_files[{i}]"] = (
                        os.fspath(filename),
                        filename.read_bytes(),
                    )
                response = self._client.api_client.rest_client.POST(
                    url,
                    post_params={"image_quality": kwargs["image_quality"], **files},
                    headers={
                        "Content-Type": "multipart/form-data",
                        "Upload-Multiple": "",
                        **self._client.api_client.get_common_headers(),
                    },
                )
                expect_status(200, response)

                pbar.advance(group_size)

            for filename in separate_files:
                self._upload_file_data_with_tus(
                    url,
                    filename,
                    meta={"filename": filename.name},
                    pbar=pbar,
                    logger=self._client.logger.debug,
                )

        return self._tus_finish_upload(url, fields=kwargs)

    def _split_files_by_requests(
        self, filenames: list[Path]
    ) -> tuple[list[tuple[list[Path], int]], list[Path], int]:
        bulk_files: dict[str, int] = {}
        separate_files: dict[str, int] = {}
        max_request_size = self.max_request_size

        # sort by size
        for filename in filenames:
            filename = filename.resolve()
            file_size = filename.stat().st_size
            if max_request_size < file_size:
                separate_files[filename] = file_size
            else:
                bulk_files[filename] = file_size

        total_size = sum(bulk_files.values()) + sum(separate_files.values())

        # group small files by requests
        bulk_file_groups: list[tuple[list[str], int]] = []
        current_group_size: int = 0
        current_group: list[str] = []
        for filename, file_size in bulk_files.items():
            if max_request_size < current_group_size + file_size:
                bulk_file_groups.append((current_group, current_group_size))
                current_group_size = 0
                current_group = []

            current_group.append(filename)
            current_group_size += file_size
        if current_group:
            bulk_file_groups.append((current_group, current_group_size))

        return bulk_file_groups, separate_files, total_size


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import contextlib
import itertools
import os
from collections.abc import Generator, Sequence
from typing import IO, Any, BinaryIO, Literal, TextIO, Union, overload


def filter_dict(
    d: dict[str, Any], *, keep: Sequence[str] = None, drop: Sequence[str] = None
) -> dict[str, Any]:
    return {k: v for k, v in d.items() if (not keep or k in keep) and (not drop or k not in drop)}


@overload
def atomic_writer(
    path: Union[os.PathLike, str], mode: Literal["wb"]
) -> contextlib.AbstractContextManager[BinaryIO]: ...


@overload
def atomic_writer(
    path: Union[os.PathLike, str], mode: Literal["w"], encoding: str = "UTF-8"
) -> contextlib.AbstractContextManager[TextIO]: ...


@contextlib.contextmanager
def atomic_writer(
    path: Union[os.PathLike, str], mode: Literal["w", "wb"], encoding: str = "UTF-8"
) -> Generator[IO, None, None]:
    """
    Returns a context manager that, when entered, returns a handle to a temporary
    file opened with the specified `mode` and `encoding`. If the context manager
    is exited via an exception, the temporary file is deleted. If the context manager
    is exited normally, the file is renamed to `path`.

    In other words, this function works like `open()`, but the file does not appear
    at the specified path until and unless the context manager is exited
    normally.
    """

    path_str = os.fspath(path)

    for counter in itertools.count():
        tmp_path = f"{path_str}.tmp{counter}"

        try:
            if mode == "w":
                tmp_file = open(tmp_path, "xt", encoding=encoding)
            elif mode == "wb":
                tmp_file = open(tmp_path, "xb")
            else:
                raise ValueError(f"Unsupported mode: {mode!r}")

            break
        except FileExistsError:
            pass  # try next counter value

    try:
        with tmp_file:
            yield tmp_file
        os.replace(tmp_path, path)
    except:
        os.unlink(tmp_path)
        raise


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from cvat_sdk.core.client import Client, Config, make_client
from cvat_sdk.version import VERSION as __version__


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\annotations.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from abc import ABC
from collections.abc import Sequence
from enum import Enum
from typing import Optional

from cvat_sdk import models
from cvat_sdk.core.proxies.model_proxy import _EntityT


class AnnotationUpdateAction(Enum):
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"


class AnnotationCrudMixin(ABC):
    # TODO: refactor

    @property
    def _put_annotations_data_param(self) -> str: ...

    def get_annotations(self: _EntityT) -> models.ILabeledData:
        (annotations, _) = self.api.retrieve_annotations(getattr(self, self._model_id_field))
        return annotations

    def set_annotations(self: _EntityT, data: models.ILabeledDataRequest):
        self.api.update_annotations(
            getattr(self, self._model_id_field), **{self._put_annotations_data_param: data}
        )

    def update_annotations(
        self: _EntityT,
        data: models.IPatchedLabeledDataRequest,
        *,
        action: AnnotationUpdateAction = AnnotationUpdateAction.UPDATE,
    ):
        self.api.partial_update_annotations(
            action=action.value,
            id=getattr(self, self._model_id_field),
            patched_labeled_data_request=data,
        )

    def remove_annotations(self: _EntityT, *, ids: Optional[Sequence[int]] = None):
        if ids:
            anns = self.get_annotations()

            if not isinstance(ids, set):
                ids = set(ids)

            anns_to_remove = models.PatchedLabeledDataRequest(
                tags=[models.LabeledImageRequest(**a.to_dict()) for a in anns.tags if a.id in ids],
                tracks=[
                    models.LabeledTrackRequest(**a.to_dict()) for a in anns.tracks if a.id in ids
                ],
                shapes=[
                    models.LabeledShapeRequest(**a.to_dict()) for a in anns.shapes if a.id in ids
                ],
            )

            self.update_annotations(anns_to_remove, action=AnnotationUpdateAction.DELETE)
        else:
            self.api.destroy_annotations(getattr(self, self._model_id_field))


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\issues.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from cvat_sdk.api_client import apis, models
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.proxies.model_proxy import (
    ModelCreateMixin,
    ModelDeleteMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)

_CommentEntityBase, _CommentRepoBase = build_model_bases(
    models.CommentRead, apis.CommentsApi, api_member_name="comments_api"
)


class Comment(
    models.ICommentRead,
    _CommentEntityBase,
    ModelUpdateMixin[models.IPatchedCommentWriteRequest],
    ModelDeleteMixin,
):
    _model_partial_update_arg = "patched_comment_write_request"


class CommentsRepo(
    _CommentRepoBase,
    ModelListMixin[Comment],
    ModelCreateMixin[Comment, models.ICommentWriteRequest],
    ModelRetrieveMixin[Comment],
):
    _entity_type = Comment


_IssueEntityBase, _IssueRepoBase = build_model_bases(
    models.IssueRead, apis.IssuesApi, api_member_name="issues_api"
)


class Issue(
    models.IIssueRead,
    _IssueEntityBase,
    ModelUpdateMixin[models.IPatchedIssueWriteRequest],
    ModelDeleteMixin,
):
    _model_partial_update_arg = "patched_issue_write_request"

    def get_comments(self) -> list[Comment]:
        return [
            Comment(self._client, m)
            for m in get_paginated_collection(
                self._client.api_client.comments_api.list_endpoint, issue_id=self.id
            )
        ]


class IssuesRepo(
    _IssueRepoBase,
    ModelListMixin[Issue],
    ModelCreateMixin[Issue, models.IIssueWriteRequest],
    ModelRetrieveMixin[Issue],
):
    _entity_type = Issue


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\jobs.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import mimetypes
from collections.abc import Sequence
from pathlib import Path
from typing import TYPE_CHECKING, Optional

from PIL import Image

from cvat_sdk.api_client import apis, models
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.progress import ProgressReporter
from cvat_sdk.core.proxies.annotations import AnnotationCrudMixin
from cvat_sdk.core.proxies.issues import Issue
from cvat_sdk.core.proxies.model_proxy import (
    ExportDatasetMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)
from cvat_sdk.core.uploading import AnnotationUploader

if TYPE_CHECKING:
    from _typeshed import StrPath

_JobEntityBase, _JobRepoBase = build_model_bases(
    models.JobRead, apis.JobsApi, api_member_name="jobs_api"
)


class Job(
    models.IJobRead,
    _JobEntityBase,
    ModelUpdateMixin[models.IPatchedJobWriteRequest],
    AnnotationCrudMixin,
    ExportDatasetMixin,
):
    _model_partial_update_arg = "patched_job_write_request"
    _put_annotations_data_param = "job_annotations_update_request"

    def import_annotations(
        self,
        format_name: str,
        filename: StrPath,
        *,
        conv_mask_to_poly: Optional[bool] = None,
        status_check_period: Optional[int] = None,
        pbar: Optional[ProgressReporter] = None,
    ):
        """
        Upload annotations for a job in the specified format (e.g. 'YOLO 1.1').
        """

        filename = Path(filename)

        AnnotationUploader(self._client).upload_file_and_wait(
            self.api.create_annotations_endpoint,
            filename,
            format_name,
            conv_mask_to_poly=conv_mask_to_poly,
            url_params={"id": self.id},
            pbar=pbar,
            status_check_period=status_check_period,
        )

        self._client.logger.info(f"Annotation file '{filename}' for job #{self.id} uploaded")

    def get_frame(
        self,
        frame_id: int,
        *,
        quality: Optional[str] = None,
    ) -> io.RawIOBase:
        (_, response) = self.api.retrieve_data(
            self.id, number=frame_id, quality=quality, type="frame"
        )
        return io.BytesIO(response.data)

    def get_preview(
        self,
    ) -> io.RawIOBase:
        (_, response) = self.api.retrieve_preview(self.id)
        return io.BytesIO(response.data)

    def download_frames(
        self,
        frame_ids: Sequence[int],
        *,
        image_extension: Optional[str] = None,
        outdir: StrPath = ".",
        quality: str = "original",
        filename_pattern: str = "frame_{frame_id:06d}{frame_ext}",
    ) -> Optional[list[Image.Image]]:
        """
        Download the requested frame numbers for a job and save images as outdir/filename_pattern
        """
        # TODO: add arg descriptions in schema

        outdir = Path(outdir)
        outdir.mkdir(parents=True, exist_ok=True)

        for frame_id in frame_ids:
            frame_bytes = self.get_frame(frame_id, quality=quality)

            im = Image.open(frame_bytes)
            if image_extension is None:
                mime_type = im.get_format_mimetype() or "image/jpg"
                im_ext = mimetypes.guess_extension(mime_type)

                # FIXME It is better to use meta information from the server
                # to determine the extension
                # replace '.jpe' or '.jpeg' with a more used '.jpg'
                if im_ext in (".jpe", ".jpeg", None):
                    im_ext = ".jpg"
            else:
                im_ext = f".{image_extension.strip('.')}"

            outfile = filename_pattern.format(frame_id=frame_id, frame_ext=im_ext)
            im.save(outdir / outfile)

    def get_meta(self) -> models.IDataMetaRead:
        (meta, _) = self.api.retrieve_data_meta(self.id)
        return meta

    def get_labels(self) -> list[models.ILabel]:
        return get_paginated_collection(
            self._client.api_client.labels_api.list_endpoint, job_id=self.id
        )

    def get_frames_info(self) -> list[models.IFrameMeta]:
        return self.get_meta().frames

    def remove_frames_by_ids(self, ids: Sequence[int]) -> None:
        self.api.partial_update_data_meta(
            self.id,
            patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                deleted_frames=ids
            ),
        )

    def get_issues(self) -> list[Issue]:
        return [
            Issue(self._client, m)
            for m in get_paginated_collection(
                self._client.api_client.issues_api.list_endpoint, job_id=self.id
            )
        ]


class JobsRepo(
    _JobRepoBase,
    ModelListMixin[Job],
    ModelRetrieveMixin[Job],
):
    _entity_type = Job


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\model_proxy.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import json
from abc import ABC
from collections.abc import Sequence
from copy import deepcopy
from pathlib import Path
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Generic,
    Literal,
    Optional,
    TypeVar,
    Union,
    overload,
)

from typing_extensions import Self

from cvat_sdk.api_client import exceptions
from cvat_sdk.api_client.model_utils import IModelData, ModelNormal, to_json
from cvat_sdk.core.downloading import Downloader
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.progress import ProgressReporter
from cvat_sdk.core.proxies.types import Location

if TYPE_CHECKING:
    from _typeshed import StrPath

    from cvat_sdk.core.client import Client

IModel = TypeVar("IModel", bound=IModelData)
ModelType = TypeVar("ModelType", bound=ModelNormal)
ApiType = TypeVar("ApiType")


class ModelProxy(ABC, Generic[ModelType, ApiType]):
    _client: Client

    @property
    def _api_member_name(self) -> str: ...

    def __init__(self, client: Client) -> None:
        self.__dict__["_client"] = client

    @classmethod
    def get_api(cls, client: Client) -> ApiType:
        return getattr(client.api_client, cls._api_member_name)

    @property
    def api(self) -> ApiType:
        return self.get_api(self._client)


class Entity(ModelProxy[ModelType, ApiType]):
    """
    Represents a single object. Implements related operations and provides read access
    to data members.
    """

    _model: ModelType

    def __init__(self, client: Client, model: ModelType) -> None:
        super().__init__(client)
        self.__dict__["_model"] = model

    @property
    def _model_id_field(self) -> str:
        return "id"

    def __getattr__(self, __name: str) -> Any:
        # NOTE: be aware of potential problems with throwing AttributeError from @property
        # in derived classes!
        # https://medium.com/@ceshine/python-debugging-pitfall-mixed-use-of-property-and-getattr-f89e0ede13f1
        return self._model[__name]

    def __str__(self) -> str:
        return str(self._model)

    def __repr__(self) -> str:
        return f"<{self.__class__.__name__}: id={getattr(self, self._model_id_field)}>"


class Repo(ModelProxy[ModelType, ApiType]):
    """
    Represents a collection of corresponding Entity objects.
    Implements group and management operations for entities.
    """

    _entity_type: type[Entity[ModelType, ApiType]]


### Utilities


def build_model_bases(
    mt: type[ModelType], at: type[ApiType], *, api_member_name: Optional[str] = None
) -> tuple[type[Entity[ModelType, ApiType]], type[Repo[ModelType, ApiType]]]:
    """
    Helps to remove code duplication in declarations of derived classes
    """

    class _EntityBase(Entity[ModelType, ApiType]):
        if api_member_name:
            _api_member_name = api_member_name

    class _RepoBase(Repo[ModelType, ApiType]):
        if api_member_name:
            _api_member_name = api_member_name

    return _EntityBase, _RepoBase


### CRUD mixins

_EntityT = TypeVar("_EntityT", bound=Entity)

#### Repo mixins


class ModelCreateMixin(Generic[_EntityT, IModel]):
    def create(self: Repo, spec: Union[dict[str, Any], IModel]) -> _EntityT:
        """
        Creates a new object on the server and returns the corresponding local object
        """

        (model, _) = self.api.create(spec)
        return self._entity_type(self._client, model)


class ModelRetrieveMixin(Generic[_EntityT]):
    def retrieve(self: Repo, obj_id: int) -> _EntityT:
        """
        Retrieves an object from the server by ID
        """

        (model, _) = self.api.retrieve(id=obj_id)
        return self._entity_type(self._client, model)


class ModelListMixin(Generic[_EntityT]):
    @overload
    def list(self: Repo, *, return_json: Literal[False] = False) -> list[_EntityT]: ...

    @overload
    def list(self: Repo, *, return_json: Literal[True] = False) -> list[Any]: ...

    def list(self: Repo, *, return_json: bool = False) -> list[Union[_EntityT, Any]]:
        """
        Retrieves all objects from the server and returns them in basic or JSON format.
        """

        results = get_paginated_collection(endpoint=self.api.list_endpoint, return_json=return_json)

        if return_json:
            return json.dumps(results)
        return [self._entity_type(self._client, model) for model in results]


class ModelBatchDeleteMixin(Repo):
    def remove_by_ids(self, ids: Sequence[int], /) -> None:
        """
        Delete a list of objects from the server, ignoring those which don't exist.
        """
        type_name = self._entity_type.__name__

        for object_id in ids:
            (_, response) = self.api.destroy(object_id, _check_status=False)

            if 200 <= response.status <= 299:
                self._client.logger.info(f"{type_name} #{object_id} deleted")
            elif response.status == 404:
                self._client.logger.info(f"{type_name} #{object_id} not found")
            else:
                self._client.logger.error(
                    f"Failed to delete {type_name} #{object_id}: "
                    f"{response.msg} (status {response.status})"
                )


#### Entity mixins


class ModelUpdateMixin(ABC, Generic[IModel]):
    @property
    def _model_partial_update_arg(self: Entity) -> str: ...

    def _export_update_fields(
        self: Entity, overrides: Optional[Union[dict[str, Any], IModel]] = None
    ) -> dict[str, Any]:
        # TODO: support field conversion and assignment updating
        # fields = to_json(self._model)

        if isinstance(overrides, ModelNormal):
            overrides = to_json(overrides)
        fields = deepcopy(overrides)

        return fields

    def fetch(self: Entity) -> Self:
        """
        Updates the current object from the server
        """

        # TODO: implement revision checking
        (self._model, _) = self.api.retrieve(id=getattr(self, self._model_id_field))
        return self

    def update(self: Entity, values: Union[dict[str, Any], IModel]) -> Self:
        """
        Commits model changes to the server

        The local object is updated from the server after this operation.
        """

        # TODO: implement revision checking
        self.api.partial_update(
            id=getattr(self, self._model_id_field),
            **{self._model_partial_update_arg: self._export_update_fields(values)},
        )

        # TODO: use the response model, once input and output models are same
        return self.fetch()


class ModelDeleteMixin:
    def remove(self: Entity) -> None:
        """
        Removes current object on the server
        """

        self.api.destroy(id=getattr(self, self._model_id_field))


class _ExportMixin(Generic[_EntityT]):
    def export(
        self,
        endpoint: Callable,
        filename: StrPath,
        *,
        pbar: Optional[ProgressReporter] = None,
        status_check_period: Optional[int] = None,
        location: Optional[Location] = None,
        cloud_storage_id: Optional[int] = None,
        **query_params,
    ) -> None:
        query_params = {
            **query_params,
            **({"location": location} if location else {}),
        }

        if location == Location.CLOUD_STORAGE:
            if not cloud_storage_id:
                raise ValueError(
                    f"Cloud storage ID must be specified when {location!r} location is used"
                )

            query_params["cloud_storage_id"] = cloud_storage_id

        local_downloading = (
            location == Location.LOCAL
            or not location
            and (not self.target_storage or self.target_storage.location.value == Location.LOCAL)
        )

        if not local_downloading:
            query_params["filename"] = str(filename)

        downloader = Downloader(self._client)
        export_request = downloader.prepare_file(
            endpoint,
            url_params={"id": self.id},
            query_params=query_params,
            status_check_period=status_check_period,
        )

        result_url = export_request.result_url

        if (
            location == Location.LOCAL
            and not result_url
            or location == Location.CLOUD_STORAGE
            and result_url
        ):
            raise exceptions.ServiceException(500, "Server handled export parameters incorrectly")
        elif not location and (
            (not self.target_storage or self.target_storage.location.value == Location.LOCAL)
            and not result_url
            or (
                self.target_storage
                and self.target_storage.location.value == Location.CLOUD_STORAGE
                and result_url
            )
        ):
            # SDK should not raise an exception here, because most likely
            # a SDK model was outdated while export finished successfully
            self._client.logger.warn(
                f"{self.__class__.__name__.title()} was outdated. "
                f"Use .fetch() method to obtain {self.__class__.__name__.lower()!r} actual version"
            )

        if result_url:
            downloader.download_file(result_url, output_path=Path(filename), pbar=pbar)


class ExportDatasetMixin(_ExportMixin):
    def export_dataset(
        self,
        format_name: str,
        filename: StrPath,
        *,
        pbar: Optional[ProgressReporter] = None,
        status_check_period: Optional[int] = None,
        include_images: bool = True,
        location: Optional[Location] = None,
        cloud_storage_id: Optional[int] = None,
    ) -> None:
        """
        Export a dataset in the specified format (e.g. 'YOLO 1.1').
        By default, a result file will be downloaded based on the default configuration.
        To force file downloading, pass `location=Location.LOCAL`.
        To save a file to a specific cloud storage, use the `location` and `cloud_storage_id` arguments.

        Args:
            filename (StrPath): A path to which a file will be downloaded
            status_check_period (int, optional): Sleep interval in seconds between status checks.
                Defaults to None, which means the `Config.status_check_period` is used.
            pbar (Optional[ProgressReporter], optional): Can be used to show a progress when downloading file locally.
                Defaults to None.
            location (Optional[Location], optional): Location to which a file will be uploaded.
                Can be Location.LOCAL or Location.CLOUD_STORAGE. Defaults to None.
            cloud_storage_id (Optional[int], optional): ID of cloud storage to which a file should be uploaded. Defaults to None.

        Raises:
            ValueError: When location is Location.CLOUD_STORAGE but no cloud_storage_id is passed
        """

        self.export(
            self.api.create_dataset_export_endpoint,
            filename,
            pbar=pbar,
            status_check_period=status_check_period,
            location=location,
            cloud_storage_id=cloud_storage_id,
            format=format_name,
            save_images=include_images,
        )

        self._client.logger.info(
            f"Dataset for {self.__class__.__name__.lower()} {self.id} has been downloaded to {filename}"
        )


class DownloadBackupMixin(_ExportMixin):
    def download_backup(
        self,
        filename: StrPath,
        *,
        status_check_period: int = None,
        pbar: Optional[ProgressReporter] = None,
        location: Optional[str] = None,
        cloud_storage_id: Optional[int] = None,
    ) -> None:
        """
        Create a resource backup and download it locally or upload to a cloud storage.
        By default, a result file will be downloaded based on the default configuration.
        To force file downloading, pass `location=Location.LOCAL`.
        To save a file to a specific cloud storage, use the `location` and `cloud_storage_id` arguments.

        Args:
            filename (StrPath): A path to which a file will be downloaded
            status_check_period (int, optional): Sleep interval in seconds between status checks.
                Defaults to None, which means the `Config.status_check_period` is used.
            pbar (Optional[ProgressReporter], optional): Can be used to show a progress when downloading file locally.
                Defaults to None.
            location (Optional[Location], optional): Location to which a file will be uploaded.
                Can be Location.LOCAL or Location.CLOUD_STORAGE. Defaults to None.
            cloud_storage_id (Optional[int], optional): ID of cloud storage to which a file should be uploaded. Defaults to None.

        Raises:
            ValueError: When location is Location.CLOUD_STORAGE but no cloud_storage_id is passed
        """

        self.export(
            self.api.create_backup_export_endpoint,
            filename,
            pbar=pbar,
            status_check_period=status_check_period,
            location=location,
            cloud_storage_id=cloud_storage_id,
        )

        self._client.logger.info(
            f"Backup for {self.__class__.__name__.lower()} {self.id} has been downloaded to {filename}"
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\organizations.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from cvat_sdk.api_client import apis, models
from cvat_sdk.core.proxies.model_proxy import (
    ModelCreateMixin,
    ModelDeleteMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)

_OrganizationEntityBase, _OrganizationRepoBase = build_model_bases(
    models.OrganizationRead, apis.OrganizationsApi, api_member_name="organizations_api"
)


class Organization(
    models.IOrganizationRead,
    _OrganizationEntityBase,
    ModelUpdateMixin[models.IPatchedOrganizationWriteRequest],
    ModelDeleteMixin,
):
    _model_partial_update_arg = "patched_organization_write_request"


class OrganizationsRepo(
    _OrganizationRepoBase,
    ModelCreateMixin[Organization, models.IOrganizationWriteRequest],
    ModelListMixin[Organization],
    ModelRetrieveMixin[Organization],
):
    _entity_type = Organization


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\projects.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import json
from pathlib import Path
from typing import TYPE_CHECKING, Optional

from cvat_sdk.api_client import apis, models
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.progress import ProgressReporter
from cvat_sdk.core.proxies.model_proxy import (
    DownloadBackupMixin,
    ExportDatasetMixin,
    ModelBatchDeleteMixin,
    ModelCreateMixin,
    ModelDeleteMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)
from cvat_sdk.core.proxies.tasks import Task
from cvat_sdk.core.uploading import DatasetUploader, Uploader

if TYPE_CHECKING:
    from _typeshed import StrPath

_ProjectEntityBase, _ProjectRepoBase = build_model_bases(
    models.ProjectRead, apis.ProjectsApi, api_member_name="projects_api"
)


class Project(
    _ProjectEntityBase,
    models.IProjectRead,
    ModelUpdateMixin[models.IPatchedProjectWriteRequest],
    ModelDeleteMixin,
    ExportDatasetMixin,
    DownloadBackupMixin,
):
    _model_partial_update_arg = "patched_project_write_request"

    def import_dataset(
        self,
        format_name: str,
        filename: StrPath,
        *,
        conv_mask_to_poly: Optional[bool] = None,
        status_check_period: Optional[int] = None,
        pbar: Optional[ProgressReporter] = None,
    ):
        """
        Import dataset for a project in the specified format (e.g. 'YOLO 1.1').
        """

        filename = Path(filename)

        DatasetUploader(self._client).upload_file_and_wait(
            self.api.create_dataset_endpoint,
            filename,
            format_name,
            url_params={"id": self.id},
            conv_mask_to_poly=conv_mask_to_poly,
            pbar=pbar,
            status_check_period=status_check_period,
        )

        self._client.logger.info(f"Annotation file '{filename}' for project #{self.id} uploaded")

    def get_annotations(self) -> models.ILabeledData:
        (annotations, _) = self.api.retrieve_annotations(self.id)
        return annotations

    def get_tasks(self) -> list[Task]:
        return [
            Task(self._client, m)
            for m in get_paginated_collection(
                self._client.api_client.tasks_api.list_endpoint, project_id=self.id
            )
        ]

    def get_labels(self) -> list[models.ILabel]:
        return get_paginated_collection(
            self._client.api_client.labels_api.list_endpoint, project_id=self.id
        )

    def get_preview(
        self,
    ) -> io.RawIOBase:
        (_, response) = self.api.retrieve_preview(self.id)
        return io.BytesIO(response.data)


class ProjectsRepo(
    _ProjectRepoBase,
    ModelCreateMixin[Project, models.IProjectWriteRequest],
    ModelListMixin[Project],
    ModelRetrieveMixin[Project],
    ModelBatchDeleteMixin,
):
    _entity_type = Project

    def create_from_dataset(
        self,
        spec: models.IProjectWriteRequest,
        *,
        dataset_path: str = "",
        dataset_format: str = "CVAT XML 1.1",
        status_check_period: int = None,
        pbar: Optional[ProgressReporter] = None,
        conv_mask_to_poly: Optional[bool] = None,
    ) -> Project:
        """
        Create a new project with the given name and labels JSON and
        add the files to it.

        Returns: id of the created project
        """
        project = self.create(spec=spec)
        self._client.logger.info("Created project ID: %s NAME: %s", project.id, project.name)

        if dataset_path:
            project.import_dataset(
                format_name=dataset_format,
                filename=dataset_path,
                pbar=pbar,
                status_check_period=status_check_period,
                conv_mask_to_poly=conv_mask_to_poly,
            )

        project.fetch()
        return project

    def create_from_backup(
        self,
        filename: StrPath,
        *,
        status_check_period: int = None,
        pbar: Optional[ProgressReporter] = None,
    ) -> Project:
        """
        Import a project from a backup file
        """

        filename = Path(filename)

        if status_check_period is None:
            status_check_period = self._client.config.status_check_period

        params = {"filename": filename.name}
        url = self._client.api_map.make_endpoint_url(self.api.create_backup_endpoint.path)

        uploader = Uploader(self._client)
        response = uploader.upload_file(
            url,
            filename,
            meta=params,
            query_params=params,
            pbar=pbar,
            logger=self._client.logger.debug,
        )

        rq_id = json.loads(response.data).get("rq_id")
        assert rq_id, "The rq_id was not found in server response"
        request, response = self._client.wait_for_completion(
            rq_id, status_check_period=status_check_period
        )

        project_id = request.result_id
        self._client.logger.info(
            f"Project has been imported successfully. Project ID: {project_id}"
        )

        return self.retrieve(project_id)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\tasks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import io
import json
import mimetypes
import os
import shutil
from collections.abc import Sequence
from enum import Enum
from pathlib import Path
from time import sleep
from typing import TYPE_CHECKING, Any, Optional

from PIL import Image

from cvat_sdk.api_client import apis, exceptions, models
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.progress import ProgressReporter
from cvat_sdk.core.proxies.annotations import AnnotationCrudMixin
from cvat_sdk.core.proxies.jobs import Job
from cvat_sdk.core.proxies.model_proxy import (
    DownloadBackupMixin,
    ExportDatasetMixin,
    ModelBatchDeleteMixin,
    ModelCreateMixin,
    ModelDeleteMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)
from cvat_sdk.core.uploading import AnnotationUploader, DataUploader, Uploader
from cvat_sdk.core.utils import filter_dict

if TYPE_CHECKING:
    from _typeshed import StrPath, SupportsWrite


class ResourceType(Enum):
    LOCAL = 0
    SHARE = 1
    REMOTE = 2

    def __str__(self):
        return self.name.lower()

    def __repr__(self):
        return str(self)


_TaskEntityBase, _TaskRepoBase = build_model_bases(
    models.TaskRead, apis.TasksApi, api_member_name="tasks_api"
)


class Task(
    _TaskEntityBase,
    models.ITaskRead,
    ModelUpdateMixin[models.IPatchedTaskWriteRequest],
    ModelDeleteMixin,
    AnnotationCrudMixin,
    ExportDatasetMixin,
    DownloadBackupMixin,
):
    _model_partial_update_arg = "patched_task_write_request"
    _put_annotations_data_param = "task_annotations_update_request"

    def upload_data(
        self,
        resources: Sequence[StrPath],
        *,
        resource_type: ResourceType = ResourceType.LOCAL,
        pbar: Optional[ProgressReporter] = None,
        params: Optional[dict[str, Any]] = None,
        wait_for_completion: bool = True,
        status_check_period: Optional[int] = None,
    ) -> None:
        """
        Add local, remote, or shared files to an existing task.
        """
        params = params or {}

        data = {"image_quality": 70}

        data.update(
            filter_dict(
                params,
                keep=[
                    "chunk_size",
                    "copy_data",
                    "image_quality",
                    "sorting_method",
                    "start_frame",
                    "stop_frame",
                    "use_cache",
                    "use_zip_chunks",
                    "job_file_mapping",
                    "filename_pattern",
                    "cloud_storage_id",
                    "server_files_exclude",
                    "validation_params",
                ],
            )
        )
        if params.get("frame_step") is not None:
            data["frame_filter"] = f"step={params.get('frame_step')}"

        if resource_type in [ResourceType.REMOTE, ResourceType.SHARE]:

            str_resources = list(map(os.fspath, resources))

            if resource_type is ResourceType.REMOTE:
                data["remote_files"] = str_resources
            elif resource_type is ResourceType.SHARE:
                data["server_files"] = str_resources

            result, _ = self.api.create_data(
                self.id,
                data_request=models.DataRequest(**data),
            )
            rq_id = result.rq_id
        elif resource_type == ResourceType.LOCAL:
            url = self._client.api_map.make_endpoint_url(
                self.api.create_data_endpoint.path, kwsub={"id": self.id}
            )

            response = DataUploader(self._client).upload_files(
                url, list(map(Path, resources)), pbar=pbar, **data
            )
            response = json.loads(response.data)
            rq_id = response.get("rq_id")
            assert rq_id, "The rq_id param was not found in the response"

        if wait_for_completion:
            if status_check_period is None:
                status_check_period = self._client.config.status_check_period

            self._client.logger.info("Awaiting for task %s creation...", self.id)
            while True:
                sleep(status_check_period)
                request_details, response = self._client.api_client.requests_api.retrieve(rq_id)
                status, message = request_details.status, request_details.message

                self._client.logger.info(
                    "Task %s creation status: %s (message=%s)",
                    self.id,
                    status,
                    message,
                )

                if status.value == models.RequestStatus.allowed_values[("value",)]["FINISHED"]:
                    break

                elif status.value == models.RequestStatus.allowed_values[("value",)]["FAILED"]:
                    raise exceptions.ApiException(status=status, reason=message, http_resp=response)

            self.fetch()

    def import_annotations(
        self,
        format_name: str,
        filename: StrPath,
        *,
        conv_mask_to_poly: Optional[bool] = None,
        status_check_period: Optional[int] = None,
        pbar: Optional[ProgressReporter] = None,
    ):
        """
        Upload annotations for a task in the specified format (e.g. 'YOLO 1.1').
        """

        filename = Path(filename)

        AnnotationUploader(self._client).upload_file_and_wait(
            self.api.create_annotations_endpoint,
            filename,
            format_name,
            url_params={"id": self.id},
            conv_mask_to_poly=conv_mask_to_poly,
            pbar=pbar,
            status_check_period=status_check_period,
        )

        self._client.logger.info(f"Annotation file '{filename}' for task #{self.id} uploaded")

    def get_frame(
        self,
        frame_id: int,
        *,
        quality: Optional[str] = None,
    ) -> io.RawIOBase:
        params = {}
        if quality:
            params["quality"] = quality
        (_, response) = self.api.retrieve_data(self.id, number=frame_id, **params, type="frame")
        return io.BytesIO(response.data)

    def get_preview(
        self,
    ) -> io.RawIOBase:
        (_, response) = self.api.retrieve_preview(self.id)
        return io.BytesIO(response.data)

    def download_chunk(
        self,
        chunk_id: int,
        output_file: SupportsWrite[bytes],
        *,
        quality: Optional[str] = None,
    ) -> None:
        params = {}
        if quality:
            params["quality"] = quality
        (_, response) = self.api.retrieve_data(
            self.id, number=chunk_id, **params, type="chunk", _parse_response=False
        )

        with response:
            shutil.copyfileobj(response, output_file)

    def download_frames(
        self,
        frame_ids: Sequence[int],
        *,
        image_extension: Optional[str] = None,
        outdir: StrPath = ".",
        quality: str = "original",
        filename_pattern: str = "frame_{frame_id:06d}{frame_ext}",
    ) -> Optional[list[Image.Image]]:
        """
        Download the requested frame numbers for a task and save images as outdir/filename_pattern
        """

        outdir = Path(outdir)
        outdir.mkdir(parents=True, exist_ok=True)

        for frame_id in frame_ids:
            frame_bytes = self.get_frame(frame_id, quality=quality)

            im = Image.open(frame_bytes)
            if image_extension is None:
                mime_type = im.get_format_mimetype() or "image/jpg"
                im_ext = mimetypes.guess_extension(mime_type)

                # FIXME It is better to use meta information from the server
                # to determine the extension
                # replace '.jpe' or '.jpeg' with a more used '.jpg'
                if im_ext in (".jpe", ".jpeg", None):
                    im_ext = ".jpg"
            else:
                im_ext = f".{image_extension.strip('.')}"

            outfile = filename_pattern.format(frame_id=frame_id, frame_ext=im_ext)
            im.save(outdir / outfile)

    def get_jobs(self) -> list[Job]:
        return [
            Job(self._client, model=m)
            for m in get_paginated_collection(
                self._client.api_client.jobs_api.list_endpoint, task_id=self.id
            )
        ]

    def get_meta(self) -> models.IDataMetaRead:
        (meta, _) = self.api.retrieve_data_meta(self.id)
        return meta

    def get_labels(self) -> list[models.ILabel]:
        return get_paginated_collection(
            self._client.api_client.labels_api.list_endpoint, task_id=self.id
        )

    def get_frames_info(self) -> list[models.IFrameMeta]:
        return self.get_meta().frames

    def remove_frames_by_ids(self, ids: Sequence[int]) -> None:
        self.api.partial_update_data_meta(
            self.id,
            patched_data_meta_write_request=models.PatchedDataMetaWriteRequest(deleted_frames=ids),
        )


class TasksRepo(
    _TaskRepoBase,
    ModelCreateMixin[Task, models.ITaskWriteRequest],
    ModelRetrieveMixin[Task],
    ModelListMixin[Task],
    ModelBatchDeleteMixin,
):
    _entity_type = Task

    def create_from_data(
        self,
        spec: models.ITaskWriteRequest,
        resources: Sequence[StrPath],
        *,
        resource_type: ResourceType = ResourceType.LOCAL,
        data_params: Optional[dict[str, Any]] = None,
        annotation_path: str = "",
        annotation_format: str = "CVAT XML 1.1",
        status_check_period: int = None,
        pbar: Optional[ProgressReporter] = None,
    ) -> Task:
        """
        Create a new task with the given name and labels JSON and
        add the files to it.

        Returns: id of the created task
        """
        if getattr(spec, "project_id", None) and getattr(spec, "labels", None):
            raise exceptions.ApiValueError(
                "Can't set labels to a task inside a project. "
                "Tasks inside a project use project's labels.",
                ["labels"],
            )

        task = self.create(spec=spec)
        self._client.logger.info("Created task ID: %s NAME: %s", task.id, task.name)

        task.upload_data(
            resource_type=resource_type,
            resources=resources,
            pbar=pbar,
            params=data_params,
            wait_for_completion=True,
            status_check_period=status_check_period,
        )

        if annotation_path:
            task.import_annotations(annotation_format, annotation_path, pbar=pbar)

        task.fetch()

        return task

    # This is a backwards compatibility wrapper to support calls which pass
    # the task_ids parameter by keyword (the base class implementation is generic,
    # so it doesn't support this).
    # pylint: disable-next=arguments-differ
    def remove_by_ids(self, task_ids: Sequence[int]) -> None:
        """
        Delete a list of tasks, ignoring those which don't exist.
        """

        super().remove_by_ids(task_ids)

    def create_from_backup(
        self,
        filename: StrPath,
        *,
        status_check_period: int = None,
        pbar: Optional[ProgressReporter] = None,
    ) -> Task:
        """
        Import a task from a backup file
        """

        filename = Path(filename)

        if status_check_period is None:
            status_check_period = self._client.config.status_check_period

        params = {"filename": filename.name}
        url = self._client.api_map.make_endpoint_url(self.api.create_backup_endpoint.path)
        uploader = Uploader(self._client)
        response = uploader.upload_file(
            url,
            filename,
            meta=params,
            query_params=params,
            pbar=pbar,
            logger=self._client.logger.debug,
        )

        rq_id = json.loads(response.data).get("rq_id")
        assert rq_id, "The rq_id was not found in server response"

        request, response = self._client.wait_for_completion(
            rq_id, status_check_period=status_check_period
        )

        task_id = request.result_id
        self._client.logger.info(f"Task has been imported successfully. Task ID: {task_id}")

        return self.retrieve(task_id)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\types.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import enum

from cvat_sdk.models import LocationEnum as RawLocationEnum


class Location(str, enum.Enum):
    LOCAL = "local"
    CLOUD_STORAGE = "cloud_storage"

    def __str__(self) -> str:
        return self.value


assert all(
    [getattr(Location, k) == v for k, v in RawLocationEnum.allowed_values[("value",)].items()]
), 'SDK "Location" enum should be updated'


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\users.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

from cvat_sdk.api_client import apis, models
from cvat_sdk.core.proxies.model_proxy import (
    ModelDeleteMixin,
    ModelListMixin,
    ModelRetrieveMixin,
    ModelUpdateMixin,
    build_model_bases,
)

_UserEntityBase, _UserRepoBase = build_model_bases(
    models.User, apis.UsersApi, api_member_name="users_api"
)


class User(
    models.IUser, _UserEntityBase, ModelUpdateMixin[models.IPatchedUserRequest], ModelDeleteMixin
):
    _model_partial_update_arg = "patched_user_request"


class UsersRepo(
    _UserRepoBase,
    ModelListMixin[User],
    ModelRetrieveMixin[User],
):
    _entity_type = User

    def retrieve_current_user(self) -> User:
        return User(self._client, self.api.retrieve_self()[0])


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\core\proxies\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\datasets\caching.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import base64
import json
import shutil
from abc import ABCMeta, abstractmethod
from collections.abc import Mapping
from enum import Enum, auto
from pathlib import Path
from typing import Any, Callable, TypeVar, Union, cast

from attrs import define

import cvat_sdk.models as models
from cvat_sdk.api_client.model_utils import OpenApiModel, to_json
from cvat_sdk.core.client import Client
from cvat_sdk.core.proxies.projects import Project
from cvat_sdk.core.proxies.tasks import Task
from cvat_sdk.core.utils import atomic_writer


class UpdatePolicy(Enum):
    """
    Defines policies for when the local cache is updated from the CVAT server.
    """

    IF_MISSING_OR_STALE = auto()
    """
    Update the cache whenever cached data is missing or the server has a newer version.
    """

    NEVER = auto()
    """
    Never update the cache. If an operation requires data that is not cached,
    it will fail.

    No network access will be performed if this policy is used.
    """


_CacheObject = dict[str, Any]


class _CacheObjectModel(metaclass=ABCMeta):
    @abstractmethod
    def dump(self) -> _CacheObject: ...

    @classmethod
    @abstractmethod
    def load(cls, obj: _CacheObject): ...


_ModelType = TypeVar("_ModelType", bound=Union[OpenApiModel, _CacheObjectModel])


class CacheManager(metaclass=ABCMeta):
    def __init__(self, client: Client) -> None:
        self._client = client
        self._logger = client.logger

        self._server_dir = client.config.cache_dir / f"servers/{self.server_dir_name}"

    @property
    def server_dir_name(self) -> str:
        # Base64-encode the name to avoid FS-unsafe characters (like slashes)
        return base64.urlsafe_b64encode(self._client.api_map.host.encode()).rstrip(b"=").decode()

    def task_dir(self, task_id: int) -> Path:
        return self._server_dir / f"tasks/{task_id}"

    def task_json_path(self, task_id: int) -> Path:
        return self.task_dir(task_id) / "task.json"

    def chunk_dir(self, task_id: int) -> Path:
        return self.task_dir(task_id) / "chunks"

    def project_dir(self, project_id: int) -> Path:
        return self._server_dir / f"projects/{project_id}"

    def project_json_path(self, project_id: int) -> Path:
        return self.project_dir(project_id) / "project.json"

    def _load_object(self, path: Path) -> _CacheObject:
        with open(path, "rb") as f:
            return json.load(f)

    def _save_object(self, path: Path, obj: _CacheObject) -> None:
        with atomic_writer(path, "w", encoding="UTF-8") as f:
            json.dump(obj, f, indent=4)
            print(file=f)  # add final newline

    def _deserialize_model(self, obj: _CacheObject, model_type: _ModelType) -> _ModelType:
        if issubclass(model_type, OpenApiModel):
            return cast(OpenApiModel, model_type)._new_from_openapi_data(**obj)
        elif issubclass(model_type, _CacheObjectModel):
            return cast(_CacheObjectModel, model_type).load(obj)
        else:
            raise NotImplementedError("Unexpected model type")

    def _serialize_model(self, model: _ModelType) -> _CacheObject:
        if isinstance(model, OpenApiModel):
            return to_json(model)
        elif isinstance(model, _CacheObjectModel):
            return model.dump()
        else:
            raise NotImplementedError("Unexpected model type")

    def load_model(self, path: Path, model_type: type[_ModelType]) -> _ModelType:
        return self._deserialize_model(self._load_object(path), model_type)

    def save_model(self, path: Path, model: _ModelType) -> None:
        return self._save_object(path, self._serialize_model(model))

    @abstractmethod
    def retrieve_task(self, task_id: int) -> Task: ...

    @abstractmethod
    def ensure_task_model(
        self,
        task_id: int,
        filename: str,
        model_type: type[_ModelType],
        downloader: Callable[[], _ModelType],
        model_description: str,
    ) -> _ModelType: ...

    @abstractmethod
    def ensure_chunk(self, task: Task, chunk_index: int) -> None: ...

    @abstractmethod
    def retrieve_project(self, project_id: int) -> Project: ...


class _CacheManagerOnline(CacheManager):
    def retrieve_task(self, task_id: int) -> Task:
        self._logger.info(f"Fetching task {task_id}...")
        task = self._client.tasks.retrieve(task_id)

        self._initialize_task_dir(task)
        return task

    def _initialize_task_dir(self, task: Task) -> None:
        task_dir = self.task_dir(task.id)
        task_json_path = self.task_json_path(task.id)

        try:
            saved_task = self.load_model(task_json_path, _OfflineTaskModel)
        except Exception:
            self._logger.info(f"Task {task.id} is not yet cached or the cache is corrupted")

            # If the cache was corrupted, the directory might already be there; clear it.
            if task_dir.exists():
                shutil.rmtree(task_dir)
        else:
            if saved_task.api_model.updated_date < task.updated_date:
                self._logger.info(
                    f"Task {task.id} has been updated on the server since it was cached; purging the cache"
                )
                shutil.rmtree(task_dir)

        task_dir.mkdir(exist_ok=True, parents=True)
        self.save_model(task_json_path, _OfflineTaskModel.from_entity(task))

    def ensure_task_model(
        self,
        task_id: int,
        filename: str,
        model_type: type[_ModelType],
        downloader: Callable[[], _ModelType],
        model_description: str,
    ) -> _ModelType:
        path = self.task_dir(task_id) / filename

        try:
            model = self.load_model(path, model_type)
            self._logger.info(f"Loaded {model_description} from cache")
            return model
        except FileNotFoundError:
            pass
        except Exception:
            self._logger.warning(f"Failed to load {model_description} from cache", exc_info=True)

        self._logger.info(f"Downloading {model_description}...")
        model = downloader()
        self._logger.info(f"Downloaded {model_description}")

        self.save_model(path, model)

        return model

    def ensure_chunk(self, task: Task, chunk_index: int) -> None:
        chunk_path = self.chunk_dir(task.id) / f"{chunk_index}.zip"
        if chunk_path.exists():
            return  # already downloaded previously

        self._logger.info(f"Downloading chunk #{chunk_index}...")

        with atomic_writer(chunk_path, "wb") as chunk_file:
            task.download_chunk(chunk_index, chunk_file, quality="original")

    def retrieve_project(self, project_id: int) -> Project:
        self._logger.info(f"Fetching project {project_id}...")
        project = self._client.projects.retrieve(project_id)

        project_dir = self.project_dir(project_id)
        project_dir.mkdir(parents=True, exist_ok=True)
        project_json_path = self.project_json_path(project_id)

        # There are currently no files cached alongside project.json,
        # so we don't need to check if we need to purge them.

        self.save_model(project_json_path, _OfflineProjectModel.from_entity(project))

        return project


class _CacheManagerOffline(CacheManager):
    def retrieve_task(self, task_id: int) -> Task:
        self._logger.info(f"Retrieving task {task_id} from cache...")
        cached_model = self.load_model(self.task_json_path(task_id), _OfflineTaskModel)
        return _OfflineTaskProxy(self._client, cached_model, cache_manager=self)

    def ensure_task_model(
        self,
        task_id: int,
        filename: str,
        model_type: type[_ModelType],
        downloader: Callable[[], _ModelType],
        model_description: str,
    ) -> _ModelType:
        self._logger.info(f"Loading {model_description} from cache...")
        return self.load_model(self.task_dir(task_id) / filename, model_type)

    def ensure_chunk(self, task: Task, chunk_index: int) -> None:
        chunk_path = self.chunk_dir(task.id) / f"{chunk_index}.zip"

        if not chunk_path.exists():
            raise FileNotFoundError(f"Chunk {chunk_index} of task {task.id} is not cached")

    def retrieve_project(self, project_id: int) -> Project:
        self._logger.info(f"Retrieving project {project_id} from cache...")
        cached_model = self.load_model(self.project_json_path(project_id), _OfflineProjectModel)
        return _OfflineProjectProxy(self._client, cached_model, cache_manager=self)


@define
class _OfflineTaskModel(_CacheObjectModel):
    api_model: models.ITaskRead
    labels: list[models.ILabel]

    def dump(self) -> _CacheObject:
        return {
            "model": to_json(self.api_model),
            "labels": to_json(self.labels),
        }

    @classmethod
    def load(cls, obj: _CacheObject):
        return cls(
            api_model=models.TaskRead._from_openapi_data(**obj["model"]),
            labels=[models.Label._from_openapi_data(**label) for label in obj["labels"]],
        )

    @classmethod
    def from_entity(cls, entity: Task):
        return cls(
            api_model=entity._model,
            labels=entity.get_labels(),
        )


class _OfflineTaskProxy(Task):
    def __init__(
        self, client: Client, cached_model: _OfflineTaskModel, *, cache_manager: CacheManager
    ) -> None:
        super().__init__(client, cached_model.api_model)
        self._offline_model = cached_model
        self._cache_manager = cache_manager

    def get_labels(self) -> list[models.ILabel]:
        return self._offline_model.labels


@define
class _OfflineProjectModel(_CacheObjectModel):
    api_model: models.IProjectRead
    task_ids: list[int]
    labels: list[models.ILabel]

    def dump(self) -> _CacheObject:
        return {
            "model": to_json(self.api_model),
            "tasks": self.task_ids,
            "labels": to_json(self.labels),
        }

    @classmethod
    def load(cls, obj: _CacheObject):
        return cls(
            api_model=models.ProjectRead._from_openapi_data(**obj["model"]),
            task_ids=obj["tasks"],
            labels=[models.Label._from_openapi_data(**label) for label in obj["labels"]],
        )

    @classmethod
    def from_entity(cls, entity: Project):
        return cls(
            api_model=entity._model,
            task_ids=[t.id for t in entity.get_tasks()],
            labels=entity.get_labels(),
        )


class _OfflineProjectProxy(Project):
    def __init__(
        self, client: Client, cached_model: _OfflineProjectModel, *, cache_manager: CacheManager
    ) -> None:
        super().__init__(client, cached_model.api_model)
        self._offline_model = cached_model
        self._cache_manager = cache_manager

    def get_tasks(self) -> list[Task]:
        return [self._cache_manager.retrieve_task(t) for t in self._offline_model.task_ids]

    def get_labels(self) -> list[models.ILabel]:
        return self._offline_model.labels


_CACHE_MANAGER_CLASSES: Mapping[UpdatePolicy, type[CacheManager]] = {
    UpdatePolicy.IF_MISSING_OR_STALE: _CacheManagerOnline,
    UpdatePolicy.NEVER: _CacheManagerOffline,
}


def make_cache_manager(client: Client, update_policy: UpdatePolicy) -> CacheManager:
    return _CACHE_MANAGER_CLASSES[update_policy](client)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\datasets\common.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import abc
from typing import Optional

import attrs
import attrs.validators
import PIL.Image

import cvat_sdk.core
import cvat_sdk.core.exceptions
import cvat_sdk.models as models


class UnsupportedDatasetError(cvat_sdk.core.exceptions.CvatSdkException):
    pass


@attrs.frozen
class FrameAnnotations:
    """
    Contains annotations that pertain to a single frame.
    """

    tags: list[models.LabeledImage] = attrs.Factory(list)
    shapes: list[models.LabeledShape] = attrs.Factory(list)


class MediaElement(metaclass=abc.ABCMeta):
    """
    The media part of a dataset sample.
    """

    @abc.abstractmethod
    def load_image(self) -> PIL.Image.Image:
        """
        Loads the media data and returns it as a PIL Image object.
        """
        ...


@attrs.frozen
class Sample:
    """
    Represents an element of a dataset.
    """

    frame_index: int
    """Index of the corresponding frame in its task."""

    frame_name: str
    """File name of the frame in its task."""

    annotations: Optional[FrameAnnotations]
    """
    Annotations belonging to the frame.

    Will be None if the dataset was created without loading annotations.
    """

    media: MediaElement
    """Media data of the frame."""


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\datasets\task_dataset.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from __future__ import annotations

import zipfile
from collections.abc import Iterable, Sequence
from concurrent.futures import ThreadPoolExecutor

import PIL.Image

import cvat_sdk.core
import cvat_sdk.core.exceptions
import cvat_sdk.models as models
from cvat_sdk.datasets.caching import CacheManager, UpdatePolicy, make_cache_manager
from cvat_sdk.datasets.common import FrameAnnotations, MediaElement, Sample, UnsupportedDatasetError

_NUM_DOWNLOAD_THREADS = 4


class TaskDataset:
    """
    Represents a task on a CVAT server as a collection of samples.

    Each sample corresponds to one frame in the task, and provides access to
    the corresponding annotations and media data. Deleted frames are omitted.

    This class caches all data and annotations for the task on the local file system
    during construction.

    Limitations:

    * Only tasks with image (not video) data are supported at the moment.
    * Track annotations are currently not accessible.
    """

    class _TaskMediaElement(MediaElement):
        def __init__(self, dataset: TaskDataset, frame_index: int) -> None:
            self._dataset = dataset
            self._frame_index = frame_index

        def load_image(self) -> PIL.Image.Image:
            return self._dataset._load_frame_image(self._frame_index)

    def __init__(
        self,
        client: cvat_sdk.core.Client,
        task_id: int,
        *,
        update_policy: UpdatePolicy = UpdatePolicy.IF_MISSING_OR_STALE,
        load_annotations: bool = True,
    ) -> None:
        """
        Creates a dataset corresponding to the task with ID `task_id` on the
        server that `client` is connected to.

        `update_policy` determines when and if the local cache will be updated.

        `load_annotations` determines whether annotations will be loaded from
        the server. If set to False, the `annotations` field in the samples will
        be set to None.
        """

        self._logger = client.logger

        cache_manager = make_cache_manager(client, update_policy)
        self._task = cache_manager.retrieve_task(task_id)

        if not self._task.size or not self._task.data_chunk_size:
            raise UnsupportedDatasetError("The task has no data")

        if self._task.data_original_chunk_type != "imageset":
            raise UnsupportedDatasetError(
                f"{self.__class__.__name__} only supports tasks with image chunks;"
                f" current chunk type is {self._task.data_original_chunk_type!r}"
            )

        self._logger.info("Fetching labels...")
        self._labels = tuple(self._task.get_labels())

        data_meta = cache_manager.ensure_task_model(
            self._task.id,
            "data_meta.json",
            models.DataMetaRead,
            self._task.get_meta,
            "data metadata",
        )

        active_frame_indexes = set(range(self._task.size)) - set(data_meta.deleted_frames)

        self._logger.info("Downloading chunks...")

        self._chunk_dir = cache_manager.chunk_dir(task_id)
        self._chunk_dir.mkdir(exist_ok=True, parents=True)

        needed_chunks = {index // self._task.data_chunk_size for index in active_frame_indexes}

        with ThreadPoolExecutor(_NUM_DOWNLOAD_THREADS) as pool:

            def ensure_chunk(chunk_index):
                cache_manager.ensure_chunk(self._task, chunk_index)

            for _ in pool.map(ensure_chunk, sorted(needed_chunks)):
                # just need to loop through all results so that any exceptions are propagated
                pass

        self._logger.info("All chunks downloaded")

        if load_annotations:
            self._load_annotations(cache_manager, sorted(active_frame_indexes))
        else:
            self._frame_annotations = {
                frame_index: None for frame_index in sorted(active_frame_indexes)
            }

        # TODO: tracks?

        self._samples = [
            Sample(
                frame_index=k,
                frame_name=data_meta.frames[k].name,
                annotations=v,
                media=self._TaskMediaElement(self, k),
            )
            for k, v in self._frame_annotations.items()
        ]

    def _load_annotations(self, cache_manager: CacheManager, frame_indexes: Iterable[int]) -> None:
        annotations = cache_manager.ensure_task_model(
            self._task.id,
            "annotations.json",
            models.LabeledData,
            self._task.get_annotations,
            "annotations",
        )

        self._frame_annotations = {frame_index: FrameAnnotations() for frame_index in frame_indexes}

        for tag in annotations.tags:
            # Some annotations may belong to deleted frames; skip those.
            if tag.frame in self._frame_annotations:
                self._frame_annotations[tag.frame].tags.append(tag)

        for shape in annotations.shapes:
            if shape.frame in self._frame_annotations:
                self._frame_annotations[shape.frame].shapes.append(shape)

    @property
    def labels(self) -> Sequence[models.ILabel]:
        """
        Returns the labels configured in the task.

        Clients must not modify the object returned by this property or its components.
        """
        return self._labels

    @property
    def samples(self) -> Sequence[Sample]:
        """
        Returns a sequence of all samples, in order of their frame indices.

        Note that the frame indices may not be contiguous, as deleted frames will not be included.

        Clients must not modify the object returned by this property or its components.
        """
        return self._samples

    def _load_frame_image(self, frame_index: int) -> PIL.Image:
        assert frame_index in self._frame_annotations

        chunk_index = frame_index // self._task.data_chunk_size
        member_index = frame_index % self._task.data_chunk_size

        with zipfile.ZipFile(self._chunk_dir / f"{chunk_index}.zip", "r") as chunk_zip:
            with chunk_zip.open(chunk_zip.infolist()[member_index]) as chunk_member:
                image = PIL.Image.open(chunk_member)
                image.load()

        return image


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\datasets\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from .caching import UpdatePolicy
from .common import FrameAnnotations, MediaElement, Sample, UnsupportedDatasetError
from .task_dataset import TaskDataset


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\pytorch\common.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from collections.abc import Mapping

import attrs

from cvat_sdk.datasets.common import FrameAnnotations


@attrs.frozen
class Target:
    """
    Non-image data for a dataset sample.
    """

    annotations: FrameAnnotations
    """Annotations for the frame corresponding to the sample."""

    label_id_to_index: Mapping[int, int]
    """
    A mapping from label_id values in `LabeledImage` and `LabeledShape` objects
    to an integer index. This mapping is consistent across all samples for a given task.
    """


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\pytorch\project_dataset.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
from collections.abc import Container, Mapping
from typing import Callable, Optional

import torch
import torch.utils.data
import torchvision.datasets

import cvat_sdk.core
import cvat_sdk.core.exceptions
import cvat_sdk.models as models
from cvat_sdk.datasets.caching import UpdatePolicy, make_cache_manager
from cvat_sdk.pytorch.task_dataset import TaskVisionDataset


class ProjectVisionDataset(torchvision.datasets.VisionDataset):
    """
    Represents a project on a CVAT server as a PyTorch Dataset.

    The dataset contains one sample for each frame of each task in the project
    (except for tasks that are filtered out - see the description of `task_filter`
    in the constructor). The sequence of samples is formed by concatening sequences
    of samples from all included tasks in an arbitrary order that's consistent
    between executions. Each task's sequence of samples corresponds to the sequence
    of frames on the server.

    See `TaskVisionDataset` for information on sample format, caching, and
    current limitations.
    """

    def __init__(
        self,
        client: cvat_sdk.core.Client,
        project_id: int,
        *,
        transforms: Optional[Callable] = None,
        transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        label_name_to_index: Mapping[str, int] = None,
        task_filter: Optional[Callable[[models.ITaskRead], bool]] = None,
        include_subsets: Optional[Container[str]] = None,
        update_policy: UpdatePolicy = UpdatePolicy.IF_MISSING_OR_STALE,
    ) -> None:
        """
        Creates a dataset corresponding to the project with ID `project_id` on the
        server that `client` is connected to.

        `transforms`, `transform` and `target_transforms` are optional transformation
        functions; see the documentation for `torchvision.datasets.VisionDataset` for
        more information.

        See `TaskVisionDataset.__init__` for information on `label_name_to_index`.

        By default, all of the project's tasks will be included in the dataset.
        The following parameters can be specified to exclude some tasks:

        * If `task_filter` is set to a callable object, it will be applied to every task.
          Tasks for which it returns a false value will be excluded.

        * If `include_subsets` is set to a container, then tasks whose subset is
          not a member of this container will be excluded.

        `update_policy` determines when and if the local cache will be updated.
        """

        self._logger = client.logger

        cache_manager = make_cache_manager(client, update_policy)
        project = cache_manager.retrieve_project(project_id)

        super().__init__(
            os.fspath(cache_manager.project_dir(project_id)),
            transforms=transforms,
            transform=transform,
            target_transform=target_transform,
        )

        self._logger.info("Fetching project tasks...")
        tasks = project.get_tasks()

        if task_filter is not None:
            tasks = list(filter(task_filter, tasks))

        if include_subsets is not None:
            tasks = [task for task in tasks if task.subset in include_subsets]

        tasks.sort(key=lambda t: t.id)  # ensure consistent order between executions

        self._underlying = torch.utils.data.ConcatDataset(
            [
                TaskVisionDataset(
                    client,
                    task.id,
                    label_name_to_index=label_name_to_index,
                    update_policy=update_policy,
                )
                for task in tasks
            ]
        )

    def __getitem__(self, sample_index: int):
        """
        Returns the sample with index `sample_index`.

        `sample_index` must satisfy the condition `0 <= sample_index < len(self)`.
        """

        sample_image, sample_target = self._underlying[sample_index]

        if self.transforms:
            sample_image, sample_target = self.transforms(sample_image, sample_target)

        return sample_image, sample_target

    def __len__(self) -> int:
        """Returns the number of samples in the dataset."""
        return len(self._underlying)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\pytorch\task_dataset.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import types
from collections.abc import Mapping
from typing import Callable, Optional

import torchvision.datasets

import cvat_sdk.core
import cvat_sdk.core.exceptions
from cvat_sdk.datasets.caching import UpdatePolicy, make_cache_manager
from cvat_sdk.datasets.task_dataset import TaskDataset
from cvat_sdk.pytorch.common import Target

_NUM_DOWNLOAD_THREADS = 4


class TaskVisionDataset(torchvision.datasets.VisionDataset):
    """
    Represents a task on a CVAT server as a PyTorch Dataset.

    This dataset contains one sample for each frame in the task, in the same
    order as the frames are in the task. Deleted frames are omitted.
    Before transforms are applied, each sample is a tuple of
    (image, target), where:

    * image is a `PIL.Image.Image` object for the corresponding frame.
    * target is a `Target` object containing annotations for the frame.

    This class caches all data and annotations for the task on the local file system
    during construction.

    Limitations:

    * Only tasks with image (not video) data are supported at the moment.
    * Track annotations are currently not accessible.
    """

    def __init__(
        self,
        client: cvat_sdk.core.Client,
        task_id: int,
        *,
        transforms: Optional[Callable] = None,
        transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        label_name_to_index: Mapping[str, int] = None,
        update_policy: UpdatePolicy = UpdatePolicy.IF_MISSING_OR_STALE,
    ) -> None:
        """
        Creates a dataset corresponding to the task with ID `task_id` on the
        server that `client` is connected to.

        `transforms`, `transform` and `target_transforms` are optional transformation
        functions; see the documentation for `torchvision.datasets.VisionDataset` for
        more information.

        `label_name_to_index` affects the `label_id_to_index` member in `Target` objects
        returned by the dataset. If it is specified, then it must contain an entry for
        each label name in the task. The `label_id_to_index` mapping will be constructed
        so that each label will be mapped to the index corresponding to the label's name
        in `label_name_to_index`.

        If `label_name_to_index` is unspecified or set to `None`, then `label_id_to_index`
        will map each label ID to a distinct integer in the range [0, `num_labels`), where
        `num_labels` is the number of labels defined in the task. This mapping will be
        generally unpredictable, but consistent for a given task.

        `update_policy` determines when and if the local cache will be updated.
        """

        self._underlying = TaskDataset(client, task_id, update_policy=update_policy)

        cache_manager = make_cache_manager(client, update_policy)

        super().__init__(
            os.fspath(cache_manager.task_dir(task_id)),
            transforms=transforms,
            transform=transform,
            target_transform=target_transform,
        )

        if label_name_to_index is None:
            self._label_id_to_index = types.MappingProxyType(
                {
                    label.id: label_index
                    for label_index, label in enumerate(
                        sorted(self._underlying.labels, key=lambda l: l.id)
                    )
                }
            )
        else:
            self._label_id_to_index = types.MappingProxyType(
                {label.id: label_name_to_index[label.name] for label in self._underlying.labels}
            )

    def __getitem__(self, sample_index: int):
        """
        Returns the sample with index `sample_index`.

        `sample_index` must satisfy the condition `0 <= sample_index < len(self)`.
        """

        sample = self._underlying.samples[sample_index]

        sample_image = sample.media.load_image()
        sample_target = Target(sample.annotations, self._label_id_to_index)

        if self.transforms:
            sample_image, sample_target = self.transforms(sample_image, sample_target)
        return sample_image, sample_target

    def __len__(self) -> int:
        """Returns the number of samples in the dataset."""
        return len(self._underlying.samples)


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\pytorch\transforms.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from typing import TypedDict

import attrs
import attrs.validators
import torch
import torch.utils.data

from cvat_sdk.datasets.common import UnsupportedDatasetError
from cvat_sdk.pytorch.common import Target


@attrs.frozen
class ExtractSingleLabelIndex:
    """
    A target transform that takes a `Target` object and produces a single label index
    based on the tag in that object, as a 0-dimensional tensor.

    This makes the dataset samples compatible with the image classification networks
    in torchvision.

    If the annotations contain no tags, or multiple tags, raises a `ValueError`.
    """

    def __call__(self, target: Target) -> int:
        tags = target.annotations.tags
        if not tags:
            raise ValueError("sample has no tags")

        if len(tags) > 1:
            raise ValueError("sample has multiple tags")

        return torch.tensor(target.label_id_to_index[tags[0].label_id], dtype=torch.long)


class LabeledBoxes(TypedDict):
    boxes: torch.Tensor
    labels: torch.Tensor


_SUPPORTED_SHAPE_TYPES = frozenset(["rectangle", "polygon", "polyline", "points", "ellipse"])


@attrs.frozen
class ExtractBoundingBoxes:
    """
    A target transform that takes a `Target` object and returns a dictionary compatible
    with the object detection networks in torchvision.

    The dictionary contains the following entries:

    "boxes": a tensor with shape [N, 4], where each row represents a bounding box of a shape
    in the annotations in the (xmin, ymin, xmax, ymax) format.
    "labels": a tensor with shape [N] containing corresponding label indices.

    Limitations:

    * Only the following shape types are supported: rectangle, polygon, polyline,
      points, ellipse.
    * Rotated shapes are not supported.
    """

    include_shape_types: frozenset[str] = attrs.field(
        converter=frozenset,
        validator=attrs.validators.deep_iterable(attrs.validators.in_(_SUPPORTED_SHAPE_TYPES)),
        kw_only=True,
    )
    """Shapes whose type is not in this set will be ignored."""

    def __call__(self, target: Target) -> LabeledBoxes:
        boxes = []
        labels = []

        for shape in target.annotations.shapes:
            if shape.type.value not in self.include_shape_types:
                continue

            if shape.rotation != 0:
                raise UnsupportedDatasetError("Rotated shapes are not supported")

            x_coords = shape.points[0::2]
            y_coords = shape.points[1::2]

            boxes.append((min(x_coords), min(y_coords), max(x_coords), max(y_coords)))
            labels.append(target.label_id_to_index[shape.label_id])

        return LabeledBoxes(
            boxes=torch.tensor(boxes, dtype=torch.float),
            labels=torch.tensor(labels, dtype=torch.long),
        )


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\cvat_sdk\pytorch\__init__.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from .common import Target
from .project_dataset import ProjectVisionDataset
from .task_dataset import TaskVisionDataset
from .transforms import ExtractBoundingBoxes, ExtractSingleLabelIndex, LabeledBoxes

# isort: split
# Compatibility imports
from ..datasets.caching import UpdatePolicy
from ..datasets.common import FrameAnnotations, UnsupportedDatasetError


# ===== 文件: D:\wow_ai\docker\cvat\cvat-sdk\gen\postprocess.py =====
#!/usr/bin/env python3

# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import os.path as osp
import re
import sys
from glob import glob

from inflection import underscore
from ruamel.yaml import YAML


def collect_operations(schema):
    endpoints = schema.get("paths", {})

    operations = {}

    for endpoint_name, endpoint_schema in endpoints.items():
        for method_name, method_schema in endpoint_schema.items():
            method_schema = dict(method_schema)
            method_schema["method"] = method_name
            method_schema["endpoint"] = endpoint_name
            operations[method_schema["operationId"]] = method_schema

    return operations


class Replacer:
    REPLACEMENT_TOKEN = r"%%%"  # nosec: hardcoded_password_string
    ARGS_TOKEN = r"!!!"  # nosec: hardcoded_password_string

    def __init__(self, schema):
        self._schema = schema
        self._operations = collect_operations(self._schema)

    def make_operation_id(self, name: str) -> str:
        operation = self._operations[name]

        new_name = name

        tokenized_path = operation["endpoint"].split("/")
        assert 3 <= len(tokenized_path)
        assert tokenized_path[0] == "" and tokenized_path[1] == "api"
        tokenized_path = tokenized_path[2:]

        prefix = tokenized_path[0] + "_"
        if new_name.startswith(prefix) and tokenized_path[0] in operation["tags"]:
            new_name = new_name[len(prefix) :]

        return new_name

    def make_api_name(self, name: str) -> str:
        return underscore(name)

    def make_type_annotation(self, type_repr: str) -> str:
        type_repr = type_repr.replace("[", "list[")
        type_repr = type_repr.replace("(", "typing.Union[").replace(")", "]")
        type_repr = type_repr.replace("{", "dict[").replace(":", ",").replace("}", "]")

        ANY_pattern = "bool, date, datetime, dict, float, int, list, str"
        type_repr = type_repr.replace(ANY_pattern, "typing.Any")

        # single optional arg pattern
        type_repr = re.sub(r"^(.+, none_type)$", r"typing.Union[\1]", type_repr)

        return type_repr

    allowed_actions = {
        "make_operation_id",
        "make_api_name",
        "make_type_annotation",
    }

    def _process_file(self, contents: str):
        processor_pattern = re.compile(
            f"{self.REPLACEMENT_TOKEN}(.*?){self.ARGS_TOKEN}(.*?){self.REPLACEMENT_TOKEN}"
        )

        matches = list(processor_pattern.finditer(contents))
        for match in reversed(matches):
            action = match.group(1)
            args = match.group(2).split(self.ARGS_TOKEN)

            if action not in self.allowed_actions:
                raise Exception(f"Replacement action '{action}' is not allowed")

            replacement = getattr(self, action)(*args)
            contents = contents[: match.start(0)] + replacement + contents[match.end(0) :]

        return contents

    def process_file(self, src_path: str):
        with open(src_path, "r") as f:
            contents = f.read()

        contents = self._process_file(contents)

        with open(src_path, "w") as f:
            f.write(contents)

    def process_dir(self, dir_path: str, *, file_ext: str = ".py"):
        for filename in glob(dir_path + f"/**/*{file_ext}", recursive=True):
            try:
                self.process_file(filename)
            except Exception as e:
                raise RuntimeError(f"Failed to process file {filename!r}") from e


def parse_schema(path):
    yaml = YAML(typ="safe")
    with open(path, "r") as f:
        return yaml.load(f)


def parse_args(args=None):
    parser = argparse.ArgumentParser(
        add_help=True,
        formatter_class=argparse.RawTextHelpFormatter,
        description="""\
Processes generator output files in a custom way, saves results inplace.

Replacement token: '%(repl_token)s'.
Arg separator token: '%(args_token)s'.
Replaces the following patterns in files:
    '%(repl_token)sREPLACER%(args_token)sARG1%(args_token)sARG2...%(repl_token)s'
    ->
    REPLACER(ARG1, ARG2, ...) value

Available REPLACERs:
    %(replacers)s
        """
        % {
            "repl_token": Replacer.REPLACEMENT_TOKEN,
            "args_token": Replacer.ARGS_TOKEN,
            "replacers": "\n    ".join(Replacer.allowed_actions),
        },
    )
    parser.add_argument("--schema", required=True, help="Path to server schema yaml")
    parser.add_argument("--input-path", required=True, help="Path to target file or directory")
    parser.add_argument(
        "--file-ext",
        default=".py",
        help="If working on a directory, look for "
        "files with the specified extension (default: %(default)s)",
    )

    return parser.parse_args(args)


def main(args=None):
    args = parse_args(args)

    schema = parse_schema(args.schema)
    processor = Replacer(schema=schema)

    if osp.isdir(args.input_path):
        processor.process_dir(args.input_path, file_ext=args.file_ext)
    elif osp.isfile(args.input_path):
        processor.process_file(args.input_path)
    else:
        return f"error: input {args.input_path} is neither a file nor a directory"

    return 0


if __name__ == "__main__":
    sys.exit(main())


# ===== 文件: D:\wow_ai\docker\cvat\dev\check_changelog_fragments.py =====
#!/usr/bin/env python3

import configparser
import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]


def main():
    scriv_config = configparser.ConfigParser()
    scriv_config.read(REPO_ROOT / "changelog.d/scriv.ini")

    scriv_section = scriv_config["scriv"]
    assert scriv_section["format"] == "md"

    md_header_level = int(scriv_section["md_header_level"])
    md_header_prefix = "#" * md_header_level + "# "

    categories = {s.strip() for s in scriv_section["categories"].split(",")}

    success = True

    def complain(message):
        nonlocal success
        success = False
        print(f"{fragment_path.relative_to(REPO_ROOT)}:{line_index+1}: {message}", file=sys.stderr)

    for fragment_path in REPO_ROOT.glob("changelog.d/*.md"):
        with open(fragment_path) as fragment_file:
            for line_index, line in enumerate(fragment_file):
                line = line.rstrip("\n")

                if line_index == 0:
                    # The first line should always be a header.
                    if not line.startswith("#"):
                        complain("line should be a header")
                elif (
                    line
                    and not line.startswith("#")
                    and not line.startswith("-")
                    and not line.startswith("  ")
                ):
                    complain("line should be a header, a list item, or indented")

                if line.startswith("#"):
                    if line.startswith(md_header_prefix):
                        category = line.removeprefix(md_header_prefix).strip()
                        if category not in categories:
                            complain(f"unknown category: {category}")
                    else:
                        # All headers should be of the same level.
                        complain(f"header should start with {md_header_prefix!r}")

    sys.exit(0 if success else 1)


main()


# ===== 文件: D:\wow_ai\docker\cvat\dev\update_version.py =====
#!/usr/bin/env python3

import argparse
import functools
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from re import Match, Pattern
from typing import Callable

SUCCESS_CHAR = "\u2714"
FAIL_CHAR = "\u2716"

CVAT_VERSION_PATTERN = re.compile(
    r"VERSION\s*=\s*\((\d+),\s*(\d*),\s*(\d+),\s*[\',\"](\w+)[\',\"],\s*(\d+)\)"
)

REPO_ROOT_DIR = Path(__file__).resolve().parents[1]

CVAT_INIT_PY_REL_PATH = "cvat/__init__.py"
CVAT_INIT_PY_PATH = REPO_ROOT_DIR / CVAT_INIT_PY_REL_PATH


@dataclass()
class Version:
    major: int = 0
    minor: int = 0
    patch: int = 0
    prerelease: str = ""
    prerelease_number: int = 0

    def __str__(self) -> str:
        return f"{self.major}.{self.minor}.{self.patch}-{self.prerelease}.{self.prerelease_number}"

    def cvat_repr(self):
        return f'({self.major}, {self.minor}, {self.patch}, "{self.prerelease}", {self.prerelease_number})'

    def compose_repr(self):
        if self.prerelease != "final":
            return "dev"
        return f"v{self.major}.{self.minor}.{self.patch}"

    def increment_prerelease_number(self) -> None:
        self.prerelease_number += 1

    def increment_prerelease(self) -> None:
        flow = ("alpha", "beta", "rc", "final")
        idx = flow.index(self.prerelease)
        if idx == len(flow) - 1:
            raise ValueError(f"Cannot increment current '{self.prerelease}' prerelease version")

        self.prerelease = flow[idx + 1]
        self._set_default_prerelease_number()

    def set_prerelease(self, value: str) -> None:
        values = ("alpha", "beta", "rc", "final")
        if value not in values:
            raise ValueError(f"{value} is a wrong, must be one of {values}")

        self.prerelease = value
        self._set_default_prerelease_number()

    def increment_patch(self) -> None:
        self.patch += 1
        self._set_default_prerelease()

    def increment_minor(self) -> None:
        self.minor += 1
        self._set_default_patch()

    def increment_major(self) -> None:
        self.major += 1
        self._set_default_minor()

    def set(self, v: str) -> None:
        self.major, self.minor, self.patch = map(int, v.split("."))
        self.prerelease = "final"
        self.prerelease_number = 0

    def _set_default_prerelease_number(self) -> None:
        self.prerelease_number = 0

    def _set_default_prerelease(self) -> None:
        self.prerelease = "alpha"
        self._set_default_prerelease_number()

    def _set_default_patch(self) -> None:
        self.patch = 0
        self._set_default_prerelease()

    def _set_default_minor(self) -> None:
        self.minor = 0
        self._set_default_patch()


@dataclass(frozen=True)
class ReplacementRule:
    rel_path: str
    pattern: Pattern[str]
    replacement: Callable[[Version, Match[str]], str]

    def apply(self, new_version: Version, *, verify_only: bool) -> bool:
        path = REPO_ROOT_DIR / self.rel_path
        text = path.read_text()

        new_text, num_replacements = self.pattern.subn(
            functools.partial(self.replacement, new_version), text
        )

        if not num_replacements:
            print(f"{FAIL_CHAR} {self.rel_path}: failed to match version pattern.")
            return False

        if text == new_text:
            if verify_only:
                print(f"{SUCCESS_CHAR} {self.rel_path}: verified.")
            else:
                print(f"{SUCCESS_CHAR} {self.rel_path}: no need to update.")
        else:
            if verify_only:
                print(f"{FAIL_CHAR} {self.rel_path}: verification failed.")
                return False
            else:
                path.write_text(new_text)
                print(f"{SUCCESS_CHAR} {self.rel_path}: updated.")

        return True


REPLACEMENT_RULES = [
    ReplacementRule(
        CVAT_INIT_PY_REL_PATH, CVAT_VERSION_PATTERN, lambda v, m: f"VERSION = {v.cvat_repr()}"
    ),
    ReplacementRule(
        "docker-compose.yml",
        re.compile(r"(\$\{CVAT_VERSION:-)([\w.]+)(\})"),
        lambda v, m: m[1] + v.compose_repr() + m[3],
    ),
    ReplacementRule(
        "helm-chart/values.yaml",
        re.compile(r"(^    image: cvat/(?:ui|server)\n    tag: )([\w.]+)", re.M),
        lambda v, m: m[1] + v.compose_repr(),
    ),
    ReplacementRule(
        "cvat-sdk/gen/generate.sh",
        re.compile(r'^VERSION="[\d.]+"$', re.M),
        lambda v, m: f'VERSION="{v.major}.{v.minor}.{v.patch}"',
    ),
    ReplacementRule(
        "cvat/schema.yml",
        re.compile(r"^  version: [\d.]+$", re.M),
        lambda v, m: f"  version: {v.major}.{v.minor}.{v.patch}",
    ),
    ReplacementRule(
        "cvat-cli/src/cvat_cli/version.py",
        re.compile(r'^VERSION = "[\d.]+"$', re.M),
        lambda v, m: f'VERSION = "{v.major}.{v.minor}.{v.patch}"',
    ),
    ReplacementRule(
        "cvat-cli/requirements/base.txt",
        re.compile(r"^cvat-sdk==[\d.]+$", re.M),
        lambda v, m: f"cvat-sdk=={v.major}.{v.minor}.{v.patch}",
    ),
    ReplacementRule(
        "cvat-ui/package.json",
        re.compile(r'^  "version": "[\d.]+",$', re.M),
        lambda v, m: f'  "version": "{v.major}.{v.minor}.{v.patch}",',
    ),
]


def get_current_version() -> Version:
    version_text = CVAT_INIT_PY_PATH.read_text()

    match = re.search(CVAT_VERSION_PATTERN, version_text)
    if not match:
        raise RuntimeError(f"Failed to find version in {CVAT_INIT_PY_PATH}")

    return Version(int(match[1]), int(match[2]), int(match[3]), match[4], int(match[5]))


def main() -> None:
    parser = argparse.ArgumentParser(description="Bump CVAT version")

    action_group = parser.add_mutually_exclusive_group(required=True)

    action_group.add_argument(
        "--major", action="store_true", help="Increment the existing major version by 1"
    )
    action_group.add_argument(
        "--minor", action="store_true", help="Increment the existing minor version by 1"
    )
    action_group.add_argument(
        "--patch", action="store_true", help="Increment the existing patch version by 1"
    )
    action_group.add_argument(
        "--prerelease",
        nargs="?",
        const="increment",
        help="""Increment prerelease version alpha->beta->rc->final,
                Also it's possible to pass value explicitly""",
    )
    action_group.add_argument(
        "--prerelease_number", action="store_true", help="Increment prerelease number by 1"
    )

    action_group.add_argument(
        "--current", "--show-current", action="store_true", help="Display current version"
    )
    action_group.add_argument(
        "--verify-current",
        action="store_true",
        help="Check that all version numbers are consistent",
    )

    action_group.add_argument(
        "--set", metavar="X.Y.Z", help="Set the version to the specified version"
    )

    args = parser.parse_args()

    version = get_current_version()
    verify_only = False

    if args.current:
        print(version)
        return

    elif args.verify_current:
        verify_only = True

    elif args.prerelease_number:
        version.increment_prerelease_number()

    elif args.prerelease:
        if args.prerelease == "increment":
            version.increment_prerelease()
        else:
            version.set_prerelease(args.prerelease)

    elif args.patch:
        version.increment_patch()

    elif args.minor:
        version.increment_minor()

    elif args.major:
        version.increment_major()

    elif args.set is not None:
        version.set(args.set)

    else:
        assert False, "Unreachable code"

    if verify_only:
        print(f"Verifying that version is {version}...")
    else:
        print(f"Bumping version to {version}...")
    print()

    success = True

    for rule in REPLACEMENT_RULES:
        if not rule.apply(version, verify_only=verify_only):
            success = False

    if not success:
        if verify_only:
            sys.exit("\nFailed to verify one or more files!")
        else:
            sys.exit("\nFailed to update one or more files!")


if __name__ == "__main__":
    main()


# ===== 文件: D:\wow_ai\docker\cvat\serverless\onnx\WongKinYiu\yolov7\nuclio\main.py =====
import base64
import io
import json

import yaml
from model_handler import ModelHandler
from PIL import Image


def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)

    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")


def handler(context, event):
    context.logger.info("Run YoloV7 ONNX model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf).convert("RGB")

    results = context.user_data.model.infer(image, threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\onnx\WongKinYiu\yolov7\nuclio\model_handler.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import cv2
import numpy as np
import onnxruntime as ort


class ModelHandler:
    def __init__(self, labels):
        self.model = None
        self.load_network(model="yolov7-nms-640.onnx")
        self.labels = labels

    def load_network(self, model):
        device = ort.get_device()
        cuda = True if device == 'GPU' else False
        try:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']
            so = ort.SessionOptions()
            so.log_severity_level = 3

            self.model = ort.InferenceSession(model, providers=providers, sess_options=so)
            self.output_details = [i.name for i in self.model.get_outputs()]
            self.input_details = [i.name for i in self.model.get_inputs()]

            self.is_inititated = True
        except Exception as e:
            raise Exception(f"Cannot load model {model}: {e}")

    def letterbox(self, im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):
        # Resize and pad image while meeting stride-multiple constraints
        shape = im.shape[:2]  # current shape [height, width]
        if isinstance(new_shape, int):
            new_shape = (new_shape, new_shape)

        # Scale ratio (new / old)
        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])
        if not scaleup:  # only scale down, do not scale up (for better val mAP)
            r = min(r, 1.0)

        # Compute padding
        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))
        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding

        if auto:  # minimum rectangle
            dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding

        dw /= 2  # divide padding into 2 sides
        dh /= 2

        if shape[::-1] != new_unpad:  # resize
            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)
        top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))
        left, right = int(round(dw - 0.1)), int(round(dw + 0.1))
        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border
        return im, r, (dw, dh)

    def _infer(self, inputs: np.ndarray):
        try:
            img = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB)
            image = img.copy()
            image, ratio, dwdh = self.letterbox(image, auto=False)
            image = image.transpose((2, 0, 1))
            image = np.expand_dims(image, 0)
            image = np.ascontiguousarray(image)

            im = image.astype(np.float32)
            im /= 255

            inp = {self.input_details[0]: im}
            # ONNX inference
            output = list()
            detections = self.model.run(self.output_details, inp)[0]

            # for det in detections:
            boxes = detections[:, 1:5]
            labels = detections[:, 5]
            scores = detections[:, -1]

            boxes -= np.array(dwdh * 2)
            boxes /= ratio
            boxes = boxes.round().astype(np.int32)
            output.append(boxes)
            output.append(labels)
            output.append(scores)
            return output

        except Exception as e:
            print(e)

    def infer(self, image, threshold):
        image = np.array(image)
        image = image[:, :, ::-1].copy()
        h, w, _ = image.shape
        detections = self._infer(image)

        results = []
        if detections:
            boxes = detections[0]
            labels = detections[1]
            scores = detections[2]

            for label, score, box in zip(labels, scores, boxes):
                if score >= threshold:
                    xtl = max(int(box[0]), 0)
                    ytl = max(int(box[1]), 0)
                    xbr = min(int(box[2]), w)
                    ybr = min(int(box[3]), h)

                    results.append({
                        "confidence": str(score),
                        "label": self.labels.get(label, "unknown"),
                        "points": [xtl, ytl, xbr, ybr],
                        "type": "rectangle",
                    })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\base\model_loader.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import cv2
import numpy as np
from openvino.inference_engine import IECore

class ModelLoader:
    def __init__(self, model, weights):
        ie_core = IECore()
        network = ie_core.read_network(model, weights)
        self._network = network

        # Initialize input blobs
        self._input_info_name = None
        for blob_name in network.input_info:
            if len(network.input_info[blob_name].tensor_desc.dims) == 4:
                self._input_blob_name = blob_name
                self._input_layout = network.input_info[blob_name].tensor_desc.dims
            elif len(network.input_info[blob_name].tensor_desc.dims) == 2:
                self._input_info_name = blob_name
            else:
                raise RuntimeError(
                    "Unsupported {}D input layer '{}'. Only 2D and 4D input layers are supported"
                    .format(len(network.input_info[blob_name].tensor_desc.dims), blob_name))

        # Initialize output blob
        self._output_blob_name = next(iter(network.outputs))

        # Load network
        self._net = ie_core.load_network(network, "CPU", num_requests=2)

    def _prepare_inputs(self, image, preprocessing):
        image = np.array(image)
        _, _, h, w = self._input_layout
        if preprocessing:
            image = image if image.shape[:-1] == (h, w) else cv2.resize(image, (w, h))
            if len(image.shape) < 3: # grayscale image
                image = image[:, :, np.newaxis]
            else:
                if image.shape[2] == 4: # the image has alpha channel
                    image = image[:, :, :3]

            image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW

        inputs = {self._input_blob_name: image}
        if self._input_info_name:
            inputs[self._input_info_name] = [h, w, 1]
        return inputs

    def infer(self, image, preprocessing=True):
        inputs = self._prepare_inputs(image, preprocessing)
        results = self._net.infer(inputs)
        if len(results) == 1:
            return results[self._output_blob_name].copy()
        else:
            return results.copy()

    def async_infer(self, image, preprocessing=True, request_id=0):
        inputs = self._prepare_inputs(image, preprocessing)
        return self._net.start_async(request_id=request_id, inputs=inputs)

    def input_size(self):
        return self._input_layout[2:]

    @property
    def network(self):
        return self._network


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\base\shared.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

def to_cvat_mask(box: list, mask):
    xtl, ytl, xbr, ybr = box
    flattened = mask[ytl:ybr + 1, xtl:xbr + 1].flat[:].tolist()
    flattened.extend([xtl, ytl, xbr, ybr])
    return flattened


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\dextr\nuclio\export.py =====
#!/usr/bin/env python3

import sys

import torch
import torch.nn
import torch.onnx

import networks.deeplab_resnet as resnet

net = resnet.resnet101(1, nInputChannels=4, classifier='psp')

state_dict_checkpoint = torch.load(sys.argv[1], map_location=torch.device('cpu'))

net.load_state_dict(state_dict_checkpoint)

full_net = torch.nn.Sequential(
    net,
    torch.nn.Upsample((512, 512), mode='bilinear', align_corners=True),
    torch.nn.Sigmoid(),
)
full_net.eval()

input_tensor = torch.randn((1, 4, 512, 512))

torch.onnx.export(full_net, input_tensor, sys.argv[2])


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\dextr\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    model = ModelHandler()
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("call handler")
    data = event.body
    points = data["pos_points"]
    buf = io.BytesIO(base64.b64decode(data["image"]))
    image = Image.open(buf)

    mask, polygon = context.user_data.model.handle(image, points)
    return context.Response(body=json.dumps({
            'points': polygon,
            'mask': mask.tolist(),
        }),
        headers={},
        content_type='application/json',
        status_code=200
    )


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\dextr\nuclio\model_handler.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import cv2
import numpy as np
import os
from model_loader import ModelLoader

class ModelHandler:
    def __init__(self):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH", "/opt/nuclio"))
        model_xml = os.path.join(base_dir, "dextr.xml")
        model_bin = os.path.join(base_dir, "dextr.bin")
        self.model = ModelLoader(model_xml, model_bin)

    # Input:
    #   image: PIL image
    #   points: [[x1,y1], [x2,y2], [x3,y3], [x4,y4], ...]
    # Output:
    #   polygon: [[x1,y1], [x2,y2], [x3,y3], [x4,y4], ...]
    #   mask: [[a, a, a, a, a, ...], [a, a, a, a, ...], ...]
    def handle(self, image, points):
        DEXTR_PADDING = 50
        DEXTR_TRESHOLD = 0.8
        DEXTR_SIZE = 512

        numpy_image = np.array(image)
        points = np.asarray(points, dtype=int)
        bounding_box = (
            max(min(points[:, 0]) - DEXTR_PADDING, 0),
            max(min(points[:, 1]) - DEXTR_PADDING, 0),
            min(max(points[:, 0]) + DEXTR_PADDING, numpy_image.shape[1] - 1),
            min(max(points[:, 1]) + DEXTR_PADDING, numpy_image.shape[0] - 1)
        )

        # Prepare an image
        numpy_cropped = np.array(image.crop(bounding_box))
        resized = cv2.resize(numpy_cropped, (DEXTR_SIZE, DEXTR_SIZE),
            interpolation = cv2.INTER_CUBIC).astype(np.float32)
        if len(resized.shape) == 2: # support grayscale images
            resized = cv2.cvtColor(resized, cv2.COLOR_GRAY2RGB)
        elif resized.shape[2] == 4: # remove alpha channel
            resized = resized[:, :, :3]

        # Make a heatmap
        points = points - [bounding_box[0], bounding_box[1]]
        points = (points * [DEXTR_SIZE / numpy_cropped.shape[1], DEXTR_SIZE / numpy_cropped.shape[0]]).astype(int)
        heatmap = np.zeros(shape=resized.shape[:2], dtype=np.float64)
        for point in points:
            gaussian_x_axis = np.arange(0, DEXTR_SIZE, 1, float) - point[0]
            gaussian_y_axis = np.arange(0, DEXTR_SIZE, 1, float)[:, np.newaxis] - point[1]
            gaussian = np.exp(-4 * np.log(2) * ((gaussian_x_axis ** 2 + gaussian_y_axis ** 2) / 100)).astype(np.float64)
            heatmap = np.maximum(heatmap, gaussian)
        cv2.normalize(heatmap, heatmap, 0, 255, cv2.NORM_MINMAX)

        # Concat an image and a heatmap
        input_dextr = np.concatenate((resized, heatmap[:, :, np.newaxis].astype(resized.dtype)), axis=2)
        input_dextr = input_dextr.transpose((2,0,1))

        pred = self.model.infer(input_dextr[np.newaxis, ...], False)[0, 0, :, :]
        pred = (pred > DEXTR_TRESHOLD).astype(np.uint8)
        pred = cv2.resize(pred, tuple(reversed(numpy_cropped.shape[:2])), interpolation = cv2.INTER_NEAREST)
        result = np.zeros(numpy_image.shape[:2]).astype(np.uint8)
        result[bounding_box[1]:bounding_box[1] + pred.shape[0], bounding_box[0]:bounding_box[0] + pred.shape[1]] = pred

        # Convert a mask to a polygon
        contours = None
        if int(cv2.__version__.split('.')[0]) > 3:
            contours = cv2.findContours(result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[0]
        else:
            contours = cv2.findContours(result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[1]

        contours = max(contours, key=lambda arr: arr.size)
        if contours.shape.count(1):
            contours = np.squeeze(contours)
        if contours.size < 3 * 2:
            raise Exception('Less then three point have been detected. Can not build a polygon.')

        polygon = []
        for point in contours:
            polygon.append([int(point[0]), int(point[1])])

        return result, polygon


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\face-detection-0205\nuclio\main.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
from PIL import Image
import io
from model_handler import FaceDetectorHandler, AttributesExtractorHandler

def init_context(context):
    context.logger.info("Init context...  0%")

     # Read the DL model
    context.user_data.detector_model = FaceDetectorHandler()
    context.user_data.attributes_model = AttributesExtractorHandler()

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run face-detection-0206 model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf)

    results, faces = context.user_data.detector_model.infer(image, threshold)
    for idx, face in enumerate(faces):
        attributes =  context.user_data.attributes_model.infer(face)
        results[idx].update(attributes)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\face-detection-0205\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os
import numpy as np
from model_loader import ModelLoader

class FaceDetectorHandler:
    def __init__(self):
        base_dir = os.path.abspath(os.environ.get("DETECTOR_MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/face-detection-0205/FP32"))
        model_xml = os.path.join(base_dir, "face-detection-0205.xml")
        model_bin = os.path.join(base_dir, "face-detection-0205.bin")
        self.model = ModelLoader(model_xml, model_bin)

    def infer(self, image, threshold):
        infer_res = self.model.infer(image)["boxes"]
        infer_res = infer_res[infer_res[:,4] > threshold]

        results = []
        faces = []
        h_scale = image.height / 416
        w_scale = image.width / 416
        for face in infer_res:
            xmin = int(face[0] * w_scale)
            ymin = int(face[1] * h_scale)
            xmax = int(face[2] * w_scale)
            ymax = int(face[3] * h_scale)
            confidence = face[4]

            faces.append(np.array(image)[ymin:ymax, xmin:xmax])
            results.append({
                "confidence": str(confidence),
                "label": "face",
                "points": [xmin, ymin, xmax, ymax],
                "type": "rectangle",
                "attributes": []
            })

        return results, faces

class AttributesExtractorHandler:
    def __init__(self):
        age_gender_base_dir = os.path.abspath(os.environ.get("AGE_GENDER_MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/age-gender-recognition-retail-0013/FP32"))
        age_gender_model_xml = os.path.join(age_gender_base_dir, "age-gender-recognition-retail-0013.xml")
        age_gender_model_bin = os.path.join(age_gender_base_dir, "age-gender-recognition-retail-0013.bin")
        self.age_gender_model = ModelLoader(age_gender_model_xml, age_gender_model_bin)
        emotions_base_dir = os.path.abspath(os.environ.get("EMOTIONS_MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/emotions-recognition-retail-0003/FP32"))
        emotions_model_xml = os.path.join(emotions_base_dir, "emotions-recognition-retail-0003.xml")
        emotions_model_bin = os.path.join(emotions_base_dir, "emotions-recognition-retail-0003.bin")
        self.emotions_model = ModelLoader(emotions_model_xml, emotions_model_bin)
        self.genders_map = ["female", "male"]
        self.emotions_map = ["neutral", "happy", "sad", "surprise", "anger"]

    def infer(self, image):
        age_gender_request = self.age_gender_model.async_infer(image)
        emotions_request = self.emotions_model.async_infer(image)
        # Wait until both age_gender and emotion recognition async inferences finish
        while not (age_gender_request.wait(0) == 0 and emotions_request.wait(0) == 0):
            continue
        age = int(np.squeeze(age_gender_request.output_blobs["age_conv3"].buffer) * 100)
        gender = self.genders_map[np.argmax(np.squeeze(age_gender_request.output_blobs["prob"].buffer))]
        emotion = self.emotions_map[np.argmax(np.squeeze(emotions_request.output_blobs['prob_emotion'].buffer))]
        return {"attributes": [
            {"name": "age", "value": str(age)},
            {"name": "gender", "value": gender},
            {"name": "emotion", "value": emotion}
        ]}


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\person-reidentification-retail-0277\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    model = ModelHandler()
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run person-reidentification-retail-0277 model")
    data = event.body
    buf0 = io.BytesIO(base64.b64decode(data["image0"]))
    buf1 = io.BytesIO(base64.b64decode(data["image1"]))
    threshold = float(data.get("threshold", 0.5))
    max_distance = float(data.get("max_distance", 50))
    image0 = Image.open(buf0)
    image1 = Image.open(buf1)
    boxes0 = data["boxes0"]
    boxes1 = data["boxes1"]

    results = context.user_data.model.infer(image0, boxes0,
        image1, boxes1, threshold, max_distance)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\person-reidentification-retail-0277\nuclio\model_handler.py =====
# Copyright (C) 2018-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import math
import numpy
import os
from scipy.optimize import linear_sum_assignment
from scipy.spatial.distance import euclidean, cosine

from model_loader import ModelLoader

class ModelHandler:
    def __init__(self):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/person-reidentification-retail-0277/FP32"))
        model_xml = os.path.join(base_dir, "person-reidentification-retail-0277.xml")
        model_bin = os.path.join(base_dir, "person-reidentification-retail-0277.bin")

        self.model = ModelLoader(model_xml, model_bin)

    def infer(self, image0, boxes0, image1, boxes1, threshold, distance):
        similarity_matrix = self._compute_similarity_matrix(image0,
            boxes0, image1, boxes1, distance)
        row_idx, col_idx = linear_sum_assignment(similarity_matrix)
        results = [-1] * len(boxes0)
        for idx0, idx1 in zip(row_idx, col_idx):
            if similarity_matrix[idx0, idx1] <= threshold:
                results[idx0] = int(idx1)

        return results

    def _match_boxes(self, box0, box1, distance):
        cx0 = (box0["points"][0] + box0["points"][2]) / 2
        cy0 = (box0["points"][1] + box0["points"][3]) / 2
        cx1 = (box1["points"][0] + box1["points"][2]) / 2
        cy1 = (box1["points"][1] + box1["points"][3]) / 2
        is_good_distance = euclidean([cx0, cy0], [cx1, cy1]) <= distance
        is_same_label = box0["label_id"] == box1["label_id"]

        return is_good_distance and is_same_label

    def _match_crops(self, crop0, crop1):
        embedding0 = self.model.infer(crop0)
        embedding1 = self.model.infer(crop1)

        embedding0 = embedding0.reshape(embedding0.size)
        embedding1 = embedding1.reshape(embedding1.size)

        return cosine(embedding0, embedding1)

    def _compute_similarity_matrix(self, image0, boxes0, image1, boxes1,
        distance):
        def _int(number, upper):
            return math.floor(numpy.clip(number, 0, upper - 1))

        DISTANCE_INF = 1000.0

        matrix = numpy.full([len(boxes0), len(boxes1)], DISTANCE_INF, dtype=float)
        for row, box0 in enumerate(boxes0):
            w0, h0 = image0.size
            xtl0, xbr0, ytl0, ybr0 = (
                _int(box0["points"][0], w0), _int(box0["points"][2], w0),
                _int(box0["points"][1], h0), _int(box0["points"][3], h0)
            )

            for col, box1 in enumerate(boxes1):
                w1, h1 = image1.size
                xtl1, xbr1, ytl1, ybr1 = (
                    _int(box1["points"][0], w1), _int(box1["points"][2], w1),
                    _int(box1["points"][1], h1), _int(box1["points"][3], h1)
                )

                if not self._match_boxes(box0, box1, distance):
                    continue

                crop0 = image0.crop((xtl0, ytl0, xbr0, ybr0))
                crop1 = image1.crop((xtl1, ytl1, xbr1, ybr1))
                matrix[row][col] = self._match_crops(crop0, crop1)

        return matrix


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\semantic-segmentation-adas-0001\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler
import yaml

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)

    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run semantic-segmentation-adas-0001 model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf)

    results = context.user_data.model.infer(image, threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\semantic-segmentation-adas-0001\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os

import cv2
import numpy as np
from model_loader import ModelLoader
from shared import to_cvat_mask


class ModelHandler:
    def __init__(self, labels):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/semantic-segmentation-adas-0001/FP32"))
        model_xml = os.path.join(base_dir, "semantic-segmentation-adas-0001.xml")
        model_bin = os.path.join(base_dir, "semantic-segmentation-adas-0001.bin")
        self.model = ModelLoader(model_xml, model_bin)
        self.labels = labels

    def infer(self, image, threshold):
        output_layer = self.model.infer(image)

        results = []
        mask = output_layer[0, 0, :, :]
        width, height = mask.shape

        for i in range(len(self.labels)):
            mask_by_label = np.zeros((width, height), dtype=np.uint8)
            mask_by_label = ((mask == float(i)) * 255).astype(np.uint8)
            mask_by_label = cv2.resize(mask_by_label,
                dsize=(image.width, image.height),
                interpolation=cv2.INTER_NEAREST)

            contours, _  = cv2.findContours(mask_by_label, cv2.RETR_EXTERNAL,
                cv2.CHAIN_APPROX_SIMPLE)

            for contour in contours:
                contour = np.flip(contour, axis=1)
                if len(contour) < 3:
                    continue

                x_min = max(0, int(np.min(contour[:,:,0])))
                x_max = max(0, int(np.max(contour[:,:,0])))
                y_min = max(0, int(np.min(contour[:,:,1])))
                y_max = max(0, int(np.max(contour[:,:,1])))

                cvat_mask = to_cvat_mask((x_min, y_min, x_max, y_max), mask_by_label)

                results.append({
                    "confidence": None,
                    "label": self.labels.get(i, "unknown"),
                    "points": contour.ravel().tolist(),
                    "mask": cvat_mask,
                    "type": "mask",
                })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\text-detection-0004\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler
import yaml

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)
    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run text-detection-0004 model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    pixel_threshold = float(data.get("pixel_threshold", 0.8))
    link_threshold = float(data.get("link_threshold", 0.8))
    image = Image.open(buf)

    results = context.user_data.model.infer(image,
        pixel_threshold, link_threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\intel\text-detection-0004\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import cv2
import numpy as np
from model_loader import ModelLoader
from shared import to_cvat_mask


class PixelLinkDecoder():
    def __init__(self, pixel_threshold, link_threshold):
        four_neighbours = False
        if four_neighbours:
            self._get_neighbours = self._get_neighbours_4
        else:
            self._get_neighbours = self._get_neighbours_8
        self.pixel_conf_threshold = pixel_threshold
        self.link_conf_threshold = link_threshold

    def decode(self, height, width, detections: dict):
        self.image_height = height
        self.image_width = width
        self.pixel_scores = self._set_pixel_scores(detections['model/segm_logits/add'])
        self.link_scores = self._set_link_scores(detections['model/link_logits_/add'])

        self.pixel_mask = self.pixel_scores >= self.pixel_conf_threshold
        self.link_mask = self.link_scores >= self.link_conf_threshold
        self.points = list(zip(*np.where(self.pixel_mask)))
        self.h, self.w = np.shape(self.pixel_mask)
        self.group_mask = dict.fromkeys(self.points, -1)
        self.bboxes = None
        self.root_map = None
        self.mask = None

        self._decode()

    def _softmax(self, x, axis=None):
        return np.exp(x - self._logsumexp(x, axis=axis, keepdims=True))

    # pylint: disable=no-self-use
    def _logsumexp(self, a, axis=None, b=None, keepdims=False, return_sign=False):
        if b is not None:
            a, b = np.broadcast_arrays(a, b)
            if np.any(b == 0):
                a = a + 0.  # promote to at least float
                a[b == 0] = -np.inf

        a_max = np.amax(a, axis=axis, keepdims=True)

        if a_max.ndim > 0:
            a_max[~np.isfinite(a_max)] = 0
        elif not np.isfinite(a_max):
            a_max = 0

        if b is not None:
            b = np.asarray(b)
            tmp = b * np.exp(a - a_max)
        else:
            tmp = np.exp(a - a_max)

        # suppress warnings about log of zero
        with np.errstate(divide='ignore'):
            s = np.sum(tmp, axis=axis, keepdims=keepdims)
            if return_sign:
                sgn = np.sign(s)
                s *= sgn  # /= makes more sense but we need zero -> zero
            out = np.log(s)

        if not keepdims:
            a_max = np.squeeze(a_max, axis=axis)
        out += a_max

        if return_sign:
            return out, sgn
        else:
            return out

    def _set_pixel_scores(self, pixel_scores):
        "get softmaxed properly shaped pixel scores"
        tmp = np.transpose(pixel_scores, (0, 2, 3, 1))
        return self._softmax(tmp, axis=-1)[0, :, :, 1]

    def _set_link_scores(self, link_scores):
        "get softmaxed properly shaped links scores"
        tmp = np.transpose(link_scores, (0, 2, 3, 1))
        tmp_reshaped = tmp.reshape(tmp.shape[:-1] + (8, 2))
        return self._softmax(tmp_reshaped, axis=-1)[0, :, :, :, 1]

    def _find_root(self, point):
        root = point
        update_parent = False
        tmp = self.group_mask[root]
        while tmp != -1:
            root = tmp
            tmp = self.group_mask[root]
            update_parent = True
        if update_parent:
            self.group_mask[point] = root
        return root

    def _join(self, p1, p2):
        root1 = self._find_root(p1)
        root2 = self._find_root(p2)
        if root1 != root2:
            self.group_mask[root2] = root1

    def _get_index(self, root):
        if root not in self.root_map:
            self.root_map[root] = len(self.root_map) + 1
        return self.root_map[root]

    def _get_all(self):
        self.root_map = {}
        self.mask = np.zeros_like(self.pixel_mask, dtype=np.int32)

        for point in self.points:
            point_root = self._find_root(point)
            bbox_idx = self._get_index(point_root)
            self.mask[point] = bbox_idx

    def _get_neighbours_8(self, x, y):
        w, h = self.w, self.h
        tmp = [(0, x - 1, y - 1), (1, x, y - 1),
               (2, x + 1, y - 1), (3, x - 1, y),
               (4, x + 1, y), (5, x - 1, y + 1),
               (6, x, y + 1), (7, x + 1, y + 1)]

        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]

    def _get_neighbours_4(self, x, y):
        w, h = self.w, self.h
        tmp = [(1, x, y - 1),
               (3, x - 1, y),
               (4, x + 1, y),
               (6, x, y + 1)]

        return [i for i in tmp if i[1] >= 0 and i[1] < w and i[2] >= 0 and i[2] < h]

    def _mask_to_bboxes(self, min_area=300, min_height=10):
        self.bboxes = []
        max_bbox_idx = self.mask.max()
        mask_tmp = cv2.resize(self.mask, (self.image_width, self.image_height), interpolation=cv2.INTER_NEAREST)

        for bbox_idx in range(1, max_bbox_idx + 1):
            bbox_mask = mask_tmp == bbox_idx
            cnts, _ = cv2.findContours(bbox_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
            if len(cnts) == 0:
                continue
            cnt = cnts[0]
            rect, w, h = self._min_area_rect(cnt)
            if min(w, h) < min_height:
                continue
            if w * h < min_area:
                continue
            self.bboxes.append(self._order_points(rect))

    # pylint: disable=no-self-use
    def _min_area_rect(self, cnt):
        rect = cv2.minAreaRect(cnt)
        w, h = rect[1]
        box = cv2.boxPoints(rect)
        box = np.int0(box)
        return box, w, h

    # pylint: disable=no-self-use
    def _order_points(self, rect):
        """ (x, y)
            Order: TL, TR, BR, BL
        """
        tmp = np.zeros_like(rect)
        sums = rect.sum(axis=1)
        tmp[0] = rect[np.argmin(sums)]
        tmp[2] = rect[np.argmax(sums)]
        diff = np.diff(rect, axis=1)
        tmp[1] = rect[np.argmin(diff)]
        tmp[3] = rect[np.argmax(diff)]
        return tmp

    def _decode(self):
        for point in self.points:
            y, x = point
            neighbours = self._get_neighbours(x, y)
            for n_idx, nx, ny in neighbours:
                link_value = self.link_mask[y, x, n_idx]
                pixel_cls = self.pixel_mask[ny, nx]
                if link_value and pixel_cls:
                    self._join(point, (ny, nx))

        self._get_all()
        self._mask_to_bboxes()

class ModelHandler:
    def __init__(self, labels):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/intel/text-detection-0004/FP32"))
        model_xml = os.path.join(base_dir, "text-detection-0004.xml")
        model_bin = os.path.join(base_dir, "text-detection-0004.bin")
        self.model = ModelLoader(model_xml, model_bin)
        self.labels = labels

    def infer(self, image, pixel_threshold, link_threshold):
        output_layer = self.model.infer(image)

        results = []
        obj_class = 1
        pcd = PixelLinkDecoder(pixel_threshold, link_threshold)

        pcd.decode(image.height, image.width, output_layer)
        for box in pcd.bboxes:
            mask = pcd.pixel_mask
            mask = np.array(mask, dtype=np.uint8)
            mask = cv2.resize(mask, dsize=(image.width, image.height), interpolation=cv2.INTER_CUBIC)
            cv2.normalize(mask, mask, 0, 255, cv2.NORM_MINMAX)

            box = box.ravel().tolist()
            x_min = min(box[::2])
            x_max = max(box[::2])
            y_min = min(box[1::2])
            y_max = max(box[1::2])
            cvat_mask = to_cvat_mask((x_min, y_min, x_max, y_max), mask)

            results.append({
                "confidence": None,
                "label": self.labels.get(obj_class, "unknown"),
                "points": box,
                "mask": cvat_mask,
                "type": "mask",
            })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\faster_rcnn_inception_resnet_v2_atrous_coco\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler
import yaml

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)

    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run faster_rcnn_inception_resnet_v2_atrous_coco model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf)

    results = context.user_data.model.infer(image, threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\faster_rcnn_inception_resnet_v2_atrous_coco\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os
from model_loader import ModelLoader

class ModelHandler:
    def __init__(self, labels):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/public/faster_rcnn_inception_resnet_v2_atrous_coco/FP32"))
        model_xml = os.path.join(base_dir, "faster_rcnn_inception_resnet_v2_atrous_coco.xml")
        model_bin = os.path.join(base_dir, "faster_rcnn_inception_resnet_v2_atrous_coco.bin")
        self.model = ModelLoader(model_xml, model_bin)
        self.labels = labels

    def infer(self, image, threshold):
        output_layer = self.model.infer(image)

        results = []
        prediction = output_layer[0][0]
        for obj in prediction:
            obj_class = int(obj[1])
            obj_value = obj[2]
            obj_label = self.labels.get(obj_class, "unknown")
            if obj_value >= threshold:
                xtl = obj[3] * image.width
                ytl = obj[4] * image.height
                xbr = obj[5] * image.width
                ybr = obj[6] * image.height

                results.append({
                    "confidence": str(obj_value),
                    "label": obj_label,
                    "points": [xtl, ytl, xbr, ybr],
                    "type": "rectangle",
                })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\mask_rcnn_inception_resnet_v2_atrous_coco\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler
import yaml

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)

    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run mask_rcnn_inception_resnet_v2_atrous_coco model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.2))
    image = Image.open(buf)

    results = context.user_data.model.infer(image, threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\mask_rcnn_inception_resnet_v2_atrous_coco\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import math
import os
import cv2
import numpy as np
from model_loader import ModelLoader
from shared import to_cvat_mask
from skimage.measure import approximate_polygon, find_contours

MASK_THRESHOLD = 0.5

# Ref: https://software.intel.com/en-us/forums/computer-vision/topic/804895
def segm_postprocess(box: list, raw_cls_mask, im_h, im_w):
    xmin, ymin, xmax, ymax = box

    width = xmax - xmin + 1
    height = ymax - ymin + 1

    result = np.zeros((im_h, im_w), dtype=np.uint8)
    resized_mask = cv2.resize(raw_cls_mask, dsize=(width, height), interpolation=cv2.INTER_CUBIC)

    # extract the ROI of the image
    result[ymin:ymax + 1, xmin:xmax + 1] = (resized_mask > MASK_THRESHOLD).astype(np.uint8) * 255

    return result

class ModelHandler:
    def __init__(self, labels):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/public/mask_rcnn_inception_resnet_v2_atrous_coco/FP32"))
        model_xml = os.path.join(base_dir, "mask_rcnn_inception_resnet_v2_atrous_coco.xml")
        model_bin = os.path.join(base_dir, "mask_rcnn_inception_resnet_v2_atrous_coco.bin")
        self.model = ModelLoader(model_xml, model_bin)
        self.labels = labels

    def infer(self, image, threshold):
        output_layer = self.model.infer(image)

        results = []
        masks = output_layer['masks']
        boxes = output_layer['reshape_do_2d']

        for index, box in enumerate(boxes):
            obj_class = int(box[1])
            obj_value = box[2]
            obj_label = self.labels.get(obj_class, "unknown")
            if obj_value >= threshold:
                xtl = math.trunc(box[3] * image.width)
                ytl = math.trunc(box[4] * image.height)
                xbr = math.trunc(box[5] * image.width)
                ybr = math.trunc(box[6] * image.height)
                mask = masks[index][obj_class - 1]

                mask = segm_postprocess((xtl, ytl, xbr, ybr), mask, image.height, image.width)
                cvat_mask = to_cvat_mask((xtl, ytl, xbr, ybr), mask)

                contours = find_contours(mask, MASK_THRESHOLD)
                contour = contours[0]
                contour = np.flip(contour, axis=1)
                contour = approximate_polygon(contour, tolerance=2.5)

                if len(contour) < 3:
                    continue

                results.append({
                    "confidence": str(obj_value),
                    "label": obj_label,
                    "points": contour.ravel().tolist(),
                    "mask": cvat_mask,
                    "type": "mask",
                })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\yolo-v3-tf\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler
import yaml

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read labels
    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)

    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}

    # Read the DL model
    model = ModelHandler(labels)
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run yolo-v3-tf model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf)

    results = context.user_data.model.infer(image, threshold)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\openvino\omz\public\yolo-v3-tf\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os
from math import exp

import ngraph

from model_loader import ModelLoader

class YoloParams:
    # ------------------------------------------- Extracting layer parameters ------------------------------------------
    # Magic numbers are copied from yolo samples
    def __init__(self, param, side):
        self.num = param.get('num', 3)
        self.coords = param.get('coords', 4)
        self.classes = param.get('classes', 80)
        self.side = side
        self.anchors = param.get('anchors', [
            10.0, 13.0, 16.0, 30.0, 33.0, 23.0, 30.0, 61.0, 62.0, 45.0, 59.0,
            119.0, 116.0, 90.0, 156.0, 198.0, 373.0, 326.0])

        self.isYoloV3 = False

        if mask := param.get('mask'):
            self.num = len(mask)

            maskedAnchors = []
            for idx in mask:
                maskedAnchors += [self.anchors[idx * 2], self.anchors[idx * 2 + 1]]
            self.anchors = maskedAnchors

            self.isYoloV3 = True # Weak way to determine but the only one.


def entry_index(side, coord, classes, location, entry):
    side_power_2 = side ** 2
    n = location // side_power_2
    loc = location % side_power_2
    return int(side_power_2 * (n * (coord + classes + 1) + entry) + loc)


def scale_bbox(x, y, h, w, class_id, confidence, h_scale, w_scale):
    xmin = int((x - w / 2) * w_scale)
    ymin = int((y - h / 2) * h_scale)
    xmax = int(xmin + w * w_scale)
    ymax = int(ymin + h * h_scale)
    return dict(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, class_id=class_id, confidence=confidence)


def parse_yolo_region(blob, resized_image_shape, original_im_shape, params, threshold):
    # ------------------------------------------ Validating output parameters ------------------------------------------
    _, _, out_blob_h, out_blob_w = blob.shape
    assert out_blob_w == out_blob_h, "Invalid size of output blob. It should be in NCHW layout and height should " \
                                     "be equal to width. Current height = {}, current width = {}" \
                                     "".format(out_blob_h, out_blob_w)

    # ------------------------------------------ Extracting layer parameters -------------------------------------------
    orig_im_h, orig_im_w = original_im_shape
    resized_image_h, resized_image_w = resized_image_shape
    objects = list()
    predictions = blob.flatten()
    side_square = params.side * params.side

    # ------------------------------------------- Parsing YOLO Region output -------------------------------------------
    for i in range(side_square):
        row = i // params.side
        col = i % params.side
        for n in range(params.num):
            obj_index = entry_index(params.side, params.coords, params.classes, n * side_square + i, params.coords)
            scale = predictions[obj_index]
            if scale < threshold:
                continue
            box_index = entry_index(params.side, params.coords, params.classes, n * side_square + i, 0)
            # Network produces location predictions in absolute coordinates of feature maps.
            # Scale it to relative coordinates.
            x = (col + predictions[box_index + 0 * side_square]) / params.side
            y = (row + predictions[box_index + 1 * side_square]) / params.side
            # Value for exp is very big number in some cases so following construction is using here
            try:
                w_exp = exp(predictions[box_index + 2 * side_square])
                h_exp = exp(predictions[box_index + 3 * side_square])
            except OverflowError:
                continue
            # Depends on topology we need to normalize sizes by feature maps (up to YOLOv3) or by input shape (YOLOv3)
            w = w_exp * params.anchors[2 * n] / (resized_image_w if params.isYoloV3 else params.side)
            h = h_exp * params.anchors[2 * n + 1] / (resized_image_h if params.isYoloV3 else params.side)
            for j in range(params.classes):
                class_index = entry_index(params.side, params.coords, params.classes, n * side_square + i,
                                          params.coords + 1 + j)
                confidence = scale * predictions[class_index]
                if confidence < threshold:
                    continue
                objects.append(scale_bbox(x=x, y=y, h=h, w=w, class_id=j, confidence=confidence,
                                          h_scale=orig_im_h, w_scale=orig_im_w))
    return objects


def intersection_over_union(box_1, box_2):
    width_of_overlap_area = min(box_1['xmax'], box_2['xmax']) - max(box_1['xmin'], box_2['xmin'])
    height_of_overlap_area = min(box_1['ymax'], box_2['ymax']) - max(box_1['ymin'], box_2['ymin'])
    if width_of_overlap_area < 0 or height_of_overlap_area < 0:
        area_of_overlap = 0
    else:
        area_of_overlap = width_of_overlap_area * height_of_overlap_area
    box_1_area = (box_1['ymax'] - box_1['ymin']) * (box_1['xmax'] - box_1['xmin'])
    box_2_area = (box_2['ymax'] - box_2['ymin']) * (box_2['xmax'] - box_2['xmin'])
    area_of_union = box_1_area + box_2_area - area_of_overlap
    if area_of_union == 0:
        return 0
    return area_of_overlap / area_of_union


class ModelHandler:
    def __init__(self, labels):
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/open_model_zoo/public/yolo-v3-tf/FP32"))
        model_xml = os.path.join(base_dir, "yolo-v3-tf.xml")
        model_bin = os.path.join(base_dir, "yolo-v3-tf.bin")
        self.model = ModelLoader(model_xml, model_bin)
        self.labels = labels

        ng_func = ngraph.function_from_cnn(self.model.network)

        self.output_info = {}

        for node in ng_func.get_ordered_ops():
            layer_name = node.get_friendly_name()
            if layer_name not in self.model.network.outputs:
                continue
            parent_node = node.inputs()[0].get_source_output().get_node()
            shape = list(parent_node.shape)
            yolo_params = YoloParams(node._get_attributes(), shape[2])
            self.output_info[layer_name] = (shape, yolo_params)

    def infer(self, image, threshold):
        output_layer = self.model.infer(image)

        # Collecting object detection results
        objects = []
        origin_im_size = (image.height, image.width)
        for layer_name, out_blob in output_layer.items():
            shape, yolo_params = self.output_info[layer_name]
            out_blob = out_blob.reshape(shape)
            objects += parse_yolo_region(out_blob, self.model.input_size(),
                origin_im_size, yolo_params, threshold)

        # Filtering overlapping boxes (non-maximum suppression)
        IOU_THRESHOLD = 0.4
        objects = sorted(objects, key=lambda obj : obj['confidence'], reverse=True)
        for i, obj in enumerate(objects):
            if obj['confidence'] == 0:
                continue
            for j in range(i + 1, len(objects)):
                if intersection_over_union(obj, objects[j]) > IOU_THRESHOLD:
                    objects[j]['confidence'] = 0

        results = []
        for obj in objects:
            if obj['confidence'] >= threshold:
                xtl = max(obj['xmin'], 0)
                ytl = max(obj['ymin'], 0)
                xbr = min(obj['xmax'], image.width)
                ybr = min(obj['ymax'], image.height)
                obj_class = int(obj['class_id'])

                results.append({
                    "confidence": str(obj['confidence']),
                    "label": self.labels.get(obj_class, "unknown"),
                    "points": [xtl, ytl, xbr, ybr],
                    "type": "rectangle",
                })

        return results


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\dschoerk\transt\nuclio\main.py =====
import base64
import io
import json

import numpy as np
from model_handler import ModelHandler
from PIL import Image


def init_context(context):
    context.logger.info("Init context...  0%")
    model = ModelHandler()
    context.user_data.model = model
    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run TransT model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    shapes = data.get("shapes")
    states = data.get("states")

    image = Image.open(buf).convert('RGB')
    image = np.array(image)[:, :, ::-1].copy()

    results = {
        'shapes': [],
        'states': []
    }
    for i, shape in enumerate(shapes):
        shape, state = context.user_data.model.infer(image, shape, states[i] if i < len(states) else None)
        results['shapes'].append(shape)
        results['states'].append(state)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\dschoerk\transt\nuclio\model_handler.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import jsonpickle
import numpy as np
import torch
from pysot_toolkit.bbox import get_axis_aligned_bbox
from pysot_toolkit.trackers.net_wrappers import NetWithBackbone
from pysot_toolkit.trackers.tracker import Tracker


class ModelHandler:
    def __init__(self):
        use_gpu = torch.cuda.is_available()
        net_path = '/transt.pth' # Absolute path of the model
        net = NetWithBackbone(net_path=net_path, use_gpu=use_gpu)
        self.tracker = Tracker(name='transt', net=net, window_penalty=0.49, exemplar_size=128, instance_size=256)

    def decode_state(self, state):
        # The server ensures that `state` is one of the values that the function itself
        # has previously output. Therefore it should be safe to use jsonpickle.
        decode = jsonpickle.decode  # nosec: B301

        self.tracker.net.net.zf = decode(state['model.net.net.zf'])
        self.tracker.net.net.pos_template = decode(state['model.net.net.pos_template'])

        self.tracker.window = decode(state['model.window'])
        self.tracker.center_pos = decode(state['model.center_pos'])
        self.tracker.size = decode(state['model.size'])
        self.tracker.channel_average = decode(state['model.channel_average'])
        self.tracker.mean = decode(state['model.mean'])
        self.tracker.std = decode(state['model.std'])
        self.tracker.inplace = decode(state['model.inplace'])

        self.tracker.features_initialized = False
        if 'model.features_initialized' in state:
            self.tracker.features_initialized = decode(state['model.features_initialized'])

    def encode_state(self):
        state = {}
        state['model.net.net.zf'] = jsonpickle.encode(self.tracker.net.net.zf)
        state['model.net.net.pos_template'] = jsonpickle.encode(self.tracker.net.net.pos_template)
        state['model.window'] = jsonpickle.encode(self.tracker.window)
        state['model.center_pos'] = jsonpickle.encode(self.tracker.center_pos)
        state['model.size'] = jsonpickle.encode(self.tracker.size)
        state['model.channel_average'] = jsonpickle.encode(self.tracker.channel_average)
        state['model.mean'] = jsonpickle.encode(self.tracker.mean)
        state['model.std'] = jsonpickle.encode(self.tracker.std)
        state['model.inplace'] = jsonpickle.encode(self.tracker.inplace)
        state['model.features_initialized'] = jsonpickle.encode(getattr(self.tracker, 'features_initialized', False))

        return state

    def init_tracker(self, img, bbox):
        cx, cy, w, h = get_axis_aligned_bbox(np.array(bbox))
        gt_bbox_ = [cx - w / 2, cy - h / 2, w, h]
        init_info = {'init_bbox': gt_bbox_}
        self.tracker.initialize(img, init_info)

    def track(self, img):
        outputs = self.tracker.track(img)
        prediction_bbox = outputs['target_bbox']

        left = prediction_bbox[0]
        top = prediction_bbox[1]
        right = prediction_bbox[0] + prediction_bbox[2]
        bottom = prediction_bbox[1] + prediction_bbox[3]
        return (left, top, right, bottom)

    def infer(self, image, shape, state):
        if state is None:
            init_shape = (shape[0], shape[1], shape[2] - shape[0], shape[3] - shape[1])

            self.init_tracker(image, init_shape)
            state = self.encode_state()
        else:
            self.decode_state(state)
            shape = self.track(image)
            state = self.encode_state()

        return shape, state


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\facebookresearch\detectron2\retinanet_r101\nuclio\main.py =====
import json
import base64
import io
from PIL import Image

import torch
from detectron2.model_zoo import get_config
from detectron2.data.detection_utils import convert_PIL_to_numpy
from detectron2.engine.defaults import DefaultPredictor
from detectron2.data.datasets.builtin_meta import COCO_CATEGORIES

CONFIG_OPTS = ["MODEL.WEIGHTS", "model_final_971ab9.pkl"]
CONFIDENCE_THRESHOLD = 0.5

def init_context(context):
    context.logger.info("Init context...  0%")

    cfg = get_config('COCO-Detection/retinanet_R_101_FPN_3x.yaml')
    if torch.cuda.is_available():
        CONFIG_OPTS.extend(['MODEL.DEVICE', 'cuda'])
    else:
        CONFIG_OPTS.extend(['MODEL.DEVICE', 'cpu'])

    cfg.merge_from_list(CONFIG_OPTS)
    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = CONFIDENCE_THRESHOLD
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = CONFIDENCE_THRESHOLD
    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = CONFIDENCE_THRESHOLD
    cfg.freeze()
    predictor = DefaultPredictor(cfg)

    context.user_data.model_handler = predictor

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run retinanet-R101 model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = convert_PIL_to_numpy(Image.open(buf), format="BGR")

    predictions = context.user_data.model_handler(image)

    instances = predictions['instances']
    pred_boxes = instances.pred_boxes
    scores = instances.scores
    pred_classes = instances.pred_classes
    results = []
    for box, score, label in zip(pred_boxes, scores, pred_classes):
        label = COCO_CATEGORIES[int(label)]["name"]
        if score >= threshold:
            results.append({
                "confidence": str(float(score)),
                "label": label,
                "points": box.tolist(),
                "type": "rectangle",
            })

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\facebookresearch\sam\nuclio\main.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")
    model = ModelHandler()
    context.user_data.model = model
    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("call handler")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    image = Image.open(buf)
    image = image.convert("RGB")  #  to make sure image comes in RGB
    features = context.user_data.model.handle(image)

    return context.Response(body=json.dumps({
            'blob': base64.b64encode((features.cpu().numpy() if features.is_cuda else features.numpy())).decode(),
        }),
        headers={},
        content_type='application/json',
        status_code=200
    )


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\facebookresearch\sam\nuclio\model_handler.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import numpy as np
import torch
from segment_anything import sam_model_registry, SamPredictor

class ModelHandler:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.sam_checkpoint = "/opt/nuclio/sam/sam_vit_h_4b8939.pth"
        self.model_type = "vit_h"
        self.latest_image = None
        sam_model = sam_model_registry[self.model_type](checkpoint=self.sam_checkpoint)
        sam_model.to(device=self.device)
        self.predictor = SamPredictor(sam_model)

    def handle(self, image):
        self.predictor.set_image(np.array(image))
        features = self.predictor.get_image_embedding()
        return features


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\foolwood\siammask\nuclio\main.py =====
import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    # Read the DL model
    model = ModelHandler()
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run SiamMask model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    shapes = data.get("shapes")
    states = data.get("states")
    image = Image.open(buf)
    image = image.convert("RGB")  #  to make sure image comes in RGB

    results = {
        'shapes': [],
        'states': []
    }
    for i, shape in enumerate(shapes):
        shape, state = context.user_data.model.infer(image, shape, states[i] if i < len(states) else None)
        results['shapes'].append(shape)
        results['states'].append(state)

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\foolwood\siammask\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import os
from copy import copy

import jsonpickle
import numpy as np
import torch

from tools.test import siamese_init, siamese_track
from utils.config_helper import load_config
from utils.load_helper import load_pretrain

class ModelHandler:
    def __init__(self):
        # Setup device
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        torch.backends.cudnn.benchmark = True

        base_dir = os.path.abspath(os.environ.get("MODEL_PATH",
            "/opt/nuclio/SiamMask/experiments/siammask_sharp"))
        class configPath:
            config = os.path.join(base_dir, "config_davis.json")

        self.config = load_config(configPath)
        from custom import Custom
        siammask = Custom(anchors=self.config['anchors'])
        self.siammask = load_pretrain(siammask, os.path.join(base_dir, "SiamMask_DAVIS.pth"))
        self.siammask.eval().to(self.device)

    def encode_state(self, state):
        state['net.zf'] = state['net'].zf
        state.pop('net', None)
        state.pop('mask', None)

        for k,v in state.items():
            state[k] = jsonpickle.encode(v)

        return state

    def decode_state(self, state):
        for k,v in state.items():
            # The server ensures that `state` is one of the values that the function itself
            # has previously output. Therefore it should be safe to use jsonpickle.
            state[k] = jsonpickle.decode(v)  # nosec: B301

        state['net'] = copy(self.siammask)
        state['net'].zf = state['net.zf']
        del state['net.zf']

        return state

    def infer(self, image, shape, state):
        image = np.array(image)
        if state is None: # init tracking
            xtl, ytl, xbr, ybr = shape
            target_pos = np.array([(xtl + xbr) / 2, (ytl + ybr) / 2])
            target_sz = np.array([xbr - xtl, ybr - ytl])
            siammask = copy(self.siammask) # don't modify self.siammask
            state = siamese_init(image, target_pos, target_sz, siammask,
                self.config['hp'], device=self.device)
            state = self.encode_state(state)
        else: # track
            state = self.decode_state(state)
            state = siamese_track(state, image, mask_enable=True,
                refine_enable=True, device=self.device)
            shape = state['ploygon'].flatten().tolist()
            state = self.encode_state(state)

        return shape, state



# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\mmpose\hrnet32\nuclio\main.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
import io
import yaml
import numpy as np
from PIL import Image

from mmpose.apis import MMPoseInferencer

def init_context(context):
    context.logger.info("Init detector...")

    det_config = "/opt/nuclio/mmpose/projects/rtmpose/rtmdet/person/rtmdet_nano_320-8xb32_coco-person.py"
    det_checkpoint = "/opt/nuclio/rtmdet_nano_8xb32-100e_coco-obj365-person-05d8511e.pth"
    pose_config = "/opt/nuclio/mmpose/configs/wholebody_2d_keypoint/topdown_heatmap/ubody2d/td-hm_hrnet-w32_8xb64-210e_ubody-256x192.py"
    pose_checkpoint = "/opt/nuclio/td-hm_hrnet-w32_8xb64-210e_ubody-coco-256x192-7c227391_20230807.pth"

    inferencer = MMPoseInferencer(
        pose2d=pose_config,
        pose2d_weights=pose_checkpoint,
        det_model=det_config,
        det_weights=det_checkpoint,
        det_cat_ids=[0],  # the category id of 'human' class
        device='cpu'
    )

    context.logger.info("Init labels...")
    with open("/opt/nuclio/function.yaml", "rb") as function_file:
        functionconfig = yaml.safe_load(function_file)
        labels_spec = functionconfig["metadata"]["annotations"]["spec"]
        labels = json.loads(labels_spec)

    context.user_data.labels = labels
    context.user_data.inferencer = inferencer
    context.logger.info("Function initialized")

def handler(context, event):
    context.logger.info("Run mmpose ubody-2d model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = data.get('threshold', 0.55)
    image = Image.open(buf).convert("RGB")

    results = []
    pred_instances = next(context.user_data.inferencer(np.array(image)[...,::-1]))["predictions"][0]

    for pred_instance in pred_instances:
        keypoints = pred_instance["keypoints"]
        keypoint_scores = pred_instance["keypoint_scores"]
        for label in context.user_data.labels:
            skeleton = {
                "confidence": str(pred_instance["bbox_score"]),
                "label": label["name"],
                "type": "skeleton",
                "elements": [{
                    "label": element["name"],
                    "type": "points",
                    "outside": 0 if threshold < keypoint_scores[element["id"]] else 1,
                    "points": [
                        float(keypoints[element["id"]][0]),
                        float(keypoints[element["id"]][1])
                    ],
                    "confidence": str(keypoint_scores[element["id"]]),
                } for element in label["sublabels"]],
            }

            if not all([element['outside'] for element in skeleton["elements"]]):
                results.append(skeleton)

    return context.Response(body=json.dumps(results), headers={}, content_type="application/json", status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\saic-vul\fbrs\nuclio\main.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    model = ModelHandler() # pylint: disable=no-value-for-parameter
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("call handler")
    data = event.body
    pos_points = data["pos_points"]
    neg_points = data["neg_points"]
    threshold = data.get("threshold", 0.5)
    buf = io.BytesIO(base64.b64decode(data["image"]))
    image = Image.open(buf).convert('RGB')

    mask = context.user_data.model.handle(image, pos_points,
        neg_points, threshold)
    return context.Response(
        body=json.dumps({ 'mask': mask.tolist() }),
        headers={},
        content_type='application/json',
        status_code=200
    )


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\saic-vul\fbrs\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import torch
import numpy as np
from torchvision import transforms
import cv2
import os

from isegm.inference.predictors import get_predictor
from isegm.inference.utils import load_deeplab_is_model, load_hrnet_is_model
from isegm.inference.clicker import Clicker, Click

class ModelHandler:
    def __init__(self):
        torch.backends.cudnn.deterministic = True
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH", "/opt/nuclio/fbrs"))
        model_path = os.path.join(base_dir, "resnet101_dh256_sbd.pth")
        state_dict = torch.load(model_path, map_location='cpu')

        self.net = None
        backbone = 'auto'
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        for k in state_dict.keys():
            if 'feature_extractor.stage2.0.branches' in k:
                self.net = load_hrnet_is_model(state_dict, self.device, backbone)
                break

        if self.net is None:
            self.net = load_deeplab_is_model(state_dict, self.device, backbone)
        self.net.to(self.device)

    def handle(self, image, pos_points, neg_points, threshold):
        input_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([.485, .456, .406], [.229, .224, .225])
        ])

        image_nd = input_transform(image).to(self.device)

        clicker = Clicker()
        for x, y in pos_points:
            click = Click(is_positive=True, coords=(y, x))
            clicker.add_click(click)

        for x, y in neg_points:
            click = Click(is_positive=False, coords=(y, x))
            clicker.add_click(click)

        predictor_params = {
            'brs_mode': 'f-BRS-B',
            'brs_opt_func_params': {'min_iou_diff': 0.001},
            'lbfgs_params': {'maxfun': 20},
            'predictor_params': {'max_size': 800, 'net_clicks_limit': 8},
            'prob_thresh': threshold,
            'zoom_in_params': {'expansion_ratio': 1.4, 'skip_clicks': 1, 'target_size': 480}}
        predictor = get_predictor(self.net, device=self.device,
            **predictor_params)
        predictor.set_input_image(image_nd)

        object_prob = predictor.get_prediction(clicker)
        if self.device == 'cuda':
            torch.cuda.empty_cache()
        mask = object_prob > threshold
        mask = np.array(mask, dtype=np.uint8)
        cv2.normalize(mask, mask, 0, 255, cv2.NORM_MINMAX)

        return mask


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\saic-vul\hrnet\nuclio\main.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
from PIL import Image
import io
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    model = ModelHandler() # pylint: disable=no-value-for-parameter
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("call handler")
    data = event.body
    pos_points = data["pos_points"]
    neg_points = data["neg_points"]
    threshold = data.get("threshold", 0.5)
    buf = io.BytesIO(base64.b64decode(data["image"]))
    image = Image.open(buf).convert('RGB')

    mask = context.user_data.model.handle(image, pos_points, neg_points, threshold)
    return context.Response(
        body=json.dumps({ 'mask': mask.tolist() }),
        headers={},
        content_type='application/json',
        status_code=200
    )


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\saic-vul\hrnet\nuclio\model_handler.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import torch
import numpy as np
import cv2
import os

from isegm.inference import utils
from isegm.inference.predictors import get_predictor
from isegm.inference.clicker import Clicker, Click

class ModelHandler:
    def __init__(self):
        torch.backends.cudnn.deterministic = True
        base_dir = os.path.abspath(os.environ.get("MODEL_PATH", "/opt/nuclio/hrnet"))
        model_path = os.path.join(base_dir)

        self.net = None
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'

        checkpoint_path = utils.find_checkpoint(model_path, "coco_lvis_h18_itermask.pth")
        self.net = utils.load_is_model(checkpoint_path, self.device)

    def handle(self, image, pos_points, neg_points, threshold):
        image_nd = np.array(image)

        clicker = Clicker()
        for x, y in pos_points:
            click = Click(is_positive=True, coords=(y, x))
            clicker.add_click(click)

        for x, y in neg_points:
            click = Click(is_positive=False, coords=(y, x))
            clicker.add_click(click)

        predictor = get_predictor(self.net, 'NoBRS', device=self.device, prob_thresh=0.49)
        predictor.set_input_image(image_nd)

        object_prob = predictor.get_prediction(clicker)
        if self.device == 'cuda':
            torch.cuda.empty_cache()
        mask = object_prob > threshold
        mask = np.array(mask, dtype=np.uint8)
        cv2.normalize(mask, mask, 0, 255, cv2.NORM_MINMAX)

        return mask


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\shiyinzhang\iog\nuclio\main.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import base64
from PIL import Image
import io
import numpy as np
from model_handler import ModelHandler

def init_context(context):
    context.logger.info("Init context...  0%")

    model = ModelHandler() # pylint: disable=no-value-for-parameter
    context.user_data.model = model

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("call handler")
    data = event.body
    pos_points = data["pos_points"]
    neg_points = data["neg_points"]
    obj_bbox = data.get("obj_bbox", None)
    threshold = data.get("threshold", 0.8)
    buf = io.BytesIO(base64.b64decode(data["image"]))
    image = Image.open(buf)

    if obj_bbox is None:
        x, y = np.split(np.transpose(np.array(neg_points)), 2)
        obj_bbox = [np.min(x), np.min(y), np.max(x), np.max(y)]
        neg_points = []

    mask = context.user_data.model.handle(image, obj_bbox, pos_points, neg_points, threshold)
    return context.Response(
        body=json.dumps({ 'mask': mask.tolist() }),
        headers={},
        content_type='application/json',
        status_code=200
    )


# ===== 文件: D:\wow_ai\docker\cvat\serverless\pytorch\shiyinzhang\iog\nuclio\model_handler.py =====
# Copyright (C) 2020-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import numpy as np
import os
import cv2
import torch
from networks.mainnetwork import Network
from dataloaders import helpers

class ModelHandler:
    def __init__(self):
        base_dir = os.environ.get("MODEL_PATH", "/opt/nuclio/iog")
        model_path = os.path.join(base_dir, "IOG_PASCAL_SBD.pth")
        self.device = torch.device("cpu")

        # Number of input channels (RGB + heatmap of IOG points)
        self.net = Network(nInputChannels=5, num_classes=1, backbone='resnet101',
            output_stride=16, sync_bn=None, freeze_bn=False)

        pretrain_dict = torch.load(model_path)
        self.net.load_state_dict(pretrain_dict)
        self.net.to(self.device)
        self.net.eval()

    def handle(self, image, bbox, pos_points, neg_points, threshold):
        with torch.no_grad():
            # extract a crop with padding from the image
            crop_padding = 30
            crop_bbox = [
                max(bbox[0][0] - crop_padding, 0),
                max(bbox[0][1] - crop_padding, 0),
                min(bbox[1][0] + crop_padding, image.width - 1),
                min(bbox[1][1] + crop_padding, image.height - 1)
            ]
            crop_shape = (
                int(crop_bbox[2] - crop_bbox[0] + 1), # width
                int(crop_bbox[3] - crop_bbox[1] + 1), # height
            )

            # try to use crop_from_bbox(img, bbox, zero_pad) here
            input_crop = np.array(image.crop(crop_bbox)).astype(np.float32)

            # resize the crop
            input_crop = cv2.resize(input_crop, (512, 512), interpolation=cv2.INTER_NEAREST)
            crop_scale = (512 / crop_shape[0], 512 / crop_shape[1])

            def translate_points_to_crop(points):
                points = [
                    ((p[0] - crop_bbox[0]) * crop_scale[0], # x
                     (p[1] - crop_bbox[1]) * crop_scale[1]) # y
                    for p in points]

                return points

            pos_points = translate_points_to_crop(pos_points)
            neg_points = translate_points_to_crop(neg_points)

            # Create IOG image
            pos_gt = np.zeros(shape=input_crop.shape[:2], dtype=np.float64)
            neg_gt = np.zeros(shape=input_crop.shape[:2], dtype=np.float64)
            for p in pos_points:
                pos_gt = np.maximum(pos_gt, helpers.make_gaussian(pos_gt.shape, center=p))
            for p in neg_points:
                neg_gt = np.maximum(neg_gt, helpers.make_gaussian(neg_gt.shape, center=p))
            iog_image = np.stack((pos_gt, neg_gt), axis=2).astype(dtype=input_crop.dtype)

            # Convert iog_image to an image (0-255 values)
            cv2.normalize(iog_image, iog_image, 0, 255, cv2.NORM_MINMAX)

            # Concatenate input crop and IOG image
            input_blob = np.concatenate((input_crop, iog_image), axis=2)

            # numpy image: H x W x C
            # torch image: C X H X W
            input_blob = input_blob.transpose((2, 0, 1))
            # batch size is 1
            input_blob = np.array([input_blob])
            input_tensor = torch.from_numpy(input_blob)

            input_tensor = input_tensor.to(self.device)
            output_mask = self.net.forward(input_tensor)[4]
            output_mask = output_mask.to(self.device)
            pred = np.transpose(output_mask.data.numpy()[0, :, :, :], (1, 2, 0))
            pred = pred > threshold
            pred = np.squeeze(pred)

            # Convert a mask to a polygon
            pred = np.array(pred, dtype=np.uint8)
            pred = cv2.resize(pred, dsize=(crop_shape[0], crop_shape[1]),
                interpolation=cv2.INTER_CUBIC)
            cv2.normalize(pred, pred, 0, 255, cv2.NORM_MINMAX)

            mask = np.zeros((image.height, image.width), dtype=np.uint8)
            x = int(crop_bbox[0])
            y = int(crop_bbox[1])
            mask[y : y + crop_shape[1], x : x + crop_shape[0]] = pred

            return mask


# ===== 文件: D:\wow_ai\docker\cvat\serverless\tensorflow\faster_rcnn_inception_v2_coco\nuclio\main.py =====
import json
import base64
import io
from PIL import Image
import yaml
from model_loader import ModelLoader


def init_context(context):
    context.logger.info("Init context...  0%")
    model_path = "/opt/nuclio/faster_rcnn/frozen_inference_graph.pb"
    model_handler = ModelLoader(model_path)
    context.user_data.model_handler = model_handler

    with open("/opt/nuclio/function.yaml", 'rb') as function_file:
        functionconfig = yaml.safe_load(function_file)
    labels_spec = functionconfig['metadata']['annotations']['spec']
    labels = {item['id']: item['name'] for item in json.loads(labels_spec)}
    context.user_data.labels = labels

    context.logger.info("Init context...100%")

def handler(context, event):
    context.logger.info("Run faster_rcnn_inception_v2_coco model")
    data = event.body
    buf = io.BytesIO(base64.b64decode(data["image"]))
    threshold = float(data.get("threshold", 0.5))
    image = Image.open(buf)

    (boxes, scores, classes, num_detections) = context.user_data.model_handler.infer(image)

    results = []
    for i in range(int(num_detections[0])):
        obj_class = int(classes[0][i])
        obj_score = scores[0][i]
        obj_label = context.user_data.labels.get(obj_class, "unknown")
        if obj_score >= threshold:
            xtl = boxes[0][i][1] * image.width
            ytl = boxes[0][i][0] * image.height
            xbr = boxes[0][i][3] * image.width
            ybr = boxes[0][i][2] * image.height

            results.append({
                "confidence": str(obj_score),
                "label": obj_label,
                "points": [xtl, ytl, xbr, ybr],
                "type": "rectangle",
            })

    return context.Response(body=json.dumps(results), headers={},
        content_type='application/json', status_code=200)


# ===== 文件: D:\wow_ai\docker\cvat\serverless\tensorflow\faster_rcnn_inception_v2_coco\nuclio\model_loader.py =====

import numpy as np
from PIL import Image
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

class ModelLoader:
    def __init__(self, model_path):
        self.session = None

        detection_graph = tf.Graph()
        with detection_graph.as_default():
            od_graph_def = tf.GraphDef()
            with tf.gfile.GFile(model_path, 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')
            gpu_fraction = 0.333
            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction,
                                        allow_growth=True)
            config = tf.ConfigProto(gpu_options=gpu_options)
            self.session = tf.Session(graph=detection_graph, config=config)

            self.image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
            self.boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
            self.scores = detection_graph.get_tensor_by_name('detection_scores:0')
            self.classes = detection_graph.get_tensor_by_name('detection_classes:0')
            self.num_detections = detection_graph.get_tensor_by_name('num_detections:0')

    def __del__(self):
        if self.session:
            self.session.close()
            del self.session

    def infer(self, image):
        width, height = image.size
        if width > 1920 or height > 1080:
            image = image.resize((width // 2, height // 2), Image.LANCZOS)
        image_np = np.array(image.getdata())[:, :3].reshape(
            (image.height, image.width, -1)).astype(np.uint8)
        image_np = np.expand_dims(image_np, axis=0)

        return self.session.run(
            [self.boxes, self.scores, self.classes, self.num_detections],
            feed_dict={self.image_tensor: image_np})


# ===== 文件: D:\wow_ai\docker\cvat\site\build_docs.py =====
#!/usr/bin/env python3

# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import shutil
import subprocess
import tempfile
from pathlib import Path
from typing import Optional

import git
import toml
from packaging import version

# the initial version for the documentation site
MINIMUM_VERSION = version.Version("1.5.0")

# apply new hugo version starting from release 2.9.2
UPDATED_HUGO_FROM = version.Version("2.9.2")

# Start the name with HUGO_ for Hugo default security checks
VERSION_URL_ENV_VAR = "HUGO_VERSION_REL_URL"

# Hugo binaries for different versions
hugo110 = "hugo-0.110"  # required for new docs
hugo83 = "hugo-0.83"  # required for older docs


def prepare_tags(repo: git.Repo):
    tags = {}
    for tag in repo.tags:
        tag_version = version.parse(tag.name)
        if tag_version >= MINIMUM_VERSION and not tag_version.is_prerelease:
            release_version = (tag_version.major, tag_version.minor)
            if release_version not in tags or tag_version > version.parse(
                tags[release_version].name
            ):
                tags[release_version] = tag

    return tags.values()


def generate_versioning_config(filename, versions, url_prefix=""):
    def write_version_item(file_object, version, url):
        file_object.write("[[params.versions]]\n")
        file_object.write('version = "{}"\n'.format(version))
        file_object.write('url = "{}"\n\n'.format(url))

    with open(filename, "w") as f:
        write_version_item(f, "Latest version", "{}/".format(url_prefix))
        for v in versions:
            write_version_item(f, v, "{}/{}".format(url_prefix, v))


def git_checkout(ref: str, temp_repo: git.Repo, temp_dir: Path):
    # We need to checkout with submodules, recursively

    subdirs = [
        "site/content",
        "site/assets",
        "site/layouts/partials",
        "site/layouts/shortcodes",
        "site/themes",
    ]

    temp_repo.git.checkout(ref, recurse_submodules=True, force=True)
    temp_repo.git.submodule("update", "--init", "--recursive")
    tmp_repo_root = Path(temp_repo.working_tree_dir)

    for subdir in subdirs:
        dst_dir = temp_dir / subdir
        shutil.rmtree(dst_dir)
        shutil.copytree(tmp_repo_root / subdir, dst_dir, symlinks=True)


def change_version_menu_toml(filename, version):
    data = toml.load(filename)
    data["params"]["version_menu"] = version

    with open(filename, "w") as f:
        toml.dump(data, f)


def generate_docs(repo: git.Repo, output_dir: os.PathLike, tags):
    repo_root = Path(repo.working_tree_dir)

    with tempfile.TemporaryDirectory() as temp_dir:
        content_loc = Path(temp_dir, "site")
        shutil.copytree(repo_root / "site", content_loc, symlinks=True)

        def run_npm_install():
            subprocess.run(["npm", "install"], cwd=content_loc)  # nosec

        def run_hugo(
            destination_dir: os.PathLike,
            *,
            extra_env_vars: dict[str, str] = None,
            executable: Optional[str] = "hugo",
        ):
            extra_kwargs = {}

            if extra_env_vars:
                extra_kwargs["env"] = os.environ.copy()
                extra_kwargs["env"].update(extra_env_vars)

            subprocess.run(  # nosec
                [
                    executable,
                    "--destination",
                    str(destination_dir),
                    "--config",
                    "config.toml,versioning.toml",
                ],
                cwd=content_loc,
                check=True,
                **extra_kwargs,
            )

        versioning_toml_path = content_loc / "versioning.toml"

        # Process the develop version
        generate_versioning_config(versioning_toml_path, (t.name for t in tags))
        change_version_menu_toml(versioning_toml_path, "develop")
        run_hugo(output_dir, executable=hugo110)

        # Create a temp repo for checkouts
        temp_repo_path = Path(temp_dir) / "tmp_repo"
        shutil.copytree(repo_root, temp_repo_path, symlinks=True)
        temp_repo = git.Repo(temp_repo_path)
        temp_repo.git.reset(hard=True, recurse_submodules=True)

        # Process older versions
        generate_versioning_config(versioning_toml_path, (t.name for t in tags), "/..")
        for tag in tags:
            git_checkout(tag.name, temp_repo, Path(temp_dir))
            change_version_menu_toml(versioning_toml_path, tag.name)
            run_npm_install()

            # find correct hugo version
            hugo = hugo110 if version.parse(tag.name) >= UPDATED_HUGO_FROM else hugo83

            run_hugo(
                output_dir / tag.name,
                # This variable is no longer needed by the current version,
                # but it was required in v2.11.2 and older.
                extra_env_vars={VERSION_URL_ENV_VAR: f"/{tag.name}/docs"},
                executable=hugo,
            )


def validate_env():
    for hugo in [hugo83, hugo110]:
        try:
            subprocess.run([hugo, "version"], capture_output=True)  # nosec
        except (subprocess.CalledProcessError, FileNotFoundError) as ex:
            raise Exception(f"Failed to run '{hugo}', please make sure it exists.") from ex


if __name__ == "__main__":
    repo_root = Path(__file__).resolve().parents[1]
    output_dir = repo_root / "public"

    validate_env()

    with git.Repo(repo_root) as repo:
        tags = prepare_tags(repo)
        generate_docs(repo, output_dir, tags)


# ===== 文件: D:\wow_ai\docker\cvat\site\process_sdk_docs.py =====
#!/usr/bin/env python3

# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import os
import os.path as osp
import re
import shutil
import sys
import textwrap
from glob import iglob
from typing import Callable

from inflection import underscore


class Processor:
    _reference_files: list[str]

    def __init__(self, *, input_dir: str, site_root: str) -> None:
        self._input_dir = input_dir
        self._site_root = site_root

        self._content_dir = osp.join(self._site_root, "content")
        self._sdk_reference_dir = osp.join(self._content_dir, "en/docs/api_sdk/sdk/reference")
        self._templates_dir = osp.join(self._site_root, "templates")

    @staticmethod
    def _copy_files(src_dir: str, glob_pattern: str, dst_dir: str) -> list[str]:
        copied_files = []

        for src_path in iglob(osp.join(src_dir, glob_pattern), recursive=True):
            src_filename = osp.relpath(src_path, src_dir)
            dst_path = osp.join(dst_dir, src_filename)
            # assume dst dir exists
            shutil.copy(src_path, dst_path, follow_symlinks=True)

            copied_files.append(dst_path)

        return copied_files

    def _copy_pages(self):
        self._reference_files = self._copy_files(
            self._input_dir, "*/**/*.md", self._sdk_reference_dir
        )

    def _add_page_headers(self):
        """
        Adds headers required by hugo to docs pages
        """

        HEADER_SEPARATOR = "---"

        for p in self._reference_files:
            with open(p) as f:
                contents = f.read()

            assert not contents.startswith(HEADER_SEPARATOR), p

            lines = contents.splitlines()

            assert lines[0].startswith("#")
            classname = lines[0][1:].strip()

            header = textwrap.dedent(
                """\
                %(header_separator)s
                title: '%(classname)s class reference'
                linkTitle: '%(classname)s'
                weight: 10
                description: ''
                %(header_separator)s
            """
                % {"header_separator": HEADER_SEPARATOR, "classname": classname}
            )

            contents = header + "\n".join(lines[1:])

            with open(p, "w") as f:
                f.write(contents)

    def _move_api_summary(self):
        """
        Moves API summary section from README to apis/_index
        """

        SUMMARY_REPLACE_TOKEN = "{{REPLACEME:apis_summary}}"  # nosec

        with open(osp.join(self._input_dir, "api_summary.md")) as f:
            apis_summary = f.read()

        apis_index_filename = osp.join(
            osp.relpath(self._sdk_reference_dir, self._content_dir), "apis/_index.md"
        )
        apis_index_path = osp.join(self._templates_dir, apis_index_filename + ".template")
        with open(apis_index_path) as f:
            contents = f.read()

        contents = contents.replace(SUMMARY_REPLACE_TOKEN, apis_summary)

        with open(osp.join(self._content_dir, apis_index_filename), "w") as f:
            f.write(contents)

    def _fix_page_links_and_references(self):
        """
        Replaces reference page links from full lowercase (which is generated by hugo from the
        original camelcase and creates broken links) ('authapi') to the minus-case ('auth-api'),
        which is more readable and works.
        Adds an extra parent directory part to links ('../') as hugo requires, even for neighbor
        files.
        """

        mapping = {}

        for src_path in self._reference_files:
            src_filename = osp.relpath(src_path, self._sdk_reference_dir)
            dst_filename = underscore(src_filename).replace("_", "-")
            dst_path = osp.join(self._sdk_reference_dir, dst_filename)
            os.rename(src_path, dst_path)
            mapping[src_filename] = dst_filename

        self._reference_files = [osp.join(self._sdk_reference_dir, p) for p in mapping.values()]

        for p in iglob(self._sdk_reference_dir + "/**/*.md", recursive=True):
            with open(p) as f:
                contents = f.read()

            for src_filename, dst_filename in mapping.items():
                src_dir, src_filename = osp.split(osp.splitext(src_filename)[0])
                dst_filename = osp.basename(osp.splitext(dst_filename)[0])
                contents = re.sub(
                    rf"(\[.*?\]\()((?:\.\./)?(?:{src_dir}/)?){src_filename}((?:#[^\)]*?)?\))",
                    rf"\1../\2{dst_filename}\3",
                    contents,
                )

            with open(p, "w") as f:
                f.write(contents)

    def _process_non_code_blocks(self, text: str, handlers: list[Callable[[str], str]]) -> str:
        """
        Allows to process Markdown documents with passed callbacks. Callbacks are only
        executed outside code blocks.
        """

        used_quotes = ""
        block_start_pos = 0
        inside_code_block = False
        while block_start_pos < len(text):
            pattern = re.compile(used_quotes or "```|`")
            next_code_block_quote = pattern.search(text, pos=block_start_pos)
            if next_code_block_quote is not None:
                if not used_quotes:
                    inside_code_block = False
                    block_end_pos = next_code_block_quote.start(0)
                    used_quotes = next_code_block_quote.group(0)
                else:
                    inside_code_block = True
                    block_end_pos = next_code_block_quote.end(0)
                    used_quotes = None
            else:
                block_end_pos = len(text)

            if not inside_code_block:
                block = text[block_start_pos:block_end_pos]

                for handler in handlers:
                    block = handler(block)

                text = text[:block_start_pos] + block + text[block_end_pos:]
                block_end_pos = block_start_pos + len(block) + len(used_quotes)

            block_start_pos = block_end_pos

        return text

    def _escape_free_square_brackets(self, text: str) -> str:
        return re.sub(r"\[([^\[\]]*?)\]([^\(])", r"\[\1\]\2", text)

    def _add_angle_brackets_to_free_links(self, text: str) -> str:
        # Adapted from https://stackoverflow.com/a/31952097
        URL_REGEX = (
            # Scheme (HTTP, HTTPS):
            r"(?:https?:\/\/)"
            r"(?:"
            # www:
            r"(?:www\.)?"
            # Host and domain (including ccSLD):
            r"(?:(?:[a-zA-Z0-9][a-zA-Z0-9-]{0,61}[a-zA-Z0-9]\.)+)"
            # TLD:
            r"(?:[a-zA-Z]{2,6})"
            # IP Address:
            r"|(?:\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})"
            r")"
            # Port:
            r"(?::\d{1,5})?"
            # Query path:
            r"(?:(?:\/\S+)*|\/)"
        )

        text = re.sub(
            r"(\A|[\.\s])(" + URL_REGEX + r")([\.\s]|\Z)",
            r"\1<\2>\3",
            text,
            flags=re.MULTILINE,
        )

        return text

    def _fix_parsing_problems(self):
        """
        Adds angle brackets to freestanding links, as the linter requires. Such links can appear
        from the generated model and api descriptions.
        Adds escapes to freestanding square brackets to make parsing correct.
        """

        for p in iglob(self._sdk_reference_dir + "/**/*.md", recursive=True):
            with open(p) as f:
                contents = f.read()

            contents = self._process_non_code_blocks(
                contents,
                [
                    self._add_angle_brackets_to_free_links,
                    self._escape_free_square_brackets,
                ],
            )

            with open(p, "w") as f:
                f.write(contents)

    def run(self):
        assert osp.isdir(self._input_dir), self._input_dir
        assert osp.isdir(self._site_root), self._site_root
        assert osp.isdir(self._sdk_reference_dir), self._sdk_reference_dir
        assert osp.isdir(self._templates_dir), self._templates_dir

        self._copy_pages()
        self._move_api_summary()
        self._add_page_headers()
        self._fix_page_links_and_references()
        self._fix_parsing_problems()


def parse_args(args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input-dir",
        type=osp.abspath,
        default="cvat-sdk/docs/",
        help="Path to the cvat-sdk/docs/ directory",
    )
    parser.add_argument(
        "--site-root",
        type=osp.abspath,
        default="site/",
    )

    return parser.parse_args(args)


def main(args=None):
    args = parse_args(args)
    processor = Processor(input_dir=args.input_dir, site_root=args.site_root)
    processor.run()
    return 0


if __name__ == "__main__":
    sys.exit(main())


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\conftest.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Force execution of fixture definitions
from shared.fixtures.data import *  # pylint: disable=wildcard-import
from shared.fixtures.init import *  # pylint: disable=wildcard-import
from shared.fixtures.util import *  # pylint: disable=wildcard-import


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\cmtp_function.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models
import PIL.Image

spec = cvataa.DetectionFunctionSpec(
    labels=[
        cvataa.label_spec("car", 0),
    ],
)


def detect(
    context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
) -> list[models.LabeledShapeRequest]:
    if context.conv_mask_to_poly:
        return [cvataa.polygon(0, [0, 0, 0, 1, 1, 1])]
    else:
        return [cvataa.mask(0, [1, 0, 0, 0, 0])]


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\conftest.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Force execution of fixture definitions
from sdk.fixtures import *  # pylint: disable=wildcard-import


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\conf_threshold_function.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models
import PIL.Image

spec = cvataa.DetectionFunctionSpec(
    labels=[
        cvataa.label_spec("car", 0),
    ],
)


def detect(
    context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
) -> list[models.LabeledShapeRequest]:
    return [
        cvataa.rectangle(0, [context.conf_threshold, 1, 1, 1]),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\example_function.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models
import PIL.Image

spec = cvataa.DetectionFunctionSpec(
    labels=[
        cvataa.label_spec("car", 0),
    ],
)


def detect(
    context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
) -> list[models.LabeledShapeRequest]:
    return [
        cvataa.rectangle(0, [1, 2, 3, 4]),
    ]


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\example_parameterized_function.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from types import SimpleNamespace as namespace

import cvat_sdk.auto_annotation as cvataa
import cvat_sdk.models as models
import PIL.Image


def create(s: str, i: int, f: float, b: bool) -> cvataa.DetectionFunction:
    assert s == "string"
    assert i == 123
    assert f == 5.5
    assert b is False

    spec = cvataa.DetectionFunctionSpec(
        labels=[
            cvataa.label_spec("car", 0),
        ],
    )

    def detect(
        context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
    ) -> list[models.LabeledShapeRequest]:
        return [
            cvataa.rectangle(0, [1, 2, 3, 4]),
        ]

    return namespace(spec=spec, detect=detect)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\test_cli_misc.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import os
from datetime import timedelta
from io import BytesIO

import packaging.version as pv
import pytest
from cvat_cli._internal.agent import _Event, _NewReconnectionDelay, _parse_event_stream
from cvat_sdk import Client
from cvat_sdk.api_client import models
from cvat_sdk.core.proxies.tasks import ResourceType

from .util import TestCliBase, generate_images, https_reverse_proxy, run_cli


class TestCliMisc(TestCliBase):
    def test_can_warn_on_mismatching_server_version(self, monkeypatch, caplog):
        def mocked_version(_):
            return pv.Version("0")

        # We don't actually run a separate process in the tests here, so it works
        monkeypatch.setattr(Client, "get_server_version", mocked_version)

        self.run_cli("task", "ls")

        assert "Server version '0' is not compatible with SDK version" in caplog.text

    @pytest.mark.parametrize("verify", [True, False])
    def test_can_control_ssl_verification_with_arg(self, verify: bool):
        with https_reverse_proxy() as proxy_url:
            if verify:
                insecure_args = []
            else:
                insecure_args = ["--insecure"]

            run_cli(
                self,
                f"--auth={self.user}:{self.password}",
                f"--server-host={proxy_url}",
                *insecure_args,
                "task",
                "ls",
                expected_code=1 if verify else 0,
            )
            stdout = self.stdout.getvalue()

        if not verify:
            for line in stdout.splitlines():
                int(line)

    def test_can_control_organization_context(self):
        org = "cli-test-org"
        self.client.organizations.create(models.OrganizationWriteRequest(org))

        files = generate_images(self.tmp_path, 1)

        stdout = self.run_cli(
            "task",
            "create",
            "personal_task",
            ResourceType.LOCAL.name,
            *map(os.fspath, files),
            "--labels=" + json.dumps([{"name": "person"}]),
            "--completion_verification_period=0.01",
            organization="",
        )

        personal_task_id = int(stdout.split()[-1])

        stdout = self.run_cli(
            "task",
            "create",
            "org_task",
            ResourceType.LOCAL.name,
            *map(os.fspath, files),
            "--labels=" + json.dumps([{"name": "person"}]),
            "--completion_verification_period=0.01",
            organization=org,
        )

        org_task_id = int(stdout.split()[-1])

        personal_task_ids = list(map(int, self.run_cli("task", "ls", organization="").split()))
        assert personal_task_id in personal_task_ids
        assert org_task_id not in personal_task_ids

        org_task_ids = list(map(int, self.run_cli("task", "ls", organization=org).split()))
        assert personal_task_id not in org_task_ids
        assert org_task_id in org_task_ids

        all_task_ids = list(map(int, self.run_cli("task", "ls").split()))
        assert personal_task_id in all_task_ids
        assert org_task_id in all_task_ids


@pytest.mark.parametrize(
    ["lines", "messages"],
    [
        # empty
        ([], []),
        ([""], [_Event("", "")]),
        # event only
        (["event: test", ""], [_Event("test", "")]),
        (["event: foo", "event: bar", ""], [_Event("bar", "")]),
        # data only
        (["data: test", ""], [_Event("", "test")]),
        (["data: foo", "data: bar", ""], [_Event("", "foo\nbar")]),
        # event and data
        (["event: test", "data: foo", "data: bar", ""], [_Event("test", "foo\nbar")]),
        (["data: foo", "event: test", "data: bar", ""], [_Event("test", "foo\nbar")]),
        (["data: foo", "data: bar", "event: test", ""], [_Event("test", "foo\nbar")]),
        # fields without values
        (["event: test", "event", ""], [_Event("", "")]),
        (["data: test", "data", ""], [_Event("", "test\n")]),
        # incomplete event
        (["event: test", "data: foo"], []),
        # multiple events
        (
            ["event: test1", "data: foo", "", "event: test2", "data: bar", ""],
            [_Event("test1", "foo"), _Event("test2", "bar")],
        ),
        # comments
        ([":"], []),
        ([":1", "event: test", ":2", "data: foo", ":3", ""], [_Event("test", "foo")]),
        # retry
        (["retry: 1234"], [_NewReconnectionDelay(timedelta(milliseconds=1234))]),
        (["retry", "retry:", "retry: a"], []),
        # no space
        (["event:test", "data:foo", ""], [_Event("test", "foo")]),
        # two spaces
        (["event:  test", "data:  foo", ""], [_Event(" test", " foo")]),
        # carriage return
        (["event: test\r", "data: foo\r", "\r"], [_Event("test", "foo")]),
    ],
)
def test_parse_event_stream(lines, messages):
    stream = BytesIO(b"".join(line.encode() + b"\n" for line in lines))
    assert list(_parse_event_stream(stream)) == messages


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\test_cli_projects.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import os

import pytest
from cvat_sdk.api_client import exceptions
from cvat_sdk.core.proxies.projects import Project

from .util import TestCliBase


class TestCliProjects(TestCliBase):
    @pytest.fixture
    def fxt_new_project(self):
        project = self.client.projects.create(
            spec={
                "name": "test_project",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
        )

        return project

    def test_can_create_project(self):
        stdout = self.run_cli(
            "project",
            "create",
            "new_project",
            "--labels",
            json.dumps([{"name": "car"}, {"name": "person"}]),
            "--bug_tracker",
            "https://bugs.example/",
        )

        project_id = int(stdout.rstrip("\n"))
        created_project = self.client.projects.retrieve(project_id)
        assert created_project.name == "new_project"
        assert created_project.bug_tracker == "https://bugs.example/"
        assert {label.name for label in created_project.get_labels()} == {"car", "person"}

    def test_can_create_project_from_dataset(self, fxt_coco_dataset):
        stdout = self.run_cli(
            "project",
            "create",
            "new_project",
            "--dataset_path",
            os.fspath(fxt_coco_dataset),
            "--dataset_format",
            "COCO 1.0",
        )

        project_id = int(stdout.rstrip("\n"))
        created_project = self.client.projects.retrieve(project_id)
        assert created_project.name == "new_project"
        assert {label.name for label in created_project.get_labels()} == {"car", "person"}
        assert created_project.tasks.count == 1

    def test_can_list_projects_in_simple_format(self, fxt_new_project: Project):
        output = self.run_cli("project", "ls")

        results = output.split("\n")
        assert any(str(fxt_new_project.id) in r for r in results)

    def test_can_list_project_in_json_format(self, fxt_new_project: Project):
        output = self.run_cli("project", "ls", "--json")

        results = json.loads(output)
        assert any(r["id"] == fxt_new_project.id for r in results)

    def test_can_delete_project(self, fxt_new_project: Project):
        self.run_cli("project", "delete", str(fxt_new_project.id))

        with pytest.raises(exceptions.NotFoundException):
            fxt_new_project.fetch()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\test_cli_tasks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import os
from pathlib import Path

import pytest
from cvat_sdk.api_client import exceptions
from cvat_sdk.core.proxies.tasks import ResourceType, Task
from PIL import Image

from sdk.util import generate_coco_json
from shared.utils.helpers import generate_image_file

from .util import TestCliBase, generate_images


class TestCliTasks(TestCliBase):
    @pytest.fixture
    def fxt_image_file(self):
        img_path = self.tmp_path / "img_0.png"
        with img_path.open("wb") as f:
            f.write(generate_image_file(filename=str(img_path)).getvalue())

        return img_path

    @pytest.fixture
    def fxt_coco_file(self, fxt_image_file: Path):
        img_filename = fxt_image_file
        img_size = Image.open(img_filename).size
        ann_filename = self.tmp_path / "coco.json"
        generate_coco_json(ann_filename, img_info=(img_filename, *img_size))

        yield ann_filename

    @pytest.fixture
    def fxt_backup_file(self, fxt_new_task: Task, fxt_coco_file: str):
        backup_path = self.tmp_path / "backup.zip"

        fxt_new_task.import_annotations("COCO 1.0", filename=fxt_coco_file)
        fxt_new_task.download_backup(backup_path)

        yield backup_path

    @pytest.fixture
    def fxt_new_task(self):
        files = generate_images(self.tmp_path, 5)

        task = self.client.tasks.create_from_data(
            spec={
                "name": "test_task",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
            resource_type=ResourceType.LOCAL,
            resources=files,
        )

        return task

    def test_can_create_task_from_local_images(self):
        files = generate_images(self.tmp_path, 5)

        stdout = self.run_cli(
            "task",
            "create",
            "test_task",
            ResourceType.LOCAL.name,
            *map(os.fspath, files),
            "--labels",
            json.dumps([{"name": "car"}, {"name": "person"}]),
            "--completion_verification_period",
            "0.01",
        )

        task_id = int(stdout.rstrip("\n"))
        assert self.client.tasks.retrieve(task_id).size == 5

    def test_can_create_task_from_local_images_with_parameters(self):
        # Checks for regressions of <https://github.com/cvat-ai/cvat/issues/4962>

        files = generate_images(self.tmp_path, 7)
        files.sort(reverse=True)
        frame_step = 3

        stdout = self.run_cli(
            "task",
            "create",
            "test_task",
            ResourceType.LOCAL.name,
            *map(os.fspath, files),
            "--labels",
            json.dumps([{"name": "car"}, {"name": "person"}]),
            "--completion_verification_period",
            "0.01",
            "--sorting-method",
            "predefined",
            "--frame_step",
            str(frame_step),
            "--bug_tracker",
            "http://localhost/bug",
        )

        task_id = int(stdout.rstrip("\n"))
        task = self.client.tasks.retrieve(task_id)
        frames = task.get_frames_info()
        assert [f.name for f in frames] == [
            f.name for i, f in enumerate(files) if i % frame_step == 0
        ]
        assert task.get_meta().frame_filter == f"step={frame_step}"
        assert task.bug_tracker == "http://localhost/bug"

    def test_can_list_tasks_in_simple_format(self, fxt_new_task: Task):
        output = self.run_cli("task", "ls")

        results = output.split("\n")
        assert any(str(fxt_new_task.id) in r for r in results)

    def test_can_list_tasks_in_json_format(self, fxt_new_task: Task):
        output = self.run_cli("task", "ls", "--json")

        results = json.loads(output)
        assert any(r["id"] == fxt_new_task.id for r in results)

    def test_can_delete_task(self, fxt_new_task: Task):
        self.run_cli("task", "delete", str(fxt_new_task.id))

        with pytest.raises(exceptions.NotFoundException):
            fxt_new_task.fetch()

    def test_can_download_task_annotations(self, fxt_new_task: Task):
        filename = self.tmp_path / "task_{fxt_new_task.id}-cvat.zip"
        self.run_cli(
            "task",
            "export-dataset",
            str(fxt_new_task.id),
            str(filename),
            "--format",
            "CVAT for images 1.1",
            "--with-images",
            "no",
            "--completion_verification_period",
            "0.01",
        )

        assert 0 < filename.stat().st_size

    def test_can_download_task_backup(self, fxt_new_task: Task):
        filename = self.tmp_path / "task_{fxt_new_task.id}-cvat.zip"
        self.run_cli(
            "task",
            "backup",
            str(fxt_new_task.id),
            str(filename),
            "--completion_verification_period",
            "0.01",
        )

        assert 0 < filename.stat().st_size

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    def test_can_download_task_frames(self, fxt_new_task: Task, quality: str):
        out_dir = str(self.tmp_path / "downloads")
        self.run_cli(
            "task",
            "frames",
            str(fxt_new_task.id),
            "0",
            "1",
            "--outdir",
            out_dir,
            "--quality",
            quality,
        )

        assert set(os.listdir(out_dir)) == {
            "task_{}_frame_{:06d}.jpg".format(fxt_new_task.id, i) for i in range(2)
        }

    def test_can_upload_annotations(self, fxt_new_task: Task, fxt_coco_file: Path):
        self.run_cli(
            "task",
            "import-dataset",
            str(fxt_new_task.id),
            str(fxt_coco_file),
            "--format",
            "COCO 1.0",
        )

    def test_can_create_from_backup(self, fxt_new_task: Task, fxt_backup_file: Path):
        stdout = self.run_cli("task", "create-from-backup", str(fxt_backup_file))

        task_id = int(stdout.rstrip("\n"))
        assert task_id
        assert task_id != fxt_new_task.id
        assert self.client.tasks.retrieve(task_id).size == fxt_new_task.size

    def test_auto_annotate_with_module(self, fxt_new_task: Task):
        annotations = fxt_new_task.get_annotations()
        assert not annotations.shapes

        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-module={__package__}.example_function",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes

    def test_auto_annotate_with_file(self, fxt_new_task: Task):
        annotations = fxt_new_task.get_annotations()
        assert not annotations.shapes

        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-file={Path(__file__).with_name('example_function.py')}",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes

    def test_auto_annotate_with_parameters(self, fxt_new_task: Task):
        annotations = fxt_new_task.get_annotations()
        assert not annotations.shapes

        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-module={__package__}.example_parameterized_function",
            "-ps=str:string",
            "-pi=int:123",
            "-pf=float:5.5",
            "-pb=bool:false",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes

    def test_auto_annotate_with_threshold(self, fxt_new_task: Task):
        annotations = fxt_new_task.get_annotations()
        assert not annotations.shapes

        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-module={__package__}.conf_threshold_function",
            "--conf-threshold=0.75",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes[0].points[0] == 0.75  # python:S1244 NOSONAR

    def test_auto_annotate_with_cmtp(self, fxt_new_task: Task):
        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-module={__package__}.cmtp_function",
            "--clear-existing",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes[0].type.value == "mask"

        self.run_cli(
            "task",
            "auto-annotate",
            str(fxt_new_task.id),
            f"--function-module={__package__}.cmtp_function",
            "--clear-existing",
            "--conv-mask-to-poly",
        )

        annotations = fxt_new_task.get_annotations()
        assert annotations.shapes[0].type.value == "polygon"

    def test_legacy_alias(self, caplog):
        # All legacy aliases are implemented the same way;
        # no need to test every single one.
        self.run_cli("ls")

        assert "deprecated" in caplog.text
        assert "task ls" in caplog.text


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\util.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


import contextlib
import http.server
import io
import ssl
import threading
import unittest
from collections.abc import Generator
from pathlib import Path
from typing import Any, Optional, Union

import pytest
import requests
from cvat_sdk import make_client

from shared.utils.config import BASE_URL, USER_PASS
from shared.utils.helpers import generate_image_file


def run_cli(test: Union[unittest.TestCase, Any], *args: str, expected_code: int = 0) -> None:
    from cvat_cli.__main__ import main

    if isinstance(test, unittest.TestCase):
        # Unittest
        test.assertEqual(expected_code, main(args), str(args))
    else:
        # Pytest case
        assert expected_code == main(args)


def generate_images(dst_dir: Path, count: int) -> list[Path]:
    filenames = []
    dst_dir.mkdir(parents=True, exist_ok=True)
    for i in range(count):
        filename = dst_dir / f"img_{i}.jpg"
        filename.write_bytes(generate_image_file(filename.name).getvalue())
        filenames.append(filename)
    return filenames


@contextlib.contextmanager
def https_reverse_proxy() -> Generator[str, None, None]:
    ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
    ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2
    cert_dir = Path(__file__).parent
    ssl_context.load_cert_chain(cert_dir / "self-signed.crt", cert_dir / "self-signed.key")

    with http.server.HTTPServer(("localhost", 0), _ProxyHttpRequestHandler) as proxy_server:
        proxy_server.socket = ssl_context.wrap_socket(
            proxy_server.socket,
            server_side=True,
        )
        server_thread = threading.Thread(target=proxy_server.serve_forever)
        server_thread.start()
        try:
            yield f"https://localhost:{proxy_server.server_port}"
        finally:
            proxy_server.shutdown()
            server_thread.join()


class _ProxyHttpRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        response = requests.get(**self._shared_request_args())
        self._translate_response(response)

    def do_POST(self):
        body_length = int(self.headers["Content-Length"])

        response = requests.post(data=self.rfile.read(body_length), **self._shared_request_args())
        self._translate_response(response)

    def _shared_request_args(self) -> dict[str, Any]:
        headers = {k.lower(): v for k, v in self.headers.items()}
        del headers["host"]

        return {"url": BASE_URL + self.path, "headers": headers, "timeout": 60, "stream": True}

    def _translate_response(self, response: requests.Response) -> None:
        self.send_response(response.status_code)
        for key, value in response.headers.items():
            self.send_header(key, value)
        self.end_headers()
        # Need to use raw here to prevent requests from handling Content-Encoding.
        self.wfile.write(response.raw.read())


class TestCliBase:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        restore_db_per_function,  # force fixture call order to allow DB setup
        restore_redis_inmem_per_function,
        restore_redis_ondisk_per_function,
        fxt_stdout: io.StringIO,
        tmp_path: Path,
        admin_user: str,
    ):
        self.tmp_path = tmp_path
        self.stdout = fxt_stdout
        self.host, self.port = BASE_URL.rsplit(":", maxsplit=1)
        self.user = admin_user
        self.password = USER_PASS
        self.client = make_client(
            host=self.host, port=self.port, credentials=(self.user, self.password)
        )
        self.client.config.status_check_period = 0.01

        yield

    def run_cli(
        self, cmd: str, *args: str, expected_code: int = 0, organization: Optional[str] = None
    ) -> str:
        common_args = [
            f"--auth={self.user}:{self.password}",
            f"--server-host={self.host}",
            f"--server-port={self.port}",
        ]

        if organization is not None:
            common_args.append(f"--organization={organization}")

        run_cli(
            self,
            *common_args,
            cmd,
            *args,
            expected_code=expected_code,
        )
        return self.stdout.getvalue()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\cli\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_analytics.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import csv
import json
import uuid
from collections import Counter
from datetime import datetime, timedelta, timezone
from http import HTTPStatus
from io import StringIO
from time import sleep

import pytest
from dateutil import parser as datetime_parser

from shared.utils.config import delete_method, make_api_client, server_get
from shared.utils.helpers import generate_image_files

from .utils import create_task


class TestGetAnalytics:
    endpoint = "analytics"

    def _test_can_see(self, user):
        response = server_get(user, self.endpoint)

        assert response.status_code == HTTPStatus.OK

    def _test_cannot_see(self, user):
        response = server_get(user, self.endpoint)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize(
        "conditions, is_allow",
        [
            (dict(privilege="admin"), True),
            (dict(privilege="worker", has_analytics_access=False), False),
            (dict(privilege="worker", has_analytics_access=True), True),
            (dict(privilege="user", has_analytics_access=False), False),
            (dict(privilege="user", has_analytics_access=True), True),
        ],
    )
    def test_can_see(self, conditions, is_allow, find_users):
        user = find_users(**conditions)[0]["username"]

        if is_allow:
            self._test_can_see(user)
        else:
            self._test_cannot_see(user)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetAuditEvents:
    _USERNAME = "admin1"

    @staticmethod
    def _create_project(user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (project, response) = api_client.projects_api.create(spec, **kwargs)
            assert response.status == HTTPStatus.CREATED
        return project.id, response.headers.get("X-Request-Id")

    @pytest.fixture(autouse=True)
    def setup(self, restore_clickhouse_db_per_function, restore_redis_inmem_per_function):
        project_spec = {
            "name": f"Test project created by {self._USERNAME}",
            "labels": [
                {
                    "name": "car",
                    "color": "#ff00ff",
                    "attributes": [
                        {
                            "name": "a",
                            "mutable": True,
                            "input_type": "number",
                            "default_value": "5",
                            "values": ["4", "5", "6"],
                        }
                    ],
                }
            ],
        }
        self.project_id, project_request_id = TestGetAuditEvents._create_project(
            self._USERNAME, project_spec
        )
        task_spec = {
            "name": f"test {self._USERNAME} to create a task",
            "segment_size": 2,
            "project_id": self.project_id,
        }
        task_ids = [
            create_task(
                self._USERNAME,
                task_spec,
                {
                    "image_quality": 10,
                    "client_files": generate_image_files(3),
                },
            ),
            create_task(
                self._USERNAME,
                task_spec,
                {
                    "image_quality": 10,
                    "client_files": generate_image_files(3),
                },
            ),
        ]

        self.task_ids = [t[0] for t in task_ids]

        assert project_request_id is not None
        assert all(t[1] is not None for t in task_ids)

        event_filters = [
            (
                (lambda e: json.loads(e["payload"])["request"]["id"], [project_request_id]),
                ("scope", ["create:project"]),
            ),
        ]
        for task_id in task_ids:
            event_filters.extend(
                (
                    (
                        (lambda e: json.loads(e["payload"])["request"]["id"], [task_id[1]]),
                        ("scope", ["create:task"]),
                    ),
                    (("scope", ["create:job"]),),
                )
            )
        self._wait_for_request_ids(event_filters)

    def _wait_for_request_ids(self, event_filters):
        MAX_RETRIES = 5
        SLEEP_INTERVAL = 2
        while MAX_RETRIES > 0:
            data = self._test_get_audit_logs_as_csv()
            events = self._csv_to_dict(data)
            if all(self._filter_events(events, filter) for filter in event_filters):
                break
            MAX_RETRIES -= 1
            sleep(SLEEP_INTERVAL)
        else:
            assert False, "Could not wait for expected request IDs"

    @staticmethod
    def _export_events(endpoint, *, max_retries: int = 20, interval: float = 0.1, **kwargs):
        query_id = ""
        for _ in range(max_retries):
            (_, response) = endpoint.call_with_http_info(
                **kwargs, query_id=query_id, _parse_response=False
            )
            if response.status == HTTPStatus.CREATED:
                break
            assert response.status == HTTPStatus.ACCEPTED
            if not query_id:
                response_json = json.loads(response.data)
                query_id = response_json["query_id"]
            sleep(interval)
        assert response.status == HTTPStatus.CREATED

        (_, response) = endpoint.call_with_http_info(
            **kwargs, query_id=query_id, action="download", _parse_response=False
        )
        assert response.status == HTTPStatus.OK

        return response.data

    @staticmethod
    def _csv_to_dict(csv_data):
        res = []
        with StringIO(csv_data.decode()) as f:
            reader = csv.DictReader(f)
            for row in reader:
                res.append(row)

        return res

    @staticmethod
    def _filter_events(events, filters):
        res = []
        get_value = lambda getter, e: getter(e) if callable(getter) else e.get(getter, None)
        for e in events:
            if all(get_value(getter, e) in expected_values for getter, expected_values in filters):
                res.append(e)

        return res

    def _test_get_audit_logs_as_csv(self, **kwargs):
        with make_api_client(self._USERNAME) as api_client:
            return self._export_events(api_client.events_api.list_endpoint, **kwargs)

    def test_entry_to_time_interval(self):
        now = datetime.now(timezone.utc)
        to_datetime = now
        from_datetime = now - timedelta(minutes=3)

        query_params = {
            "_from": from_datetime.isoformat(),
            "to": to_datetime.isoformat(),
        }

        data = self._test_get_audit_logs_as_csv(**query_params)
        events = self._csv_to_dict(data)
        assert len(events)

        for event in events:
            event_timestamp = datetime_parser.isoparse(event["timestamp"])
            assert from_datetime <= event_timestamp <= to_datetime

    def test_filter_by_project(self):
        query_params = {
            "project_id": self.project_id,
        }

        data = self._test_get_audit_logs_as_csv(**query_params)
        events = self._csv_to_dict(data)

        filtered_events = self._filter_events(events, [("project_id", [str(self.project_id)])])
        assert len(filtered_events)
        assert len(events) == len(filtered_events)

        event_count = Counter([e["scope"] for e in filtered_events])
        assert event_count["create:project"] == 1
        assert event_count["create:task"] == 2
        assert event_count["create:job"] == 4

    def test_filter_by_task(self):
        for task_id in self.task_ids:
            query_params = {
                "task_id": task_id,
            }

            data = self._test_get_audit_logs_as_csv(**query_params)
            events = self._csv_to_dict(data)

            filtered_events = self._filter_events(events, [("task_id", [str(task_id)])])
            assert len(filtered_events)
            assert len(events) == len(filtered_events)

            event_count = Counter([e["scope"] for e in filtered_events])
            assert event_count["create:task"] == 1
            assert event_count["create:job"] == 2

    def test_filter_by_non_existent_project(self):
        query_params = {
            "project_id": self.project_id + 100,
        }

        data = self._test_get_audit_logs_as_csv(**query_params)
        events = self._csv_to_dict(data)
        assert len(events) == 0

    def test_user_and_request_id_not_empty(self):
        query_params = {
            "project_id": self.project_id,
        }
        data = self._test_get_audit_logs_as_csv(**query_params)
        events = self._csv_to_dict(data)

        for event in events:
            assert event["user_id"]
            assert event["user_name"]
            assert event["user_email"]

            payload = json.loads(event["payload"])
            request_id = payload["request"]["id"]
            assert request_id
            uuid.UUID(request_id)

    def test_delete_project(self):
        response = delete_method("admin1", f"projects/{self.project_id}")
        assert response.status_code == HTTPStatus.NO_CONTENT

        event_filters = (
            (
                (
                    lambda e: json.loads(e["payload"])["request"]["id"],
                    [response.headers.get("X-Request-Id")],
                ),
                ("scope", ["delete:project"]),
            ),
            (
                (
                    lambda e: json.loads(e["payload"])["request"]["id"],
                    [response.headers.get("X-Request-Id")],
                ),
                ("scope", ["delete:task"]),
            ),
        )

        self._wait_for_request_ids(event_filters)

        query_params = {
            "project_id": self.project_id,
        }

        data = self._test_get_audit_logs_as_csv(**query_params)
        events = self._csv_to_dict(data)

        filtered_events = self._filter_events(events, [("project_id", [str(self.project_id)])])
        assert len(filtered_events)
        assert len(events) == len(filtered_events)

        event_count = Counter([e["scope"] for e in filtered_events])
        assert event_count["delete:project"] == 1
        assert event_count["delete:task"] == 2
        assert event_count["delete:job"] == 4


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_auth.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from http import HTTPStatus

import pytest
from cvat_sdk.api_client import ApiClient, Configuration, models

from shared.utils.config import BASE_URL, USER_PASS, make_api_client


@pytest.mark.usefixtures("restore_db_per_class")
class TestBasicAuth:
    def test_can_do_basic_auth(self, admin_user: str):
        username = admin_user
        config = Configuration(host=BASE_URL, username=username, password=USER_PASS)
        with ApiClient(config) as client:
            (user, response) = client.users_api.retrieve_self()
            assert response.status == HTTPStatus.OK
            assert user.username == username


@pytest.mark.usefixtures("restore_db_per_function")
class TestTokenAuth:
    @staticmethod
    def login(client: ApiClient, username: str) -> models.Token:
        (auth, _) = client.auth_api.create_login(
            models.LoginSerializerExRequest(username=username, password=USER_PASS)
        )
        client.set_default_header("Authorization", "Token " + auth.key)
        return auth

    @classmethod
    def make_client(cls, username: str) -> ApiClient:
        with ApiClient(Configuration(host=BASE_URL)) as client:
            cls.login(client, username)
            return client

    def test_can_do_token_auth_and_manage_cookies(self, admin_user: str):
        username = admin_user
        with ApiClient(Configuration(host=BASE_URL)) as api_client:
            auth = self.login(api_client, username=username)
            assert "sessionid" in api_client.cookies
            assert "csrftoken" in api_client.cookies
            assert auth.key

            (user, response) = api_client.users_api.retrieve_self()
            assert response.status == HTTPStatus.OK
            assert user.username == username

    def test_can_do_token_auth_from_config(self, admin_user: str):
        username = admin_user

        with make_api_client(username) as api_client:
            auth = self.login(api_client, username=username)

            config = Configuration(
                host=BASE_URL,
                api_key={
                    "sessionAuth": api_client.cookies["sessionid"].value,
                    "csrfAuth": api_client.cookies["csrftoken"].value,
                    "tokenAuth": auth.key,
                },
            )

        with ApiClient(config) as api_client:
            auth = self.login(api_client, username=username)
            assert "sessionid" in api_client.cookies
            assert "csrftoken" in api_client.cookies
            assert auth.key

            (user, response) = api_client.users_api.retrieve_self()
            assert response.status == HTTPStatus.OK
            assert user.username == username

    def test_can_do_logout(self, admin_user: str):
        username = admin_user
        with self.make_client(username) as api_client:
            (_, response) = api_client.auth_api.create_logout()
            assert response.status == HTTPStatus.OK

            (_, response) = api_client.users_api.retrieve_self(
                _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.UNAUTHORIZED


@pytest.mark.usefixtures("restore_db_per_function")
class TestCredentialsManagement:
    def test_can_register(self):
        username = "newuser"
        email = "123@456.com"
        with ApiClient(Configuration(host=BASE_URL)) as api_client:
            (user, response) = api_client.auth_api.create_register(
                models.RegisterSerializerExRequest(
                    username=username, password1=USER_PASS, password2=USER_PASS, email=email
                )
            )
            assert response.status == HTTPStatus.CREATED
            assert user.username == username

        with make_api_client(username) as api_client:
            (user, response) = api_client.users_api.retrieve_self()
            assert response.status == HTTPStatus.OK
            assert user.username == username
            assert user.email == email

    def test_can_change_password(self, admin_user: str):
        username = admin_user
        new_pass = "5w4knrqaW#$@gewa"
        with make_api_client(username) as api_client:
            (info, response) = api_client.auth_api.create_password_change(
                models.PasswordChangeRequest(
                    old_password=USER_PASS, new_password1=new_pass, new_password2=new_pass
                )
            )
            assert response.status == HTTPStatus.OK
            assert info.detail == "New password has been saved."

            (_, response) = api_client.users_api.retrieve_self(
                _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.UNAUTHORIZED

            api_client.configuration.password = new_pass
            (user, response) = api_client.users_api.retrieve_self()
            assert response.status == HTTPStatus.OK
            assert user.username == username

    def test_can_report_weak_password(self, admin_user: str):
        username = admin_user
        new_pass = "pass"
        with make_api_client(username) as api_client:
            (_, response) = api_client.auth_api.create_password_change(
                models.PasswordChangeRequest(
                    old_password=USER_PASS, new_password1=new_pass, new_password2=new_pass
                ),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.BAD_REQUEST
            assert json.loads(response.data) == {
                "new_password2": [
                    "This password is too short. It must contain at least 8 characters.",
                    "This password is too common.",
                ]
            }

    def test_can_report_mismatching_passwords(self, admin_user: str):
        username = admin_user
        with make_api_client(username) as api_client:
            (_, response) = api_client.auth_api.create_password_change(
                models.PasswordChangeRequest(
                    old_password=USER_PASS, new_password1="3j4tb13/T$#", new_password2="q#@$n34g5"
                ),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.BAD_REQUEST
            assert json.loads(response.data) == {
                "new_password2": ["The two password fields didn’t match."]
            }


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_cache_policy.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import re
from http import HTTPStatus

from shared.utils.config import server_get


class TestCachePolicy:
    @staticmethod
    def _get_js_bundle_url(response):
        match = re.search(r'<script.* src="(/assets/cvat-ui.\w+.min.js)".*></script>', response)
        if match:
            return match.group(1)

    def _test_cache_policy_enabled(self, response):
        assert response.status_code == HTTPStatus.OK
        assert (
            "public" in response.headers["Cache-Control"]
            and "max-age" in response.headers["Cache-Control"]
        )

    def _test_cache_policy_disabled(self, response):
        assert response.status_code == HTTPStatus.OK
        assert "no-cache" in response.headers["Cache-Control"]

    def test_index_not_cached(self, find_users):
        user = find_users(privilege="user")[0]["username"]
        index_page_response = server_get(user, "/")

        self._test_cache_policy_disabled(index_page_response)

    def test_asset_cached(self, find_users):
        user = find_users(privilege="user")[0]["username"]
        index_page_response = server_get(user, "/")
        js_asset_url = self._get_js_bundle_url(index_page_response.content.decode("utf-8"))
        js_asset_response = server_get(user, js_asset_url)

        self._test_cache_policy_enabled(js_asset_response)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_check_objects_integrity.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from pathlib import Path

import pytest
from deepdiff import DeepDiff

from shared.utils import config


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetResources:
    @pytest.mark.parametrize("path", config.ASSETS_DIR.glob("*.json"))
    def test_check_objects_integrity(self, path: Path):
        with open(path) as f:
            endpoint = path.stem
            if endpoint in [
                "quality_settings",
                "quality_reports",
                "quality_conflicts",
                "consensus_settings",
            ]:
                endpoint = "/".join(endpoint.split("_"))

            if endpoint == "annotations":
                objects = json.load(f)
                for jid, annotations in objects["job"].items():
                    response = config.get_method("admin1", f"jobs/{jid}/annotations").json()
                    assert (
                        DeepDiff(
                            annotations,
                            response,
                            ignore_order=True,
                            exclude_paths="root['version']",
                        )
                        == {}
                    )
            else:
                response = config.get_method("admin1", endpoint, page_size="all")
                json_objs = json.load(f)
                resp_objs = response.json()

                assert (
                    DeepDiff(
                        json_objs,
                        resp_objs,
                        ignore_order=True,
                        exclude_regex_paths=r"root\['results'\]\[\d+\]\['last_login'\]",
                    )
                    == {}
                )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_cloud_storages.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import json
from functools import partial
from http import HTTPStatus
from typing import Any, Optional

import pytest
from cvat_sdk.api_client import ApiClient, models
from cvat_sdk.api_client.api_client import Endpoint
from cvat_sdk.api_client.model.file_info import FileInfo
from deepdiff import DeepDiff
from PIL import Image

from shared.utils.config import get_method, make_api_client
from shared.utils.s3 import make_client as make_s3_client

from .utils import CollectionSimpleFilterTestBase

# https://docs.pytest.org/en/7.1.x/example/markers.html#marking-whole-classes-or-modules
pytestmark = [pytest.mark.with_external_services]


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetCloudStorage:
    def _test_can_see(self, user, storage_id, data):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.retrieve(
                id=storage_id,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.OK
            response_json = json.loads(response.data)

            assert (
                DeepDiff(
                    data, response_json, ignore_order=True, exclude_paths="root['updated_date']"
                )
                == {}
            )

    def _test_cannot_see(self, user, storage_id):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.retrieve(
                id=storage_id,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("storage_id", [1])
    @pytest.mark.parametrize(
        "group, is_owner, is_allow",
        [
            ("admin", False, True),
            ("user", True, True),
        ],
    )
    def test_sandbox_user_get_cloud_storage(
        self, storage_id, group, is_owner, is_allow, users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in users
                    if group in u["groups"] and u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_see(username, storage_id, cloud_storage)
        else:
            self._test_cannot_see(username, storage_id)

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize("storage_id", [2])
    @pytest.mark.parametrize(
        "role, is_owner, is_allow",
        [
            ("worker", True, True),
            ("supervisor", False, True),
            ("worker", False, False),
        ],
    )
    def test_org_user_get_cloud_storage(
        self, org_id, storage_id, role, is_owner, is_allow, find_users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in find_users(role=role, org=org_id)
                    if u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_see(username, storage_id, cloud_storage)
        else:
            self._test_cannot_see(username, storage_id)

    def test_can_remove_owner_and_fetch_with_sdk(self, admin_user, cloud_storages):
        # test for API schema regressions
        source_storage = next(
            s for s in cloud_storages if s.get("owner") and s["owner"]["username"] != admin_user
        ).copy()

        with make_api_client(admin_user) as api_client:
            api_client.users_api.destroy(source_storage["owner"]["id"])

            (_, response) = api_client.cloudstorages_api.retrieve(source_storage["id"])
            fetched_storage = json.loads(response.data)

        source_storage["owner"] = None
        assert DeepDiff(source_storage, fetched_storage, ignore_order=True) == {}


class TestCloudStoragesListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
        "name": ["display_name"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, cloud_storages):
        self.user = admin_user
        self.samples = cloud_storages

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.cloudstorages_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("provider_type", "name", "resource", "credentials_type", "owner"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostCloudStorage:
    _SPEC = {
        "provider_type": "AWS_S3_BUCKET",
        "resource": "test",
        "display_name": "Bucket",
        "credentials_type": "KEY_SECRET_KEY_PAIR",
        "key": "minio_access_key",
        "secret_key": "minio_secret_key",
        "specific_attributes": "endpoint_url=http://minio:9000",
        "description": "Some description",
        "manifests": ["manifest.jsonl"],
    }
    _EXCLUDE_PATHS = [
        f"root['{extra_field}']"
        for extra_field in {
            # unchanged fields
            "created_date",
            "id",
            "organization",
            "owner",
            "updated_date",
            # credentials that server doesn't return
            "key",
            "secret_key",
        }
    ]

    def _test_can_create(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.create(
                models.CloudStorageWriteRequest(**spec),
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.CREATED
            response_json = json.loads(response.data)

            assert (
                DeepDiff(
                    self._SPEC, response_json, ignore_order=True, exclude_paths=self._EXCLUDE_PATHS
                )
                == {}
            )

    def _test_cannot_create(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.create(
                models.CloudStorageWriteRequest(**spec),
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("group, is_allow", [("user", True), ("worker", False)])
    def test_sandbox_user_create_cloud_storage(self, group, is_allow, users):
        org = ""
        username = [u for u in users if group in u["groups"]][0]["username"]

        if is_allow:
            self._test_can_create(username, self._SPEC, org=org)
        else:
            self._test_cannot_create(username, self._SPEC, org=org)

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize(
        "role, is_allow",
        [
            ("owner", True),
            ("maintainer", True),
            ("worker", False),
            ("supervisor", False),
        ],
    )
    def test_org_user_create_cloud_storage(self, org_id, role, is_allow, find_users):
        username = find_users(role=role, org=org_id)[0]["username"]

        if is_allow:
            self._test_can_create(username, self._SPEC, org_id=org_id)
        else:
            self._test_cannot_create(username, self._SPEC, org_id=org_id)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchCloudStorage:
    _SPEC = {
        "display_name": "New display name",
        "description": "New description",
        "manifests": [
            "manifest_1.jsonl",
            "manifest_2.jsonl",
        ],
    }
    _PRIVATE_BUCKET_SPEC = {
        "display_name": "New display name",
        "description": "New description",
        "manifests": [
            "sub/manifest_1.jsonl",
            "sub/manifest_2.jsonl",
        ],
    }
    _EXCLUDE_PATHS = [
        f"root['{extra_field}']"
        for extra_field in {
            # unchanged fields
            "created_date",
            "credentials_type",
            "id",
            "organization",
            "owner",
            "provider_type",
            "resource",
            "specific_attributes",
            "updated_date",
        }
    ]

    def _test_can_update(self, user, storage_id, spec):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.partial_update(
                id=storage_id,
                patched_cloud_storage_write_request=models.PatchedCloudStorageWriteRequest(**spec),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.OK
            response_json = json.loads(response.data)

            assert (
                DeepDiff(spec, response_json, ignore_order=True, exclude_paths=self._EXCLUDE_PATHS)
                == {}
            )

    def _test_cannot_update(self, user, storage_id, spec):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.partial_update(
                id=storage_id,
                patched_cloud_storage_write_request=models.PatchedCloudStorageWriteRequest(**spec),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("storage_id", [1])
    @pytest.mark.parametrize(
        "group, is_owner, is_allow",
        [
            ("admin", False, True),
            ("worker", True, True),
        ],
    )
    def test_sandbox_user_update_cloud_storage(
        self, storage_id, group, is_owner, is_allow, users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in users
                    if group in u["groups"] and u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_update(username, storage_id, self._SPEC)
        else:
            self._test_cannot_update(username, storage_id, self._SPEC)

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize("storage_id", [2])
    @pytest.mark.parametrize(
        "role, is_owner, is_allow",
        [
            ("worker", True, True),
            ("maintainer", False, True),
            ("supervisor", False, False),
        ],
    )
    def test_org_user_update_cloud_storage(
        self, org_id, storage_id, role, is_owner, is_allow, find_users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in find_users(role=role, org=org_id)
                    if u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_update(username, storage_id, self._PRIVATE_BUCKET_SPEC)
        else:
            self._test_cannot_update(username, storage_id, self._PRIVATE_BUCKET_SPEC)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetCloudStoragePreview:
    def _test_can_see(self, user, storage_id):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.retrieve_preview(
                id=storage_id,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.OK

            (width, height) = Image.open(io.BytesIO(response.data)).size
            assert width > 0 and height > 0

    def _test_cannot_see(self, user, storage_id):
        with make_api_client(user) as api_client:
            (_, response) = api_client.cloudstorages_api.retrieve_preview(
                id=storage_id,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("storage_id", [1])
    @pytest.mark.parametrize(
        "group, is_owner, is_allow",
        [
            ("admin", False, True),
            ("user", True, True),
        ],
    )
    def test_sandbox_user_get_cloud_storage_preview(
        self, storage_id, group, is_owner, is_allow, users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in users
                    if group in u["groups"] and u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_see(username, storage_id)
        else:
            self._test_cannot_see(username, storage_id)

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize("storage_id", [2])
    @pytest.mark.parametrize(
        "role, is_owner, is_allow",
        [
            ("worker", True, True),
            ("supervisor", False, True),
            ("worker", False, False),
        ],
    )
    def test_org_user_get_cloud_storage_preview(
        self, org_id, storage_id, role, is_owner, is_allow, find_users, cloud_storages
    ):
        cloud_storage = cloud_storages[storage_id]
        username = (
            cloud_storage["owner"]["username"]
            if is_owner
            else next(
                (
                    u
                    for u in find_users(role=role, org=org_id)
                    if u["id"] != cloud_storage["owner"]["id"]
                )
            )["username"]
        )

        if is_allow:
            self._test_can_see(username, storage_id)
        else:
            self._test_cannot_see(username, storage_id)


@pytest.mark.usefixtures("restore_db_per_function")
class TestGetCloudStorageContent:
    USER = "admin1"

    def _test_get_cloud_storage_content(
        self,
        cloud_storage_id: int,
        manifest: Optional[str] = None,
        **kwargs,
    ):
        with make_api_client(self.USER) as api_client:
            content_kwargs = {"manifest_path": manifest} if manifest else {}

            for item in ["next_token", "prefix", "page_size"]:
                if item_value := kwargs.get(item):
                    content_kwargs[item] = item_value

            (data, _) = api_client.cloudstorages_api.retrieve_content_v2(
                cloud_storage_id, **content_kwargs
            )

            return data

    @pytest.mark.parametrize("cloud_storage_id", [2])
    @pytest.mark.parametrize(
        "manifest, prefix, default_bucket_prefix, page_size, expected_content",
        [
            (
                # [v2] list the top level of bucket with based on manifest
                "sub/manifest.jsonl",
                None,
                None,
                None,
                [FileInfo(mime_type="DIR", name="sub", type="DIR")],
            ),
            (
                # [v2] search by some prefix in bucket content based on manifest
                "sub/manifest.jsonl",
                "sub/image_case_65_1",
                None,
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list the second layer (directory "sub") of bucket content based on manifest
                "sub/manifest.jsonl",
                "sub/",
                None,
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_2.png", type="REG"),
                ],
            ),
            (
                # [v2] list the top layer of real bucket content
                None,
                None,
                None,
                None,
                [FileInfo(mime_type="DIR", name="sub", type="DIR")],
            ),
            (
                # [v2] list the second layer (directory "sub") of real bucket content
                None,
                "sub/",
                None,
                2,
                [
                    FileInfo(mime_type="unknown", name="demo_manifest.jsonl", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                None,
                "/sub/",  # cover case: API is identical to share point API
                None,
                None,
                [
                    FileInfo(mime_type="unknown", name="demo_manifest.jsonl", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_2.png", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest.jsonl", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest_1.jsonl", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest_2.jsonl", type="REG"),
                ],
            ),
            (
                # [v2] list bucket content based on manifest when default bucket prefix is set to directory
                "sub/manifest.jsonl",
                None,
                "sub/",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_2.png", type="REG"),
                ],
            ),
            (
                # [v2] list bucket content based on manifest when default bucket prefix
                # is set to template from which the files should start
                "sub/manifest.jsonl",
                None,
                "sub/image_case_65_1",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list bucket content based on manifest when specified prefix is stricter than default bucket prefix
                "sub/manifest.jsonl",
                "sub/image_case_65_1",
                "sub/image_case",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list bucket content based on manifest when default bucket prefix is stricter than specified prefix
                "sub/manifest.jsonl",
                "sub/image_case",
                "sub/image_case_65_1",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list bucket content based on manifest when default bucket prefix and specified prefix have no intersection
                "sub/manifest.jsonl",
                "sub/image_case_65_1",
                "sub/image_case_65_2",
                None,
                [],
            ),
            (
                # [v2] list bucket content based on manifest when default bucket prefix contains dirs and prefix starts with it
                "sub/manifest.jsonl",
                "s",
                "sub/",
                None,
                [
                    FileInfo(mime_type="DIR", name="sub", type="DIR"),
                ],
            ),
            (
                # [v2] list real bucket content when default bucket prefix is set to directory
                None,
                None,
                "sub/",
                None,
                [
                    FileInfo(mime_type="unknown", name="demo_manifest.jsonl", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                    FileInfo(mime_type="image", name="image_case_65_2.png", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest.jsonl", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest_1.jsonl", type="REG"),
                    FileInfo(mime_type="unknown", name="manifest_2.jsonl", type="REG"),
                ],
            ),
            (
                # [v2] list real bucket content when default bucket prefix
                # is set to template from which the files should start
                None,
                None,
                "sub/demo",
                None,
                [
                    FileInfo(mime_type="unknown", name="demo_manifest.jsonl", type="REG"),
                ],
            ),
            (
                # [v2] list real bucket content when specified prefix is stricter than default bucket prefix
                None,
                "sub/image_case_65_1",
                "sub/image_case",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list real bucket content when default bucket prefix is stricter than specified prefix
                None,
                "sub/image_case",
                "sub/image_case_65_1",
                None,
                [
                    FileInfo(mime_type="image", name="image_case_65_1.png", type="REG"),
                ],
            ),
            (
                # [v2] list real bucket content when default bucket prefix and specified prefix have no intersection
                None,
                "sub/image_case_65_1",
                "sub/image_case_65_2",
                None,
                [],
            ),
            (
                # [v2] list real bucket content when default bucket prefix contains dirs and prefix starts with it
                None,
                "s",
                "sub/",
                None,
                [
                    FileInfo(mime_type="DIR", name="sub", type="DIR"),
                ],
            ),
        ],
    )
    def test_get_cloud_storage_content(
        self,
        cloud_storage_id: int,
        manifest: Optional[str],
        prefix: Optional[str],
        default_bucket_prefix: Optional[str],
        page_size: Optional[int],
        expected_content: Optional[Any],
        cloud_storages,
    ):
        if default_bucket_prefix:
            cloud_storage = cloud_storages[cloud_storage_id]

            with make_api_client(self.USER) as api_client:
                (_, response) = api_client.cloudstorages_api.partial_update(
                    cloud_storage_id,
                    patched_cloud_storage_write_request={
                        "specific_attributes": f'{cloud_storage["specific_attributes"]}&prefix={default_bucket_prefix}'
                    },
                )
                assert response.status == HTTPStatus.OK

        result = self._test_get_cloud_storage_content(
            cloud_storage_id, manifest, prefix=prefix, page_size=page_size
        )
        if expected_content:
            assert result["content"] == expected_content
        if page_size:
            assert len(result["content"]) <= page_size

    @pytest.mark.parametrize("cloud_storage_id, prefix, page_size", [(2, "sub/", 2)])
    def test_iterate_over_cloud_storage_content(
        self, cloud_storage_id: int, prefix: str, page_size: int
    ):
        expected_content = self._test_get_cloud_storage_content(cloud_storage_id, prefix=prefix)[
            "content"
        ]

        current_content = []
        next_token = None
        while True:
            result = self._test_get_cloud_storage_content(
                cloud_storage_id,
                prefix=prefix,
                page_size=page_size,
                next_token=next_token,
            )
            content = result["content"]
            assert len(content) <= page_size
            current_content.extend(content)

            next_token = result["next"]
            if not next_token:
                break

        assert expected_content == current_content

    @pytest.mark.parametrize("cloud_storage_id", [2])
    def test_can_get_storage_content_with_manually_created_dirs(
        self,
        cloud_storage_id: int,
        request,
        cloud_storages,
    ):
        initial_content = self._test_get_cloud_storage_content(cloud_storage_id)["content"]
        cs_name = cloud_storages[cloud_storage_id]["resource"]
        s3_client = make_s3_client(bucket=cs_name)
        new_directory = "manually_created_directory/"

        # directory is 0 size object that has a name ending with a forward slash
        s3_client.create_file(
            filename=new_directory,
        )
        request.addfinalizer(
            partial(
                s3_client.remove_file,
                filename=new_directory,
            )
        )

        content = self._test_get_cloud_storage_content(
            cloud_storage_id,
        )["content"]
        assert len(initial_content) + 1 == len(content)
        assert any(
            new_directory.strip("/") == x["name"] and "DIR" == str(x["type"]) for x in content
        )

        content = self._test_get_cloud_storage_content(
            cloud_storage_id,
            prefix=new_directory,
        )["content"]
        assert not len(content)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListCloudStorages:
    def _test_can_see_cloud_storages(self, user, data, **kwargs):
        response = get_method(user, "cloudstorages", **kwargs)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(data, response.json()["results"]) == {}

    def test_admin_can_see_all_cloud_storages(self, cloud_storages):
        self._test_can_see_cloud_storages("admin2", cloud_storages.raw, page_size="all")

    @pytest.mark.parametrize("field_value, query_value", [(2, 2), (None, "")])
    def test_can_filter_by_org_id(self, field_value, query_value, cloud_storages):
        cloud_storages = filter(lambda i: i["organization"] == field_value, cloud_storages)
        self._test_can_see_cloud_storages(
            "admin2", list(cloud_storages), page_size="all", org_id=query_value
        )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_consensus.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from copy import deepcopy
from functools import partial
from http import HTTPStatus
from typing import Any, Dict, Optional, Tuple

import pytest
import urllib3
from cvat_sdk.api_client import exceptions, models
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff

from shared.utils.config import make_api_client

from .utils import CollectionSimpleFilterTestBase, compare_annotations


class _PermissionTestBase:
    def merge(
        self,
        *,
        task_id: Optional[int] = None,
        job_id: Optional[int] = None,
        user: str,
        raise_on_error: bool = True,
        wait_result: bool = True,
    ) -> urllib3.HTTPResponse:
        assert task_id is not None or job_id is not None

        kwargs = {}
        if task_id is not None:
            kwargs["task_id"] = task_id
        if job_id is not None:
            kwargs["job_id"] = job_id

        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.create_merge(
                consensus_merge_create_request=models.ConsensusMergeCreateRequest(**kwargs),
                _parse_response=False,
                _check_status=raise_on_error,
            )

            if not raise_on_error and response.status != HTTPStatus.ACCEPTED:
                return response
            assert response.status == HTTPStatus.ACCEPTED

            rq_id = json.loads(response.data)["rq_id"]

            while wait_result:
                (_, response) = api_client.consensus_api.create_merge(
                    rq_id=rq_id, _parse_response=False
                )
                assert response.status in [HTTPStatus.CREATED, HTTPStatus.ACCEPTED]

                if response.status == HTTPStatus.CREATED:
                    break

            return response

    def request_merge(
        self,
        *,
        task_id: Optional[int] = None,
        job_id: Optional[int] = None,
        user: str,
    ) -> str:
        response = self.merge(user=user, task_id=task_id, job_id=job_id, wait_result=False)
        return json.loads(response.data)["rq_id"]

    @pytest.fixture
    def find_sandbox_task(self, tasks, jobs, users, is_task_staff):
        def _find(
            is_staff: bool, *, has_consensus_jobs: Optional[bool] = None
        ) -> tuple[dict[str, Any], dict[str, Any]]:
            task = next(
                t
                for t in tasks
                if t["organization"] is None
                and not users[t["owner"]["id"]]["is_superuser"]
                and (
                    has_consensus_jobs is None
                    or has_consensus_jobs
                    == any(
                        j
                        for j in jobs
                        if j["task_id"] == t["id"] and j["type"] == "consensus_replica"
                    )
                )
            )

            if is_staff:
                user = task["owner"]
            else:
                user = next(u for u in users if not is_task_staff(u["id"], task["id"]))

            return task, user

        return _find

    @pytest.fixture
    def find_sandbox_task_with_consensus(self, find_sandbox_task):
        return partial(find_sandbox_task, has_consensus_jobs=True)

    @pytest.fixture
    def find_org_task(
        self, restore_db_per_function, tasks, jobs, users, is_org_member, is_task_staff, admin_user
    ):
        def _find(
            is_staff: bool, user_org_role: str, *, has_consensus_jobs: Optional[bool] = None
        ) -> tuple[dict[str, Any], dict[str, Any]]:
            for user in users:
                if user["is_superuser"]:
                    continue

                task = next(
                    (
                        t
                        for t in tasks
                        if t["organization"] is not None
                        and is_task_staff(user["id"], t["id"]) == is_staff
                        and is_org_member(user["id"], t["organization"], role=user_org_role)
                        and (
                            has_consensus_jobs is None
                            or has_consensus_jobs
                            == any(
                                j
                                for j in jobs
                                if j["task_id"] == t["id"] and j["type"] == "consensus_replica"
                            )
                        )
                    ),
                    None,
                )
                if task is not None:
                    break

            if not task:
                task = next(
                    t
                    for t in tasks
                    if t["organization"] is not None
                    if has_consensus_jobs is None or has_consensus_jobs == t["consensus_enabled"]
                )
                user = next(
                    u
                    for u in users
                    if is_org_member(u["id"], task["organization"], role=user_org_role)
                )

                if is_staff:
                    with make_api_client(admin_user) as api_client:
                        api_client.tasks_api.partial_update(
                            task["id"],
                            patched_task_write_request=models.PatchedTaskWriteRequest(
                                assignee_id=user["id"]
                            ),
                        )

            return task, user

        return _find

    @pytest.fixture
    def find_org_task_with_consensus(self, find_org_task):
        return partial(find_org_task, has_consensus_jobs=True)

    _default_sandbox_cases = ("is_staff, allow", [(True, True), (False, False)])

    _default_org_cases = (
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, True),
            ("worker", False, False),
        ],
    )

    _default_org_roles = ("owner", "maintainer", "supervisor", "worker")


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestPostConsensusMerge(_PermissionTestBase):
    def test_can_merge_task_with_consensus_jobs(self, admin_user, tasks):
        task_id = next(t["id"] for t in tasks if t["consensus_enabled"])

        assert self.merge(user=admin_user, task_id=task_id).status == HTTPStatus.CREATED

    def test_can_merge_consensus_job(self, admin_user, jobs):
        job_id = next(
            j["id"] for j in jobs if j["type"] == "annotation" and j["consensus_replicas"] > 0
        )

        assert self.merge(user=admin_user, job_id=job_id).status == HTTPStatus.CREATED

    def test_cannot_merge_task_without_consensus_jobs(self, admin_user, tasks):
        task_id = next(t["id"] for t in tasks if not t["consensus_enabled"])

        with pytest.raises(exceptions.ApiException) as capture:
            self.merge(user=admin_user, task_id=task_id)

        assert "Consensus is not enabled in this task" in capture.value.body

    def test_cannot_merge_task_without_mergeable_parent_jobs(self, admin_user, tasks, jobs):
        task_id = next(t["id"] for t in tasks if t["consensus_enabled"])

        for j in jobs:
            if (j["stage"] != "annotation" or j["state"] != "new") and (
                j["task_id"] == task_id and j["type"] in ("annotation", "consensus_replica")
            ):
                with make_api_client(admin_user) as api_client:
                    api_client.jobs_api.partial_update(
                        j["id"],
                        patched_job_write_request=models.PatchedJobWriteRequest(
                            state="new", stage="annotation"
                        ),
                    )

        with pytest.raises(exceptions.ApiException) as capture:
            self.merge(user=admin_user, task_id=task_id)

        assert "No annotation jobs in the annotation stage" in capture.value.body

    def test_cannot_merge_replica_job(self, admin_user, tasks, jobs):
        job_id = next(
            j["id"]
            for j in jobs
            if j["type"] == "consensus_replica"
            if tasks.map[j["task_id"]]["consensus_enabled"]
        )

        with pytest.raises(exceptions.ApiException) as capture:
            self.merge(user=admin_user, job_id=job_id)

        assert "No annotated consensus jobs found for parent job" in capture.value.body

    def _test_merge_200(
        self, user: str, *, task_id: Optional[int] = None, job_id: Optional[int] = None
    ):
        return self.merge(user=user, task_id=task_id, job_id=job_id)

    def _test_merge_403(
        self, user: str, *, task_id: Optional[int] = None, job_id: Optional[int] = None
    ):
        response = self.merge(user=user, task_id=task_id, job_id=job_id, raise_on_error=False)
        assert response.status == HTTPStatus.FORBIDDEN
        return response

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_merge_in_sandbox_task(self, is_staff, allow, find_sandbox_task_with_consensus):
        task, user = find_sandbox_task_with_consensus(is_staff)

        if allow:
            self._test_merge_200(user["username"], task_id=task["id"])
        else:
            self._test_merge_403(user["username"], task_id=task["id"])

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_merge_in_org_task(
        self,
        find_org_task_with_consensus,
        org_role,
        is_staff,
        allow,
    ):
        task, user = find_org_task_with_consensus(is_staff, org_role)

        if allow:
            self._test_merge_200(user["username"], task_id=task["id"])
        else:
            self._test_merge_403(user["username"], task_id=task["id"])

    # only rq job owner or admin now has the right to check status of report creation
    def _test_check_merge_status_by_non_rq_job_owner(
        self,
        rq_id: str,
        *,
        staff_user: str,
        other_user: str,
    ):
        with make_api_client(other_user) as api_client:
            (_, response) = api_client.consensus_api.create_merge(
                rq_id=rq_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.NOT_FOUND
            assert json.loads(response.data)["detail"] == "Unknown request id"

        with make_api_client(staff_user) as api_client:
            (_, response) = api_client.consensus_api.create_merge(
                rq_id=rq_id, _parse_response=False, _check_status=False
            )
            assert response.status in {HTTPStatus.ACCEPTED, HTTPStatus.CREATED}

    def test_non_rq_job_owner_cannot_check_status_of_merge_in_sandbox(
        self,
        find_sandbox_task_with_consensus,
        users,
    ):
        task, task_staff = find_sandbox_task_with_consensus(is_staff=True)

        other_user = next(
            u
            for u in users
            if (
                u["id"] != task_staff["id"]
                and not u["is_superuser"]
                and u["id"] != task["owner"]["id"]
            )
        )

        rq_id = self.request_merge(task_id=task["id"], user=task_staff["username"])
        self._test_check_merge_status_by_non_rq_job_owner(
            rq_id, staff_user=task_staff["username"], other_user=other_user["username"]
        )

    @pytest.mark.parametrize("role", _PermissionTestBase._default_org_roles)
    def test_non_rq_job_owner_cannot_check_status_of_merge_in_org(
        self,
        find_org_task_with_consensus,
        find_users,
        role: str,
    ):
        task, task_staff = find_org_task_with_consensus(is_staff=True, user_org_role="supervisor")

        other_user = next(
            u
            for u in find_users(role=role, org=task["organization"])
            if (
                u["id"] != task_staff["id"]
                and not u["is_superuser"]
                and u["id"] != task["owner"]["id"]
            )
        )
        rq_id = self.request_merge(task_id=task["id"], user=task_staff["username"])
        self._test_check_merge_status_by_non_rq_job_owner(
            rq_id, staff_user=task_staff["username"], other_user=other_user["username"]
        )

    @pytest.mark.parametrize("is_sandbox", (True, False))
    def test_admin_can_check_status_of_merge(
        self,
        find_org_task_with_consensus,
        find_sandbox_task_with_consensus,
        users,
        is_sandbox: bool,
    ):
        if is_sandbox:
            task, task_staff = find_sandbox_task_with_consensus(is_staff=True)
        else:
            task, task_staff = find_org_task_with_consensus(is_staff=True, user_org_role="owner")

        admin = next(
            u
            for u in users
            if (
                u["is_superuser"]
                and u["id"] != task_staff["id"]
                and u["id"] != task["owner"]["id"]
                and u["id"] != (task["assignee"] or {}).get("id")
            )
        )

        rq_id = self.request_merge(task_id=task["id"], user=task_staff["username"])

        with make_api_client(admin["username"]) as api_client:
            (_, response) = api_client.consensus_api.create_merge(
                rq_id=rq_id, _parse_response=False
            )
            assert response.status in {HTTPStatus.ACCEPTED, HTTPStatus.CREATED}


class TestSimpleConsensusSettingsFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(self, admin_user, consensus_settings):
        self.user = admin_user
        self.samples = consensus_settings

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.consensus_api.list_settings_endpoint

    @pytest.mark.parametrize("field", ("task_id",))
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


class TestListSettings(_PermissionTestBase):
    def _test_list_settings_200(
        self, user: str, task_id: int, *, expected_data: Optional[Dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            actual = get_paginated_collection(
                api_client.consensus_api.list_settings_endpoint,
                task_id=task_id,
                **kwargs,
                return_json=True,
            )

        if expected_data is not None:
            assert DeepDiff(expected_data, actual, ignore_order=True) == {}

    def _test_list_settings_403(self, user: str, task_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.list_settings(
                task_id=task_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_list_settings_in_sandbox_task(
        self, is_staff, allow, find_sandbox_task_with_consensus, consensus_settings
    ):
        task, user = find_sandbox_task_with_consensus(is_staff)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])

        if allow:
            self._test_list_settings_200(user["username"], task["id"], expected_data=[settings])
        else:
            self._test_list_settings_403(user["username"], task["id"])

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_list_settings_in_org_task(
        self,
        consensus_settings,
        find_org_task_with_consensus,
        org_role: str,
        is_staff,
        allow: bool,
    ):
        task, user = find_org_task_with_consensus(is_staff, org_role)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])
        org_id = task["organization"]

        if allow:
            self._test_list_settings_200(
                user["username"], task["id"], expected_data=[settings], org_id=org_id
            )
        else:
            self._test_list_settings_403(user["username"], task["id"], org_id=org_id)


class TestGetSettings(_PermissionTestBase):
    def _test_get_settings_200(
        self, user: str, obj_id: int, *, expected_data: Optional[Dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.retrieve_settings(obj_id, **kwargs)
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_get_settings_403(self, user: str, obj_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.retrieve_settings(
                obj_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    def test_can_get_settings(self, admin_user, consensus_settings):
        settings = next(iter(consensus_settings))
        self._test_get_settings_200(admin_user, settings["id"], expected_data=settings)

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_get_settings_in_sandbox_task(
        self, is_staff, allow, find_sandbox_task_with_consensus, consensus_settings
    ):
        task, user = find_sandbox_task_with_consensus(is_staff)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])

        if allow:
            self._test_get_settings_200(user["username"], settings["id"], expected_data=settings)
        else:
            self._test_get_settings_403(user["username"], settings["id"])

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_get_settings_in_org_task(
        self,
        consensus_settings,
        find_org_task_with_consensus,
        org_role: str,
        is_staff,
        allow: bool,
    ):
        task, user = find_org_task_with_consensus(is_staff, org_role)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])

        if allow:
            self._test_get_settings_200(user["username"], settings["id"], expected_data=settings)
        else:
            self._test_get_settings_403(user["username"], settings["id"])


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchSettings(_PermissionTestBase):
    def _test_patch_settings_200(
        self,
        user: str,
        obj_id: int,
        data: Dict[str, Any],
        *,
        expected_data: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.partial_update_settings(
                obj_id, patched_consensus_settings_request=data, **kwargs
            )
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_patch_settings_403(self, user: str, obj_id: int, data: Dict[str, Any], **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.consensus_api.partial_update_settings(
                obj_id,
                patched_consensus_settings_request=data,
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    def _get_request_data(self, data: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        patched_data = deepcopy(data)

        for field, value in data.items():
            if isinstance(value, bool):
                patched_data[field] = not value
            elif isinstance(value, float):
                patched_data[field] = 1 - value

        expected_data = deepcopy(patched_data)

        return patched_data, expected_data

    def test_can_patch_settings(self, admin_user, consensus_settings):
        settings = next(iter(consensus_settings))
        data, expected_data = self._get_request_data(settings)
        self._test_patch_settings_200(admin_user, settings["id"], data, expected_data=expected_data)

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_patch_settings_in_sandbox_task(
        self, consensus_settings, find_sandbox_task_with_consensus, is_staff: bool, allow: bool
    ):
        task, user = find_sandbox_task_with_consensus(is_staff)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])
        request_data, expected_data = self._get_request_data(settings)

        if allow:
            self._test_patch_settings_200(
                user["username"], settings["id"], request_data, expected_data=expected_data
            )
        else:
            self._test_patch_settings_403(user["username"], settings["id"], request_data)

    @pytest.mark.parametrize(
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, True),
            ("worker", False, False),
        ],
    )
    def test_user_patch_settings_in_org_task(
        self,
        consensus_settings,
        find_org_task_with_consensus,
        org_role: str,
        is_staff: bool,
        allow: bool,
    ):
        task, user = find_org_task_with_consensus(is_staff, org_role)
        settings = next(s for s in consensus_settings if s["task_id"] == task["id"])
        request_data, expected_data = self._get_request_data(settings)

        if allow:
            self._test_patch_settings_200(
                user["username"], settings["id"], request_data, expected_data=expected_data
            )
        else:
            self._test_patch_settings_403(user["username"], settings["id"], request_data)


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestMerging(_PermissionTestBase):
    @pytest.mark.parametrize("task_id", [31])
    def test_quorum_is_applied(self, admin_user, jobs, labels, consensus_settings, task_id: int):
        task_labels = [l for l in labels if l.get("task_id") == task_id]
        settings = next(s for s in consensus_settings if s["task_id"] == task_id)

        task_jobs = [j for j in jobs if j["task_id"] == task_id]
        parent_job = next(
            j for j in task_jobs if j["type"] == "annotation" if j["consensus_replicas"] > 0
        )
        replicas = [
            j
            for j in task_jobs
            if j["type"] == "consensus_replica"
            if j["parent_job_id"] == parent_job["id"]
        ]
        assert len(replicas) == 2

        with make_api_client(admin_user) as api_client:
            api_client.tasks_api.destroy_annotations(task_id)

            for replica in replicas:
                api_client.jobs_api.destroy_annotations(replica["id"])

            api_client.consensus_api.partial_update_settings(
                settings["id"],
                patched_consensus_settings_request=models.PatchedConsensusSettingsRequest(
                    quorum=0.6
                ),
            )

            # Should be used > quorum times, must be present in the resulting dataset
            bbox1 = models.LabeledShapeRequest(
                type="rectangle",
                frame=parent_job["start_frame"],
                label_id=task_labels[0]["id"],
                points=[0, 0, 2, 2],
                attributes=[
                    {"spec_id": attr["id"], "value": attr["default_value"]}
                    for attr in task_labels[0]["attributes"]
                ],
                rotation=0,
                z_order=0,
                occluded=False,
                outside=False,
                group=0,
            )

            # Should be used < quorum times
            bbox2 = models.LabeledShapeRequest(
                type="rectangle",
                frame=parent_job["start_frame"],
                label_id=task_labels[0]["id"],
                points=[4, 0, 6, 2],
            )

            api_client.jobs_api.update_annotations(
                replicas[0]["id"],
                job_annotations_update_request=models.JobAnnotationsUpdateRequest(shapes=[bbox1]),
            )
            api_client.jobs_api.update_annotations(
                replicas[1]["id"],
                job_annotations_update_request=models.JobAnnotationsUpdateRequest(
                    shapes=[bbox1, bbox2]
                ),
            )

            self.merge(job_id=parent_job["id"], user=admin_user)

            merged_annotations = json.loads(
                api_client.jobs_api.retrieve_annotations(parent_job["id"])[1].data
            )
            assert (
                compare_annotations(
                    merged_annotations,
                    {"version": 0, "tags": [], "shapes": [bbox1.to_dict()], "tracks": []},
                )
                == {}
            )

    @pytest.mark.parametrize("job_id", [42])
    def test_unmodified_job_produces_same_annotations(self, admin_user, annotations, job_id: int):
        old_annotations = annotations["job"][str(job_id)]

        self.merge(job_id=job_id, user=admin_user)

        with make_api_client(admin_user) as api_client:
            new_annotations = json.loads(api_client.jobs_api.retrieve_annotations(job_id)[1].data)

            assert compare_annotations(old_annotations, new_annotations) == {}

    @pytest.mark.parametrize("job_id", [42])
    def test_modified_job_produces_different_annotations(
        self, admin_user, annotations, jobs, consensus_settings, job_id: int
    ):
        settings = next(
            s for s in consensus_settings if s["task_id"] == jobs.map[job_id]["task_id"]
        )
        old_annotations = annotations["job"][str(job_id)]

        with make_api_client(admin_user) as api_client:
            api_client.consensus_api.partial_update_settings(
                settings["id"],
                patched_consensus_settings_request=models.PatchedConsensusSettingsRequest(
                    quorum=0.6
                ),
            )

        self.merge(job_id=job_id, user=admin_user)

        with make_api_client(admin_user) as api_client:
            new_annotations = json.loads(api_client.jobs_api.retrieve_annotations(job_id)[1].data)

            assert compare_annotations(old_annotations, new_annotations) != {}


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_invitations.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from http import HTTPStatus

import pytest
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from deepdiff import DeepDiff

from shared.utils.config import get_method, make_api_client, post_method

from .utils import CollectionSimpleFilterTestBase


class TestCreateInvitations:
    ROLES = ["worker", "supervisor", "maintainer", "owner"]

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_function, organizations, memberships, admin_user):
        self.org_id = 2
        self.owner = self.get_member("owner", memberships, self.org_id)

    def _test_post_invitation_201(self, user, data, invitee, **kwargs):
        response = post_method(user, "invitations", data, **kwargs)

        assert response.status_code == HTTPStatus.CREATED, response.content
        assert data["role"] == response.json()["role"]
        assert invitee["id"] == response.json()["user"]["id"]
        assert kwargs["org_id"] == response.json()["organization"]

    def _test_post_invitation_403(self, user, data, **kwargs):
        response = post_method(user, "invitations", data, **kwargs)
        assert response.status_code == HTTPStatus.FORBIDDEN, response.content
        assert "You do not have permission" in str(response.content)

    @staticmethod
    def get_non_member_users(memberships, users):
        organization_users = set(m["user"]["id"] for m in memberships if m["user"] is not None)
        non_member_users = [u for u in users if u["id"] not in organization_users]

        return non_member_users

    @staticmethod
    def get_member(role, memberships, org_id):
        member = [
            m["user"]
            for m in memberships
            if m["role"] == role and m["organization"] == org_id and m["user"] is not None
        ][0]

        return member

    @pytest.mark.parametrize("org_role", ROLES)
    @pytest.mark.parametrize("invitee_role", ROLES)
    def test_create_invitation(self, organizations, memberships, users, org_role, invitee_role):
        org_id = self.org_id
        inviter_user = self.get_member(org_role, memberships, org_id)
        invitee_user = self.get_non_member_users(memberships, users)[0]

        if org_role in ["worker", "supervisor"]:
            self._test_post_invitation_403(
                inviter_user["username"],
                {"role": invitee_role, "email": invitee_user["email"]},
                org_id=org_id,
            )

        elif invitee_role in ["worker", "supervisor"]:
            self._test_post_invitation_201(
                inviter_user["username"],
                {"role": invitee_role, "email": invitee_user["email"]},
                invitee_user,
                org_id=org_id,
            )

        elif invitee_role == "maintainer":
            if org_role == "owner":
                # only the owner can invite a maintainer
                self._test_post_invitation_201(
                    inviter_user["username"],
                    {"role": invitee_role, "email": invitee_user["email"]},
                    invitee_user,
                    org_id=org_id,
                )
            else:
                self._test_post_invitation_403(
                    inviter_user["username"],
                    {"role": invitee_role, "email": invitee_user["email"]},
                    org_id=org_id,
                )

        elif invitee_role == "owner":
            # nobody can invite an owner
            self._test_post_invitation_403(
                inviter_user["username"],
                {"role": invitee_role, "email": invitee_user["email"]},
                org_id=org_id,
            )

        else:
            assert False, "Unknown role"


class TestInvitationsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, invitations):
        self.user = admin_user
        self.samples = invitations

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.invitations_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("owner",),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListInvitations:
    def _test_can_see_invitations(self, user, data, **kwargs):
        response = get_method(user, "invitations", **kwargs)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(data, response.json()["results"]) == {}

    def test_admin_can_see_all_invitations(self, invitations):
        self._test_can_see_invitations("admin2", invitations.raw, page_size="all")

    @pytest.mark.parametrize("field_value, query_value", [(1, 1), (None, "")])
    def test_can_filter_by_org_id(self, field_value, query_value, invitations):
        invitations = filter(lambda i: i["organization"] == field_value, invitations)
        self._test_can_see_invitations(
            "admin2", list(invitations), page_size="all", org_id=query_value
        )


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetInvitations:
    def test_can_remove_owner_and_fetch_with_sdk(self, admin_user, invitations):
        # test for API schema regressions
        source_inv = next(
            invitations
            for invitations in invitations
            if invitations.get("owner") and invitations["owner"]["username"] != admin_user
        ).copy()

        with make_api_client(admin_user) as api_client:
            api_client.users_api.destroy(source_inv["owner"]["id"])

            (_, response) = api_client.invitations_api.retrieve(source_inv["key"])
            fetched_inv = json.loads(response.data)

        source_inv["owner"] = None
        assert DeepDiff(source_inv, fetched_inv, ignore_order=True) == {}


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_issues.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from copy import deepcopy
from http import HTTPStatus
from typing import Any

import pytest
from cvat_sdk import models
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from deepdiff import DeepDiff

from shared.utils.config import get_method, make_api_client

from .utils import CollectionSimpleFilterTestBase


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostIssues:
    def _test_check_response(self, user, data, is_allow, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.issues_api.create(
                models.IssueWriteRequest(**data),
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )

        if is_allow:
            assert response.status == HTTPStatus.CREATED
            response_json = json.loads(response.data)
            assert user == response_json["owner"]["username"]

            with make_api_client(user) as client:
                (comments, _) = client.comments_api.list(issue_id=response_json["id"])
            assert data["message"] == comments.results[0].message

            assert (
                DeepDiff(
                    data,
                    response_json,
                    exclude_regex_paths=r"root\['created_date|updated_date|comments|id|owner|message'\]",
                )
                == {}
            )
        else:
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "privilege, job_staff, is_allow",
        [
            ("admin", True, True),
            ("admin", False, True),
            ("worker", True, True),
            ("worker", False, False),
            ("user", True, True),
            ("user", False, False),
        ],
    )
    def test_user_create_issue(
        self, org, privilege, job_staff, is_allow, find_job_staff_user, find_users, jobs_by_org
    ):
        users = find_users(privilege=privilege)
        jobs = jobs_by_org[org]
        username, jid = find_job_staff_user(jobs, users, job_staff)
        (job,) = filter(lambda job: job["id"] == jid, jobs)

        data = {
            "assignee": None,
            "comments": [],
            "job": jid,
            "frame": job["start_frame"],
            "position": [
                0.0,
                0.0,
                1.0,
                1.0,
            ],
            "resolved": False,
            "message": "lorem ipsum",
        }

        self._test_check_response(username, data, is_allow)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, job_staff, is_allow",
        [
            ("maintainer", False, True),
            ("owner", False, True),
            ("supervisor", False, False),
            ("worker", False, False),
            ("maintainer", True, True),
            ("owner", True, True),
            ("supervisor", True, True),
            ("worker", True, True),
        ],
    )
    def test_member_create_issue(
        self, org, role, job_staff, is_allow, find_job_staff_user, find_users, jobs_by_org, jobs
    ):
        users = find_users(role=role, org=org)
        username, jid = find_job_staff_user(jobs_by_org[org], users, job_staff)
        job = jobs[jid]

        data = {
            "assignee": None,
            "comments": [],
            "job": jid,
            "frame": job["start_frame"],
            "position": [
                0.0,
                0.0,
                1.0,
                1.0,
            ],
            "resolved": False,
            "message": "lorem ipsum",
        }

        self._test_check_response(username, data, is_allow, org_id=org)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchIssues:
    def _test_check_response(self, user, issue_id, data, is_allow, **kwargs):
        request_data, expected_response_data = data
        with make_api_client(user) as client:
            (_, response) = client.issues_api.partial_update(
                issue_id,
                patched_issue_write_request=models.PatchedIssueWriteRequest(**request_data),
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )

        if is_allow:
            assert response.status == HTTPStatus.OK
            assert (
                DeepDiff(
                    expected_response_data,
                    json.loads(response.data),
                    exclude_regex_paths=r"root\['created_date|updated_date|comments|id|owner'\]",
                )
                == {}
            )
        else:
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.fixture(scope="class")
    def request_and_response_data(self, issues, users):
        def get_data(issue_id, *, username: str = None):
            request_data = deepcopy(issues[issue_id])
            request_data["resolved"] = not request_data["resolved"]

            response_data = deepcopy(request_data)

            request_data.pop("comments")
            request_data.pop("updated_date")
            request_data.pop("id")
            request_data.pop("owner")

            if username:
                assignee = next(u for u in users if u["username"] == username)
                request_data["assignee"] = assignee["id"]
                response_data["assignee"] = {
                    k: assignee[k] for k in ["id", "username", "url", "first_name", "last_name"]
                }
            else:
                request_data["assignee"] = None

            return request_data, response_data

        return get_data

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "privilege, issue_staff, issue_admin, is_allow",
        [
            ("admin", True, None, True),
            ("admin", False, None, True),
            ("user", True, None, True),
            ("user", False, None, False),
            ("worker", False, True, True),
            ("worker", True, False, True),
            ("worker", False, False, False),
        ],
    )
    def test_user_update_issue(
        self,
        org,
        privilege,
        issue_staff,
        issue_admin,
        is_allow,
        find_issue_staff_user,
        find_users,
        issues_by_org,
        request_and_response_data,
    ):
        users = find_users(privilege=privilege)
        issues = issues_by_org[org]
        username, issue_id = find_issue_staff_user(issues, users, issue_staff, issue_admin)

        data = request_and_response_data(issue_id, username=username)
        self._test_check_response(username, issue_id, data, is_allow)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, issue_staff, issue_admin, is_allow",
        [
            ("maintainer", True, None, True),
            ("maintainer", False, None, True),
            ("supervisor", True, None, True),
            ("supervisor", False, None, False),
            ("owner", True, None, True),
            ("owner", False, None, True),
            ("worker", False, True, True),
            ("worker", True, False, True),
            ("worker", False, False, False),
        ],
    )
    def test_member_update_issue(
        self,
        org,
        role,
        issue_staff,
        issue_admin,
        is_allow,
        find_issue_staff_user,
        find_users,
        issues_by_org,
        request_and_response_data,
    ):
        users = find_users(role=role, org=org)
        issues = issues_by_org[org]
        username, issue_id = find_issue_staff_user(issues, users, issue_staff, issue_admin)

        data = request_and_response_data(issue_id, username=username)
        self._test_check_response(username, issue_id, data, is_allow)


@pytest.mark.usefixtures("restore_db_per_function")
class TestDeleteIssues:
    def _test_check_response(self, user, issue_id, expect_success, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.issues_api.destroy(
                issue_id,
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )

        if expect_success:
            assert response.status == HTTPStatus.NO_CONTENT

            (_, response) = client.issues_api.retrieve(
                issue_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.NOT_FOUND
        else:
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "privilege, issue_staff, issue_admin, expect_success",
        [
            ("admin", True, None, True),
            ("admin", False, None, True),
            ("user", True, None, True),
            ("user", False, None, False),
            ("worker", False, True, True),
            ("worker", True, False, False),
            ("worker", False, False, False),
        ],
    )
    def test_user_delete_issue(
        self,
        org,
        privilege,
        issue_staff,
        issue_admin,
        expect_success,
        find_issue_staff_user,
        find_users,
        issues_by_org,
    ):
        users = find_users(privilege=privilege)
        issues = issues_by_org[org]
        username, issue_id = find_issue_staff_user(issues, users, issue_staff, issue_admin)

        self._test_check_response(username, issue_id, expect_success)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, issue_staff, issue_admin, expect_success",
        [
            ("maintainer", True, None, True),
            ("maintainer", False, None, True),
            ("supervisor", True, None, True),
            ("supervisor", False, None, False),
            ("owner", True, None, True),
            ("owner", False, None, True),
            ("worker", False, True, True),
            ("worker", True, False, False),
            ("worker", False, False, False),
        ],
    )
    def test_org_member_delete_issue(
        self,
        org,
        role,
        issue_staff,
        issue_admin,
        expect_success,
        find_issue_staff_user,
        find_users,
        issues_by_org,
    ):
        users = find_users(role=role, org=org)
        issues = issues_by_org[org]
        username, issue_id = find_issue_staff_user(issues, users, issue_staff, issue_admin)

        self._test_check_response(username, issue_id, expect_success)


class TestIssuesListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
        "assignee": ["assignee", "username"],
        "job_id": ["job"],
        "frame_id": ["frame"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, issues):
        self.user = admin_user
        self.samples = issues

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.issues_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("owner", "assignee", "job_id", "resolved", "frame_id"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


class TestCommentsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
        "issue_id": ["issue"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, comments, issues):
        self.user = admin_user
        self.samples = comments
        self.sample_issues = issues

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.comments_api.list_endpoint

    def _get_field_samples(self, field: str) -> tuple[Any, list[dict[str, Any]]]:
        if field == "job_id":
            issue_id, issue_comments = super()._get_field_samples("issue_id")
            issue = next((s for s in self.sample_issues if s["id"] == issue_id))
            return issue["job"], issue_comments
        elif field == "frame_id":
            frame_id = self._find_valid_field_value(self.sample_issues, ["frame"])
            issues = [s["id"] for s in self.sample_issues if s["frame"] == frame_id]
            comments = [
                s for s in self.samples if self._get_field(s, self._map_field("issue_id")) in issues
            ]
            return frame_id, comments
        else:
            return super()._get_field_samples(field)

    @pytest.mark.parametrize(
        "field",
        ("owner", "issue_id", "job_id", "frame_id"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListIssues:
    def _test_can_see_issues(self, user, data, **kwargs):
        response = get_method(user, "issues", **kwargs)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(data, response.json()["results"]) == {}

    def test_admin_can_see_all_issues(self, issues):
        self._test_can_see_issues("admin2", issues.raw, page_size="all")

    @pytest.mark.parametrize("field_value, query_value", [(1, 1), (None, "")])
    def test_can_filter_by_org_id(self, field_value, query_value, issues, jobs):
        issues = filter(lambda i: jobs[i["job"]]["organization"] == field_value, issues)
        self._test_can_see_issues("admin2", list(issues), page_size="all", org_id=query_value)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_jobs.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import json
import math
import operator
import os
import xml.etree.ElementTree as ET
import zipfile
from copy import deepcopy
from datetime import datetime
from http import HTTPStatus
from io import BytesIO
from itertools import groupby, product
from typing import Any, Optional

import numpy as np
import pytest
from cvat_sdk import models
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.api_client.exceptions import ForbiddenException
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff
from PIL import Image
from pytest_cases import parametrize

from shared.utils.config import make_api_client
from shared.utils.helpers import generate_image_files

from .utils import (
    CollectionSimpleFilterTestBase,
    compare_annotations,
    create_task,
    export_job_dataset,
    parse_frame_step,
)


def get_job_staff(job, tasks, projects):
    job_staff = []
    job_staff.append(job["assignee"])
    tid = job["task_id"]
    job_staff.append(tasks[tid]["owner"])
    job_staff.append(tasks[tid]["assignee"])

    pid = job["project_id"]
    if pid:
        job_staff.append(projects[pid]["owner"])
        job_staff.append(projects[pid]["assignee"])
    job_staff = set(u["id"] for u in job_staff if u is not None)

    return job_staff


def filter_jobs(jobs, tasks, org):
    if isinstance(org, int):
        kwargs = {"org_id": org}
        jobs = [job for job in jobs if tasks[job["task_id"]]["organization"] == org]
    elif org == "":
        kwargs = {"org": ""}
        jobs = [job for job in jobs if tasks[job["task_id"]]["organization"] is None]
    else:
        kwargs = {}
        jobs = jobs.raw

    return jobs, kwargs


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostJobs:
    def _test_create_job_ok(self, user: str, data: dict[str, Any], **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.jobs_api.create(
                models.JobWriteRequest(**deepcopy(data)), **kwargs
            )
            assert response.status == HTTPStatus.CREATED
        return response

    def _test_create_job_fails(
        self, user: str, data: dict[str, Any], *, expected_status: int, **kwargs
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.jobs_api.create(
                models.JobWriteRequest(**deepcopy(data)),
                **kwargs,
                _check_status=False,
                _parse_response=False,
            )
            assert response.status == expected_status
        return response

    @parametrize(
        "frame_selection_method, method_params",
        [
            *tuple(product(["random_uniform"], [{"frame_count"}, {"frame_share"}])),
            *tuple(
                product(["random_per_job"], [{"frames_per_job_count"}, {"frames_per_job_share"}])
            ),
            ("manual", {}),
        ],
        idgen=lambda **args: "-".join([args["frame_selection_method"], *args["method_params"]]),
    )
    @pytest.mark.parametrize("task_mode", ["annotation", "interpolation"])
    def test_can_create_gt_job_in_a_task(
        self,
        admin_user,
        tasks,
        task_mode: str,
        frame_selection_method: str,
        method_params: set[str],
    ):
        required_task_size = 15

        task = next(
            t
            for t in tasks
            if t["mode"] == task_mode
            if required_task_size <= t["size"]
            if not t["validation_mode"]
        )
        task_id = task["id"]

        segment_size = task["segment_size"]
        total_frame_count = task["size"]

        job_params = {
            "task_id": task_id,
            "type": "ground_truth",
            "frame_selection_method": frame_selection_method,
        }

        if "random" in frame_selection_method:
            job_params["random_seed"] = 42

        if frame_selection_method == "random_uniform":
            validation_frames_count = 5

            for method_param in method_params:
                if method_param == "frame_count":
                    job_params[method_param] = validation_frames_count
                elif method_param == "frame_share":
                    job_params[method_param] = validation_frames_count / total_frame_count
                else:
                    assert False
        elif frame_selection_method == "random_per_job":
            validation_per_job_count = 2
            validation_frames_count = validation_per_job_count * math.ceil(
                total_frame_count / segment_size
            )

            for method_param in method_params:
                if method_param == "frames_per_job_count":
                    job_params[method_param] = validation_per_job_count
                elif method_param == "frames_per_job_share":
                    job_params[method_param] = validation_per_job_count / segment_size
                else:
                    assert False
        elif frame_selection_method == "manual":
            validation_frames_count = 5

            rng = np.random.Generator(np.random.MT19937(seed=42))
            job_params["frames"] = rng.choice(
                range(total_frame_count), validation_frames_count, replace=False
            ).tolist()
        else:
            assert False

        with make_api_client(admin_user) as api_client:
            (gt_job, _) = api_client.jobs_api.create(job_write_request=job_params)

            # GT jobs occupy the whole task frame range
            assert gt_job.start_frame == 0
            assert gt_job.stop_frame + 1 == task["size"]
            assert gt_job.type == "ground_truth"
            assert gt_job.task_id == task_id

            annotation_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="annotation"
                )
            ]
            gt_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
                )
            ]

            assert len(gt_job_metas) == 1

        frame_step = parse_frame_step(gt_job_metas[0].frame_filter)
        validation_frames = [
            abs_frame_id
            for abs_frame_id in range(
                gt_job_metas[0].start_frame,
                gt_job_metas[0].stop_frame + 1,
                frame_step,
            )
            if abs_frame_id in gt_job_metas[0].included_frames
        ]

        if frame_selection_method == "random_per_job":
            # each job must have the specified number of validation frames
            for job_meta in annotation_job_metas:
                assert (
                    len(
                        set(
                            range(job_meta.start_frame, job_meta.stop_frame + 1, frame_step)
                        ).intersection(validation_frames)
                    )
                    == validation_per_job_count
                )
        else:
            assert len(validation_frames) == validation_frames_count

    @pytest.mark.parametrize(
        "task_id, frame_ids",
        [
            # The results have to be the same in different CVAT revisions,
            # so the task ids are fixed
            (21, [3, 5, 7]),  # annotation task
            (5, [11, 14, 20]),  # interpolation task
        ],
    )
    def test_can_create_gt_job_with_random_frames_and_seed(self, admin_user, task_id, frame_ids):
        user = admin_user
        job_spec = {
            "task_id": task_id,
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 3,
            "seed": 42,
        }

        response = self._test_create_job_ok(user, job_spec)
        job_id = json.loads(response.data)["id"]

        with make_api_client(user) as api_client:
            (gt_job_meta, _) = api_client.jobs_api.retrieve_data_meta(job_id)

        assert frame_ids == gt_job_meta.included_frames

    @pytest.mark.parametrize("task_mode", ["annotation", "interpolation"])
    def test_can_create_gt_job_with_all_frames(self, admin_user, tasks, jobs, task_mode):
        user = admin_user
        task = next(
            t
            for t in tasks
            if t["mode"] == task_mode
            and t["size"]
            and not any(j for j in jobs if j["task_id"] == t["id"] and j["type"] == "ground_truth")
        )
        task_id = task["id"]

        job_spec = {
            "task_id": task_id,
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": task["size"],
        }

        response = self._test_create_job_ok(user, job_spec)
        job_id = json.loads(response.data)["id"]

        with make_api_client(user) as api_client:
            (gt_job_meta, _) = api_client.jobs_api.retrieve_data_meta(job_id)

        assert task["size"] == gt_job_meta.size

    @pytest.mark.parametrize("validation_mode", ["gt", "gt_pool"])
    def test_can_create_no_more_than_1_gt_job(self, admin_user, tasks, jobs, validation_mode):
        user = admin_user
        task_id = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            if tasks[j["task_id"]]["validation_mode"] == validation_mode
        )["task_id"]

        job_spec = {
            "task_id": task_id,
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        response = self._test_create_job_fails(
            user, job_spec, expected_status=HTTPStatus.BAD_REQUEST
        )

        assert (
            f'Task with validation mode \\"{validation_mode}\\" '
            "cannot have more than 1 GT job".encode() in response.data
        )

    def test_can_create_gt_job_in_sandbox_task(self, tasks, jobs, users):
        task = next(
            t
            for t in tasks
            if t["organization"] is None
            and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
            and not users[t["owner"]["id"]]["is_superuser"]
        )
        user = task["owner"]["username"]

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        self._test_create_job_ok(user, job_spec)

    @pytest.mark.parametrize(
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, False),
            ("worker", False, False),
        ],
    )
    def test_create_gt_job_in_org_task(
        self, tasks, jobs, users, is_org_member, is_task_staff, org_role, is_staff, allow
    ):
        for user in users:
            if user["is_superuser"]:
                continue

            task = next(
                (
                    t
                    for t in tasks
                    if t["organization"] is not None
                    and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
                    and is_task_staff(user["id"], t["id"]) == is_staff
                    and is_org_member(user["id"], t["organization"], role=org_role)
                ),
                None,
            )
            if task is not None:
                break

        assert task

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        if allow:
            self._test_create_job_ok(user["username"], job_spec)
        else:
            self._test_create_job_fails(
                user["username"], job_spec, expected_status=HTTPStatus.FORBIDDEN
            )

    def test_create_response_matches_get(self, tasks, jobs, users):
        task = next(
            t
            for t in tasks
            if t["organization"] is None
            and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
            and not users[t["owner"]["id"]]["is_superuser"]
        )
        user = task["owner"]["username"]

        spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        response = self._test_create_job_ok(user, spec)
        job = json.loads(response.data)

        with make_api_client(user) as api_client:
            (_, response) = api_client.jobs_api.retrieve(job["id"])
            assert DeepDiff(job, json.loads(response.data), ignore_order=True) == {}

    @pytest.mark.parametrize("assignee", [None, "admin1"])
    def test_can_create_with_assignee(self, admin_user, tasks, jobs, users_by_name, assignee):
        task = next(
            t
            for t in tasks
            if t["size"] > 0
            if all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
        )

        spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
            "assignee": users_by_name[assignee]["id"] if assignee else None,
        }

        with make_api_client(admin_user) as api_client:
            (job, _) = api_client.jobs_api.create(job_write_request=spec)

            if assignee:
                assert job.assignee.username == assignee
                assert job.assignee_updated_date
            else:
                assert job.assignee is None
                assert job.assignee_updated_date is None


@pytest.mark.usefixtures("restore_db_per_function")
class TestDeleteJobs:
    def _test_destroy_job_ok(self, user, job_id, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.jobs_api.destroy(job_id, **kwargs)
            assert response.status == HTTPStatus.NO_CONTENT

    def _test_destroy_job_fails(self, user, job_id, *, expected_status: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.jobs_api.destroy(
                job_id, **kwargs, _check_status=False, _parse_response=False
            )
            assert response.status == expected_status
        return response

    @pytest.mark.usefixtures("restore_cvat_data_per_function")
    @pytest.mark.parametrize(
        "validation_mode, job_type, allow",
        (
            (None, "annotation", False),
            ("gt", "ground_truth", True),
            ("gt", "annotation", False),
            ("gt_pool", "ground_truth", False),
            ("gt_pool", "annotation", False),
        ),
    )
    def test_destroy_job(self, admin_user, tasks, jobs, validation_mode, job_type, allow):
        job = next(
            j
            for j in jobs
            if j["type"] == job_type
            if tasks[j["task_id"]]["validation_mode"] == validation_mode
        )

        if allow:
            self._test_destroy_job_ok(admin_user, job["id"])
        else:
            self._test_destroy_job_fails(
                admin_user, job["id"], expected_status=HTTPStatus.BAD_REQUEST
            )

    def test_can_destroy_gt_job_in_sandbox_task(self, tasks, jobs, users, admin_user):
        task = next(
            t
            for t in tasks
            if t["organization"] is None
            if all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
            if not users[t["owner"]["id"]]["is_superuser"]
        )
        user = task["owner"]["username"]

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        with make_api_client(admin_user) as api_client:
            (job, _) = api_client.jobs_api.create(job_spec)

        self._test_destroy_job_ok(user, job.id)

    @pytest.mark.parametrize(
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, False),
            ("worker", False, False),
        ],
    )
    def test_destroy_gt_job_in_org_task(
        self,
        tasks,
        jobs,
        users,
        is_org_member,
        is_task_staff,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        for user in users:
            task = next(
                (
                    t
                    for t in tasks
                    if t["organization"] is not None
                    and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
                    and is_task_staff(user["id"], t["id"]) == is_staff
                    and is_org_member(user["id"], t["organization"], role=org_role)
                ),
                None,
            )
            if task is not None:
                break

        assert task

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        with make_api_client(admin_user) as api_client:
            (job, _) = api_client.jobs_api.create(job_spec)

        if allow:
            self._test_destroy_job_ok(user["username"], job.id)
        else:
            self._test_destroy_job_fails(
                user["username"], job.id, expected_status=HTTPStatus.FORBIDDEN
            )


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetJobs:
    def _test_get_job_200(
        self, user, jid, *, expected_data: Optional[dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as client:
            (_, response) = client.jobs_api.retrieve(jid, **kwargs)
            assert response.status == HTTPStatus.OK

            if expected_data is not None:
                assert compare_annotations(expected_data, json.loads(response.data)) == {}

    def _test_get_job_403(self, user, jid, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.jobs_api.retrieve(
                jid, **kwargs, _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    def test_admin_can_get_sandbox_job(self, admin_user, jobs, tasks):
        job = next(job for job in jobs if tasks[job["task_id"]]["organization"] is None)
        self._test_get_job_200(admin_user, job["id"], expected_data=job)

    def test_admin_can_get_org_job(self, admin_user, jobs, tasks):
        job = next(job for job in jobs if tasks[job["task_id"]]["organization"] is not None)
        self._test_get_job_200(admin_user, job["id"], expected_data=job)

    @pytest.mark.parametrize("groups", [["user"]])
    def test_non_admin_org_staff_can_get_job(
        self, groups, users, organizations, org_staff, jobs_by_org
    ):
        user, org_id = next(
            (user, org["id"])
            for user in users
            for org in organizations
            if user["groups"] == groups and user["id"] in org_staff(org["id"])
        )
        job = jobs_by_org[org_id][0]
        self._test_get_job_200(user["username"], job["id"], expected_data=job)

    @pytest.mark.parametrize("groups", [["user"], ["worker"]])
    def test_non_admin_job_staff_can_get_job(self, groups, users, jobs, is_job_staff):
        user, job = next(
            (user, job)
            for user in users
            for job in jobs
            if user["groups"] == groups and is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_200(user["username"], job["id"], expected_data=job)

    @pytest.mark.parametrize("groups", [["user"], ["worker"]])
    def test_non_admin_non_job_staff_non_org_staff_cannot_get_job(
        self, groups, users, organizations, org_staff, jobs, is_job_staff
    ):
        user, job_id = next(
            (user, job["id"])
            for user in users
            for org in organizations
            for job in jobs
            if user["groups"] == groups
            and user["id"] not in org_staff(org["id"])
            and not is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_403(user["username"], job_id)

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_can_get_gt_job_in_sandbox_task(self, tasks, jobs, users, admin_user):
        task = next(
            t
            for t in tasks
            if t["organization"] is None
            and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
            and not users[t["owner"]["id"]]["is_superuser"]
        )
        user = task["owner"]["username"]

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        with make_api_client(admin_user) as api_client:
            (job, _) = api_client.jobs_api.create(job_spec)

        self._test_get_job_200(user, job.id)

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, True),
            ("worker", False, False),
        ],
    )
    def test_get_gt_job_in_org_task(
        self,
        tasks,
        jobs,
        users,
        is_org_member,
        is_task_staff,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        for user in users:
            task = next(
                (
                    t
                    for t in tasks
                    if t["organization"] is not None
                    and all(j["type"] != "ground_truth" for j in jobs if j["task_id"] == t["id"])
                    and is_task_staff(user["id"], t["id"]) == is_staff
                    and is_org_member(user["id"], t["organization"], role=org_role)
                ),
                None,
            )
            if task is not None:
                break

        assert task

        job_spec = {
            "task_id": task["id"],
            "type": "ground_truth",
            "frame_selection_method": "random_uniform",
            "frame_count": 1,
        }

        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.jobs_api.create(job_spec)
            job = json.loads(response.data)

        if allow:
            self._test_get_job_200(user["username"], job["id"], expected_data=job)
        else:
            self._test_get_job_403(user["username"], job["id"])


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_redis_ondisk_per_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_class")
class TestGetGtJobData:
    def _delete_gt_job(self, user, gt_job_id):
        with make_api_client(user) as api_client:
            api_client.jobs_api.destroy(gt_job_id)

    @pytest.mark.parametrize("task_mode", ["annotation", "interpolation"])
    def test_can_get_gt_job_meta(self, admin_user, tasks, jobs, task_mode, request):
        user = admin_user
        job_frame_count = 4
        task = next(
            t
            for t in tasks
            if not t["project_id"]
            and not t["organization"]
            and t["mode"] == task_mode
            and t["size"] > job_frame_count
            and not any(j for j in jobs if j["task_id"] == t["id"] and j["type"] == "ground_truth")
        )
        task_id = task["id"]
        with make_api_client(user) as api_client:
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            frame_step = parse_frame_step(task_meta.frame_filter.split("=")[-1])

        job_frame_ids = list(range(task_meta.start_frame, task_meta.stop_frame, frame_step))[
            :job_frame_count
        ]
        gt_job = self._create_gt_job(admin_user, task_id, job_frame_ids)
        request.addfinalizer(lambda: self._delete_gt_job(user, gt_job.id))

        with make_api_client(user) as api_client:
            (gt_job_meta, _) = api_client.jobs_api.retrieve_data_meta(gt_job.id)

        # These values are relative to the resulting task frames, unlike meta values
        assert 0 == gt_job.start_frame
        assert task_meta.size - 1 == gt_job.stop_frame

        # The size is adjusted by the frame step and included frames
        assert job_frame_count == gt_job_meta.size
        assert job_frame_ids == gt_job_meta.included_frames

        # The frames themselves are the same as in the whole range
        # this is required by the UI implementation
        assert task_meta.start_frame == gt_job_meta.start_frame
        assert task_meta.stop_frame == gt_job_meta.stop_frame
        if task_mode == "annotation":
            assert (
                len(gt_job_meta.frames)
                == (gt_job_meta.stop_frame + 1 - gt_job_meta.start_frame) / frame_step
            )
        elif task_mode == "interpolation":
            assert len(gt_job_meta.frames) == 1
        else:
            assert False

    def test_can_get_gt_job_meta_with_complex_frame_setup(self, admin_user, request):
        image_count = 50
        start_frame = 3
        stop_frame = image_count - 4
        frame_step = 5

        images = generate_image_files(image_count)

        task_id, _ = create_task(
            admin_user,
            spec={
                "name": "test complex frame setup",
                "labels": [{"name": "cat"}],
            },
            data={
                "image_quality": 75,
                "start_frame": start_frame,
                "stop_frame": stop_frame,
                "frame_filter": f"step={frame_step}",
                "client_files": images,
                "sorting_method": "predefined",
            },
        )

        task_frame_ids = range(start_frame, stop_frame, frame_step)
        gt_frame_ids = list(range(len(task_frame_ids)))[::3]
        gt_job = self._create_gt_job(admin_user, task_id, gt_frame_ids)
        request.addfinalizer(lambda: self._delete_gt_job(admin_user, gt_job.id))

        with make_api_client(admin_user) as api_client:
            (gt_job_meta, _) = api_client.jobs_api.retrieve_data_meta(gt_job.id)

        # These values are relative to the resulting task frames, unlike meta values
        assert 0 == gt_job.start_frame
        assert len(task_frame_ids) - 1 == gt_job.stop_frame

        # The size is adjusted by the frame step and included frames
        assert len(gt_frame_ids) == gt_job_meta.size
        assert (
            list(task_frame_ids[gt_frame] for gt_frame in gt_frame_ids)
            == gt_job_meta.included_frames
        )

        # The frames themselves are the same as in the whole range
        # with placeholders in the frames outside the job.
        # This is required by the UI implementation
        assert start_frame == gt_job_meta.start_frame
        assert max(task_frame_ids) == gt_job_meta.stop_frame
        assert [frame_info["name"] for frame_info in gt_job_meta.frames] == [
            images[frame].name if frame in gt_job_meta.included_frames else "placeholder.jpg"
            for frame in task_frame_ids
        ]

    @pytest.mark.parametrize("task_mode", ["annotation", "interpolation"])
    @pytest.mark.parametrize("quality", ["compressed", "original"])
    @pytest.mark.parametrize("indexing", ["absolute", "relative"])
    def test_can_get_gt_job_chunk(
        self, admin_user, tasks, jobs, task_mode, quality, request, indexing
    ):
        user = admin_user
        job_frame_count = 4
        task = next(
            t
            for t in tasks
            if not t["project_id"]
            and not t["organization"]
            and t["mode"] == task_mode
            and t["size"] > job_frame_count
            and not any(j for j in jobs if j["task_id"] == t["id"] and j["type"] == "ground_truth")
        )
        task_id = task["id"]
        with make_api_client(user) as api_client:
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            frame_step = parse_frame_step(task_meta.frame_filter.split("=")[-1])

        task_frame_ids = range(task_meta.start_frame, task_meta.stop_frame + 1, frame_step)
        rng = np.random.Generator(np.random.MT19937(42))
        job_frame_ids = sorted(rng.choice(task_frame_ids, job_frame_count, replace=False).tolist())

        gt_job = self._create_gt_job(admin_user, task_id, job_frame_ids)
        request.addfinalizer(lambda: self._delete_gt_job(admin_user, gt_job.id))

        if indexing == "absolute":
            chunk_iter = groupby(task_frame_ids, key=lambda f: f // task_meta.chunk_size)
        else:
            chunk_iter = groupby(job_frame_ids, key=lambda f: f // task_meta.chunk_size)

        for chunk_id, chunk_frames in chunk_iter:
            chunk_frames = list(chunk_frames)

            if indexing == "absolute":
                kwargs = {"number": chunk_id}
            else:
                kwargs = {"index": chunk_id}

            with make_api_client(admin_user) as api_client:
                (chunk_file, response) = api_client.jobs_api.retrieve_data(
                    gt_job.id, **kwargs, quality=quality, type="chunk"
                )
                assert response.status == HTTPStatus.OK

            # The frame count is the same as in the whole range
            # with placeholders in the frames outside the job.
            # This is required by the UI implementation
            with zipfile.ZipFile(chunk_file) as chunk:
                assert set(chunk.namelist()) == set(
                    f"{i:06d}.jpeg" for i in range(len(chunk_frames))
                )

                for file_info in chunk.filelist:
                    with chunk.open(file_info) as image_file:
                        image = Image.open(image_file)

                    chunk_frame_id = int(os.path.splitext(file_info.filename)[0])
                    if chunk_frames[chunk_frame_id] not in job_frame_ids:
                        assert image.size == (1, 1)
                    else:
                        assert image.size > (1, 1)

    def _create_gt_job(self, user, task_id, frames):
        with make_api_client(user) as api_client:
            job_spec = {
                "task_id": task_id,
                "type": "ground_truth",
                "frame_selection_method": "manual",
                "frames": frames,
            }

            (gt_job, _) = api_client.jobs_api.create(job_spec)

        return gt_job

    def _get_gt_job(self, user, task_id):
        with make_api_client(user) as api_client:
            (task_jobs, _) = api_client.jobs_api.list(task_id=task_id, type="ground_truth")
            gt_job = task_jobs.results[0]

        return gt_job

    @pytest.mark.parametrize("task_mode", ["annotation", "interpolation"])
    @pytest.mark.parametrize("quality", ["compressed", "original"])
    def test_can_get_gt_job_frame(self, admin_user, tasks, jobs, task_mode, quality, request):
        user = admin_user
        job_frame_count = 4
        task = next(
            t
            for t in tasks
            if not t["project_id"]
            and not t["organization"]
            and t["mode"] == task_mode
            and t["size"] > job_frame_count
            and not any(j for j in jobs if j["task_id"] == t["id"] and j["type"] == "ground_truth")
        )
        task_id = task["id"]
        with make_api_client(user) as api_client:
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            frame_step = parse_frame_step(task_meta.frame_filter.split("=")[-1])

        job_frame_ids = list(range(task_meta.start_frame, task_meta.stop_frame, frame_step))[
            :job_frame_count
        ]
        gt_job = self._create_gt_job(admin_user, task_id, job_frame_ids)
        request.addfinalizer(lambda: self._delete_gt_job(admin_user, gt_job.id))

        frame_range = range(
            task_meta.start_frame, min(task_meta.stop_frame + 1, task_meta.chunk_size), frame_step
        )
        included_frames = job_frame_ids
        excluded_frames = list(set(frame_range).difference(included_frames))

        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.jobs_api.retrieve_data(
                gt_job.id,
                number=excluded_frames[0],
                quality=quality,
                type="frame",
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.BAD_REQUEST
            assert b"Incorrect requested frame number" in response.data

            (_, response) = api_client.jobs_api.retrieve_data(
                gt_job.id, number=included_frames[0], quality=quality, type="frame"
            )
            assert response.status == HTTPStatus.OK


@pytest.mark.usefixtures("restore_db_per_class")
class TestListJobs:
    def _test_list_jobs_200(self, user, data, **kwargs):
        with make_api_client(user) as client:
            results = get_paginated_collection(
                client.jobs_api.list_endpoint, return_json=True, **kwargs
            )
            assert compare_annotations(data, results) == {}

    def _test_list_jobs_403(self, user, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.jobs_api.list(
                **kwargs, _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("org", [None, "", 1, 2])
    def test_admin_list_jobs(self, jobs, tasks, org):
        jobs, kwargs = filter_jobs(jobs, tasks, org)
        self._test_list_jobs_200("admin1", jobs, **kwargs)

    @pytest.mark.parametrize("org_id", ["", None, 1, 2])
    @pytest.mark.parametrize("groups", [["user"], ["worker"], []])
    def test_non_admin_list_jobs(
        self, org_id, groups, users, jobs, tasks, projects, org_staff, is_org_member
    ):
        users = [u for u in users if u["groups"] == groups][:2]
        jobs, kwargs = filter_jobs(jobs, tasks, org_id)
        org_staff = org_staff(org_id)

        for user in users:
            user_jobs = []
            for job in jobs:
                job_staff = get_job_staff(job, tasks, projects)
                if user["id"] in job_staff | org_staff:
                    user_jobs.append(job)
            if is_org_member(user["id"], org_id):
                self._test_list_jobs_200(user["username"], user_jobs, **kwargs)
            else:
                self._test_list_jobs_403(user["username"], **kwargs)


class TestJobsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "assignee": ["assignee", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, jobs):
        self.user = admin_user
        self.samples = jobs

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.jobs_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        (
            "assignee",
            "state",
            "stage",
            "task_id",
            "project_id",
            "type",
            "parent_job_id",
        ),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetAnnotations:
    def _test_get_job_annotations_200(self, user, jid, data):
        with make_api_client(user) as client:
            (_, response) = client.jobs_api.retrieve_annotations(jid)
            assert response.status == HTTPStatus.OK
            assert compare_annotations(data, json.loads(response.data)) == {}

    def _test_get_job_annotations_403(self, user, jid):
        with make_api_client(user) as client:
            (_, response) = client.jobs_api.retrieve_annotations(
                jid, _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "groups, job_staff, expect_success",
        [
            (["admin"], True, True),
            (["admin"], False, True),
            (["worker"], True, True),
            (["worker"], False, False),
            (["user"], True, True),
            (["user"], False, False),
        ],
    )
    def test_user_get_job_annotations(
        self,
        org,
        groups,
        job_staff,
        expect_success,
        users,
        jobs,
        tasks,
        annotations,
        find_job_staff_user,
    ):
        users = [u for u in users if u["groups"] == groups]
        jobs, _ = filter_jobs(jobs, tasks, org)
        username, job_id = find_job_staff_user(jobs, users, job_staff)

        if expect_success:
            self._test_get_job_annotations_200(username, job_id, annotations["job"][str(job_id)])
        else:
            self._test_get_job_annotations_403(username, job_id)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, job_staff, expect_success",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, True),
            ("worker", False, False),
        ],
    )
    def test_member_get_job_annotations(
        self,
        org,
        role,
        job_staff,
        expect_success,
        jobs,
        tasks,
        find_job_staff_user,
        annotations,
        find_users,
    ):
        users = find_users(org=org, role=role)
        jobs, _ = filter_jobs(jobs, tasks, org)
        username, jid = find_job_staff_user(jobs, users, job_staff)

        if expect_success:
            data = annotations["job"][str(jid)]
            data["shapes"] = sorted(data["shapes"], key=lambda a: a["id"])
            self._test_get_job_annotations_200(username, jid, data)
        else:
            self._test_get_job_annotations_403(username, jid)

    @pytest.mark.parametrize("org", [1])
    @pytest.mark.parametrize(
        "privilege, expect_success",
        [("admin", True), ("worker", False), ("user", False)],
    )
    def test_non_member_get_job_annotations(
        self,
        org,
        privilege,
        expect_success,
        jobs,
        tasks,
        find_job_staff_user,
        annotations,
        find_users,
    ):
        users = find_users(privilege=privilege, exclude_org=org)
        jobs, _ = filter_jobs(jobs, tasks, org)
        username, job_id = find_job_staff_user(jobs, users, False)

        if expect_success:
            self._test_get_job_annotations_200(username, job_id, annotations["job"][str(job_id)])
        else:
            self._test_get_job_annotations_403(username, job_id)

    @pytest.mark.parametrize("job_type", ("ground_truth", "annotation"))
    def test_can_get_annotations(self, admin_user, jobs, annotations, job_type):
        job = next(j for j in jobs if j["type"] == job_type)
        self._test_get_job_annotations_200(
            admin_user, job["id"], annotations["job"][str(job["id"])]
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchJobAnnotations:
    def _check_response(self, username, jid, expect_success, data=None):
        with make_api_client(username) as client:
            (_, response) = client.jobs_api.partial_update_annotations(
                id=jid,
                patched_labeled_data_request=deepcopy(data),
                action="update",
                _parse_response=expect_success,
                _check_status=expect_success,
            )

            if expect_success:
                assert response.status == HTTPStatus.OK
                assert compare_annotations(data, json.loads(response.data)) == {}
            else:
                assert response.status == HTTPStatus.FORBIDDEN

    @pytest.fixture(scope="class")
    def request_data(self, annotations):
        def get_data(jid):
            data = deepcopy(annotations["job"][str(jid)])
            if data["shapes"][0]["type"] == "skeleton":
                data["shapes"][0]["elements"][0].update({"points": [2.0, 3.0, 4.0, 5.0]})
            else:
                data["shapes"][0].update({"points": [2.0, 3.0, 4.0, 5.0, 6.0, 7.0]})
            data["version"] += 1
            return data

        return get_data

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, job_staff, expect_success",
        [
            ("maintainer", False, True),
            ("owner", False, True),
            ("supervisor", False, False),
            ("worker", False, False),
            ("maintainer", True, True),
            ("owner", True, True),
            ("supervisor", True, True),
            ("worker", True, True),
        ],
    )
    def test_member_update_job_annotations(
        self,
        org,
        role,
        job_staff,
        expect_success,
        find_job_staff_user,
        find_users,
        request_data,
        jobs_by_org,
        filter_jobs_with_shapes,
    ):
        users = find_users(role=role, org=org)
        jobs = jobs_by_org[org]
        filtered_jobs = filter_jobs_with_shapes(jobs)
        username, jid = find_job_staff_user(filtered_jobs, users, job_staff)

        data = request_data(jid)
        self._check_response(username, jid, expect_success, data)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "privilege, expect_success",
        [("admin", True), ("worker", False), ("user", False)],
    )
    def test_non_member_update_job_annotations(
        self,
        org,
        privilege,
        expect_success,
        find_job_staff_user,
        find_users,
        request_data,
        jobs_by_org,
        filter_jobs_with_shapes,
    ):
        users = find_users(privilege=privilege, exclude_org=org)
        jobs = jobs_by_org[org]
        filtered_jobs = filter_jobs_with_shapes(jobs)
        username, jid = find_job_staff_user(filtered_jobs, users, False)

        data = request_data(jid)
        self._check_response(username, jid, expect_success, data)

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "privilege, job_staff, expect_success",
        [
            ("admin", True, True),
            ("admin", False, True),
            ("worker", True, True),
            ("worker", False, False),
            ("user", True, True),
            ("user", False, False),
        ],
    )
    def test_user_update_job_annotations(
        self,
        org,
        privilege,
        job_staff,
        expect_success,
        find_job_staff_user,
        find_users,
        request_data,
        jobs_by_org,
        filter_jobs_with_shapes,
    ):
        users = find_users(privilege=privilege)
        jobs = jobs_by_org[org]
        filtered_jobs = filter_jobs_with_shapes(jobs)
        username, jid = find_job_staff_user(filtered_jobs, users, job_staff)

        data = request_data(jid)
        self._check_response(username, jid, expect_success, data)

    @pytest.mark.parametrize("job_type", ("ground_truth", "annotation"))
    def test_can_update_annotations(self, admin_user, jobs_with_shapes, request_data, job_type):
        job = next(j for j in jobs_with_shapes if j["type"] == job_type)
        data = request_data(job["id"])
        self._check_response(admin_user, job["id"], True, data)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchJob:
    @pytest.fixture(scope="class")
    def find_task_staff_user(self, is_task_staff):
        def find(jobs, users, is_staff):
            for job in jobs:
                for user in users:
                    if is_staff == is_task_staff(user["id"], job["task_id"]):
                        return user, job["id"]
            return None, None

        return find

    @pytest.fixture(scope="class")
    def expected_data(self, jobs, users):
        keys = ["url", "id", "username", "first_name", "last_name"]

        def find(job_id, assignee_id):
            data = deepcopy(jobs[job_id])
            data["assignee"] = dict(filter(lambda a: a[0] in keys, users[assignee_id].items()))
            return data

        return find

    @pytest.fixture(scope="class")
    def new_assignee(self, jobs, tasks, assignee_id, org_staff):
        def find_new_assignee(jid, user_id):
            members = org_staff(tasks[jobs[jid]["task_id"]]["organization"])
            members -= {assignee_id(jobs[jid]), user_id}
            return members.pop()

        return find_new_assignee

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, task_staff, expect_success",
        [
            ("maintainer", False, True),
            ("owner", False, True),
            ("supervisor", False, False),
            ("worker", False, False),
            ("maintainer", True, True),
            ("owner", True, True),
            ("supervisor", True, True),
            ("worker", True, True),
        ],
    )
    def test_member_update_job_assignee(
        self,
        org,
        role,
        task_staff,
        expect_success,
        find_task_staff_user,
        find_users,
        jobs_by_org,
        new_assignee,
        expected_data,
    ):
        users, jobs = find_users(role=role, org=org), jobs_by_org[org]
        user, jid = find_task_staff_user(jobs, users, task_staff)

        assignee = new_assignee(jid, user["id"])
        with make_api_client(user["username"]) as client:
            (_, response) = client.jobs_api.partial_update(
                id=jid,
                patched_job_write_request={"assignee": assignee},
                _parse_response=expect_success,
                _check_status=expect_success,
            )

            if expect_success:
                assert response.status == HTTPStatus.OK
                assert (
                    DeepDiff(
                        expected_data(jid, assignee),
                        json.loads(response.data),
                        exclude_paths=["root['updated_date']", "root['assignee_updated_date']"],
                        ignore_order=True,
                    )
                    == {}
                )
            else:
                assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("has_old_assignee", [False, True])
    @pytest.mark.parametrize("new_assignee", [None, "same", "different"])
    def test_can_update_assignee_updated_date_on_assignee_updates(
        self, admin_user, jobs, users, has_old_assignee, new_assignee
    ):
        job = next(j for j in jobs if bool(j.get("assignee")) == has_old_assignee)

        old_assignee_id = (job.get("assignee") or {}).get("id")

        new_assignee_id = None
        if new_assignee == "same":
            new_assignee_id = old_assignee_id
        elif new_assignee == "different":
            new_assignee_id = next(u for u in users if u["id"] != old_assignee_id)["id"]

        with make_api_client(admin_user) as api_client:
            (updated_job, _) = api_client.jobs_api.partial_update(
                job["id"], patched_job_write_request={"assignee": new_assignee_id}
            )

            op = operator.eq if new_assignee_id == old_assignee_id else operator.ne

            if isinstance(updated_job.assignee_updated_date, datetime):
                assert op(
                    str(updated_job.assignee_updated_date.isoformat()).replace("+00:00", "Z"),
                    job["assignee_updated_date"],
                )
            else:
                assert op(updated_job.assignee_updated_date, job["assignee_updated_date"])

            if new_assignee_id:
                assert updated_job.assignee.id == new_assignee_id
            else:
                assert updated_job.assignee is None

    def test_malefactor_cannot_obtain_job_details_via_empty_partial_update_request(
        self, regular_lonely_user, jobs
    ):
        job = next(iter(jobs))

        with make_api_client(regular_lonely_user) as api_client:
            with pytest.raises(ForbiddenException):
                api_client.jobs_api.partial_update(job["id"])


def _check_coco_job_annotations(content, values_to_be_checked):
    exported_annotations = json.loads(content)
    if "shapes_length" in values_to_be_checked:
        assert values_to_be_checked["shapes_length"] == len(exported_annotations["annotations"])
    assert values_to_be_checked["job_size"] == len(exported_annotations["images"])
    assert values_to_be_checked["task_size"] > len(exported_annotations["images"])


def _check_cvat_for_images_job_annotations(content, values_to_be_checked):
    document = ET.fromstring(content)
    # check meta information
    meta = document.find("meta")
    instance = list(meta)[0]
    assert instance.tag == "job"
    assert instance.find("id").text == values_to_be_checked["job_id"]
    assert instance.find("size").text == str(values_to_be_checked["job_size"])
    assert instance.find("start_frame").text == str(values_to_be_checked["start_frame"])
    assert instance.find("stop_frame").text == str(values_to_be_checked["stop_frame"])
    assert instance.find("mode").text == values_to_be_checked["mode"]
    assert len(instance.find("segments")) == 1

    # check number of images, their sorting, number of annotations
    images = document.findall("image")
    assert len(images) == values_to_be_checked["job_size"]
    if "shapes_length" in values_to_be_checked:
        assert len(list(document.iter("box"))) == values_to_be_checked["shapes_length"]
    current_id = values_to_be_checked["start_frame"]
    for image_elem in images:
        assert image_elem.attrib["id"] == str(current_id)
        current_id += 1


def _check_cvat_for_video_job_annotations(content, values_to_be_checked):
    document = ET.fromstring(content)
    # check meta information
    meta = document.find("meta")
    instance = list(meta)[0]
    assert instance.tag == "job"
    assert instance.find("id").text == values_to_be_checked["job_id"]
    assert instance.find("size").text == str(values_to_be_checked["job_size"])
    assert instance.find("start_frame").text == str(values_to_be_checked["start_frame"])
    assert instance.find("stop_frame").text == str(values_to_be_checked["stop_frame"])
    assert instance.find("mode").text == values_to_be_checked["mode"]
    assert len(instance.find("segments")) == 1

    # check number of annotations
    if values_to_be_checked.get("shapes_length") is not None:
        assert len(list(document.iter("track"))) == values_to_be_checked["tracks_length"]


@pytest.mark.usefixtures("restore_redis_inmem_per_function")
@pytest.mark.usefixtures("restore_db_per_class")
class TestJobDataset:

    @pytest.fixture(autouse=True)
    def setup(self, tasks):
        self.tasks = tasks

    @staticmethod
    def _test_export_dataset(
        username: str,
        jid: int,
        *,
        local_download: bool = True,
        **kwargs,
    ) -> Optional[bytes]:
        dataset = export_job_dataset(username, save_images=True, id=jid, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(dataset))
        else:
            assert dataset is None

        return dataset

    @staticmethod
    def _test_export_annotations(
        username: str, jid: int, *, local_download: bool = True, **kwargs
    ) -> Optional[bytes]:
        dataset = export_job_dataset(username, save_images=False, id=jid, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(dataset))
        else:
            assert dataset is None

        return dataset

    def test_non_admin_can_export_dataset(self, users, jobs_with_shapes):
        job, username = next(
            (
                (job, self.tasks[job["task_id"]]["owner"]["username"])
                for job in jobs_with_shapes
                if "admin" not in users[self.tasks[job["task_id"]]["owner"]["id"]]["groups"]
                and self.tasks[job["task_id"]]["target_storage"] is None
                and self.tasks[job["task_id"]]["organization"] is None
            )
        )
        self._test_export_dataset(username, job["id"])

    def test_non_admin_can_export_annotations(self, users, jobs_with_shapes):
        job, username = next(
            (
                (job, self.tasks[job["task_id"]]["owner"]["username"])
                for job in jobs_with_shapes
                if "admin" not in users[self.tasks[job["task_id"]]["owner"]["id"]]["groups"]
                and self.tasks[job["task_id"]]["target_storage"] is None
                and self.tasks[job["task_id"]]["organization"] is None
            )
        )

        self._test_export_annotations(username, job["id"])

    @pytest.mark.parametrize("username, jid", [("admin1", 14)])
    @pytest.mark.parametrize(
        "anno_format, anno_file_name, check_func",
        [
            ("COCO 1.0", "annotations/instances_default.json", _check_coco_job_annotations),
            ("CVAT for images 1.1", "annotations.xml", _check_cvat_for_images_job_annotations),
        ],
    )
    def test_exported_job_dataset_structure(
        self,
        username,
        jid,
        anno_format,
        anno_file_name,
        check_func,
        jobs,
        annotations,
    ):
        job_data = jobs[jid]
        annotations_before = annotations["job"][str(jid)]

        values_to_be_checked = {
            "task_size": self.tasks[job_data["task_id"]]["size"],
            # NOTE: data step is not stored in assets, default = 1
            "job_size": job_data["stop_frame"] - job_data["start_frame"] + 1,
            "start_frame": job_data["start_frame"],
            "stop_frame": job_data["stop_frame"],
            "shapes_length": len(annotations_before["shapes"]),
            "job_id": str(jid),
            "mode": job_data["mode"],
        }

        dataset = self._test_export_dataset(
            username,
            jid,
            format=anno_format,
        )

        with zipfile.ZipFile(BytesIO(dataset)) as zip_file:
            assert (
                len(zip_file.namelist()) == values_to_be_checked["job_size"] + 1
            )  # images + annotation file
            content = zip_file.read(anno_file_name)
        check_func(content, values_to_be_checked)

    @pytest.mark.parametrize("username", ["admin1"])
    @pytest.mark.parametrize("jid", [25, 26])
    @pytest.mark.parametrize(
        "anno_format, anno_file_name, check_func",
        [
            ("CVAT for images 1.1", "annotations.xml", _check_cvat_for_images_job_annotations),
            ("CVAT for video 1.1", "annotations.xml", _check_cvat_for_video_job_annotations),
            (
                "COCO Keypoints 1.0",
                "annotations/person_keypoints_default.json",
                _check_coco_job_annotations,
            ),
        ],
    )
    def test_export_job_among_several_jobs_in_task(
        self,
        username,
        jid,
        anno_format,
        anno_file_name,
        check_func,
        jobs,
        annotations,
    ):
        job_data = jobs[jid]
        annotations_before = annotations["job"][str(jid)]

        values_to_be_checked = {
            "task_size": self.tasks[job_data["task_id"]]["size"],
            # NOTE: data step is not stored in assets, default = 1
            "job_size": job_data["stop_frame"] - job_data["start_frame"] + 1,
            "start_frame": job_data["start_frame"],
            "stop_frame": job_data["stop_frame"],
            "job_id": str(jid),
            "tracks_length": len(annotations_before["tracks"]),
            "mode": job_data["mode"],
        }

        dataset = self._test_export_dataset(
            username,
            jid,
            format=anno_format,
        )

        with zipfile.ZipFile(BytesIO(dataset)) as zip_file:
            assert (
                len(zip_file.namelist()) == values_to_be_checked["job_size"] + 1
            )  # images + annotation file
            content = zip_file.read(anno_file_name)
        check_func(content, values_to_be_checked)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetJobPreview:
    def _test_get_job_preview_200(self, username, jid, **kwargs):
        with make_api_client(username) as client:
            (_, response) = client.jobs_api.retrieve_preview(jid, **kwargs)

            assert response.status == HTTPStatus.OK
            (width, height) = Image.open(BytesIO(response.data)).size
            assert width > 0 and height > 0

    def _test_get_job_preview_403(self, username, jid, **kwargs):
        with make_api_client(username) as client:
            (_, response) = client.jobs_api.retrieve_preview(
                jid, **kwargs, _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    def test_admin_get_sandbox_job_preview(self, jobs, tasks):
        job_id = next(job["id"] for job in jobs if not tasks[job["task_id"]]["organization"])
        self._test_get_job_preview_200("admin2", job_id)

    def test_admin_get_org_job_preview(self, jobs, tasks):
        job_id = next(job["id"] for job in jobs if tasks[job["task_id"]]["organization"])
        self._test_get_job_preview_200("admin2", job_id)

    def test_user_can_get_job_preview_in_sandbox(self, find_users, jobs, is_job_staff):
        username, job_id = next(
            (user["username"], job["id"])
            for user in find_users(privilege="user")
            for job in jobs
            if is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_preview_200(username, job_id)

    def test_user_cannot_get_job_preview_in_sandbox(self, find_users, jobs, is_job_staff):
        username, job_id = next(
            (user["username"], job["id"])
            for user in find_users(privilege="user")
            for job in jobs
            if not is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_preview_403(username, job_id)

    def test_org_staff_can_get_job_preview_in_org(
        self, organizations, users, org_staff, jobs_by_org
    ):
        username, job_id = next(
            (user["username"], jobs_by_org[org["id"]][0]["id"])
            for user in users
            for org in organizations
            if user["id"] in org_staff(org["id"])
        )
        self._test_get_job_preview_200(username, job_id)

    def test_job_staff_can_get_job_preview_in_org(
        self, organizations, users, jobs_by_org, is_job_staff
    ):
        username, job_id = next(
            (user["username"], job["id"])
            for user in users
            for org in organizations
            for job in jobs_by_org[org["id"]]
            if is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_preview_200(username, job_id)

    def test_job_staff_can_get_job_preview_in_sandbox(self, users, jobs, tasks, is_job_staff):
        username, job_id = next(
            (user["username"], job["id"])
            for user in users
            for job in jobs
            if is_job_staff(user["id"], job["id"]) and tasks[job["task_id"]]["organization"] is None
        )
        self._test_get_job_preview_200(username, job_id)

    def test_non_org_staff_non_job_staff_cannot_get_job_preview_in_org(
        self, users, organizations, jobs_by_org, is_job_staff, org_staff
    ):
        username, job_id = next(
            (user["username"], job["id"])
            for user in users
            for org in organizations
            for job in jobs_by_org[org["id"]]
            if user["id"] not in org_staff(org["id"]) and not is_job_staff(user["id"], job["id"])
        )
        self._test_get_job_preview_403(username, job_id)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetJobDataMeta:
    @pytest.mark.parametrize("org_slug", [None, "", "org"])
    def test_can_get_job_meta_with_org_slug(self, admin_user, tasks, jobs, organizations, org_slug):
        # Checks for backward compatibility with org_slug parameter
        task = next(t for t in tasks if t["organization"])
        job = next(j for j in jobs if j["task_id"] == task["id"])

        if org_slug == "org":
            org_slug = organizations[task["organization"]]["slug"]

        with make_api_client(admin_user) as client:
            client.organization_slug = org_slug
            client.jobs_api.retrieve_data_meta(job["id"])


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_labels.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import itertools
import json
from copy import deepcopy
from http import HTTPStatus
from types import SimpleNamespace
from typing import Any, Optional

import pytest
from cvat_sdk import exceptions, models
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.core.helpers import get_paginated_collection
from dateutil.parser import isoparse as parse_datetime
from deepdiff import DeepDiff
from pytest_cases import fixture, fixture_ref, parametrize

from shared.utils.config import delete_method, get_method, make_api_client, patch_method

from .utils import CollectionSimpleFilterTestBase, build_exclude_paths_expr, get_attrs


class _TestLabelsPermissionsBase:
    @pytest.fixture
    def _base_setup(
        self,
        users,
        labels,
        jobs,
        tasks,
        projects,
        is_task_staff,
        is_project_staff,
        users_by_name,
        tasks_by_org,
        projects_by_org,
        memberships,
        org_staff,
    ):
        self.users = users
        self.labels = labels
        self.jobs = jobs
        self.tasks = tasks
        self.projects = projects
        self.is_task_staff = is_task_staff
        self.is_project_staff = is_project_staff
        self.users_by_name = users_by_name
        self.tasks_by_org = tasks_by_org
        self.projects_by_org = projects_by_org
        self.memberships = memberships
        self.org_staff = org_staff

    @pytest.fixture(autouse=True)
    def setup(self, _base_setup):
        """
        This function only calls the _base_setup() fixture.
        It can be overridden in derived classes.
        """

    @staticmethod
    def _labels_by_source(labels: list[dict], *, source_key: str) -> dict[int, list[dict]]:
        labels_by_source = {}
        for label in labels:
            label_source = label.get(source_key)
            if label_source:
                labels_by_source.setdefault(label_source, []).append(label)

        return labels_by_source

    def _get_source_info(self, source: str, *, org_id: Optional[int] = None):
        if source == "task":
            sources = self.tasks_by_org
            is_source_staff = self.is_task_staff
            label_source_key = "task_id"
        elif source == "project":
            sources = self.projects_by_org
            is_source_staff = self.is_project_staff
            label_source_key = "project_id"
        else:
            assert False

        sources = sources[org_id or ""]

        return SimpleNamespace(
            sources=sources, is_source_staff=is_source_staff, label_source_key=label_source_key
        )

    source_types = ["task", "project"]
    org_roles = ["worker", "supervisor", "maintainer", "owner"]

    @fixture
    @parametrize("source", source_types)
    @parametrize("user", ["admin1"])
    @parametrize("is_staff", [True, False])
    def admin_sandbox_case(self, user, source, is_staff):
        sources, is_source_staff, label_source_key = get_attrs(
            self._get_source_info(source),
            ["sources", "is_source_staff", "label_source_key"],
        )

        labels_by_source = self._labels_by_source(self.labels, source_key=label_source_key)
        sources_with_labels = [s for s in sources if labels_by_source.get(s["id"])]

        user_id = self.users_by_name[user]["id"]
        source_obj = next(
            filter(lambda s: is_source_staff(user_id, s["id"]) == is_staff, sources_with_labels)
        )
        label = labels_by_source[source_obj["id"]][0]

        yield SimpleNamespace(label=label, user=user, source=source, is_staff=is_staff)

    @fixture
    @parametrize("source", source_types)
    @parametrize("org_id", [2])
    @parametrize("user", ["admin2"])
    @parametrize("is_staff", [False])
    def admin_org_case(self, user, source, org_id, is_staff):
        sources, is_source_staff, label_source_key = get_attrs(
            self._get_source_info(source, org_id=org_id),
            ["sources", "is_source_staff", "label_source_key"],
        )

        labels_by_source = self._labels_by_source(self.labels, source_key=label_source_key)
        sources_with_labels = [s for s in sources if labels_by_source.get(s["id"])]

        user_id = self.users_by_name[user]["id"]
        source_obj = next(
            filter(lambda s: is_source_staff(user_id, s["id"]) == is_staff, sources_with_labels)
        )
        label = labels_by_source[source_obj["id"]][0]

        yield SimpleNamespace(
            label=label, user=user, source=source, org_id=org_id, is_staff=is_staff
        )

    @fixture
    @parametrize("source", source_types)
    @parametrize("is_staff", [True, False])
    def user_sandbox_case(self, source, is_staff):
        sources, label_source_key = get_attrs(
            self._get_source_info(source),
            ["sources", "label_source_key"],
        )

        users = {u["id"]: u for u in self.users if not u["is_superuser"]}
        regular_users_sources = [
            s for s in sources if s["owner"]["id"] in users and s["organization"] is None
        ]
        labels_by_source = self._labels_by_source(self.labels, source_key=label_source_key)
        source_obj = next(s for s in regular_users_sources if labels_by_source.get(s["id"]))
        label = labels_by_source[source_obj["id"]][0]
        user = next(u for u in users.values() if (u["id"] == source_obj["owner"]["id"]) == is_staff)

        yield SimpleNamespace(label=label, user=user, is_staff=is_staff)

    @fixture
    @parametrize("source", source_types)
    @parametrize("org_id", [2])
    @parametrize(
        "role, src_staff", list(itertools.product(org_roles, [True, False])) + [(None, False)]
    )
    def user_org_case(self, source, src_staff, role, org_id):
        sources, is_source_staff, label_source_key = get_attrs(
            self._get_source_info(source, org_id=org_id),
            ["sources", "is_source_staff", "label_source_key"],
        )

        labels_by_source = self._labels_by_source(self.labels, source_key=label_source_key)

        users = {u["id"]: u for u in self.users_by_name.values() if not u["is_superuser"]}

        staff_by_role = {}
        for m in self.memberships:
            if m["organization"] == org_id:
                staff_by_role.setdefault(m["role"], []).append(
                    self.users_by_name[m["user"]["username"]]
                )

        for source_obj in (s for s in sources if labels_by_source.get(s["id"])):
            user = next(
                (
                    u
                    for u in users.values()
                    if is_source_staff(u["id"], source_obj["id"]) == src_staff
                    or not role
                    or u["id"] in self.org_staff(org_id)
                    if not role or u in staff_by_role[role]
                ),
                None,
            )
            if user:
                break
        assert user

        label = labels_by_source[source_obj["id"]][0]

        yield SimpleNamespace(
            label=label,
            user=user,
            org_id=org_id,
            is_staff=src_staff or user["id"] in self.org_staff(org_id),
        )


class TestLabelsListFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, labels, jobs_wlc, tasks_wlc, projects_wlc):
        self.user = admin_user
        self.samples = labels
        self.job_samples = jobs_wlc
        self.task_samples = tasks_wlc
        self.project_samples = projects_wlc

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.labels_api.list_endpoint

    def _get_field_samples(self, field: str) -> tuple[Any, list[dict[str, Any]]]:
        if field == "parent":
            parent_id, gt_objects = self._get_field_samples("parent_id")
            parent_name = self._get_field(
                next(
                    filter(
                        lambda p: parent_id == self._get_field(p, self._map_field("id")),
                        self.samples,
                    )
                ),
                self._map_field("name"),
            )
            return parent_name, gt_objects
        elif field == "job_id":
            field_path = ["id"]
            field_value = self._find_valid_field_value(self.job_samples, field_path)
            job_sample = next(
                filter(lambda p: field_value == self._get_field(p, field_path), self.job_samples)
            )

            task_id = job_sample["task_id"]
            project_id = job_sample["project_id"]
            label_samples = filter(
                lambda p: (task_id and task_id == p.get("task_id"))
                or (project_id and project_id == p.get("project_id")),
                self.samples,
            )
            return field_value, label_samples
        else:
            return super()._get_field_samples(field)

    @pytest.mark.parametrize(
        "field",
        ("name", "job_id", "task_id", "project_id", "type", "color"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)

    @pytest.mark.parametrize(
        "key1, key2", itertools.combinations(["job_id", "task_id", "project_id"], 2)
    )
    def test_cant_mix_job_task_project_filters(self, key1, key2):
        kwargs = {}
        for key in [key1, key2]:
            if key == "job_id":
                v = self._find_valid_field_value(self.job_samples, ["id"])
            elif key == "task_id":
                v = self._find_valid_field_value(self.task_samples, ["id"])
            elif key == "project_id":
                v = self._find_valid_field_value(self.project_samples, ["id"])
            else:
                assert False

            kwargs[key] = v

        with pytest.raises(exceptions.ApiException) as capture:
            self._retrieve_collection(**kwargs)

        assert capture.value.status == 400
        assert "cannot be used together" in capture.value.body

    @pytest.mark.parametrize("org_id", [None, 2])
    @pytest.mark.parametrize("dst, src", itertools.combinations(["job", "task", "project"], 2))
    def test_can_list_inherited_labels(self, org_id, dst, src):
        kwargs = {}

        if src == "project":
            src_with_labels = next(
                p
                for p in self.project_samples
                if p["labels"]["count"] > 0
                and p["organization"] == org_id
                and p["tasks"]["count"] > 0
            )
        elif src == "task":
            src_with_labels = next(
                t
                for t in self.task_samples
                if t["labels"]["count"] > 0
                and t["organization"] == org_id
                and t["jobs"]["count"] > 0
                and not t.get("project_id")
            )
        else:
            assert False

        labels = [l for l in self.samples if l.get(f"{src}_id") == src_with_labels["id"]]

        if dst == "task":
            dst_obj = next(
                t for t in self.task_samples if t.get(f"{src}_id") == src_with_labels["id"]
            )
            kwargs["task_id"] = dst_obj["id"]
        elif dst == "job":
            dst_obj = next(
                j for j in self.job_samples if j.get(f"{src}_id") == src_with_labels["id"]
            )
            kwargs["job_id"] = dst_obj["id"]
        else:
            assert False

        if org_id:
            kwargs["org_id"] = org_id

        retrieved_data = self._retrieve_collection(**kwargs)
        self._compare_results(labels, retrieved_data)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListLabels(_TestLabelsPermissionsBase):
    def _test_list_ok(self, user, data, **kwargs):
        with make_api_client(user) as client:
            results = get_paginated_collection(
                client.labels_api.list_endpoint, **kwargs, return_json=True
            )
            assert (
                DeepDiff(
                    data,
                    results,
                    exclude_paths="root['updated_date']",
                    ignore_order=True,
                )
                == {}
            )

    def _test_list_denied(self, user, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.list(
                **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize("source_type", ["job", "task", "project"])
    @pytest.mark.parametrize("role", ["worker", "supervisor"])
    @pytest.mark.parametrize("staff", [True, False])
    def test_staff_can_list_labels_in_org(
        self,
        org_id,
        source_type,
        role,
        staff,
        labels,
        jobs_wlc,
        tasks_wlc,
        projects_wlc,
        users,
        is_project_staff,
        is_task_staff,
        is_job_staff,
        memberships,
        users_by_name,
    ):
        labels_by_project = self._labels_by_source(labels, source_key="project_id")
        labels_by_task = self._labels_by_source(labels, source_key="task_id")
        if source_type == "project":
            sources = [
                p for p in projects_wlc if p["labels"]["count"] > 0 and p["organization"] == org_id
            ]
            labels_by_source = labels_by_project
            is_staff = is_project_staff
        elif source_type == "task":
            sources = [
                t for t in tasks_wlc if t["labels"]["count"] > 0 and t["organization"] == org_id
            ]
            labels_by_source = {
                task["id"]: (
                    labels_by_task.get(task["id"]) or labels_by_project.get(task.get("project_id"))
                )
                for task in sources
            }
            is_staff = is_task_staff
        elif source_type == "job":
            sources = [
                j
                for j in jobs_wlc
                if j["labels"]["count"] > 0
                if next(t for t in tasks_wlc if t["id"] == j["task_id"])["organization"] == org_id
            ]
            labels_by_source = {
                job["id"]: (
                    labels_by_task.get(job["task_id"]) or labels_by_project.get(job["project_id"])
                )
                for job in sources
            }
            is_staff = is_job_staff
        else:
            assert False

        staff_by_role = {}
        for m in memberships:
            if m["organization"] == org_id:
                staff_by_role.setdefault(m["role"], []).append(users_by_name[m["user"]["username"]])

        for source in sources:
            user = next(
                (
                    u
                    for u in users
                    if not u["is_superuser"]
                    if is_staff(u["id"], source["id"]) == staff
                    if u in staff_by_role[role]
                ),
                None,
            )
            if user:
                break

        assert source
        assert user

        labels = labels_by_source[source["id"]]

        kwargs = {
            "org_id": org_id,
            f"{source_type}_id": source["id"],
        }

        if staff:
            self._test_list_ok(user["username"], labels, **kwargs)
        else:
            self._test_list_denied(user["username"], **kwargs)

    @pytest.mark.parametrize("org_id", [2])
    @pytest.mark.parametrize("source_type", ["job", "task", "project"])
    def test_only_1st_level_labels_included(
        self, projects_wlc, tasks_wlc, jobs_wlc, labels, admin_user, source_type, org_id
    ):
        labels_by_project = self._labels_by_source(labels, source_key="project_id")
        labels_by_task = self._labels_by_source(labels, source_key="task_id")
        if source_type == "project":
            sources = [
                p for p in projects_wlc if p["labels"]["count"] > 0 and p["organization"] == org_id
            ]
            labels_by_source = labels_by_project
        elif source_type == "task":
            sources = [
                t for t in tasks_wlc if t["labels"]["count"] > 0 and t["organization"] == org_id
            ]
            labels_by_source = {
                task["id"]: (
                    labels_by_task.get(task["id"]) or labels_by_project.get(task.get("project_id"))
                )
                for task in sources
            }
        elif source_type == "job":
            sources = [
                j
                for j in jobs_wlc
                if j["labels"]["count"] > 0
                if next(t for t in tasks_wlc if t["id"] == j["task_id"])["organization"] == org_id
            ]
            labels_by_source = {
                job["id"]: (
                    labels_by_task.get(job["task_id"]) or labels_by_project.get(job["project_id"])
                )
                for job in sources
            }
        else:
            assert False

        source = next(
            s for s in sources if any(label["sublabels"] for label in labels_by_source[s["id"]])
        )
        source_labels = labels_by_source[source["id"]]
        assert not any(label["has_parent"] for label in source_labels)

        kwargs = {
            "org_id": org_id,
            f"{source_type}_id": source["id"],
        }

        self._test_list_ok(admin_user, source_labels, **kwargs)


class TestGetLabels(_TestLabelsPermissionsBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, _base_setup):  # pylint: disable=arguments-differ
        pass

    def _test_get_ok(self, user, lid, data):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.retrieve(lid)
            assert response.status == HTTPStatus.OK
            assert (
                DeepDiff(
                    data,
                    json.loads(response.data),
                    exclude_paths="root['updated_date']",
                    ignore_order=True,
                )
                == {}
            )

    def _test_get_denied(self, user, lid):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.retrieve(
                lid, _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    def test_admin_get_sandbox_label(self, admin_sandbox_case):
        label, user = get_attrs(admin_sandbox_case, ["label", "user"])

        self._test_get_ok(user, label["id"], label)

    def test_admin_get_org_label(self, admin_org_case):
        label, user = get_attrs(admin_org_case, ["label", "user"])

        self._test_get_ok(user, label["id"], label)

    def test_regular_user_get_sandbox_label(self, user_sandbox_case):
        label, user, is_staff = get_attrs(user_sandbox_case, ["label", "user", "is_staff"])

        if is_staff:
            self._test_get_ok(user["username"], label["id"], label)
        else:
            self._test_get_denied(user["username"], label["id"])

    def test_regular_user_get_org_label(self, user_org_case):
        label, user, is_staff = get_attrs(user_org_case, ["label", "user", "is_staff"])

        if is_staff:
            self._test_get_ok(user["username"], label["id"], label)
        else:
            self._test_get_denied(user["username"], label["id"])


class TestPatchLabels(_TestLabelsPermissionsBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_function, _base_setup):  # pylint: disable=arguments-differ
        self.ignore_fields = ["updated_date"]

    def _build_exclude_paths_expr(self, ignore_fields=None):
        if ignore_fields is None:
            ignore_fields = self.ignore_fields
        return build_exclude_paths_expr(ignore_fields)

    def _test_update_ok(self, user, lid, data, *, expected_data=None, ignore_fields=None, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.partial_update(
                lid, patched_label_request=models.PatchedLabelRequest(**deepcopy(data)), **kwargs
            )
            assert response.status == HTTPStatus.OK
            assert (
                DeepDiff(
                    expected_data if expected_data is not None else data,
                    json.loads(response.data),
                    exclude_regex_paths=self._build_exclude_paths_expr(ignore_fields),
                    ignore_order=True,
                )
                == {}
            )
        return response

    def _test_update_denied(self, user, lid, data, expected_status=HTTPStatus.FORBIDDEN, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.partial_update(
                lid,
                patched_label_request=models.PatchedLabelRequest(**deepcopy(data)),
                **kwargs,
                _check_status=False,
                _parse_response=False,
            )
            assert response.status == expected_status
        return response

    def _get_patch_data(
        self, original_data: dict[str, Any], **overrides
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        result = deepcopy(original_data)
        result.update(overrides)

        ignore_fields = self.ignore_fields.copy()
        if overrides:
            payload = deepcopy(overrides)

            if overridden_attributes := deepcopy(overrides.get("attributes", [])):
                combined_attributes = deepcopy(original_data.get("attributes", []))

                mapping = {attr["id"]: attr for attr in overridden_attributes if "id" in attr}

                # no attributes to update
                if not mapping:
                    ignore_fields.append("attributes.id")

                for attr in combined_attributes:
                    if attr["id"] in mapping:
                        attr.update(mapping[attr["id"]])

                for attr in overridden_attributes:
                    if attr not in combined_attributes:
                        combined_attributes.append(attr)

                payload["attributes"] = deepcopy(combined_attributes)
                result["attributes"] = deepcopy(combined_attributes)

            # Changing skeletons is not supported
            if overrides.get("type") == "skeleton":
                result["type"] = original_data["type"]

            if "name" in overrides:
                ignore_fields.append("color")
        else:
            payload = deepcopy(original_data)

        return result, payload, ignore_fields

    @parametrize(
        "param, newvalue",
        list(
            itertools.chain.from_iterable(
                itertools.product([k], values)
                for k, values in {
                    "attributes": [
                        [
                            {
                                "default_value": "mazda_new",
                                "input_type": "select",
                                "mutable": True,
                                "name": "model_new",
                                "values": ["mazda_new", "bmw"],
                            }
                        ],
                    ],
                    "color": ["#2000c0"],
                    "name": ["modified"],
                    "type": [
                        "any",
                        "cuboid",
                        "ellipse",
                        "mask",
                        "points",
                        "polygon",
                        "polyline",
                        "rectangle",
                        "skeleton",
                        "tag",
                    ],
                }.items()
            )
        ),
    )
    @parametrize("source", _TestLabelsPermissionsBase.source_types)
    def test_can_patch_label_field(self, source, admin_user, param, newvalue):
        user = admin_user
        label = next(
            iter(
                self._labels_by_source(
                    self.labels, source_key=self._get_source_info(source).label_source_key
                ).values()
            )
        )[0]

        expected_data, patch_data, ignore_fields = self._get_patch_data(label, **{param: newvalue})

        self._test_update_ok(
            user, label["id"], patch_data, expected_data=expected_data, ignore_fields=ignore_fields
        )

    @parametrize("source", _TestLabelsPermissionsBase.source_types)
    def test_can_patch_attribute_name(self, source: str, admin_user: str):
        source_key = self._get_source_info(source).label_source_key
        label = next(
            (
                l
                for l in self.labels
                if l.get(source_key) and not l["has_parent"] and l.get("attributes")
            )
        )

        attributes = deepcopy(label["attributes"])

        for attribute in attributes:
            attribute["name"] += "_updated"

        expected_data, patch_data, ignore_fields = self._get_patch_data(
            label, attributes=attributes
        )

        self._test_update_ok(
            admin_user,
            label["id"],
            patch_data,
            expected_data=expected_data,
            ignore_fields=ignore_fields,
        )

    @parametrize("source", _TestLabelsPermissionsBase.source_types)
    def test_cannot_patch_sublabel_directly(self, admin_user, source):
        user = admin_user
        label = next(
            sublabel
            for source_labels in self._labels_by_source(
                self.labels, source_key=self._get_source_info(source).label_source_key
            ).values()
            for label in source_labels
            for sublabel in label["sublabels"]
        )

        with make_api_client(user) as client:
            (_, response) = client.labels_api.partial_update(
                label["id"],
                patched_label_request=models.PatchedLabelRequest(**label),
                _parse_response=False,
                _check_status=False,
            )

        assert response.status == HTTPStatus.BAD_REQUEST
        assert "Sublabels cannot be modified this way." in response.data.decode()

    @parametrize("user", [fixture_ref("admin_user")])
    @parametrize("source_type", _TestLabelsPermissionsBase.source_types)
    def test_cannot_rename_label_to_duplicate_name(self, source_type, user):
        source_info = self._get_source_info(source_type)
        labels_by_source = self._labels_by_source(
            self.labels, source_key=source_info.label_source_key
        )
        labels = next(ls for ls in labels_by_source.values() if len(ls) >= 2)

        payload = {"name": labels[1]["name"]}

        response = self._test_update_denied(
            user, lid=labels[0]["id"], data=payload, expected_status=HTTPStatus.BAD_REQUEST
        )
        assert "All label names must be unique" in response.data.decode()

    def test_admin_patch_sandbox_label(self, admin_sandbox_case):
        label, user = get_attrs(admin_sandbox_case, ["label", "user"])

        expected_data, patch_data, *_ = self._get_patch_data(label)

        self._test_update_ok(user, label["id"], patch_data, expected_data=expected_data)

    def test_admin_patch_org_label(self, admin_org_case):
        label, user = get_attrs(admin_org_case, ["label", "user"])

        expected_data, patch_data, *_ = self._get_patch_data(label)

        self._test_update_ok(user, label["id"], patch_data, expected_data=expected_data)

    def test_regular_user_patch_sandbox_label(self, user_sandbox_case):
        label, user, is_staff = get_attrs(user_sandbox_case, ["label", "user", "is_staff"])

        expected_data, patch_data, *_ = self._get_patch_data(label)

        if is_staff:
            self._test_update_ok(
                user["username"], label["id"], patch_data, expected_data=expected_data
            )
        else:
            self._test_update_denied(user["username"], label["id"], patch_data)

    def test_regular_user_patch_org_label(self, user_org_case):
        label, user, is_staff = get_attrs(user_org_case, ["label", "user", "is_staff"])

        expected_data, patch_data, *_ = self._get_patch_data(label)

        if is_staff:
            self._test_update_ok(
                user["username"], label["id"], patch_data, expected_data=expected_data
            )
        else:
            self._test_update_denied(user["username"], label["id"], patch_data)


class TestDeleteLabels(_TestLabelsPermissionsBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_function, _base_setup):  # pylint: disable=arguments-differ
        pass

    def _test_delete_ok(self, user, lid, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.destroy(lid, **kwargs)
            assert response.status == HTTPStatus.NO_CONTENT

    def _test_delete_denied(self, user, lid, **kwargs):
        with make_api_client(user) as client:
            (_, response) = client.labels_api.partial_update(
                lid,
                **kwargs,
                _check_status=False,
                _parse_response=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

    @parametrize("source", _TestLabelsPermissionsBase.source_types)
    def test_can_delete_label(self, admin_user, source):
        user = admin_user
        label = next(
            iter(
                self._labels_by_source(
                    self.labels, source_key=self._get_source_info(source).label_source_key
                ).values()
            )
        )[0]

        with make_api_client(user) as client:
            (_, response) = client.labels_api.destroy(label["id"])
            assert response.status == HTTPStatus.NO_CONTENT

            (_, response) = client.labels_api.retrieve(
                label["id"], _check_status=False, _parse_response=False
            )
            assert response.status == HTTPStatus.NOT_FOUND

    @parametrize("source", _TestLabelsPermissionsBase.source_types)
    def test_cannot_delete_sublabel_directly(self, admin_user, source):
        user = admin_user
        label = next(
            sublabel
            for source_labels in self._labels_by_source(
                self.labels, source_key=self._get_source_info(source).label_source_key
            ).values()
            for label in source_labels
            for sublabel in label["sublabels"]
        )

        with make_api_client(user) as client:
            (_, response) = client.labels_api.destroy(label["id"], _check_status=False)

        assert response.status == HTTPStatus.BAD_REQUEST
        assert "Sublabels cannot be deleted this way." in response.data.decode()

    def test_admin_delete_sandbox_label(self, admin_sandbox_case):
        label, user = get_attrs(admin_sandbox_case, ["label", "user"])

        self._test_delete_ok(user, label["id"])

    def test_admin_delete_org_label(self, admin_org_case):
        label, user = get_attrs(admin_org_case, ["label", "user"])

        self._test_delete_ok(user, label["id"])

    def test_regular_user_delete_sandbox_label(self, user_sandbox_case):
        label, user, is_staff = get_attrs(user_sandbox_case, ["label", "user", "is_staff"])

        if is_staff:
            self._test_delete_ok(user["username"], label["id"])
        else:
            self._test_delete_denied(user["username"], label["id"])

    def test_regular_user_delete_org_label(self, user_org_case):
        label, user, is_staff = get_attrs(user_org_case, ["label", "user", "is_staff"])

        if is_staff:
            self._test_delete_ok(user["username"], label["id"])
        else:
            self._test_delete_denied(user["username"], label["id"])


@pytest.mark.usefixtures("restore_db_per_function")
class TestLabelUpdates:
    @pytest.mark.parametrize("update_kind", ["addition", "removal", "modification"])
    def test_project_label_update_triggers_nested_task_and_job_update(
        self, update_kind, admin_user, labels, projects_wlc, tasks, jobs
    ):
        # Checks for regressions against the issue https://github.com/cvat-ai/cvat/issues/6871

        project = next(p for p in projects_wlc if p["tasks"]["count"] and p["labels"]["count"])
        project_labels = [l for l in labels if l.get("project_id") == project["id"]]
        nested_tasks = [t for t in tasks if t["project_id"] == project["id"]]
        nested_task_ids = set(t["id"] for t in nested_tasks)
        nested_jobs = [j for j in jobs if j["task_id"] in nested_task_ids]

        if update_kind == "addition":
            response = patch_method(
                admin_user, f'projects/{project["id"]}', {"labels": [{"name": "dog2"}]}
            )
            updated_project = response.json()
        elif update_kind == "modification":
            label = project_labels[0]
            patch_method(admin_user, f'labels/{label["id"]}', {"name": label["name"] + "-updated"})

            response = get_method(admin_user, f'projects/{project["id"]}')
            updated_project = response.json()
        elif update_kind == "removal":
            label = project_labels[0]
            delete_method(admin_user, f'labels/{label["id"]}')

            response = get_method(admin_user, f'projects/{project["id"]}')
            updated_project = response.json()
        else:
            assert False

        with make_api_client(admin_user) as api_client:
            updated_tasks = get_paginated_collection(
                api_client.tasks_api.list_endpoint, project_id=project["id"], return_json=True
            )

            updated_jobs = [
                j
                for j in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, return_json=True
                )
                if j["task_id"] in nested_task_ids
            ]

        assert parse_datetime(project["updated_date"]) < parse_datetime(
            updated_project["updated_date"]
        )
        assert len(updated_tasks) == len(nested_tasks)
        assert len(updated_jobs) == len(nested_jobs)
        for entity in updated_tasks + updated_jobs:
            assert updated_project["updated_date"] == entity["updated_date"]

    @pytest.mark.parametrize("update_kind", ["addition", "removal", "modification"])
    def test_task_label_update_triggers_nested_task_and_job_update(
        self, update_kind, admin_user, labels, tasks_wlc, jobs
    ):
        # Checks for regressions against the issue https://github.com/cvat-ai/cvat/issues/6871

        task = next(
            t
            for t in tasks_wlc
            if t["jobs"]["count"] and t["labels"]["count"] and not t["project_id"]
        )
        task_labels = [l for l in labels if l.get("task_id") == task["id"]]
        nested_jobs = [j for j in jobs if j["task_id"] == task["id"]]

        if update_kind == "addition":
            response = patch_method(
                admin_user, f'tasks/{task["id"]}', {"labels": [{"name": "dog2"}]}
            )
            updated_task = response.json()
        elif update_kind == "modification":
            label = task_labels[0]
            patch_method(admin_user, f'labels/{label["id"]}', {"name": label["name"] + "-updated"})

            response = get_method(admin_user, f'tasks/{task["id"]}')
            updated_task = response.json()
        elif update_kind == "removal":
            label = task_labels[0]
            delete_method(admin_user, f'labels/{label["id"]}')

            response = get_method(admin_user, f'tasks/{task["id"]}')
            updated_task = response.json()
        else:
            assert False

        with make_api_client(admin_user) as api_client:
            updated_jobs = get_paginated_collection(
                api_client.jobs_api.list_endpoint, task_id=task["id"], return_json=True
            )

        assert parse_datetime(task["updated_date"]) < parse_datetime(updated_task["updated_date"])
        assert len(updated_jobs) == len(nested_jobs)
        for job in updated_jobs:
            assert updated_task["updated_date"] == job["updated_date"]


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_memberships.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from http import HTTPStatus
from typing import ClassVar

import pytest
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.api_client.exceptions import ForbiddenException
from deepdiff import DeepDiff

from shared.utils.config import get_method, make_api_client, patch_method

from .utils import CollectionSimpleFilterTestBase


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetMemberships:
    def _test_can_see_memberships(self, user, data, **kwargs):
        response = get_method(user, "memberships", **kwargs)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(data, response.json()["results"]) == {}

    def _test_cannot_see_memberships(self, user, **kwargs):
        response = get_method(user, "memberships", **kwargs)

        assert response.status_code == HTTPStatus.FORBIDDEN

    def test_admin_can_see_all_memberships(self, memberships):
        self._test_can_see_memberships("admin2", memberships.raw, page_size="all")

    @pytest.mark.parametrize("field_value, query_value", [(1, 1), (None, "")])
    def test_can_filter_by_org_id(self, field_value, query_value, memberships):
        memberships = filter(lambda m: m["organization"] == field_value, memberships)
        self._test_can_see_memberships(
            "admin2", list(memberships), page_size="all", org_id=query_value
        )

    def test_non_admin_can_see_only_self_memberships(self, memberships):
        non_admins = ["user1", "dummy1", "worker2"]
        for username in non_admins:
            data = [obj for obj in memberships if obj["user"]["username"] == username]
            self._test_can_see_memberships(username, data)

    def test_all_members_can_see_other_members_membership(self, memberships):
        data = [obj for obj in memberships if obj["organization"] == 1]
        for membership in data:
            self._test_can_see_memberships(membership["user"]["username"], data, org_id=1)

    def test_non_members_cannot_see_members_membership(self):
        non_org1_users = ["user2", "worker3"]
        for user in non_org1_users:
            self._test_cannot_see_memberships(user, org_id=1)


class TestMembershipsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "user": ["user", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, memberships):
        self.user = admin_user
        self.samples = memberships

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.memberships_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("role", "user"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchMemberships:
    _ORG: ClassVar[int] = 1
    ROLES: ClassVar[list[str]] = ["worker", "supervisor", "maintainer", "owner"]

    def _test_can_change_membership(self, user, membership_id, new_role):
        response = patch_method(
            user, f"memberships/{membership_id}", {"role": new_role}, org_id=self._ORG
        )

        assert response.status_code == HTTPStatus.OK
        assert response.json()["role"] == new_role

    def _test_cannot_change_membership(self, user, membership_id, new_role):
        response = patch_method(
            user, f"memberships/{membership_id}", {"role": new_role}, org_id=self._ORG
        )

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize(
        "who, whom, new_role, is_allow",
        [
            ("worker", "worker", "supervisor", False),
            ("worker", "supervisor", "worker", False),
            ("worker", "maintainer", "worker", False),
            ("worker", "owner", "worker", False),
            ("supervisor", "worker", "supervisor", False),
            ("supervisor", "supervisor", "worker", False),
            ("supervisor", "maintainer", "supervisor", False),
            ("supervisor", "owner", "worker", False),
            ("maintainer", "maintainer", "worker", False),
            ("maintainer", "owner", "worker", False),
            ("maintainer", "supervisor", "worker", True),
            ("maintainer", "worker", "supervisor", True),
            ("owner", "maintainer", "worker", True),
            ("owner", "supervisor", "worker", True),
            ("owner", "worker", "supervisor", True),
        ],
    )
    def test_user_can_change_role_of_member(self, who, whom, new_role, is_allow, find_users):
        user = find_users(org=self._ORG, role=who)[0]["username"]
        membership_id = find_users(org=self._ORG, role=whom, exclude_username=user)[0][
            "membership_id"
        ]

        if is_allow:
            self._test_can_change_membership(user, membership_id, new_role)
        else:
            self._test_cannot_change_membership(user, membership_id, new_role)

    @pytest.mark.parametrize(
        "who",
        ROLES,
    )
    def test_user_cannot_change_self_role(self, who: str, find_users):
        user = find_users(org=self._ORG, role=who)[0]
        self._test_cannot_change_membership(
            user["username"], user["membership_id"], self.ROLES[abs(self.ROLES.index(who) - 1)]
        )

    def test_malefactor_cannot_obtain_membership_details_via_empty_partial_update_request(
        self, regular_lonely_user, memberships
    ):
        membership = next(iter(memberships))

        with make_api_client(regular_lonely_user) as api_client:
            with pytest.raises(ForbiddenException):
                api_client.memberships_api.partial_update(membership["id"])

    def test_user_cannot_update_unknown_field(self, admin_user, memberships):
        membership = next(iter(memberships))

        response = patch_method(
            admin_user, f"memberships/{membership['id']}", {"foo": "bar"}, org_id=self._ORG
        )

        assert response.status_code == HTTPStatus.FORBIDDEN


@pytest.mark.usefixtures("restore_db_per_function")
class TestDeleteMemberships:
    _ORG: ClassVar[int] = 1

    def _test_delete_membership(
        self,
        who: str,
        membership_id: int,
        is_allow: bool,
    ) -> None:
        expected_status = HTTPStatus.NO_CONTENT if is_allow else HTTPStatus.FORBIDDEN

        with make_api_client(who) as api_client:
            (_, response) = api_client.memberships_api.destroy(membership_id, _check_status=False)
            assert response.status == expected_status

    @pytest.mark.parametrize(
        "who, is_allow",
        [
            ("worker", True),
            ("supervisor", True),
            ("maintainer", True),
            ("owner", False),
        ],
    )
    def test_member_can_leave_organization(self, who, is_allow, find_users):
        user = find_users(role=who, org=self._ORG)[0]

        self._test_delete_membership(user["username"], user["membership_id"], is_allow)

    @pytest.mark.parametrize(
        "who, whom, is_allow",
        [
            ("worker", "worker", False),
            ("worker", "supervisor", False),
            ("worker", "maintainer", False),
            ("worker", "owner", False),
            ("supervisor", "worker", False),
            ("supervisor", "supervisor", False),
            ("supervisor", "maintainer", False),
            ("supervisor", "owner", False),
            ("maintainer", "worker", True),
            ("maintainer", "supervisor", True),
            ("maintainer", "maintainer", False),
            ("maintainer", "owner", False),
            ("owner", "worker", True),
            ("owner", "supervisor", True),
            ("owner", "maintainer", True),
        ],
    )
    def test_member_can_exclude_another_member(
        self,
        who: str,
        whom: str,
        is_allow: bool,
        find_users,
    ):
        user = find_users(role=who, org=self._ORG)[0]["username"]
        membership_id = find_users(role=whom, org=self._ORG, exclude_username=user)[0][
            "membership_id"
        ]
        self._test_delete_membership(user, membership_id, is_allow)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_organizations.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from copy import deepcopy
from http import HTTPStatus
from typing import Optional

import pytest
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from deepdiff import DeepDiff

from shared.utils.config import (
    delete_method,
    get_method,
    make_api_client,
    options_method,
    patch_method,
)

from .utils import CollectionSimpleFilterTestBase


class TestMetadataOrganizations:
    _ORG = 2

    @pytest.mark.parametrize(
        "privilege, role, is_member",
        [
            ("admin", None, None),
            ("user", None, False),
            ("worker", None, False),
            (None, "owner", True),
            (None, "maintainer", True),
            (None, "worker", True),
            (None, "supervisor", True),
        ],
    )
    def test_can_send_options_request(
        self,
        privilege: Optional[str],
        role: Optional[str],
        is_member: Optional[bool],
        find_users,
        organizations,
    ):
        exclude_org = None if is_member else self._ORG
        org = self._ORG if is_member else None

        filters = {}

        for key, value in {
            "privilege": privilege,
            "role": role,
            "org": org,
            "exclude_org": exclude_org,
        }.items():
            if value is not None:
                filters[key] = value

        user = find_users(**filters)[0]["username"]

        response = options_method(user, f"organizations")
        assert response.status_code == HTTPStatus.OK

        response = options_method(user, f"organizations/{self._ORG}")
        assert response.status_code == HTTPStatus.OK


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetOrganizations:
    _ORG = 2

    @pytest.mark.parametrize(
        "privilege, role, is_member, is_allow",
        [
            ("admin", None, None, True),
            ("user", None, False, False),
            ("worker", None, False, False),
            (None, "owner", True, True),
            (None, "maintainer", True, True),
            (None, "worker", True, True),
            (None, "supervisor", True, True),
        ],
    )
    def test_can_see_specific_organization(
        self,
        privilege: Optional[str],
        role: Optional[str],
        is_member: Optional[bool],
        is_allow: bool,
        find_users,
        organizations,
    ):
        exclude_org = None if is_member else self._ORG
        org = self._ORG if is_member else None

        filters = {}

        for key, value in {
            "privilege": privilege,
            "role": role,
            "org": org,
            "exclude_org": exclude_org,
        }.items():
            if value is not None:
                filters[key] = value

        user = find_users(**filters)[0]["username"]

        response = get_method(user, f"organizations/{self._ORG}")
        if is_allow:
            assert response.status_code == HTTPStatus.OK
            assert DeepDiff(organizations[self._ORG], response.json()) == {}
        else:
            assert response.status_code == HTTPStatus.NOT_FOUND

    def test_can_remove_owner_and_fetch_with_sdk(self, admin_user, organizations):
        # test for API schema regressions
        source_org = next(
            org
            for org in organizations
            if org.get("owner") and org["owner"]["username"] != admin_user
        ).copy()

        with make_api_client(admin_user) as api_client:
            api_client.users_api.destroy(source_org["owner"]["id"])

            (_, response) = api_client.organizations_api.retrieve(source_org["id"])
            fetched_org = json.loads(response.data)

        source_org["owner"] = None
        assert DeepDiff(source_org, fetched_org, ignore_order=True) == {}


class TestOrganizationsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, organizations):
        self.user = admin_user
        self.samples = organizations

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.organizations_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("name", "owner", "slug"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchOrganizations:
    _ORG = 2

    @pytest.fixture(scope="class")
    def request_data(self):
        return {
            "slug": "new",
            "name": "new",
            "description": "new",
            "contact": {"email": "new@cvat.org"},
        }

    @pytest.fixture(scope="class")
    def expected_data(self, organizations, request_data):
        data = deepcopy(organizations[self._ORG])
        data.update(request_data)
        return data

    @pytest.mark.parametrize(
        "privilege, role, is_member, is_allow",
        [
            ("admin", None, None, True),
            ("user", None, False, False),
            ("worker", None, False, False),
            (None, "owner", True, True),
            (None, "maintainer", True, True),
            (None, "worker", True, False),
            (None, "supervisor", True, False),
        ],
    )
    def test_can_update_specific_organization(
        self,
        privilege: Optional[str],
        role: Optional[str],
        is_member: Optional[bool],
        is_allow: bool,
        find_users,
        request_data,
        expected_data,
    ):
        exclude_org = None if is_member else self._ORG
        org = self._ORG if is_member else None

        filters = {}

        for key, value in {
            "privilege": privilege,
            "role": role,
            "org": org,
            "exclude_org": exclude_org,
        }.items():
            if value is not None:
                filters[key] = value

        user = find_users(**filters)[0]["username"]
        response = patch_method(user, f"organizations/{self._ORG}", request_data)

        if is_allow:
            assert response.status_code == HTTPStatus.OK
            assert (
                DeepDiff(expected_data, response.json(), exclude_paths="root['updated_date']") == {}
            )
        else:
            assert response.status_code != HTTPStatus.OK


@pytest.mark.usefixtures("restore_db_per_function")
class TestDeleteOrganizations:
    _ORG = 2

    @pytest.mark.parametrize(
        "privilege, role, is_member, is_allow",
        [
            ("admin", None, None, True),
            (None, "owner", True, True),
            (None, "maintainer", True, False),
            (None, "worker", True, False),
            (None, "supervisor", True, False),
            ("user", None, False, False),
            ("worker", None, False, False),
        ],
    )
    def test_can_delete(
        self,
        privilege: Optional[str],
        role: Optional[str],
        is_member: Optional[bool],
        is_allow: bool,
        find_users,
    ):
        exclude_org = None if is_member else self._ORG
        org = self._ORG if is_member else None

        filters = {}

        for key, value in {
            "privilege": privilege,
            "role": role,
            "org": org,
            "exclude_org": exclude_org,
        }.items():
            if value is not None:
                filters[key] = value

        user = find_users(**filters)[0]["username"]

        response = delete_method(user, f"organizations/{self._ORG}")

        if is_allow:
            assert response.status_code == HTTPStatus.NO_CONTENT
        else:
            assert response.status_code != HTTPStatus.OK


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_projects.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import itertools
import json
import operator
import xml.etree.ElementTree as ET
import zipfile
from copy import deepcopy
from datetime import datetime
from http import HTTPStatus
from io import BytesIO
from itertools import product
from operator import itemgetter
from time import sleep
from typing import Optional

import pytest
from cvat_sdk.api_client import ApiClient, Configuration, exceptions, models
from cvat_sdk.api_client.api_client import Endpoint
from cvat_sdk.api_client.exceptions import ForbiddenException
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff
from PIL import Image

from shared.utils.config import (
    BASE_URL,
    USER_PASS,
    get_method,
    make_api_client,
    patch_method,
    post_method,
)
from shared.utils.helpers import generate_image_files

from .utils import (
    DATUMARO_FORMAT_FOR_DIMENSION,
    CollectionSimpleFilterTestBase,
    create_task,
    export_dataset,
    export_project_backup,
    export_project_dataset,
)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetProjects:
    def _find_project_by_user_org(self, user, projects, is_project_staff_flag, is_project_staff):
        for p in projects:
            if is_project_staff(user["id"], p["id"]) == is_project_staff_flag:
                return p["id"]

    def _test_response_200(self, username, project_id):
        with make_api_client(username) as api_client:
            (project, response) = api_client.projects_api.retrieve(project_id)
            assert response.status == HTTPStatus.OK
            assert project_id == project.id

    def _test_response_403(self, username, project_id):
        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.retrieve(
                project_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    # Admin can see any project even he has no ownerships for this project.
    def test_project_admin_accessibility(self, projects, find_users, is_project_staff, org_staff):
        users = find_users(privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["organization"])
            and user["id"] not in org_staff(project["organization"])
        )
        self._test_response_200(user["username"], project["id"])

    # Project owner or project assignee can see project.
    def test_project_owner_accessibility(self, projects):
        for p in projects:
            if p["owner"] is not None:
                project_with_owner = p
            if p["assignee"] is not None:
                project_with_assignee = p

        assert project_with_owner is not None
        assert project_with_assignee is not None

        self._test_response_200(project_with_owner["owner"]["username"], project_with_owner["id"])
        self._test_response_200(
            project_with_assignee["assignee"]["username"], project_with_assignee["id"]
        )

    def test_user_cannot_see_project(self, projects, find_users, is_project_staff, org_staff):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["organization"])
            and user["id"] not in org_staff(project["organization"])
        )
        self._test_response_403(user["username"], project["id"])

    @pytest.mark.parametrize("role", ("supervisor", "worker"))
    def test_if_supervisor_or_worker_cannot_see_project(
        self, projects, is_project_staff, find_users, role
    ):
        user, pid = next(
            (
                (user, project["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for project in projects
                if project["organization"] == user["org"]
                and not is_project_staff(user["id"], project["id"])
            )
        )

        self._test_response_403(user["username"], pid)

    @pytest.mark.parametrize("role", ("maintainer", "owner"))
    def test_if_maintainer_or_owner_can_see_project(
        self, find_users, projects, is_project_staff, role
    ):
        user, pid = next(
            (
                (user, project["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for project in projects
                if project["organization"] == user["org"]
                and not is_project_staff(user["id"], project["id"])
            )
        )

        self._test_response_200(user["username"], pid)

    @pytest.mark.parametrize("role", ("supervisor", "worker"))
    def test_if_org_member_supervisor_or_worker_can_see_project(
        self, projects, find_users, is_project_staff, role
    ):
        user, pid = next(
            (
                (user, project["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for project in projects
                if project["organization"] == user["org"]
                and is_project_staff(user["id"], project["id"])
            )
        )

        self._test_response_200(user["username"], pid)

    def test_can_remove_owner_and_fetch_with_sdk(self, admin_user, projects):
        # test for API schema regressions
        source_project = next(
            p for p in projects if p.get("owner") and p["owner"]["username"] != admin_user
        ).copy()

        with make_api_client(admin_user) as api_client:
            api_client.users_api.destroy(source_project["owner"]["id"])

            (_, response) = api_client.projects_api.retrieve(source_project["id"])
            fetched_project = json.loads(response.data)

        source_project["owner"] = None
        assert DeepDiff(source_project, fetched_project, ignore_order=True) == {}


class TestProjectsListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
        "assignee": ["assignee", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, projects):
        self.user = admin_user
        self.samples = projects

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.projects_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        (
            "name",
            "owner",
            "assignee",
            "status",
        ),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


class TestGetPostProjectBackup:

    @pytest.fixture(autouse=True)
    def setup(self, projects):
        self.projects = projects

    def _test_can_get_project_backup(
        self,
        username: str,
        pid: int,
        *,
        local_download: bool = True,
        **kwargs,
    ) -> Optional[bytes]:
        backup = export_project_backup(username, id=pid, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(backup))
        else:
            assert backup is None
        return backup

    def _test_cannot_get_project_backup(
        self,
        username: str,
        pid: int,
        **kwargs,
    ):
        with pytest.raises(ForbiddenException):
            export_project_backup(username, id=pid, expect_forbidden=True, **kwargs)

    def test_admin_can_get_project_backup(self):
        project = list(self.projects)[0]
        self._test_can_get_project_backup("admin1", project["id"])

    # User that not in [project:owner, project:assignee] cannot get project backup.
    def test_user_cannot_get_project_backup(self, find_users, is_project_staff):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if not is_project_staff(user["id"], project["id"])
        )

        self._test_cannot_get_project_backup(user["username"], project["id"])

    # Org worker that not in [project:owner, project:assignee] cannot get project backup.
    def test_org_worker_cannot_get_project_backup(
        self, find_users, is_project_staff, is_org_member
    ):
        users = find_users(role="worker", exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        self._test_cannot_get_project_backup(user["username"], project["id"])

    # Org worker that in [project:owner, project:assignee] can get project backup.
    def test_org_worker_can_get_project_backup(
        self,
        find_users,
        is_project_staff,
        is_org_member,
    ):
        users = find_users(role="worker", exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        self._test_can_get_project_backup(user["username"], project["id"])

    # Org supervisor that in [project:owner, project:assignee] can get project backup.
    def test_org_supervisor_can_get_project_backup(
        self, find_users, is_project_staff, is_org_member
    ):
        users = find_users(role="supervisor", exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        self._test_can_get_project_backup(user["username"], project["id"])

    # Org supervisor that not in [project:owner, project:assignee] cannot get project backup.
    def test_org_supervisor_cannot_get_project_backup(
        self,
        find_users,
        is_project_staff,
        is_org_member,
    ):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"], role="supervisor")
        )

        self._test_cannot_get_project_backup(user["username"], project["id"])

    # Org maintainer that not in [project:owner, project:assignee] can get project backup.
    def test_org_maintainer_can_get_project_backup(
        self,
        find_users,
        is_project_staff,
        is_org_member,
    ):
        users = find_users(role="maintainer", exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        self._test_can_get_project_backup(user["username"], project["id"])

    # Org owner that not in [project:owner, project:assignee] can get project backup.
    def test_org_owner_can_get_project_backup(self, find_users, is_project_staff, is_org_member):
        users = find_users(role="owner", exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, self.projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        self._test_can_get_project_backup(user["username"], project["id"])

    def test_can_get_backup_project_when_some_tasks_have_no_data(self):
        project = next((p for p in self.projects if 0 < p["tasks"]["count"]))

        # add empty task to project
        response = post_method(
            "admin1", "tasks", {"name": "empty_task", "project_id": project["id"]}
        )
        assert response.status_code == HTTPStatus.CREATED

        self._test_can_get_project_backup("admin1", project["id"])

    def test_can_get_backup_project_when_all_tasks_have_no_data(self, filter_projects):
        project = filter_projects(tasks__count=0)[0]

        # add empty tasks to empty project
        response = post_method(
            "admin1",
            "tasks",
            {"name": "empty_task1", "project_id": project["id"]},
            **({"org_id": project["organization"]} if project["organization"] else {}),
        )
        assert response.status_code == HTTPStatus.CREATED, response.text

        response = post_method(
            "admin1",
            "tasks",
            {"name": "empty_task2", "project_id": project["id"]},
            **({"org_id": project["organization"]} if project["organization"] else {}),
        )
        assert response.status_code == HTTPStatus.CREATED, response.text

        self._test_can_get_project_backup("admin1", project["id"])

    def test_can_get_backup_for_empty_project(self):
        empty_project = next((p for p in self.projects if 0 == p["tasks"]["count"]))
        self._test_can_get_project_backup("admin1", empty_project["id"])

    def test_admin_can_get_project_backup_and_create_project_by_backup(self, admin_user: str):
        project_id = 5
        backup = self._test_can_get_project_backup(admin_user, project_id)

        tmp_file = io.BytesIO(backup)
        tmp_file.name = "dataset.zip"

        import_data = {
            "project_file": tmp_file,
        }

        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.projects_api.create_backup(
                backup_write_request=deepcopy(import_data), _content_type="multipart/form-data"
            )
            assert response.status == HTTPStatus.ACCEPTED


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostProjects:
    def _test_create_project_201(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.projects_api.create(spec, **kwargs)
            assert response.status == HTTPStatus.CREATED

        return response

    def _test_create_project_403(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.projects_api.create(
                spec, **kwargs, _parse_response=False, _check_status=False
            )
        assert response.status == HTTPStatus.FORBIDDEN

        return response

    def test_if_worker_cannot_create_project(self, find_users):
        workers = find_users(privilege="worker")
        assert len(workers)

        username = workers[0]["username"]
        spec = {"name": f"test {username} tries to create a project"}
        self._test_create_project_403(username, spec)

    @pytest.mark.parametrize("privilege", ("admin", "user"))
    def test_if_user_can_create_project(self, find_users, privilege):
        privileged_users = find_users(privilege=privilege)
        assert len(privileged_users)

        username = privileged_users[0]["username"]
        spec = {"name": f"test {username} tries to create a project"}
        self._test_create_project_201(username, spec)

    def test_if_org_worker_cannot_create_project(self, find_users):
        workers = find_users(role="worker")

        worker = next(u for u in workers if u["org"])

        spec = {
            "name": f'test: worker {worker["username"]} creating a project for his organization',
        }
        self._test_create_project_403(worker["username"], spec, org_id=worker["org"])

    @pytest.mark.parametrize("role", ("supervisor", "maintainer", "owner"))
    def test_if_org_role_can_create_project(self, role, admin_user):
        # We can hit org or user limits here, so we create a new org and users
        user = self._create_user(
            ApiClient(configuration=Configuration(BASE_URL)), email="test_org_roles@localhost"
        )

        if role != "owner":
            org = self._create_org(make_api_client(admin_user), members={user["email"]: role})
        else:
            org = self._create_org(make_api_client(user["username"]))

        spec = {
            "name": f'test: worker {user["username"]} creating a project for his organization',
        }
        self._test_create_project_201(user["username"], spec, org_id=org)

    @classmethod
    def _create_user(cls, api_client: ApiClient, email: str) -> str:
        username = email.split("@", maxsplit=1)[0]
        with api_client:
            (_, response) = api_client.auth_api.create_register(
                models.RegisterSerializerExRequest(
                    username=username, password1=USER_PASS, password2=USER_PASS, email=email
                )
            )

        api_client.cookies.clear()

        return json.loads(response.data)

    @classmethod
    def _create_org(cls, api_client: ApiClient, members: Optional[dict[str, str]] = None) -> str:
        with api_client:
            (_, response) = api_client.organizations_api.create(
                models.OrganizationWriteRequest(slug="test_org_roles"), _parse_response=False
            )
            org = json.loads(response.data)["id"]

            for email, role in (members or {}).items():
                api_client.invitations_api.create(
                    models.InvitationWriteRequest(role=role, email=email),
                    org_id=org,
                    _parse_response=False,
                )

        return org

    def test_cannot_create_project_with_same_labels(self, admin_user):
        project_spec = {
            "name": "test cannot create project with same labels",
            "labels": [{"name": "l1"}, {"name": "l1"}],
        }
        response = post_method(admin_user, "projects", project_spec)
        assert response.status_code == HTTPStatus.BAD_REQUEST

        response = get_method(admin_user, "projects")
        assert response.status_code == HTTPStatus.OK

    def test_cannot_create_project_with_same_skeleton_sublabels(self, admin_user):
        project_spec = {
            "name": "test cannot create project with same skeleton sublabels",
            "labels": [
                {"name": "s1", "type": "skeleton", "sublabels": [{"name": "1"}, {"name": "1"}]}
            ],
        }
        response = post_method(admin_user, "projects", project_spec)
        assert response.status_code == HTTPStatus.BAD_REQUEST

        response = get_method(admin_user, "projects")
        assert response.status_code == HTTPStatus.OK

    @pytest.mark.parametrize(
        "storage_id",
        [
            1,  # public bucket
            2,  # private bucket
        ],
    )
    @pytest.mark.parametrize("field", ["source_storage", "target_storage"])
    def test_user_cannot_create_project_with_cloud_storage_without_access(
        self, storage_id, field, regular_lonely_user
    ):
        user = regular_lonely_user

        project_spec = {
            "name": f"Project with foreign cloud storage {storage_id} settings",
            field: {
                "location": "cloud_storage",
                "cloud_storage_id": storage_id,
            },
        }

        response = post_method(user, "projects", project_spec)
        assert response.status_code == HTTPStatus.FORBIDDEN

    def test_create_response_matches_get(self, admin_user):
        username = admin_user

        spec = {"name": "test create project", "labels": [{"name": "a"}]}

        response = self._test_create_project_201(username, spec)
        project = json.loads(response.data)

        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.retrieve(project["id"])
            assert DeepDiff(project, json.loads(response.data), ignore_order=True) == {}

    @pytest.mark.parametrize("assignee", [None, "admin1"])
    def test_can_create_with_assignee(self, admin_user, users_by_name, assignee):
        spec = {
            "name": "test project creation with assignee",
            "labels": [{"name": "car"}],
            "assignee_id": users_by_name[assignee]["id"] if assignee else None,
        }

        with make_api_client(admin_user) as api_client:
            (project, _) = api_client.projects_api.create(project_write_request=spec)

            if assignee:
                assert project.assignee.username == assignee
                assert project.assignee_updated_date
            else:
                assert project.assignee is None
                assert project.assignee_updated_date is None


def _check_cvat_for_video_project_annotations_meta(content, values_to_be_checked):
    document = ET.fromstring(content)
    instance = list(document.find("meta"))[0]
    assert instance.tag == "project"
    assert instance.find("id").text == values_to_be_checked["pid"]
    assert len(list(document.iter("task"))) == len(values_to_be_checked["tasks"])
    tasks = document.iter("task")
    for task_checking in values_to_be_checked["tasks"]:
        task_meta = next(tasks)
        assert task_meta.find("id").text == str(task_checking["id"])
        assert task_meta.find("name").text == task_checking["name"]
        assert task_meta.find("size").text == str(task_checking["size"])
        assert task_meta.find("mode").text == task_checking["mode"]
        assert task_meta.find("source").text


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
class TestImportExportDatasetProject:

    @pytest.fixture(autouse=True)
    def setup(self, projects):
        self.projects = projects

    @staticmethod
    def _test_export_dataset(
        username: str,
        pid: int,
        *,
        local_download: bool = True,
        **kwargs,
    ) -> Optional[bytes]:
        dataset = export_project_dataset(username, save_images=True, id=pid, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(dataset))
        else:
            assert dataset is None

        return dataset

    @staticmethod
    def _test_export_annotations(
        username: str, pid: int, *, local_download: bool = True, **kwargs
    ) -> Optional[bytes]:
        dataset = export_project_dataset(username, save_images=False, id=pid, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(dataset))
        else:
            assert dataset is None

        return dataset

    def _test_import_project(self, username, project_id, format_name, data):
        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.create_dataset(
                id=project_id,
                format=format_name,
                dataset_write_request=deepcopy(data),
                _content_type="multipart/form-data",
            )
            assert response.status == HTTPStatus.ACCEPTED
            rq_id = json.loads(response.data).get("rq_id")
            assert rq_id, "The rq_id was not found in the response"

            for _ in range(50):
                (background_request, response) = api_client.requests_api.retrieve(rq_id)
                assert response.status == HTTPStatus.OK
                if (
                    background_request.status.value
                    == models.RequestStatus.allowed_values[("value",)]["FINISHED"]
                ):
                    break
                sleep(0.1)
            else:
                assert (
                    False
                ), f"Import process was not finished, last status: {background_request.status.value}"

    def _test_get_annotations_from_task(self, username, task_id):
        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.retrieve_annotations(task_id)
            assert response.status == HTTPStatus.OK

            response_data = json.loads(response.data)
        return response_data

    def test_can_import_dataset_in_org(self, admin_user: str):
        project_id = 4

        dataset = self._test_export_dataset(
            admin_user,
            project_id,
        )

        tmp_file = io.BytesIO(dataset)
        tmp_file.name = "dataset.zip"

        import_data = {
            "dataset_file": tmp_file,
        }

        self._test_import_project(admin_user, project_id, "CVAT 1.1", import_data)

    @pytest.mark.parametrize(
        "export_format, import_format",
        (
            ("COCO Keypoints 1.0", "COCO Keypoints 1.0"),
            ("CVAT for images 1.1", "CVAT 1.1"),
            ("CVAT for video 1.1", "CVAT 1.1"),
            ("Datumaro 1.0", "Datumaro 1.0"),
            ("Ultralytics YOLO Pose 1.0", "Ultralytics YOLO Pose 1.0"),
        ),
    )
    def test_can_export_and_import_dataset_with_skeletons(
        self, annotations, tasks, admin_user, export_format, import_format
    ):
        tasks_with_skeletons = [
            int(task_id)
            for task_id in annotations["task"]
            for element in annotations["task"][task_id]["shapes"]
            if element["type"] == "skeleton"
        ]
        for task in tasks:
            if task["id"] in tasks_with_skeletons and task["project_id"] is not None:
                project_id = task["project_id"]
                project = next(p for p in self.projects if p["id"] == project_id)
                if (project["target_storage"] or {}).get("location") == "local":
                    break
        else:
            assert False, "Can't find suitable project"

        dataset = self._test_export_dataset(
            admin_user,
            project_id,
            format=export_format,
        )

        tmp_file = io.BytesIO(dataset)
        tmp_file.name = "dataset.zip"
        import_data = {
            "dataset_file": tmp_file,
        }

        self._test_import_project(admin_user, project_id, import_format, import_data)

    @pytest.mark.parametrize("format_name", ("Datumaro 1.0", "ImageNet 1.0", "PASCAL VOC 1.1"))
    def test_can_import_export_dataset_with_some_format(self, format_name: str):
        # https://github.com/cvat-ai/cvat/issues/4410
        # https://github.com/cvat-ai/cvat/issues/4850
        # https://github.com/cvat-ai/cvat/issues/4621
        username = "admin1"
        project_id = 4

        dataset = self._test_export_dataset(
            username,
            project_id,
            format=format_name,
        )

        tmp_file = io.BytesIO(dataset)
        tmp_file.name = "dataset.zip"

        import_data = {
            "dataset_file": tmp_file,
        }

        self._test_import_project(username, project_id, format_name, import_data)

    @pytest.mark.parametrize("username, pid", [("admin1", 8)])
    @pytest.mark.parametrize(
        "anno_format, anno_file_name, check_func",
        [
            (
                "CVAT for video 1.1",
                "annotations.xml",
                _check_cvat_for_video_project_annotations_meta,
            ),
        ],
    )
    def test_exported_project_dataset_structure(
        self,
        username,
        pid,
        anno_format,
        anno_file_name,
        check_func,
        tasks,
    ):
        project = self.projects[pid]

        values_to_be_checked = {
            "pid": str(pid),
            "name": project["name"],
            "tasks": [
                {
                    "id": task["id"],
                    "name": task["name"],
                    "size": str(task["size"]),
                    "mode": task["mode"],
                }
                for task in tasks
                if task["project_id"] == project["id"]
            ],
        }

        dataset = self._test_export_annotations(
            username,
            pid,
            format=anno_format,
        )

        with zipfile.ZipFile(BytesIO(dataset)) as zip_file:
            content = zip_file.read(anno_file_name)
        check_func(content, values_to_be_checked)

    def test_can_import_export_annotations_with_rotation(self):
        # https://github.com/cvat-ai/cvat/issues/4378
        username = "admin1"
        project_id = 4

        dataset = self._test_export_dataset(
            username,
            project_id,
        )

        tmp_file = io.BytesIO(dataset)
        tmp_file.name = "dataset.zip"

        import_data = {
            "dataset_file": tmp_file,
        }

        self._test_import_project(username, project_id, "CVAT 1.1", import_data)

        response = get_method(username, f"tasks", project_id=project_id)
        assert response.status_code == HTTPStatus.OK
        tasks = response.json()["results"]

        response_data = self._test_get_annotations_from_task(username, tasks[0]["id"])
        task1_rotation = response_data["shapes"][0]["rotation"]
        response_data = self._test_get_annotations_from_task(username, tasks[1]["id"])
        task2_rotation = response_data["shapes"][0]["rotation"]

        assert task1_rotation == task2_rotation

    def test_can_export_dataset_with_skeleton_labels_with_spaces(self):
        # https://github.com/cvat-ai/cvat/issues/5257
        # https://github.com/cvat-ai/cvat/issues/5600
        username = "admin1"
        project_id = 11

        self._test_export_dataset(
            username,
            project_id,
            format="COCO Keypoints 1.0",
        )

    def test_can_export_dataset_for_empty_project(self, filter_projects):
        empty_project = filter_projects(
            tasks__count=0, exclude_target_storage__location="cloud_storage"
        )[0]
        self._test_export_dataset(
            "admin1",
            empty_project["id"],
            format="COCO 1.0",
        )

    def test_can_export_project_dataset_when_some_tasks_have_no_data(self, filter_projects):
        project = filter_projects(
            exclude_tasks__count=0, exclude_target_storage__location="cloud_storage"
        )[0]

        # add empty task to project
        response = post_method(
            "admin1",
            "tasks",
            {
                "name": "empty_task",
                "project_id": project["id"],
            },
            **({"org_id": project["organization"]} if project["organization"] else {}),
        )
        assert response.status_code == HTTPStatus.CREATED

        self._test_export_dataset(
            "admin1",
            project["id"],
            format="COCO 1.0",
        )

    def test_can_export_project_dataset_when_all_tasks_have_no_data(self, filter_projects):
        project = filter_projects(tasks__count=0, exclude_target_storage__location="cloud_storage")[
            0
        ]

        # add empty tasks to empty project
        response = post_method(
            "admin1",
            "tasks",
            {"name": "empty_task1", "project_id": project["id"]},
            **({"org_id": project["organization"]} if project["organization"] else {}),
        )
        assert response.status_code == HTTPStatus.CREATED, response.text

        response = post_method(
            "admin1",
            "tasks",
            {"name": "empty_task2", "project_id": project["id"]},
            **({"org_id": project["organization"]} if project["organization"] else {}),
        )
        assert response.status_code == HTTPStatus.CREATED, response.text

        self._test_export_dataset(
            "admin1",
            project["id"],
            format="COCO 1.0",
        )

    @pytest.mark.parametrize("cloud_storage_id", [3])  # import/export bucket
    def test_can_export_and_import_dataset_after_deleting_related_storage(
        self, admin_user, cloud_storage_id: int
    ):
        project_id = next(
            p
            for p in self.projects
            if p["source_storage"]
            and p["source_storage"]["cloud_storage_id"] == cloud_storage_id
            and p["target_storage"]
            and p["target_storage"]["cloud_storage_id"] == cloud_storage_id
        )["id"]

        with make_api_client(admin_user) as api_client:
            _, response = api_client.cloudstorages_api.destroy(cloud_storage_id)
            assert response.status == HTTPStatus.NO_CONTENT

        result, response = api_client.projects_api.retrieve(project_id)
        assert all([not getattr(result, field) for field in ("source_storage", "target_storage")])

        dataset = self._test_export_dataset(admin_user, project_id)

        with io.BytesIO(dataset) as tmp_file:
            tmp_file.name = "dataset.zip"
            import_data = {
                "dataset_file": tmp_file,
            }

            self._test_import_project(admin_user, project_id, "CVAT 1.1", import_data)

    @pytest.mark.parametrize(
        "dimension, format_name",
        [
            *DATUMARO_FORMAT_FOR_DIMENSION.items(),
            ("2d", "CVAT 1.1"),
            ("3d", "CVAT 1.1"),
            ("2d", "COCO 1.0"),
        ],
    )
    def test_cant_import_annotations_as_project(self, admin_user, tasks, format_name, dimension):
        task = next(t for t in tasks if t.get("size") if t["dimension"] == dimension)

        def _export_task(task_id: int, format_name: str) -> io.BytesIO:
            with make_api_client(admin_user) as api_client:
                return io.BytesIO(
                    export_dataset(
                        api_client.tasks_api,
                        id=task_id,
                        format=format_name,
                        save_images=False,
                    )
                )

        if format_name in list(DATUMARO_FORMAT_FOR_DIMENSION.values()):
            with zipfile.ZipFile(_export_task(task["id"], format_name)) as zip_file:
                annotations = zip_file.read("annotations/default.json")

            dataset_file = io.BytesIO(annotations)
            dataset_file.name = "annotations.json"
        elif format_name == "CVAT 1.1":
            with zipfile.ZipFile(_export_task(task["id"], "CVAT for images 1.1")) as zip_file:
                annotations = zip_file.read("annotations.xml")

            dataset_file = io.BytesIO(annotations)
            dataset_file.name = "annotations.xml"
        elif format_name == "COCO 1.0":
            with zipfile.ZipFile(_export_task(task["id"], format_name)) as zip_file:
                annotations = zip_file.read("annotations/instances_default.json")

            dataset_file = io.BytesIO(annotations)
            dataset_file.name = "annotations.json"
        else:
            assert False

        with make_api_client(admin_user) as api_client:
            project, _ = api_client.projects_api.create(
                project_write_request=models.ProjectWriteRequest(
                    name=f"test_annotations_import_as_project {format_name}"
                )
            )

            import_data = {"dataset_file": dataset_file}

            with pytest.raises(exceptions.ApiException, match="Dataset file should be zip archive"):
                self._test_import_project(
                    admin_user,
                    project.id,
                    format_name=format_name,
                    data=import_data,
                )

    @pytest.mark.parametrize(
        "export_format, subset_path_template",
        [
            ("COCO 1.0", "images/{subset}/"),
            ("COCO Keypoints 1.0", "images/{subset}/"),
            ("CVAT for images 1.1", "images/{subset}/"),
            ("CVAT for video 1.1", "images/{subset}/"),
            ("Datumaro 1.0", "images/{subset}/"),
            ("Datumaro 3D 1.0", "point_clouds/{subset}/"),
            ("LabelMe 3.0", "{subset}/"),
            ("MOTS PNG 1.0", "{subset}/images/"),
            ("YOLO 1.1", "obj_{subset}_data/"),
            ("CamVid 1.0", "{subset}/"),
            ("WiderFace 1.0", "WIDER_{subset}/images/"),
            ("VGGFace2 1.0", "{subset}/"),
            ("Market-1501 1.0", "bounding_box_{subset}/"),
            ("ICDAR Recognition 1.0", "{subset}/images/"),
            ("ICDAR Localization 1.0", "{subset}/images/"),
            ("ICDAR Segmentation 1.0", "{subset}/images/"),
            ("KITTI 1.0", "{subset}/image_2/"),
            ("LFW 1.0", "{subset}/images/"),
            ("Cityscapes 1.0", "imgsFine/leftImg8bit/{subset}/"),
            ("Open Images V6 1.0", "images/{subset}/"),
            ("Ultralytics YOLO Detection 1.0", "images/{subset}/"),
            ("Ultralytics YOLO Oriented Bounding Boxes 1.0", "images/{subset}/"),
            ("Ultralytics YOLO Segmentation 1.0", "images/{subset}/"),
            ("Ultralytics YOLO Pose 1.0", "images/{subset}/"),
        ],
    )
    def test_creates_subfolders_for_subsets_on_export(
        self, filter_tasks, admin_user, export_format, subset_path_template
    ):
        group_key_func = itemgetter("project_id")
        subsets = ["Train", "Validation"]
        tasks = filter_tasks(exclude_target_storage__location="cloud_storage")
        project_id = next(
            project_id
            for project_id, group in itertools.groupby(
                sorted(filter(group_key_func, tasks), key=group_key_func),
                key=group_key_func,
            )
            if sorted(task["subset"] for task in group) == subsets
        )
        dataset = self._test_export_dataset(admin_user, project_id, format=export_format)
        with zipfile.ZipFile(io.BytesIO(dataset)) as zip_file:
            for subset in subsets:
                folder_prefix = subset_path_template.format(subset=subset)
                assert (
                    len([f for f in zip_file.namelist() if f.startswith(folder_prefix)]) > 0
                ), f"No {folder_prefix} in {zip_file.namelist()}"

    def test_export_project_with_honeypots(self, admin_user: str):
        project_spec = {
            "name": "Project with honeypots",
            "labels": [{"name": "cat"}],
        }

        with make_api_client(admin_user) as api_client:
            project, _ = api_client.projects_api.create(project_spec)

        image_files = generate_image_files(3)
        image_names = [i.name for i in image_files]

        task_params = {
            "name": "Task with honeypots",
            "segment_size": 1,
            "project_id": project.id,
        }

        data_params = {
            "image_quality": 70,
            "client_files": image_files,
            "sorting_method": "random",
            "validation_params": {
                "mode": "gt_pool",
                "frame_selection_method": "manual",
                "frames_per_job_count": 1,
                "frames": [image_files[-1].name],
            },
        }

        create_task(admin_user, spec=task_params, data=data_params)

        dataset = export_project_dataset(
            admin_user, save_images=True, id=project.id, format="COCO 1.0"
        )

        with zipfile.ZipFile(io.BytesIO(dataset)) as zip_file:
            subset_path = "images/default"
            assert (
                sorted(
                    [
                        f[len(subset_path) + 1 :]
                        for f in zip_file.namelist()
                        if f.startswith(subset_path)
                    ]
                )
                == image_names
            )
            with zip_file.open("annotations/instances_default.json") as anno_file:
                annotations = json.load(anno_file)
                assert sorted([a["file_name"] for a in annotations["images"]]) == image_names


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchProjectLabel:
    def _get_project_labels(self, pid, user, **kwargs) -> list[models.Label]:
        kwargs.setdefault("return_json", True)
        with make_api_client(user) as api_client:
            return get_paginated_collection(
                api_client.labels_api.list_endpoint, project_id=pid, **kwargs
            )

    def test_can_delete_label(self, projects_wlc, labels, admin_user):
        project = [p for p in projects_wlc if p["labels"]["count"] > 0][0]
        label = deepcopy([l for l in labels if l.get("project_id") == project["id"]][0])
        label_payload = {"id": label["id"], "deleted": True}

        prev_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        response = patch_method(
            admin_user, f'projects/{project["id"]}', {"labels": [label_payload]}
        )
        curr_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK, response.content
        assert curr_lc == prev_lc - 1

    def test_can_delete_skeleton_label(self, projects, labels, admin_user):
        project = next(
            p
            for p in projects
            if any(
                label
                for label in labels
                if label.get("project_id") == p["id"]
                if label["type"] == "skeleton"
            )
        )
        project_labels = deepcopy([l for l in labels if l.get("project_id") == project["id"]])
        label = next(l for l in project_labels if l["type"] == "skeleton")
        project_labels.remove(label)
        label_payload = {"id": label["id"], "deleted": True}

        prev_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        response = patch_method(
            admin_user, f'projects/{project["id"]}', {"labels": [label_payload]}
        )
        curr_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc - 1

        resulting_labels = self._get_project_labels(project["id"], admin_user)
        assert DeepDiff(resulting_labels, project_labels, ignore_order=True) == {}

    def test_can_rename_label(self, projects_wlc, labels, admin_user):
        project = [p for p in projects_wlc if p["labels"]["count"] > 0][0]
        project_labels = deepcopy([l for l in labels if l.get("project_id") == project["id"]])
        project_labels[0].update({"name": "new name"})

        response = patch_method(
            admin_user, f'projects/{project["id"]}', {"labels": [project_labels[0]]}
        )
        assert response.status_code == HTTPStatus.OK

        resulting_labels = self._get_project_labels(project["id"], admin_user)
        assert DeepDiff(resulting_labels, project_labels, ignore_order=True) == {}

    def test_cannot_rename_label_to_duplicate_name(self, projects_wlc, labels, admin_user):
        project = [p for p in projects_wlc if p["labels"]["count"] > 1][0]
        project_labels = deepcopy([l for l in labels if l.get("project_id") == project["id"]])
        project_labels[0].update({"name": project_labels[1]["name"]})

        label_payload = {"id": project_labels[0]["id"], "name": project_labels[0]["name"]}

        response = patch_method(
            admin_user, f'projects/{project["id"]}', {"labels": [label_payload]}
        )
        assert response.status_code == HTTPStatus.BAD_REQUEST
        assert "All label names must be unique" in response.text

    def test_cannot_add_foreign_label(self, projects, labels, admin_user):
        project = list(projects)[0]
        new_label = deepcopy([l for l in labels if l.get("project_id") != project["id"]][0])

        response = patch_method(admin_user, f'projects/{project["id"]}', {"labels": [new_label]})
        assert response.status_code == HTTPStatus.NOT_FOUND
        assert f"Not found label with id #{new_label['id']} to change" in response.text

    def test_admin_can_add_label(self, projects, admin_user):
        project = list(projects)[0]
        new_label = {"name": "new name"}

        prev_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        response = patch_method(admin_user, f'projects/{project["id"]}', {"labels": [new_label]})
        curr_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_non_project_staff_privileged_org_members_can_add_label(
        self,
        find_users,
        projects,
        is_project_staff,
        is_org_member,
        role,
    ):
        users = find_users(role=role, exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"])
        )

        prev_lc = get_method(user["username"], "labels", project_id=project["id"]).json()["count"]
        new_label = {"name": "new name"}
        response = patch_method(
            user["username"],
            f'projects/{project["id"]}',
            {"labels": [new_label]},
        )
        curr_lc = get_method(user["username"], "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    @pytest.mark.parametrize("role", ["supervisor", "worker"])
    def test_non_project_staff_org_members_cannot_add_label(
        self,
        find_users,
        projects,
        is_project_staff,
        is_org_member,
        role,
    ):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"], role=role)
        )

        new_label = {"name": "new name"}
        response = patch_method(
            user["username"],
            f'projects/{project["id"]}',
            {"labels": [new_label]},
        )
        assert response.status_code == HTTPStatus.FORBIDDEN

    # TODO: add supervisor too, but this leads to a test-side problem with DB restoring
    @pytest.mark.parametrize("role", ["worker"])
    def test_project_staff_org_members_can_add_label(
        self, find_users, projects, is_project_staff, is_org_member, labels, role
    ):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if is_project_staff(user["id"], project["id"])
            and project["organization"]
            and is_org_member(user["id"], project["organization"], role=role)
            and any(label.get("project_id") == project["id"] for label in labels)
        )

        prev_lc = get_method(user["username"], "labels", project_id=project["id"]).json()["count"]
        new_label = {"name": "new name"}
        response = patch_method(
            user["username"],
            f'projects/{project["id"]}',
            {"labels": [new_label]},
        )
        curr_lc = get_method(user["username"], "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    def test_admin_can_add_skeleton(self, projects, admin_user):
        project = list(projects)[0]
        new_skeleton = {
            "name": "skeleton1",
            "type": "skeleton",
            "sublabels": [
                {
                    "name": "1",
                    "type": "points",
                }
            ],
            "svg": '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="48.794559478759766" '
            'cy="36.98698806762695" stroke-width="0.1" data-type="element node" '
            'data-element-id="1" data-node-id="1" data-label-name="597501"></circle>',
        }

        prev_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        response = patch_method(admin_user, f'projects/{project["id"]}', {"labels": [new_skeleton]})
        curr_lc = get_method(admin_user, "labels", project_id=project["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetProjectPreview:
    def _test_response_200(self, username, project_id, **kwargs):
        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.retrieve_preview(project_id, **kwargs)

            assert response.status == HTTPStatus.OK
            (width, height) = Image.open(BytesIO(response.data)).size
            assert width > 0 and height > 0

    def _test_response_403(self, username, project_id):
        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.retrieve_preview(
                project_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    def _test_response_404(self, username, project_id):
        with make_api_client(username) as api_client:
            (_, response) = api_client.projects_api.retrieve_preview(
                project_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.NOT_FOUND

    # Admin can see any project preview even he has no ownerships for this project.
    def test_project_preview_admin_accessibility(
        self, projects, find_users, is_project_staff, org_staff
    ):
        users = find_users(privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["organization"])
            and user["id"] not in org_staff(project["organization"])
            and project["tasks"]["count"] > 0
        )
        self._test_response_200(user["username"], project["id"])

    # Project owner or project assignee can see project preview.
    def test_project_preview_owner_accessibility(self, projects):
        for p in projects:
            if not p["tasks"]:
                continue
            if p["owner"] is not None:
                project_with_owner = p
            if p["assignee"] is not None:
                project_with_assignee = p

        assert project_with_owner is not None
        assert project_with_assignee is not None

        self._test_response_200(project_with_owner["owner"]["username"], project_with_owner["id"])
        self._test_response_200(
            project_with_assignee["assignee"]["username"], project_with_assignee["id"]
        )

    def test_project_preview_not_found(self, projects, tasks):
        for p in projects:
            if any(t["project_id"] == p["id"] for t in tasks):
                continue
            if p["owner"] is not None:
                project_with_owner = p
            if p["assignee"] is not None:
                project_with_assignee = p

        assert project_with_owner is not None
        assert project_with_assignee is not None

        self._test_response_404(project_with_owner["owner"]["username"], project_with_owner["id"])
        self._test_response_404(
            project_with_assignee["assignee"]["username"], project_with_assignee["id"]
        )

    def test_user_cannot_see_project_preview(
        self, projects, find_users, is_project_staff, org_staff
    ):
        users = find_users(exclude_privilege="admin")

        user, project = next(
            (user, project)
            for user, project in product(users, projects)
            if not is_project_staff(user["id"], project["organization"])
            and user["id"] not in org_staff(project["organization"])
        )
        self._test_response_403(user["username"], project["id"])

    @pytest.mark.parametrize("role", ("supervisor", "worker"))
    def test_if_supervisor_or_worker_cannot_see_project_preview(
        self, projects, is_project_staff, find_users, role
    ):
        user, pid = next(
            (
                (user, project["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for project in projects
                if project["organization"] == user["org"]
                and not is_project_staff(user["id"], project["id"])
            )
        )

        self._test_response_403(user["username"], pid)

    @pytest.mark.parametrize("role", ("maintainer", "owner"))
    def test_if_maintainer_or_owner_can_see_project_preview(
        self, find_users, projects, is_project_staff, role
    ):
        user, pid = next(
            (
                (user, project["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for project in projects
                if project["organization"] == user["org"]
                and not is_project_staff(user["id"], project["id"])
                and project["tasks"]["count"] > 0
            )
        )

        self._test_response_200(user["username"], pid)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchProject:
    @pytest.mark.parametrize(
        "storage_id",
        [
            1,  # public bucket
            2,  # private bucket
        ],
    )
    @pytest.mark.parametrize("field", ["source_storage", "target_storage"])
    def test_user_cannot_update_project_with_cloud_storage_without_access(
        self, storage_id, field, regular_lonely_user
    ):
        user = regular_lonely_user

        project_spec = {
            "name": f"Project with foreign cloud storage {storage_id} settings",
        }
        response = post_method(user, "projects", project_spec)

        updated_fields = {
            field: {
                "location": "cloud_storage",
                "cloud_storage_id": storage_id,
            }
        }
        project_id = response.json()["id"]

        response = patch_method(user, f"projects/{project_id}", updated_fields)
        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("has_old_assignee", [False, True])
    @pytest.mark.parametrize("new_assignee", [None, "same", "different"])
    def test_can_update_assignee_updated_date_on_assignee_updates(
        self, admin_user, projects, users, has_old_assignee, new_assignee
    ):
        project = next(p for p in projects if bool(p.get("assignee")) == has_old_assignee)

        old_assignee_id = (project.get("assignee") or {}).get("id")

        new_assignee_id = None
        if new_assignee == "same":
            new_assignee_id = old_assignee_id
        elif new_assignee == "different":
            new_assignee_id = next(u for u in users if u["id"] != old_assignee_id)["id"]

        with make_api_client(admin_user) as api_client:
            (updated_project, _) = api_client.projects_api.partial_update(
                project["id"], patched_project_write_request={"assignee_id": new_assignee_id}
            )

            op = operator.eq if new_assignee_id == old_assignee_id else operator.ne

            # FUTURE-TODO: currently it is possible to have a project with an assignee but with assignee_updated_date == None
            # because there was no migration to set some assignee_updated_date for projects/tasks/jobs with assignee != None
            if isinstance(updated_project.assignee_updated_date, datetime):
                assert op(
                    str(updated_project.assignee_updated_date.isoformat()).replace("+00:00", "Z"),
                    project["assignee_updated_date"],
                )
            else:
                assert op(updated_project.assignee_updated_date, project["assignee_updated_date"])

            if new_assignee_id:
                assert updated_project.assignee.id == new_assignee_id
            else:
                assert updated_project.assignee is None

    def test_malefactor_cannot_obtain_project_details_via_empty_partial_update_request(
        self, regular_lonely_user, projects
    ):
        project = next(iter(projects))

        with make_api_client(regular_lonely_user) as api_client:
            with pytest.raises(ForbiddenException):
                api_client.projects_api.partial_update(project["id"])

    @staticmethod
    def _test_patch_linked_storage(
        user: str, project_id: int, *, expected_status: HTTPStatus = HTTPStatus.OK
    ) -> None:
        with make_api_client(user) as api_client:
            for associated_storage in ("source_storage", "target_storage"):
                patch_data = {
                    associated_storage: {
                        "location": "local",
                    }
                }
                (_, response) = api_client.projects_api.partial_update(
                    project_id,
                    patched_project_write_request=patch_data,
                    _check_status=False,
                    _parse_response=False,
                )
                assert response.status == expected_status, response.status

    @pytest.mark.parametrize(
        "is_project_assignee", [True, False]
    )  # being a project assignee must not change anything
    @pytest.mark.parametrize(
        "role, is_allow",
        [
            ("owner", True),
            ("maintainer", True),
            ("supervisor", False),
            ("worker", False),
        ],
    )
    def test_org_update_project_associated_storage(
        self,
        is_project_assignee: bool,
        role: str,
        is_allow: bool,
        projects,
        find_users,
    ):
        project_id: Optional[int] = None
        username: Optional[str] = None

        for project in projects:
            if project_id is not None:
                break
            for user in find_users(role=role, exclude_privilege="admin"):
                is_user_project_assignee = (project["assignee"] or {}).get("id") == user["id"]
                if (
                    project["organization"] == user["org"]
                    and project["owner"]["id"] != user["id"]
                    and (
                        is_project_assignee
                        and is_user_project_assignee
                        or not (is_project_assignee or is_user_project_assignee)
                    )
                ):
                    project_id = project["id"]
                    username = user["username"]
                    break

        assert project_id is not None

        self._test_patch_linked_storage(
            username,
            project_id,
            expected_status=HTTPStatus.OK if is_allow else HTTPStatus.FORBIDDEN,
        )

    @pytest.mark.parametrize(
        "is_owner, is_assignee, is_allow",
        [
            (True, False, True),
            (False, True, False),
            (False, False, False),
        ],
    )
    def test_sandbox_update_project_associated_storage(
        self,
        is_owner: bool,
        is_assignee: str,
        is_allow: bool,
        find_users,
        filter_projects,
    ):
        username: Optional[str] = None
        project_id: Optional[int] = None

        projects = filter_projects(organization=None)
        users = find_users(exclude_privilege="admin")

        for project in projects:
            if project_id is not None:
                break
            for user in users:
                is_user_project_owner = project["owner"]["id"] == user["id"]
                is_user_project_assignee = (project["assignee"] or {}).get("id") == user["id"]

                if (
                    (is_owner and is_user_project_owner)
                    or (is_assignee and not is_user_project_owner and is_user_project_assignee)
                    or not any(
                        [is_owner, is_assignee, is_user_project_owner, is_user_project_assignee]
                    )
                ):
                    project_id = project["id"]
                    username = user["username"]
                    break

        assert project_id is not None

        self._test_patch_linked_storage(
            username,
            project_id,
            expected_status=HTTPStatus.OK if is_allow else HTTPStatus.FORBIDDEN,
        )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_quality_control.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from collections.abc import Iterable
from copy import deepcopy
from functools import partial
from http import HTTPStatus
from itertools import groupby
from typing import Any, Callable, Optional

import pytest
from cvat_sdk.api_client import exceptions, models
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff

from shared.utils.config import make_api_client

from .utils import CollectionSimpleFilterTestBase, parse_frame_step


class _PermissionTestBase:
    def create_quality_report(self, user: str, task_id: int):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.create_report(
                quality_report_create_request=models.QualityReportCreateRequest(task_id=task_id),
                _parse_response=False,
            )
            assert response.status == HTTPStatus.ACCEPTED
            rq_id = json.loads(response.data)["rq_id"]

            while True:
                (_, response) = api_client.quality_api.create_report(
                    rq_id=rq_id, _parse_response=False
                )
                assert response.status in [HTTPStatus.CREATED, HTTPStatus.ACCEPTED]

                if response.status == HTTPStatus.CREATED:
                    break

            return json.loads(response.data)

    def create_gt_job(self, user, task_id):
        with make_api_client(user) as api_client:
            (meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            start_frame = meta.start_frame

            (job, _) = api_client.jobs_api.create(
                models.JobWriteRequest(
                    type="ground_truth",
                    task_id=task_id,
                    frame_selection_method="manual",
                    frames=[start_frame],
                )
            )

            (labels, _) = api_client.labels_api.list(task_id=task_id)
            api_client.jobs_api.update_annotations(
                job.id,
                job_annotations_update_request=dict(
                    shapes=[
                        dict(
                            frame=start_frame,
                            label_id=labels.results[0].id,
                            type="rectangle",
                            points=[1, 1, 2, 2],
                        ),
                    ],
                ),
            )

            api_client.jobs_api.partial_update(
                job.id,
                patched_job_write_request={
                    "stage": "acceptance",
                    "state": "completed",
                },
            )

        return job

    @pytest.fixture(scope="class")
    def find_sandbox_task(self, tasks, jobs, users, is_task_staff):
        def _find(
            is_staff: bool, *, has_gt_jobs: Optional[bool] = None
        ) -> tuple[dict[str, Any], dict[str, Any]]:
            task = next(
                t
                for t in tasks
                if t["organization"] is None
                and not users[t["owner"]["id"]]["is_superuser"]
                and (
                    has_gt_jobs is None
                    or has_gt_jobs
                    == any(
                        j for j in jobs if j["task_id"] == t["id"] and j["type"] == "ground_truth"
                    )
                )
            )

            if is_staff:
                user = task["owner"]
            else:
                user = next(u for u in users if not is_task_staff(u["id"], task["id"]))

            return task, user

        return _find

    @pytest.fixture(scope="class")
    def find_sandbox_task_without_gt(self, find_sandbox_task):
        return partial(find_sandbox_task, has_gt_jobs=False)

    @pytest.fixture(scope="class")
    def find_org_task(self, tasks, jobs, users, is_org_member, is_task_staff):
        def _find(
            is_staff: bool, user_org_role: str, *, has_gt_jobs: Optional[bool] = None
        ) -> tuple[dict[str, Any], dict[str, Any]]:
            for user in users:
                if user["is_superuser"]:
                    continue

                task = next(
                    (
                        t
                        for t in tasks
                        if t["organization"] is not None
                        and is_task_staff(user["id"], t["id"]) == is_staff
                        and is_org_member(user["id"], t["organization"], role=user_org_role)
                        and (
                            has_gt_jobs is None
                            or has_gt_jobs
                            == any(
                                j
                                for j in jobs
                                if j["task_id"] == t["id"] and j["type"] == "ground_truth"
                            )
                        )
                    ),
                    None,
                )
                if task is not None:
                    break

            assert task

            return task, user

        return _find

    @pytest.fixture(scope="class")
    def find_org_task_without_gt(self, find_org_task):
        return partial(find_org_task, has_gt_jobs=False)

    _default_sandbox_cases = ("is_staff, allow", [(True, True), (False, False)])

    _default_org_cases = (
        "org_role, is_staff, allow",
        [
            ("owner", True, True),
            ("owner", False, True),
            ("maintainer", True, True),
            ("maintainer", False, True),
            ("supervisor", True, True),
            ("supervisor", False, False),
            ("worker", True, True),
            ("worker", False, False),
        ],
    )


@pytest.mark.usefixtures("restore_db_per_class")
class TestListQualityReports(_PermissionTestBase):
    def _test_list_reports_200(self, user, task_id, *, expected_data=None, **kwargs):
        with make_api_client(user) as api_client:
            results = get_paginated_collection(
                api_client.quality_api.list_reports_endpoint,
                return_json=True,
                task_id=task_id,
                **kwargs,
            )

            if expected_data is not None:
                assert DeepDiff(expected_data, results) == {}

    def _test_list_reports_403(self, user, task_id, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.list_reports(
                task_id=task_id, **kwargs, _parse_response=False, _check_status=False
            )

            assert response.status == HTTPStatus.FORBIDDEN

    def test_can_list_quality_reports(self, admin_user, quality_reports):
        parent_report = next(r for r in quality_reports if r["task_id"])
        task_id = parent_report["task_id"]

        reports = [
            r
            for r in quality_reports
            if r["task_id"] == task_id
            or r["parent_id"]
            and quality_reports[r["parent_id"]]["task_id"] == task_id
        ]

        self._test_list_reports_200(admin_user, task_id, expected_data=reports)

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_list_reports_in_sandbox_task(
        self, is_staff, allow, admin_user, find_sandbox_task_without_gt
    ):
        task, user = find_sandbox_task_without_gt(is_staff)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])

        if allow:
            self._test_list_reports_200(
                user["username"], task["id"], expected_data=[report], target="task"
            )
        else:
            self._test_list_reports_403(user["username"], task["id"])

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_list_reports_in_org_task(
        self,
        find_org_task_without_gt,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        task, user = find_org_task_without_gt(is_staff, org_role)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])

        if allow:
            self._test_list_reports_200(
                user["username"], task["id"], expected_data=[report], target="task"
            )
        else:
            self._test_list_reports_403(user["username"], task["id"])


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetQualityReports(_PermissionTestBase):
    def _test_get_report_200(
        self, user: str, obj_id: int, *, expected_data: Optional[dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report(obj_id, **kwargs)
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_get_report_403(self, user: str, obj_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report(
                obj_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_get_report_in_sandbox_task(
        self, is_staff, allow, admin_user, find_sandbox_task_without_gt
    ):
        task, user = find_sandbox_task_without_gt(is_staff)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])

        if allow:
            self._test_get_report_200(user["username"], report["id"], expected_data=report)
        else:
            self._test_get_report_403(user["username"], report["id"])

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_get_report_in_org_task(
        self,
        find_org_task_without_gt,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        task, user = find_org_task_without_gt(is_staff, org_role)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])

        if allow:
            self._test_get_report_200(user["username"], report["id"], expected_data=report)
        else:
            self._test_get_report_403(user["username"], report["id"])


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetQualityReportData(_PermissionTestBase):
    def _test_get_report_data_200(
        self, user: str, obj_id: int, *, expected_data: Optional[dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report_data(obj_id, **kwargs)
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_get_report_data_403(self, user: str, obj_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report_data(
                obj_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    @pytest.mark.parametrize("target", ["task", "job"])
    def test_can_get_full_report_data(self, admin_user, target, quality_reports):
        report = next(r for r in quality_reports if (r["job_id"] is not None) == (target == "job"))
        report_id = report["id"]

        with make_api_client(admin_user) as api_client:
            (report_data, response) = api_client.quality_api.retrieve_report_data(report_id)
            assert response.status == HTTPStatus.OK

        # Just check several keys exist
        for key in ["parameters", "comparison_summary", "frame_results"]:
            assert key in report_data.keys(), key

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_get_report_data_in_sandbox_task(
        self, is_staff, allow, admin_user, find_sandbox_task_without_gt
    ):
        task, user = find_sandbox_task_without_gt(is_staff)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])
        report_data = json.loads(self._test_get_report_data_200(admin_user, report["id"]).data)

        if allow:
            self._test_get_report_data_200(
                user["username"], report["id"], expected_data=report_data
            )
        else:
            self._test_get_report_data_403(user["username"], report["id"])

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_get_report_data_in_org_task(
        self,
        find_org_task_without_gt,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        task, user = find_org_task_without_gt(is_staff, org_role)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])
        report_data = json.loads(self._test_get_report_data_200(admin_user, report["id"]).data)

        if allow:
            self._test_get_report_data_200(
                user["username"], report["id"], expected_data=report_data
            )
        else:
            self._test_get_report_data_403(user["username"], report["id"])

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize("has_assignee", [False, True])
    def test_can_get_report_data_with_job_assignees(
        self, admin_user, jobs, users_by_name, has_assignee
    ):
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            and j["stage"] == "acceptance"
            and j["state"] == "completed"
        )
        task_id = gt_job["task_id"]

        normal_job = next(j for j in jobs if j["type"] == "annotation" and j["task_id"] == task_id)
        if has_assignee:
            new_assignee = users_by_name[admin_user]
        else:
            new_assignee = None

        if bool(normal_job["assignee"]) != has_assignee:
            with make_api_client(admin_user) as api_client:
                api_client.jobs_api.partial_update(
                    normal_job["id"],
                    patched_job_write_request={
                        "assignee": new_assignee["id"] if new_assignee else None
                    },
                )

        task_report = self.create_quality_report(admin_user, task_id)

        with make_api_client(admin_user) as api_client:
            job_report = api_client.quality_api.list_reports(
                job_id=normal_job["id"], parent_id=task_report["id"]
            )[0].results[0]

        report_data = json.loads(self._test_get_report_data_200(admin_user, job_report["id"]).data)
        assert (
            DeepDiff(
                (
                    {
                        k: v
                        for k, v in new_assignee.items()
                        if k in ["id", "username", "first_name", "last_name"]
                    }
                    if new_assignee
                    else None
                ),
                report_data["assignee"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostQualityReports(_PermissionTestBase):
    def test_can_create_report(self, admin_user, jobs):
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            and j["stage"] == "acceptance"
            and j["state"] == "completed"
        )
        task_id = gt_job["task_id"]

        report = self.create_quality_report(admin_user, task_id)
        assert models.QualityReport._from_openapi_data(**report)

    @pytest.mark.parametrize("has_assignee", [False, True])
    def test_can_create_report_with_job_assignees(
        self, admin_user, jobs, users_by_name, has_assignee
    ):
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            and j["stage"] == "acceptance"
            and j["state"] == "completed"
        )
        task_id = gt_job["task_id"]

        normal_job = next(j for j in jobs if j["type"] == "annotation")
        if bool(normal_job["assignee"]) != has_assignee:
            with make_api_client(admin_user) as api_client:
                api_client.jobs_api.partial_update(
                    normal_job["id"],
                    patched_job_write_request={
                        "assignee": users_by_name[admin_user]["id"] if has_assignee else None
                    },
                )

        report = self.create_quality_report(admin_user, task_id)
        assert models.QualityReport._from_openapi_data(**report)

    def test_cannot_create_report_without_gt_job(self, admin_user, tasks):
        task_id = next(t["id"] for t in tasks if t["jobs"]["count"] == 1)

        with pytest.raises(exceptions.ApiException) as capture:
            self.create_quality_report(admin_user, task_id)

        assert (
            "Quality reports require a Ground Truth job in the task at the acceptance "
            "stage and in the completed state"
        ) in capture.value.body

    @pytest.mark.parametrize(
        "field_name, field_value",
        [
            ("stage", "annotation"),
            ("stage", "validation"),
            ("state", "new"),
            ("state", "in progress"),
            ("state", "rejected"),
        ],
    )
    def test_cannot_create_report_with_incomplete_gt_job(
        self, admin_user, jobs, field_name, field_value
    ):
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            and j["stage"] == "acceptance"
            and j["state"] == "completed"
        )
        task_id = gt_job["task_id"]

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update(
                gt_job["id"], patched_job_write_request={field_name: field_value}
            )

        with pytest.raises(exceptions.ApiException) as capture:
            self.create_quality_report(admin_user, task_id)

        assert (
            "Quality reports require a Ground Truth job in the task at the acceptance "
            "stage and in the completed state"
        ) in capture.value.body

    def _test_create_report_200(self, user: str, task_id: int):
        return self.create_quality_report(user, task_id)

    def _test_create_report_403(self, user: str, task_id: int):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.create_report(
                quality_report_create_request=models.QualityReportCreateRequest(task_id=task_id),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_create_report_in_sandbox_task(
        self, is_staff, allow, admin_user, find_sandbox_task_without_gt
    ):
        task, user = find_sandbox_task_without_gt(is_staff)

        self.create_gt_job(admin_user, task["id"])

        if allow:
            self._test_create_report_200(user["username"], task["id"])
        else:
            self._test_create_report_403(user["username"], task["id"])

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_create_report_in_org_task(
        self,
        find_org_task_without_gt,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        task, user = find_org_task_without_gt(is_staff, org_role)

        self.create_gt_job(admin_user, task["id"])

        if allow:
            self._test_create_report_200(user["username"], task["id"])
        else:
            self._test_create_report_403(user["username"], task["id"])

    @staticmethod
    def _initialize_report_creation(task_id: int, user: str) -> str:
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.create_report(
                quality_report_create_request=models.QualityReportCreateRequest(task_id=task_id),
                _parse_response=False,
            )
            rq_id = json.loads(response.data)["rq_id"]
            assert rq_id

            return rq_id

    # only rq job owner or admin now has the right to check status of report creation
    def _test_check_status_of_report_creation_by_non_rq_job_owner(
        self,
        rq_id: str,
        *,
        task_staff: str,
        another_user: str,
    ):
        with make_api_client(another_user) as api_client:
            (_, response) = api_client.quality_api.create_report(
                rq_id=rq_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.NOT_FOUND
            assert json.loads(response.data)["detail"] == "Unknown request id"

        with make_api_client(task_staff) as api_client:
            (_, response) = api_client.quality_api.create_report(
                rq_id=rq_id, _parse_response=False, _check_status=False
            )
            assert response.status in {HTTPStatus.ACCEPTED, HTTPStatus.CREATED}

    def test_non_rq_job_owner_cannot_check_status_of_report_creation_in_sandbox(
        self,
        find_sandbox_task_without_gt: Callable[[bool], tuple[dict[str, Any], dict[str, Any]]],
        admin_user: str,
        users: Iterable,
    ):
        task, task_staff = find_sandbox_task_without_gt(is_staff=True)

        self.create_gt_job(admin_user, task["id"])

        another_user = next(
            u
            for u in users
            if (
                u["id"] != task_staff["id"]
                and not u["is_superuser"]
                and u["id"] != task["owner"]["id"]
            )
        )
        rq_id = self._initialize_report_creation(task["id"], task_staff["username"])
        self._test_check_status_of_report_creation_by_non_rq_job_owner(
            rq_id, task_staff=task_staff["username"], another_user=another_user["username"]
        )

    @pytest.mark.parametrize("role", ("owner", "maintainer", "supervisor", "worker"))
    def test_non_rq_job_owner_cannot_check_status_of_report_creation_in_org(
        self,
        role: str,
        admin_user: str,
        find_org_task_without_gt: Callable[[bool, str], tuple[dict[str, Any], dict[str, Any]]],
        find_users: Callable[..., list[dict[str, Any]]],
    ):
        task, task_staff = find_org_task_without_gt(is_staff=True, user_org_role="supervisor")

        self.create_gt_job(admin_user, task["id"])

        another_user = next(
            u
            for u in find_users(role=role, org=task["organization"])
            if (
                u["id"] != task_staff["id"]
                and not u["is_superuser"]
                and u["id"] != task["owner"]["id"]
            )
        )
        rq_id = self._initialize_report_creation(task["id"], task_staff["username"])
        self._test_check_status_of_report_creation_by_non_rq_job_owner(
            rq_id, task_staff=task_staff["username"], another_user=another_user["username"]
        )

    @pytest.mark.parametrize("is_sandbox", (True, False))
    def test_admin_can_check_status_of_report_creation(
        self,
        is_sandbox: bool,
        users: Iterable,
        admin_user: str,
        find_org_task_without_gt: Callable[[bool, str], tuple[dict[str, Any], dict[str, Any]]],
        find_sandbox_task_without_gt: Callable[[bool], tuple[dict[str, Any], dict[str, Any]]],
    ):
        if is_sandbox:
            task, task_staff = find_sandbox_task_without_gt(is_staff=True)
        else:
            task, task_staff = find_org_task_without_gt(is_staff=True, user_org_role="owner")

        admin = next(
            u
            for u in users
            if (
                u["is_superuser"]
                and u["id"] != task_staff["id"]
                and u["id"] != task["owner"]["id"]
                and u["id"] != (task["assignee"] or {}).get("id")
            )
        )

        self.create_gt_job(admin_user, task["id"])

        rq_id = self._initialize_report_creation(task["id"], task_staff["username"])

        with make_api_client(admin["username"]) as api_client:
            (_, response) = api_client.quality_api.create_report(rq_id=rq_id, _parse_response=False)
            assert response.status in {HTTPStatus.ACCEPTED, HTTPStatus.CREATED}


class TestSimpleQualityReportsFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, quality_reports, jobs, tasks):
        self.user = admin_user
        self.samples = quality_reports
        self.job_samples = jobs
        self.task_samples = tasks

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.quality_api.list_reports_endpoint

    def _get_field_samples(self, field: str) -> tuple[Any, list[dict[str, Any]]]:
        if field == "task_id":
            # This filter includes both the task and nested job reports
            task_id, task_reports = super()._get_field_samples(field)
            task_job_ids = set(j["id"] for j in self.job_samples if j["task_id"] == task_id)
            task_reports = list(task_reports) + [
                r
                for r in self.samples
                if self._get_field(r, self._map_field("job_id")) in task_job_ids
            ]
            return task_id, task_reports
        elif field == "org_id":
            org_id = self.task_samples[
                next(
                    s
                    for s in self.samples
                    if s["task_id"] and self.task_samples[s["task_id"]]["organization"]
                )["task_id"]
            ]["organization"]
            return org_id, [
                s
                for s in self.samples
                if s["job_id"]
                and self.job_samples[s["job_id"]]["organization"] == org_id
                or s["task_id"]
                and self.task_samples[s["task_id"]]["organization"] == org_id
            ]
        else:
            return super()._get_field_samples(field)

    @pytest.mark.parametrize(
        "field",
        ("task_id", "job_id", "parent_id", "target", "org_id"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListQualityConflicts(_PermissionTestBase):
    def _test_list_conflicts_200(self, user, report_id, *, expected_data=None, **kwargs):
        with make_api_client(user) as api_client:
            results = get_paginated_collection(
                api_client.quality_api.list_conflicts_endpoint,
                return_json=True,
                report_id=report_id,
                **kwargs,
            )

            if expected_data is not None:
                assert DeepDiff(expected_data, results) == {}

        return results

    def _test_list_conflicts_403(self, user, report_id, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.list_conflicts(
                report_id=report_id, **kwargs, _parse_response=False, _check_status=False
            )

            assert response.status == HTTPStatus.FORBIDDEN

    def test_can_list_job_report_conflicts(self, admin_user, quality_reports, quality_conflicts):
        report = next(r for r in quality_reports if r["job_id"])
        conflicts = [c for c in quality_conflicts if c["report_id"] == report["id"]]

        self._test_list_conflicts_200(admin_user, report["id"], expected_data=conflicts)

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_list_conflicts_in_sandbox_task(
        self, is_staff, allow, admin_user, find_sandbox_task_without_gt
    ):
        task, user = find_sandbox_task_without_gt(is_staff)

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])
        conflicts = self._test_list_conflicts_200(admin_user, report_id=report["id"])
        assert conflicts

        if allow:
            self._test_list_conflicts_200(user["username"], report["id"], expected_data=conflicts)
        else:
            self._test_list_conflicts_403(user["username"], report["id"])

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_list_conflicts_in_org_task(
        self,
        find_org_task_without_gt,
        org_role,
        is_staff,
        allow,
        admin_user,
    ):
        task, user = find_org_task_without_gt(is_staff, org_role)
        user = user["username"]

        self.create_gt_job(admin_user, task["id"])
        report = self.create_quality_report(admin_user, task["id"])
        conflicts = self._test_list_conflicts_200(admin_user, report_id=report["id"])
        assert conflicts

        if allow:
            self._test_list_conflicts_200(user, report["id"], expected_data=conflicts)
        else:
            self._test_list_conflicts_403(user, report["id"])


class TestSimpleQualityConflictsFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(
        self, restore_db_per_class, admin_user, quality_conflicts, quality_reports, jobs, tasks
    ):
        self.user = admin_user
        self.samples = quality_conflicts
        self.report_samples = quality_reports
        self.task_samples = tasks
        self.job_samples = jobs

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.quality_api.list_conflicts_endpoint

    def _get_field_samples(self, field: str) -> tuple[Any, list[dict[str, Any]]]:
        if field == "job_id":
            # This field is not included in the response
            job_id = self._find_valid_field_value(self.report_samples, field_path=["job_id"])
            job_reports = set(r["id"] for r in self.report_samples if r["job_id"] == job_id)
            job_conflicts = [
                c
                for c in self.samples
                if self._get_field(c, self._map_field("report_id")) in job_reports
            ]
            return job_id, job_conflicts
        elif field == "task_id":
            # This field is not included in the response
            task_id = self._find_valid_field_value(self.report_samples, field_path=["task_id"])
            task_reports = [r for r in self.report_samples if r["task_id"] == task_id]
            task_report_ids = {r["id"] for r in task_reports}
            task_report_ids |= {
                r["id"] for r in self.report_samples if r["parent_id"] in task_report_ids
            }
            task_conflicts = [
                c
                for c in self.samples
                if self._get_field(c, self._map_field("report_id")) in task_report_ids
            ]
            return task_reports[0]["task_id"], task_conflicts
        elif field == "org_id":
            org_id = self.task_samples[
                next(
                    s
                    for s in self.report_samples
                    if s["task_id"] and self.task_samples[s["task_id"]]["organization"]
                )["task_id"]
            ]["organization"]
            report_ids = set(
                s["id"]
                for s in self.report_samples
                if s["job_id"]
                and self.job_samples[s["job_id"]]["organization"] == org_id
                or s["task_id"]
                and self.task_samples[s["task_id"]]["organization"] == org_id
            )
            return org_id, [c for c in self.samples if c["report_id"] in report_ids]
        else:
            return super()._get_field_samples(field)

    @pytest.mark.parametrize(
        "field",
        ("report_id", "severity", "type", "frame", "job_id", "task_id", "org_id"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


class TestSimpleQualitySettingsFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, quality_settings):
        self.user = admin_user
        self.samples = quality_settings

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.quality_api.list_settings_endpoint

    @pytest.mark.parametrize("field", ("task_id",))
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestListSettings(_PermissionTestBase):
    def _test_list_settings_200(
        self, user: str, task_id: int, *, expected_data: Optional[dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            actual = get_paginated_collection(
                api_client.quality_api.list_settings_endpoint,
                task_id=task_id,
                **kwargs,
                return_json=True,
            )

        if expected_data is not None:
            assert DeepDiff(expected_data, actual, ignore_order=True) == {}

    def _test_list_settings_403(self, user: str, task_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.list_settings(
                task_id=task_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_list_settings_in_sandbox(
        self, quality_settings, find_sandbox_task, is_staff, allow
    ):
        task, user = find_sandbox_task(is_staff)

        settings = [s for s in quality_settings if s["task_id"] == task["id"]]

        if allow:
            self._test_list_settings_200(
                user["username"], task_id=task["id"], expected_data=settings
            )
        else:
            self._test_list_settings_403(user["username"], task_id=task["id"])

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_list_settings_in_org_task(
        self,
        find_org_task,
        org_role,
        is_staff,
        allow,
        quality_settings,
    ):
        task, user = find_org_task(is_staff, org_role)

        settings = [s for s in quality_settings if s["task_id"] == task["id"]]
        org_id = task["organization"]

        if allow:
            self._test_list_settings_200(
                user["username"], task_id=task["id"], expected_data=settings, org_id=org_id
            )
        else:
            self._test_list_settings_403(user["username"], task_id=task["id"], org_id=org_id)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetSettings(_PermissionTestBase):
    def _test_get_settings_200(
        self, user: str, obj_id: int, *, expected_data: Optional[dict[str, Any]] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_settings(obj_id, **kwargs)
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_get_settings_403(self, user: str, obj_id: int, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.retrieve_settings(
                obj_id, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    def test_can_get_settings(self, admin_user, quality_settings):
        settings = next(iter(quality_settings))
        settings_id = settings["id"]
        self._test_get_settings_200(admin_user, settings_id, expected_data=settings)

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_get_settings_in_sandbox_task(
        self, quality_settings, find_sandbox_task, is_staff, allow
    ):
        task, user = find_sandbox_task(is_staff)

        settings = next(s for s in quality_settings if s["task_id"] == task["id"])
        settings_id = settings["id"]

        if allow:
            self._test_get_settings_200(user["username"], settings_id, expected_data=settings)
        else:
            self._test_get_settings_403(user["username"], settings_id)

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_get_settings_in_org_task(
        self,
        find_org_task,
        org_role,
        is_staff,
        allow,
        quality_settings,
    ):
        task, user = find_org_task(is_staff, org_role)

        settings = next(s for s in quality_settings if s["task_id"] == task["id"])
        settings_id = settings["id"]

        if allow:
            self._test_get_settings_200(user["username"], settings_id, expected_data=settings)
        else:
            self._test_get_settings_403(user["username"], settings_id)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchSettings(_PermissionTestBase):
    def _test_patch_settings_200(
        self,
        user: str,
        obj_id: int,
        data: dict[str, Any],
        *,
        expected_data: Optional[dict[str, Any]] = None,
        **kwargs,
    ):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.partial_update_settings(
                obj_id, patched_quality_settings_request=data, **kwargs
            )
            assert response.status == HTTPStatus.OK

        if expected_data is not None:
            assert DeepDiff(expected_data, json.loads(response.data), ignore_order=True) == {}

        return response

    def _test_patch_settings_403(self, user: str, obj_id: int, data: dict[str, Any], **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.quality_api.partial_update_settings(
                obj_id,
                patched_quality_settings_request=data,
                **kwargs,
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    def _get_request_data(self, data: dict[str, Any]) -> tuple[dict[str, Any], dict[str, Any]]:
        patched_data = deepcopy(data)

        for field, value in data.items():
            if isinstance(value, bool):
                patched_data[field] = not value
            elif isinstance(value, float):
                patched_data[field] = 1 - value

        expected_data = deepcopy(patched_data)

        return patched_data, expected_data

    def test_can_patch_settings(self, admin_user, quality_settings):
        settings = next(iter(quality_settings))
        settings_id = settings["id"]
        data, expected_data = self._get_request_data(settings)
        self._test_patch_settings_200(admin_user, settings_id, data, expected_data=expected_data)

    @pytest.mark.parametrize(*_PermissionTestBase._default_sandbox_cases)
    def test_user_patch_settings_in_sandbox_task(
        self, quality_settings, find_sandbox_task, is_staff, allow
    ):
        task, user = find_sandbox_task(is_staff)

        settings = next(s for s in quality_settings if s["task_id"] == task["id"])
        settings_id = settings["id"]
        data, expected_data = self._get_request_data(settings)

        if allow:
            self._test_patch_settings_200(
                user["username"], settings_id, data, expected_data=expected_data
            )
        else:
            self._test_patch_settings_403(user["username"], settings_id, data)

    @pytest.mark.parametrize(*_PermissionTestBase._default_org_cases)
    def test_user_patch_settings_in_org_task(
        self,
        find_org_task,
        org_role,
        is_staff,
        allow,
        quality_settings,
    ):
        task, user = find_org_task(is_staff, org_role)

        settings = next(s for s in quality_settings if s["task_id"] == task["id"])
        settings_id = settings["id"]
        data, expected_data = self._get_request_data(settings)

        if allow:
            self._test_patch_settings_200(
                user["username"], settings_id, data, expected_data=expected_data
            )
        else:
            self._test_patch_settings_403(user["username"], settings_id, data)


@pytest.mark.usefixtures("restore_db_per_function")
class TestQualityReportMetrics(_PermissionTestBase):
    demo_task_id = 22  # this task reproduces all the checkable cases
    demo_task_id_multiple_jobs = 23  # this task reproduces cases for multiple jobs

    @pytest.mark.parametrize("task_id", [demo_task_id])
    def test_report_summary(self, task_id, tasks, jobs, quality_reports):
        gt_job = next(j for j in jobs if j["task_id"] == task_id and j["type"] == "ground_truth")
        task = tasks[task_id]
        report = next(r for r in quality_reports if r["task_id"] == task_id)

        summary = report["summary"]
        assert 0 < summary["conflict_count"]
        assert all(summary["conflicts_by_type"].values())
        assert summary["conflict_count"] == sum(summary["conflicts_by_type"].values())
        assert summary["conflict_count"] == summary["warning_count"] + summary["error_count"]
        assert 0 < summary["valid_count"]
        assert summary["valid_count"] < summary["ds_count"]
        assert summary["valid_count"] < summary["gt_count"]
        assert summary["frame_count"] == gt_job["frame_count"]
        assert summary["frame_share"] == summary["frame_count"] / task["size"]

    def test_unmodified_task_produces_the_same_metrics(self, admin_user, quality_reports):
        old_report = max(
            (r for r in quality_reports if r["task_id"] == self.demo_task_id), key=lambda r: r["id"]
        )
        task_id = old_report["task_id"]

        new_report = self.create_quality_report(admin_user, task_id)

        with make_api_client(admin_user) as api_client:
            (old_report_data, _) = api_client.quality_api.retrieve_report_data(old_report["id"])
            (new_report_data, _) = api_client.quality_api.retrieve_report_data(new_report["id"])

        assert (
            DeepDiff(
                new_report,
                old_report,
                ignore_order=True,
                exclude_paths=["root['created_date']", "root['id']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                new_report_data,
                old_report_data,
                ignore_order=True,
                exclude_paths=["root['created_date']", "root['id']"],
            )
            == {}
        )

    def test_modified_task_produces_different_metrics(
        self, admin_user, quality_reports, jobs, labels
    ):
        gt_job = next(
            j for j in jobs if j["type"] == "ground_truth" and j["task_id"] == self.demo_task_id
        )
        task_id = gt_job["task_id"]
        old_report = max(
            (r for r in quality_reports if r["task_id"] == task_id), key=lambda r: r["id"]
        )
        job_labels = [
            l
            for l in labels
            if l.get("task_id") == task_id
            or gt_job.get("project_id")
            and l.get("project_id") == gt_job.get("project_id")
            if not l["parent_id"]
        ]

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update_annotations(
                "update",
                gt_job["id"],
                patched_labeled_data_request=dict(
                    shapes=[
                        dict(
                            frame=gt_job["start_frame"],
                            label_id=job_labels[0]["id"],
                            type="rectangle",
                            points=[1, 1, 2, 2],
                        ),
                    ],
                ),
            )

        new_report = self.create_quality_report(admin_user, task_id)
        assert new_report["summary"]["conflict_count"] > old_report["summary"]["conflict_count"]

    @pytest.mark.parametrize("task_id", [demo_task_id])
    @pytest.mark.parametrize(
        "parameter",
        [
            "check_covered_annotations",
            "compare_attributes",
            "compare_groups",
            "group_match_threshold",
            "iou_threshold",
            "line_orientation_threshold",
            "line_thickness",
            "low_overlap_threshold",
            "object_visibility_threshold",
            "oks_sigma",
            "compare_line_orientation",
            "panoptic_comparison",
            "point_size_base",
            "empty_is_annotated",
        ],
    )
    def test_settings_affect_metrics(
        self, admin_user, quality_reports, quality_settings, task_id, parameter
    ):
        old_report = max(
            (r for r in quality_reports if r["task_id"] == task_id), key=lambda r: r["id"]
        )
        task_id = old_report["task_id"]

        settings = deepcopy(next(s for s in quality_settings if s["task_id"] == task_id))
        if isinstance(settings[parameter], bool):
            settings[parameter] = not settings[parameter]
        elif isinstance(settings[parameter], float):
            settings[parameter] = 1 - settings[parameter]
            if parameter == "group_match_threshold":
                settings[parameter] = 0.9
        elif parameter == "point_size_base":
            settings[parameter] = next(
                v
                for v in models.PointSizeBaseEnum.allowed_values[("value",)].values()
                if v != settings[parameter]
            )
        else:
            assert False

        with make_api_client(admin_user) as api_client:
            api_client.quality_api.partial_update_settings(
                settings["id"], patched_quality_settings_request=settings
            )

        new_report = self.create_quality_report(admin_user, task_id)
        if parameter == "empty_is_annotated":
            assert new_report["summary"]["valid_count"] != old_report["summary"]["valid_count"]
            assert new_report["summary"]["total_count"] != old_report["summary"]["total_count"]
            assert new_report["summary"]["ds_count"] != old_report["summary"]["ds_count"]
            assert new_report["summary"]["gt_count"] != old_report["summary"]["gt_count"]
        else:
            assert (
                new_report["summary"]["conflict_count"] != old_report["summary"]["conflict_count"]
            )

    def test_old_report_can_be_loaded(self, admin_user, quality_reports):
        report = min((r for r in quality_reports if r["task_id"]), key=lambda r: r["id"])
        assert report["created_date"] < "2024"

        with make_api_client(admin_user) as api_client:
            (report_data, _) = api_client.quality_api.retrieve_report_data(report["id"])

        # This report should have been created before the Jaccard index was included.
        for d in [report_data["comparison_summary"], *report_data["frame_results"].values()]:
            assert d["annotations"]["confusion_matrix"]["jaccard_index"] is None

    def test_accumulation_annotation_conflicts_multiple_jobs(self, admin_user):
        report = self.create_quality_report(admin_user, self.demo_task_id_multiple_jobs)
        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK
        report_data = json.loads(response.data)
        task_confusion_matrix = report_data["comparison_summary"]["annotations"][
            "confusion_matrix"
        ]["rows"]

        expected_frame_confusion_matrix = {
            "5": [[1, 0, 0], [0, 0, 0], [0, 0, 0]],
            "7": [[1, 0, 0], [0, 0, 0], [0, 0, 0]],
            "4": [[0, 0, 1], [0, 0, 0], [1, 0, 0]],
        }
        for frame_id in report_data["frame_results"].keys():
            assert (
                report_data["frame_results"][frame_id]["annotations"]["confusion_matrix"]["rows"]
                == expected_frame_confusion_matrix[frame_id]
            )

        assert task_confusion_matrix == [[2, 0, 1], [0, 0, 0], [1, 0, 0]]

    @pytest.mark.parametrize("task_id", [8])
    def test_can_compute_quality_if_non_skeleton_label_follows_skeleton_label(
        self, admin_user, labels, task_id
    ):
        new_label_name = "non_skeleton"
        with make_api_client(admin_user) as api_client:
            task_labels = [label for label in labels if label.get("task_id") == task_id]
            assert any(label["type"] == "skeleton" for label in task_labels)
            task_labels += [{"name": new_label_name, "type": "any"}]
            api_client.tasks_api.partial_update(
                task_id,
                patched_task_write_request=models.PatchedTaskWriteRequest(labels=task_labels),
            )

            new_label_obj, _ = api_client.labels_api.list(task_id=task_id, name=new_label_name)
            new_label_id = new_label_obj.results[0].id
            api_client.tasks_api.update_annotations(
                task_id,
                task_annotations_update_request={
                    "shapes": [
                        models.LabeledShapeRequest(
                            type="rectangle",
                            frame=0,
                            label_id=new_label_id,
                            points=[0, 0, 1, 1],
                        )
                    ]
                },
            )

        self.create_gt_job(admin_user, task_id)

        report = self.create_quality_report(admin_user, task_id)
        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK

    def test_excluded_gt_job_frames_are_not_included_in_honeypot_task_quality_report(
        self, admin_user, tasks, jobs
    ):
        task_id = next(t["id"] for t in tasks if t["validation_mode"] == "gt_pool")
        gt_job = next(j for j in jobs if j["task_id"] == task_id if j["type"] == "ground_truth")
        gt_job_frames = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)

        with make_api_client(admin_user) as api_client:
            gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            gt_frame_names = [f.name for f in gt_job_meta.frames]

            task_meta, _ = api_client.tasks_api.retrieve_data_meta(task_id)
            honeypot_frames = [
                i
                for i, f in enumerate(task_meta.frames)
                if f.name in gt_frame_names and i not in gt_job_frames
            ]
            gt_frame_uses = {
                name: (gt_job["start_frame"] + gt_frame_names.index(name), list(ids))
                for name, ids in groupby(
                    sorted(
                        [
                            i
                            for i in range(task_meta.size)
                            if task_meta.frames[i].name in gt_frame_names
                        ],
                        key=lambda i: task_meta.frames[i].name,
                    ),
                    key=lambda i: task_meta.frames[i].name,
                )
            }

            api_client.jobs_api.partial_update(
                gt_job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(
                    stage="acceptance", state="completed"
                ),
            )
            report = self.create_quality_report(admin_user, task_id)

            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK
            assert honeypot_frames == json.loads(response.data)["comparison_summary"]["frames"]

            excluded_gt_frame, excluded_gt_frame_honeypots = next(
                (i, honeypots) for i, honeypots in gt_frame_uses.values() if len(honeypots) > 1
            )
            api_client.jobs_api.partial_update_data_meta(
                gt_job["id"],
                patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                    deleted_frames=[excluded_gt_frame]
                ),
            )

            report = self.create_quality_report(admin_user, task_id)

            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK
            assert [
                v for v in honeypot_frames if v not in excluded_gt_frame_honeypots
            ] == json.loads(response.data)["comparison_summary"]["frames"]

    @pytest.mark.parametrize("task_id", [23])
    def test_excluded_gt_job_frames_are_not_included_in_simple_gt_job_task_quality_report(
        self, admin_user, task_id: int, jobs
    ):
        gt_job = next(j for j in jobs if j["task_id"] == task_id if j["type"] == "ground_truth")

        with make_api_client(admin_user) as api_client:
            gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            gt_frames = [
                (f - gt_job_meta.start_frame) // parse_frame_step(gt_job_meta.frame_filter)
                for f in gt_job_meta.included_frames
            ]

            api_client.jobs_api.partial_update(
                gt_job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(
                    stage="acceptance", state="completed"
                ),
            )
            report = self.create_quality_report(admin_user, task_id)

            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK
            assert gt_frames == json.loads(response.data)["comparison_summary"]["frames"]

            excluded_gt_frame = gt_frames[0]
            api_client.jobs_api.partial_update_data_meta(
                gt_job["id"],
                patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                    deleted_frames=[excluded_gt_frame]
                ),
            )

            report = self.create_quality_report(admin_user, task_id)

            (_, response) = api_client.quality_api.retrieve_report_data(report["id"])
            assert response.status == HTTPStatus.OK
            assert [f for f in gt_frames if f != excluded_gt_frame] == json.loads(response.data)[
                "comparison_summary"
            ]["frames"]

    def test_quality_metrics_in_task_with_gt_and_tracks(
        self,
        admin_user,
        tasks,
        labels,
    ):
        task_id = next(
            t["id"]
            for t in tasks
            if not t["validation_mode"] and t["size"] >= 5 and not t["project_id"]
        )
        label_id = next(l["id"] for l in labels if l.get("task_id") == task_id)

        with make_api_client(admin_user) as api_client:
            gt_frames = [1, 3]
            gt_job = api_client.jobs_api.create(
                job_write_request=models.JobWriteRequest(
                    type="ground_truth",
                    task_id=task_id,
                    frame_selection_method="manual",
                    frames=gt_frames,
                )
            )[0]

            gt_annotations = {
                "shapes": [
                    {
                        "frame": 1,
                        "label_id": label_id,
                        "points": [0.5, 1.5, 2.5, 3.5],
                        "rotation": 0,
                        "type": "rectangle",
                        "occluded": False,
                        "outside": False,
                        "attributes": [],
                    },
                    {
                        "frame": 3,
                        "label_id": label_id,
                        "points": [3.0, 4.0, 5.0, 6.0],
                        "rotation": 0,
                        "type": "rectangle",
                        "occluded": False,
                        "outside": False,
                        "attributes": [],
                    },
                ]
            }

            normal_annotations = {
                "tracks": [
                    {
                        "type": "rectangle",
                        "frame": 0,
                        "label_id": label_id,
                        "shapes": [
                            {
                                "frame": 0,
                                "points": [1.0, 2.0, 3.0, 4.0],
                                "rotation": 0,
                                "type": "rectangle",
                                "occluded": False,
                                "outside": False,
                                "attributes": [],
                            },
                            {
                                "frame": 2,  # not included, but must affect interpolation
                                "points": [0.0, 1.0, 2.0, 3.0],
                                "rotation": 0,
                                "type": "rectangle",
                                "occluded": False,
                                "outside": False,
                                "attributes": [],
                            },
                            {
                                "frame": 4,
                                "points": [6.0, 7.0, 8.0, 9.0],
                                "rotation": 0,
                                "type": "rectangle",
                                "occluded": False,
                                "outside": False,
                                "attributes": [],
                            },
                        ],
                    }
                ]
            }

            api_client.jobs_api.update_annotations(
                gt_job.id, job_annotations_update_request=gt_annotations
            )

            api_client.tasks_api.update_annotations(
                task_id, task_annotations_update_request=normal_annotations
            )

            api_client.jobs_api.partial_update(
                gt_job.id,
                patched_job_write_request=models.PatchedJobWriteRequest(
                    stage="acceptance", state="completed"
                ),
            )

            report = self.create_quality_report(admin_user, task_id)

            assert report["summary"]["conflict_count"] == 0
            assert report["summary"]["valid_count"] == 2
            assert report["summary"]["total_count"] == 2


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_queues.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


import os
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from http import HTTPStatus

import pytest

import shared.utils.s3 as s3
from shared.utils.config import make_api_client
from shared.utils.helpers import generate_image_file

from .utils import create_task


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_cvat_data_per_function")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestRQQueueWorking:
    _USER_1 = "admin1"
    _USER_2 = "admin2"

    @pytest.mark.with_external_services
    @pytest.mark.timeout(60)
    @pytest.mark.skipif(
        os.getenv("ONE_RUNNING_JOB_IN_QUEUE_PER_USER", "false").lower() not in {"true", "yes", "1"},
        reason="The server is not configured to enable limit 1 user 1 task at a time",
    )
    @pytest.mark.parametrize("cloud_storage_id", [2])
    def test_user_cannot_clog_import_queue_with_his_tasks(
        self, cloud_storage_id: int, cloud_storages, request
    ):
        def _create_task(idx: int, username: str) -> int:
            task_spec = {
                "name": f"Test task {idx}",
                "labels": [
                    {
                        "name": "car",
                    }
                ],
            }

            task_data = {
                "image_quality": 90,
                "server_files": ["dataset/"],
                "cloud_storage_id": cloud_storage_id,
                "use_cache": False,
            }

            task_id, _ = create_task(username, task_spec, task_data)
            return task_id

        cs_name = cloud_storages[cloud_storage_id]["resource"]
        s3_client = s3.make_client(bucket=cs_name)
        dataset_size = 100

        img_content = generate_image_file(size=(1920, 1080)).getvalue()

        for i in range(dataset_size):
            filename = f"dataset/image_{i}.jpeg"
            s3_client.create_file(filename=filename, data=img_content)
            request.addfinalizer(
                partial(
                    s3_client.remove_file,
                    filename=filename,
                )
            )

        number_of_tasks = 4
        users = [self._USER_1] * (number_of_tasks - 1)
        users.append(self._USER_2)

        futures, task_ids = [], []

        with ThreadPoolExecutor(max_workers=number_of_tasks) as executor:
            for idx, user in enumerate(users):
                futures.append(executor.submit(_create_task, idx, user))

            for future in futures:
                task_ids.append(future.result())

        tasks = []

        for idx, task_id in enumerate(task_ids):
            with make_api_client(users[idx]) as api_client:
                (task, response) = api_client.tasks_api.retrieve(task_id)
            assert response.status == HTTPStatus.OK
            tasks.append(task)

        sorted_tasks = sorted(tasks, key=lambda x: x.updated_date)
        assert self._USER_2 in [t.owner.username for t in sorted_tasks[:2]]


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_remote_url.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from http import HTTPStatus
from time import sleep
from typing import Any

import pytest

from shared.utils.config import get_method, post_method


def _post_task_remote_data(username, task_id, resources):
    data = {
        "remote_files": resources,
        "image_quality": 30,
    }

    return post_method(username, f"tasks/{task_id}/data", data)


def _wait_until_task_is_created(username: str, rq_id: str) -> dict[str, Any]:
    url = f"requests/{rq_id}"

    for _ in range(100):
        response = get_method(username, url)
        request_details = response.json()
        if request_details["status"] in ("finished", "failed"):
            return request_details
        sleep(1)
    raise Exception("Cannot create task")


@pytest.mark.usefixtures("restore_db_per_function")
class TestCreateFromRemote:
    task_id = 12

    def _test_can_create(self, user, task_id, resources):
        response = _post_task_remote_data(user, task_id, resources)
        assert response.status_code == HTTPStatus.ACCEPTED
        response = response.json()
        rq_id = response.get("rq_id")
        assert rq_id, "The rq_id param was not found in the server response"

        response_json = _wait_until_task_is_created(user, rq_id)
        assert response_json["status"] == "finished"

    def _test_cannot_create(self, user, task_id, resources):
        response = _post_task_remote_data(user, task_id, resources)
        assert response.status_code == HTTPStatus.ACCEPTED
        response = response.json()
        rq_id = response.get("rq_id")
        assert rq_id, "The rq_id param was not found in the server response"

        response_json = _wait_until_task_is_created(user, rq_id)
        assert response_json["status"] == "failed"

    def test_cannot_create(self, find_users):
        user = find_users(privilege="admin")[0]["username"]
        remote_resources = ["http://localhost/favicon.ico"]

        self._test_cannot_create(user, self.task_id, remote_resources)

    def test_can_create(self, find_users):
        user = find_users(privilege="admin")[0]["username"]
        remote_resources = ["https://docs.cvat.ai/favicons/favicon-32x32.png"]

        self._test_can_create(user, self.task_id, remote_resources)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_requests.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import json
from http import HTTPStatus
from urllib.parse import urlparse

import pytest
from cvat_sdk.api_client import ApiClient, models
from cvat_sdk.api_client.api_client import Endpoint
from cvat_sdk.core.helpers import get_paginated_collection

from shared.fixtures.data import Container
from shared.fixtures.init import docker_exec_redis_inmem, kube_exec_redis_inmem
from shared.utils.config import make_api_client
from shared.utils.helpers import generate_image_files

from .utils import (
    CollectionSimpleFilterTestBase,
    create_task,
    export_job_dataset,
    export_project_backup,
    export_project_dataset,
    export_task_backup,
    export_task_dataset,
    import_task_backup,
)


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.timeout(30)
class TestRequestsListFilters(CollectionSimpleFilterTestBase):

    field_lookups = {
        "target": ["operation", "target"],
        "subresource": ["operation", "type", lambda x: x.split(":")[1]],
        "action": ["operation", "type", lambda x: x.split(":")[0]],
        "project_id": ["operation", "project_id"],
        "task_id": ["operation", "task_id"],
        "job_id": ["operation", "job_id"],
        "format": ["operation", "format"],
    }

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.requests_api.list_endpoint

    @pytest.fixture(autouse=True)
    def setup(self, find_users):
        self.user = find_users(privilege="user")[0]["username"]

    @pytest.fixture
    def fxt_resources_ids(self):
        with make_api_client(self.user) as api_client:
            project_ids = [
                api_client.projects_api.create(
                    {"name": f"Test project {idx + 1}", "labels": [{"name": "car"}]}
                )[0].id
                for idx in range(3)
            ]

            task_ids = [
                create_task(
                    self.user,
                    spec={"name": f"Test task {idx + 1}", "labels": [{"name": "car"}]},
                    data={
                        "image_quality": 75,
                        "client_files": generate_image_files(2),
                        "segment_size": 1,
                    },
                )[0]
                for idx in range(3)
            ]

            job_ids = []
            for task_id in task_ids:
                jobs, _ = api_client.jobs_api.list(task_id=task_id)
                job_ids.extend([j.id for j in jobs.results])

        return project_ids, task_ids, job_ids

    @pytest.fixture
    def fxt_make_requests(
        self,
        fxt_make_export_project_requests,
        fxt_make_export_task_requests,
        fxt_make_export_job_requests,
        fxt_download_file,
    ):
        def _make_requests(project_ids: list[int], task_ids: list[int], job_ids: list[int]):
            # make requests to export projects|tasks|jobs annotations|datasets|backups
            fxt_make_export_project_requests(project_ids[1:])
            fxt_make_export_task_requests(task_ids[1:])
            fxt_make_export_job_requests(job_ids[1:])

            # make requests to download files and then import them
            for resource_type, first_resource in zip(
                ("project", "task", "job"), (project_ids[0], task_ids[0], job_ids[0])
            ):
                for subresource in ("dataset", "annotations", "backup"):
                    if resource_type == "job" and subresource == "backup":
                        continue

                    data = fxt_download_file(resource_type, first_resource, subresource)

                    tmp_file = io.BytesIO(data)
                    tmp_file.name = f"{resource_type}_{subresource}.zip"

                    if resource_type == "task" and subresource == "backup":
                        import_task_backup(
                            self.user,
                            data={
                                "task_file": tmp_file,
                            },
                        )

            empty_file = io.BytesIO(b"empty_file")
            empty_file.name = "empty.zip"

            # import corrupted backup
            import_task_backup(
                self.user,
                data={
                    "task_file": empty_file,
                },
            )

        return _make_requests

    @pytest.fixture
    def fxt_download_file(self):
        def download_file(resource: str, rid: int, subresource: str):
            func = {
                ("project", "dataset"): lambda *args, **kwargs: export_project_dataset(
                    *args, **kwargs, save_images=True
                ),
                ("project", "annotations"): lambda *args, **kwargs: export_project_dataset(
                    *args, **kwargs, save_images=False
                ),
                ("project", "backup"): export_project_backup,
                ("task", "dataset"): lambda *args, **kwargs: export_task_dataset(
                    *args, **kwargs, save_images=True
                ),
                ("task", "annotations"): lambda *args, **kwargs: export_task_dataset(
                    *args, **kwargs, save_images=False
                ),
                ("task", "backup"): export_task_backup,
                ("job", "dataset"): lambda *args, **kwargs: export_job_dataset(
                    *args, **kwargs, save_images=True
                ),
                ("job", "annotations"): lambda *args, **kwargs: export_job_dataset(
                    *args, **kwargs, save_images=False
                ),
            }[(resource, subresource)]

            data = func(self.user, id=rid, download_result=True)
            assert data, f"Failed to download {resource} {subresource} locally"
            return data

        return download_file

    @pytest.fixture
    def fxt_make_export_project_requests(self):
        def make_requests(project_ids: list[int]):
            for project_id in project_ids:
                export_project_backup(self.user, id=project_id, download_result=False)
                export_project_dataset(
                    self.user, save_images=True, id=project_id, download_result=False
                )
                export_project_dataset(
                    self.user,
                    save_images=False,
                    id=project_id,
                    download_result=False,
                )

        return make_requests

    @pytest.fixture
    def fxt_make_export_task_requests(self):
        def make_requests(task_ids: list[int]):
            for task_id in task_ids:
                export_task_backup(self.user, id=task_id, download_result=False)
                export_task_dataset(self.user, save_images=True, id=task_id, download_result=False)
                export_task_dataset(self.user, save_images=False, id=task_id, download_result=False)

        return make_requests

    @pytest.fixture
    def fxt_make_export_job_requests(self):
        def make_requests(job_ids: list[int]):
            for job_id in job_ids:
                export_job_dataset(
                    self.user,
                    save_images=True,
                    id=job_id,
                    format="COCO 1.0",
                    download_result=False,
                )
                export_job_dataset(
                    self.user,
                    save_images=False,
                    id=job_id,
                    format="YOLO 1.1",
                    download_result=False,
                )

        return make_requests

    @pytest.mark.parametrize(
        "simple_filter, values",
        [
            ("subresource", ["annotations", "dataset", "backup"]),
            ("action", ["create", "export", "import"]),
            ("status", ["finished", "failed"]),
            ("project_id", []),
            ("task_id", []),
            ("job_id", []),
            ("format", ["CVAT for images 1.1", "COCO 1.0", "YOLO 1.1"]),
            ("target", ["project", "task", "job"]),
        ],
    )
    def test_can_use_simple_filter_for_object_list(
        self, simple_filter: str, values: list, fxt_resources_ids, fxt_make_requests
    ):
        project_ids, task_ids, job_ids = fxt_resources_ids
        fxt_make_requests(project_ids, task_ids, job_ids)

        if simple_filter in ("project_id", "task_id", "job_id"):
            # check last project|task|job
            if simple_filter == "project_id":
                values = project_ids[-1:]
            elif simple_filter == "task_id":
                values = task_ids[-1:]
            else:
                values = job_ids[-1:]

        with make_api_client(self.user) as api_client:
            self.samples = get_paginated_collection(
                self._get_endpoint(api_client), return_json=True
            )

        return super()._test_can_use_simple_filter_for_object_list(simple_filter, values)

    def test_list_requests_when_there_is_job_with_non_regular_or_corrupted_meta(
        self, jobs: Container, admin_user: str, request: pytest.FixtureRequest
    ):
        job = next(iter(jobs))

        export_job_dataset(admin_user, save_images=True, id=job["id"], download_result=False)
        export_job_dataset(admin_user, save_images=False, id=job["id"], download_result=False)

        with make_api_client(admin_user) as api_client:
            background_requests, response = api_client.requests_api.list(_check_status=False)
            assert response.status == HTTPStatus.OK
            assert 2 == background_requests.count

            corrupted_job, normal_job = background_requests.results

            remove_meta_command = f'redis-cli -e HDEL rq:job:{corrupted_job["id"]} meta'

            if request.config.getoption("--platform") == "local":
                stdout, _ = docker_exec_redis_inmem(["sh", "-c", remove_meta_command])
            else:
                stdout, _ = kube_exec_redis_inmem(
                    [
                        "sh",
                        "-c",
                        'export REDISCLI_AUTH="${REDIS_PASSWORD}" && ' + remove_meta_command,
                    ]
                )
            assert bool(int(stdout.strip()))

            _, response = api_client.requests_api.list(_check_status=False, _parse_response=False)
            assert response.status == HTTPStatus.OK

            background_requests = json.loads(response.data)
            assert 1 == background_requests["count"]
            assert normal_job.id == background_requests["results"][0]["id"]


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestGetRequests:

    def _test_get_request_200(self, api_client: ApiClient, rq_id: str, **kwargs) -> models.Request:
        (background_request, response) = api_client.requests_api.retrieve(rq_id, **kwargs)
        assert response.status == HTTPStatus.OK
        assert background_request.id == rq_id

        return background_request

    def _test_get_request_403(self, api_client: ApiClient, rq_id: str):
        (_, response) = api_client.requests_api.retrieve(
            rq_id, _parse_response=False, _check_status=False
        )
        assert response.status == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("format_name", ("CVAT for images 1.1",))
    @pytest.mark.parametrize("save_images", (True, False))
    def test_owner_can_retrieve_request(self, format_name: str, save_images: bool, projects):
        project = next(
            (
                p
                for p in projects
                if p["owner"] and (p["target_storage"] or {}).get("location") == "local"
            )
        )
        owner = project["owner"]

        subresource = "dataset" if save_images else "annotations"
        export_project_dataset(
            owner["username"],
            save_images=save_images,
            id=project["id"],
            download_result=False,
        )
        rq_id = f'export:project-{project["id"]}-{subresource}-in-{format_name.replace(" ", "_").replace(".", "@")}-format-by-{owner["id"]}'

        with make_api_client(owner["username"]) as owner_client:
            bg_request = self._test_get_request_200(owner_client, rq_id)

            assert (
                bg_request.created_date
                < bg_request.started_date
                < bg_request.finished_date
                < bg_request.expiry_date
            )
            assert bg_request.operation.format == format_name
            assert bg_request.operation.project_id == project["id"]
            assert bg_request.operation.target.value == "project"
            assert bg_request.operation.task_id is None
            assert bg_request.operation.job_id is None
            assert bg_request.operation.type == f"export:{subresource}"
            assert bg_request.owner.id == owner["id"]
            assert bg_request.owner.username == owner["username"]

            parsed_url = urlparse(bg_request.result_url)
            assert all([parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.query])

    @pytest.mark.parametrize("format_name", ("CVAT for images 1.1",))
    def test_non_owner_cannot_retrieve_request(self, find_users, projects, format_name: str):
        project = next(
            (
                p
                for p in projects
                if p["owner"] and (p["target_storage"] or {}).get("location") == "local"
            )
        )
        owner = project["owner"]
        malefactor = find_users(exclude_username=owner["username"])[0]

        export_project_dataset(
            owner["username"],
            save_images=True,
            id=project["id"],
            download_result=False,
        )
        rq_id = f'export:project-{project["id"]}-dataset-in-{format_name.replace(" ", "_").replace(".", "@")}-format-by-{owner["id"]}'

        with make_api_client(malefactor["username"]) as malefactor_client:
            self._test_get_request_403(malefactor_client, rq_id)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_resource_import_export.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from http import HTTPStatus

import pytest

from shared.utils.config import get_method, post_method
from shared.utils.resource_import_export import (
    _CloudStorageResourceTest,
    _make_export_resource_params,
    _make_import_resource_params,
)
from shared.utils.s3 import make_client as make_s3_client

from .utils import create_task

# https://docs.pytest.org/en/7.1.x/example/markers.html#marking-whole-classes-or-modules
pytestmark = [pytest.mark.with_external_services]


class _S3ResourceTest(_CloudStorageResourceTest):
    @staticmethod
    def _make_client():
        return make_s3_client()


@pytest.mark.usefixtures("restore_db_per_class")
class TestExportResourceToS3(_S3ResourceTest):
    @pytest.mark.usefixtures("restore_redis_inmem_per_function")
    @pytest.mark.parametrize("cloud_storage_id", [3])
    @pytest.mark.parametrize(
        "obj_id, obj, resource",
        [
            (2, "projects", "annotations"),
            (2, "projects", "dataset"),
            (2, "projects", "backup"),
            (11, "tasks", "annotations"),
            (11, "tasks", "dataset"),
            (11, "tasks", "backup"),
            (16, "jobs", "annotations"),
            (16, "jobs", "dataset"),
        ],
    )
    def test_save_resource_to_cloud_storage_with_specific_location(
        self, cloud_storage_id, obj_id, obj, resource, cloud_storages
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        kwargs = _make_export_resource_params(
            resource, is_default=False, obj=obj, cloud_storage_id=cloud_storage_id
        )

        self._export_resource(cloud_storage, obj_id, obj, resource, **kwargs)

    @pytest.mark.usefixtures("restore_redis_inmem_per_function")
    @pytest.mark.parametrize("user_type", ["admin", "assigned_supervisor_org_member"])
    @pytest.mark.parametrize(
        "obj_id, obj, resource",
        [
            (2, "projects", "annotations"),
            (2, "projects", "dataset"),
            (2, "projects", "backup"),
            (11, "tasks", "annotations"),
            (11, "tasks", "dataset"),
            (11, "tasks", "backup"),
            (16, "jobs", "annotations"),
            (16, "jobs", "dataset"),
        ],
    )
    def test_save_resource_to_cloud_storage_with_default_location(
        self,
        obj_id,
        obj,
        resource,
        user_type,
        projects,
        tasks,
        jobs,
        cloud_storages,
        users,
        is_project_staff,
        is_task_staff,
        is_job_staff,
        is_org_member,
    ):
        objects = {
            "projects": projects,
            "tasks": tasks,
            "jobs": jobs,
        }
        if obj in ("projects", "tasks"):
            cloud_storage_id = objects[obj][obj_id]["target_storage"]["cloud_storage_id"]
        else:
            task_id = jobs[obj_id]["task_id"]
            cloud_storage_id = tasks[task_id]["target_storage"]["cloud_storage_id"]
        cloud_storage = cloud_storages[cloud_storage_id]

        if user_type == "admin":
            user = self.user
        elif user_type == "assigned_supervisor_org_member":
            is_staff = {"projects": is_project_staff, "tasks": is_task_staff, "jobs": is_job_staff}[
                obj
            ]
            user = next(
                u
                for u in users
                if is_staff(u["id"], obj_id)
                if is_org_member(u["id"], cloud_storage["organization"], role="supervisor")
            )["username"]
        else:
            assert False

        kwargs = _make_export_resource_params(resource, obj=obj)
        kwargs["user"] = user

        self._export_resource(cloud_storage, obj_id, obj, resource, **kwargs)

    @pytest.mark.parametrize("storage_id", [3])
    @pytest.mark.parametrize(
        "obj, resource",
        [
            ("projects", "annotations"),
            ("projects", "dataset"),
            ("projects", "backup"),
            ("tasks", "annotations"),
            ("tasks", "dataset"),
            ("tasks", "backup"),
            ("jobs", "annotations"),
            ("jobs", "dataset"),
        ],
    )
    def test_user_cannot_export_to_cloud_storage_with_specific_location_without_access(
        self, storage_id, regular_lonely_user, obj, resource
    ):
        user = regular_lonely_user

        project_spec = {"name": "Test project"}
        project = post_method(user, "projects", project_spec).json()
        project_id = project["id"]

        task_spec = {
            "name": f"Task with files from foreign cloud storage {storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }
        data_spec = {
            "image_quality": 75,
            "use_cache": True,
            "server_files": ["images/image_1.jpg"],
            "project_id": project_id,
        }
        (task_id, _) = create_task(user, task_spec, data_spec)

        jobs = get_method(user, "jobs", task_id=task_id).json()["results"]
        job_id = jobs[0]["id"]

        if obj == "projects":
            obj_id = project_id
        elif obj == "tasks":
            obj_id = task_id
        elif obj == "jobs":
            obj_id = job_id
        else:
            assert False

        kwargs = _make_export_resource_params(
            resource, is_default=False, obj=obj, cloud_storage_id=storage_id
        )
        self._export_resource_to_cloud_storage(
            obj_id, obj, resource, user=user, _expect_status=HTTPStatus.FORBIDDEN, **kwargs
        )


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_cvat_data_per_function")
class TestImportResourceFromS3(_S3ResourceTest):
    @pytest.mark.usefixtures("restore_redis_inmem_per_function")
    @pytest.mark.parametrize("cloud_storage_id", [3])
    @pytest.mark.parametrize(
        "obj_id, obj, resource",
        [
            (2, "projects", "dataset"),
            (2, "projects", "backup"),
            (11, "tasks", "annotations"),
            (11, "tasks", "backup"),
            (16, "jobs", "annotations"),
        ],
    )
    def test_import_resource_from_cloud_storage_with_specific_location(
        self, cloud_storage_id, obj_id, obj, resource, cloud_storages
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        kwargs = _make_import_resource_params(
            resource, is_default=False, obj=obj, cloud_storage_id=cloud_storage_id
        )
        export_kwargs = _make_export_resource_params(
            resource, is_default=False, obj=obj, cloud_storage_id=cloud_storage_id
        )
        self._export_resource(cloud_storage, obj_id, obj, resource, **export_kwargs)
        self._import_resource(cloud_storage, resource, obj_id, obj, **kwargs)

    @pytest.mark.usefixtures("restore_redis_inmem_per_function")
    @pytest.mark.parametrize(
        "user_type",
        ["admin", "assigned_supervisor_org_member"],
    )
    @pytest.mark.parametrize(
        "obj_id, obj, resource",
        [
            (2, "projects", "dataset"),
            (11, "tasks", "annotations"),
            (16, "jobs", "annotations"),
        ],
    )
    def test_import_resource_from_cloud_storage_with_default_location(
        self,
        obj_id,
        obj,
        resource,
        user_type,
        projects,
        tasks,
        jobs,
        cloud_storages,
        users,
        is_project_staff,
        is_task_staff,
        is_job_staff,
        is_org_member,
    ):
        objects = {
            "projects": projects,
            "tasks": tasks,
            "jobs": jobs,
        }
        if obj in ("projects", "tasks"):
            cloud_storage_id = objects[obj][obj_id]["source_storage"]["cloud_storage_id"]
        else:
            task_id = jobs[obj_id]["task_id"]
            cloud_storage_id = tasks[task_id]["source_storage"]["cloud_storage_id"]
        cloud_storage = cloud_storages[cloud_storage_id]

        if user_type == "admin":
            user = self.user
        elif user_type == "assigned_supervisor_org_member":
            is_staff = {"projects": is_project_staff, "tasks": is_task_staff, "jobs": is_job_staff}[
                obj
            ]
            user = next(
                u
                for u in users
                if is_staff(u["id"], obj_id)
                if is_org_member(u["id"], cloud_storage["organization"], role="supervisor")
            )["username"]
        else:
            assert False

        export_kwargs = _make_export_resource_params(resource, obj=obj)
        import_kwargs = _make_import_resource_params(resource, obj=obj)
        export_kwargs["user"] = user
        import_kwargs["user"] = user

        self._export_resource(cloud_storage, obj_id, obj, resource, **export_kwargs)
        self._import_resource(cloud_storage, resource, obj_id, obj, **import_kwargs)

    @pytest.mark.parametrize("storage_id", [3])
    @pytest.mark.parametrize(
        "obj, resource",
        [
            ("projects", "dataset"),
            ("tasks", "annotations"),
            ("jobs", "annotations"),
            ("tasks", "backup"),
            ("projects", "backup"),
        ],
    )
    def test_user_cannot_import_from_cloud_storage_with_specific_location_without_access(
        self, storage_id, regular_lonely_user, obj, resource, cloud_storages
    ):
        user = regular_lonely_user

        project_spec = {"name": "Test project"}
        project = post_method(user, "projects", project_spec).json()
        project_id = project["id"]

        task_spec = {
            "name": f"Task with files from foreign cloud storage {storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }
        data_spec = {
            "image_quality": 75,
            "use_cache": True,
            "server_files": ["images/image_1.jpg"],
            "project_id": project_id,
        }
        (task_id, _) = create_task(user, task_spec, data_spec)

        jobs = get_method(user, "jobs", task_id=task_id).json()["results"]
        job_id = jobs[0]["id"]

        if obj == "projects":
            obj_id = project_id
        elif obj == "tasks":
            obj_id = task_id
        elif obj == "jobs":
            obj_id = job_id
        else:
            assert False

        cloud_storage = cloud_storages[storage_id]
        kwargs = _make_import_resource_params(
            resource, is_default=False, obj=obj, cloud_storage_id=storage_id
        )
        if resource == "annotations":
            kwargs["_check_uploaded"] = False

        self._import_resource(
            cloud_storage,
            resource,
            obj_id,
            obj,
            user=user,
            _expect_status=HTTPStatus.FORBIDDEN,
            **kwargs,
        )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_server.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


from http import HTTPStatus

import pytest

from shared.utils.config import make_api_client


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetServer:
    def test_can_retrieve_about_unauthenticated(self):
        with make_api_client(user=None, password=None) as api_client:
            (data, response) = api_client.server_api.retrieve_about()

            assert response.status == HTTPStatus.OK
            assert data.version

    def test_can_retrieve_formats(self, admin_user: str):
        with make_api_client(admin_user) as api_client:
            (data, response) = api_client.server_api.retrieve_annotation_formats()

            assert response.status == HTTPStatus.OK
            assert len(data.importers) != 0
            assert len(data.exporters) != 0


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetSchema:
    def test_can_get_schema_unauthenticated(self):
        with make_api_client(user=None, password=None) as api_client:
            (data, response) = api_client.schema_api.retrieve()

            assert response.status == HTTPStatus.OK
            assert data


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_tasks.py =====
# Copyright (C) 2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import itertools
import json
import math
import operator
import os
import os.path as osp
import re
import zipfile
from abc import ABCMeta, abstractmethod
from collections import Counter
from collections.abc import Generator, Iterable, Sequence
from contextlib import closing
from copy import deepcopy
from datetime import datetime
from enum import Enum
from functools import partial
from http import HTTPStatus
from itertools import chain, groupby, product
from math import ceil
from operator import itemgetter
from pathlib import Path, PurePosixPath
from tempfile import NamedTemporaryFile, TemporaryDirectory
from time import sleep, time
from typing import Any, Callable, ClassVar, Optional, Union

import attrs
import numpy as np
import pytest
from cvat_sdk import exceptions
from cvat_sdk.api_client import models
from cvat_sdk.api_client.api_client import ApiClient, ApiException, Endpoint
from cvat_sdk.api_client.exceptions import ForbiddenException
from cvat_sdk.core.helpers import get_paginated_collection
from cvat_sdk.core.progress import NullProgressReporter
from cvat_sdk.core.proxies.tasks import ResourceType, Task
from cvat_sdk.core.uploading import Uploader
from deepdiff import DeepDiff
from PIL import Image
from pytest_cases import fixture, fixture_ref, parametrize

import shared.utils.s3 as s3
from shared.fixtures.init import container_exec_cvat
from shared.utils.config import (
    delete_method,
    get_method,
    make_api_client,
    make_sdk_client,
    patch_method,
    post_method,
    put_method,
)
from shared.utils.helpers import (
    generate_image_file,
    generate_image_files,
    generate_manifest,
    generate_video_file,
    read_video_file,
)

from .utils import (
    DATUMARO_FORMAT_FOR_DIMENSION,
    CollectionSimpleFilterTestBase,
    calc_end_frame,
    compare_annotations,
    create_task,
    export_dataset,
    export_task_dataset,
    parse_frame_step,
    unique,
    wait_until_task_is_created,
)


def get_cloud_storage_content(username: str, cloud_storage_id: int, manifest: Optional[str] = None):
    with make_api_client(username) as api_client:
        kwargs = {"manifest_path": manifest} if manifest else {}

        (data, _) = api_client.cloudstorages_api.retrieve_content_v2(cloud_storage_id, **kwargs)
        return [f"{f['name']}{'/' if str(f['type']) == 'DIR' else ''}" for f in data["content"]]


def count_frame_uses(data: Sequence[int], *, included_frames: Sequence[int]) -> dict[int, int]:
    use_counts = {f: 0 for f in included_frames}
    for f in data:
        if f in included_frames:
            use_counts[f] += 1

    return use_counts


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetTasks:
    def _test_task_list_200(self, user, project_id, data, exclude_paths="", **kwargs):
        with make_api_client(user) as api_client:
            results = get_paginated_collection(
                api_client.tasks_api.list_endpoint,
                return_json=True,
                project_id=project_id,
                **kwargs,
            )
            assert DeepDiff(data, results, ignore_order=True, exclude_paths=exclude_paths) == {}

    def _test_users_to_see_task_list(
        self, project_id, tasks, users, is_staff, is_allow, is_project_staff, **kwargs
    ):
        if is_staff:
            users = [user for user in users if is_project_staff(user["id"], project_id)]
        else:
            users = [user for user in users if not is_project_staff(user["id"], project_id)]
        assert len(users)

        for user in users:
            if not is_allow:
                # Users outside project or org should not know if one exists.
                # Thus, no error should be produced on a list request.
                tasks = []

            self._test_task_list_200(user["username"], project_id, tasks, **kwargs)

    def _test_assigned_users_to_see_task_data(self, tasks, users, is_task_staff, **kwargs):
        for task in tasks:
            staff_users = [user for user in users if is_task_staff(user["id"], task["id"])]
            assert len(staff_users)

            for user in staff_users:
                with make_api_client(user["username"]) as api_client:
                    (_, response) = api_client.tasks_api.list(**kwargs)
                    assert response.status == HTTPStatus.OK
                    response_data = json.loads(response.data)

                assert any(_task["id"] == task["id"] for _task in response_data["results"])

    @pytest.mark.parametrize("project_id", [1])
    @pytest.mark.parametrize(
        "groups, is_staff, is_allow",
        [
            ("admin", False, True),
        ],
    )
    def test_project_tasks_visibility(
        self, project_id, groups, users, tasks, is_staff, is_allow, find_users, is_project_staff
    ):
        users = find_users(privilege=groups)
        tasks = list(filter(lambda x: x["project_id"] == project_id, tasks))
        assert len(tasks)

        self._test_users_to_see_task_list(
            project_id, tasks, users, is_staff, is_allow, is_project_staff
        )

    @pytest.mark.parametrize("project_id, groups", [(1, "user")])
    def test_task_assigned_to_see_task(
        self, project_id, groups, users, tasks, find_users, is_task_staff
    ):
        users = find_users(privilege=groups)
        tasks = list(filter(lambda x: x["project_id"] == project_id and x["assignee"], tasks))
        assert len(tasks)

        self._test_assigned_users_to_see_task_data(tasks, users, is_task_staff)

    @pytest.mark.parametrize("org, project_id", [({"id": 2, "slug": "org2"}, 2)])
    @pytest.mark.parametrize(
        "role, is_staff, is_allow",
        [
            ("maintainer", False, True),
            ("supervisor", False, False),
        ],
    )
    def test_org_project_tasks_visibility(
        self,
        org,
        project_id,
        role,
        is_staff,
        is_allow,
        tasks,
        is_task_staff,
        is_project_staff,
        find_users,
    ):
        users = find_users(org=org["id"], role=role)
        tasks = list(filter(lambda x: x["project_id"] == project_id, tasks))
        assert len(tasks)

        self._test_users_to_see_task_list(
            project_id, tasks, users, is_staff, is_allow, is_project_staff, org=org["slug"]
        )

    @pytest.mark.parametrize("org, project_id, role", [({"id": 2, "slug": "org2"}, 2, "worker")])
    def test_org_task_assigneed_to_see_task(
        self, org, project_id, role, users, tasks, find_users, is_task_staff
    ):
        users = find_users(org=org["id"], role=role)
        tasks = list(filter(lambda x: x["project_id"] == project_id and x["assignee"], tasks))
        assert len(tasks)

        self._test_assigned_users_to_see_task_data(tasks, users, is_task_staff, org=org["slug"])

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_can_get_job_validation_summary(self, admin_user, tasks, jobs):
        task = next(t for t in tasks if t["jobs"]["count"] > 0 if t["jobs"]["validation"] == 0)
        job = next(j for j in jobs if j["task_id"] == task["id"])

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update(
                job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(stage="validation"),
            )

            (server_task, _) = api_client.tasks_api.retrieve(task["id"])

        assert server_task.jobs.validation == 1

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_can_get_job_completed_summary(self, admin_user, tasks, jobs):
        task = next(t for t in tasks if t["jobs"]["count"] > 0 if t["jobs"]["completed"] == 0)
        job = next(j for j in jobs if j["task_id"] == task["id"])

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update(
                job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(
                    state="completed", stage="acceptance"
                ),
            )

            (server_task, _) = api_client.tasks_api.retrieve(task["id"])

        assert server_task.jobs.completed == 1

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_can_remove_owner_and_fetch_with_sdk(self, admin_user, tasks):
        # test for API schema regressions
        source_task = next(
            t for t in tasks if t.get("owner") and t["owner"]["username"] != admin_user
        ).copy()

        with make_api_client(admin_user) as api_client:
            api_client.users_api.destroy(source_task["owner"]["id"])

            (_, response) = api_client.tasks_api.retrieve(source_task["id"])
            fetched_task = json.loads(response.data)

        source_task["owner"] = None
        assert DeepDiff(source_task, fetched_task, ignore_order=True) == {}

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_check_task_status_after_changing_job_state(self, admin_user, tasks, jobs):
        task = next(t for t in tasks if t["jobs"]["count"] == 1 if t["jobs"]["completed"] == 0)
        job = next(j for j in jobs if j["task_id"] == task["id"])

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update(
                job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(stage="acceptance"),
            )

            api_client.jobs_api.partial_update(
                job["id"],
                patched_job_write_request=models.PatchedJobWriteRequest(state="completed"),
            )

            (server_task, _) = api_client.tasks_api.retrieve(task["id"])

        assert server_task.status == "completed"


class TestListTasksFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
        "assignee": ["assignee", "username"],
        "tracker_link": ["bug_tracker"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, tasks):
        self.user = admin_user
        self.samples = tasks

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.tasks_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        (
            "assignee",
            "dimension",
            "mode",
            "name",
            "owner",
            "project_id",
            "status",
            "subset",
            "tracker_link",
            "validation_mode",
        ),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostTasks:
    def _test_create_task_201(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.tasks_api.create(spec, **kwargs)
            assert response.status == HTTPStatus.CREATED

        return response

    def _test_create_task_403(self, user, spec, **kwargs):
        with make_api_client(user) as api_client:
            (_, response) = api_client.tasks_api.create(
                spec, **kwargs, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

        return response

    def _test_users_to_create_task_in_project(
        self, project_id, users, is_staff, is_allow, is_project_staff, **kwargs
    ):
        if is_staff:
            users = [user for user in users if is_project_staff(user["id"], project_id)]
        else:
            users = [user for user in users if not is_project_staff(user["id"], project_id)]
        assert len(users)

        for user in users:
            username = user["username"]
            spec = {
                "name": f"test {username} to create a task within a project",
                "project_id": project_id,
            }

            if is_allow:
                self._test_create_task_201(username, spec, **kwargs)
            else:
                self._test_create_task_403(username, spec, **kwargs)

    @pytest.mark.parametrize("project_id", [1])
    @pytest.mark.parametrize(
        "groups, is_staff, is_allow",
        [
            ("admin", False, True),
            ("user", True, True),
        ],
    )
    def test_users_to_create_task_in_project(
        self, project_id, groups, is_staff, is_allow, is_project_staff, find_users
    ):
        users = find_users(privilege=groups)
        self._test_users_to_create_task_in_project(
            project_id, users, is_staff, is_allow, is_project_staff
        )

    @pytest.mark.parametrize("org, project_id", [({"id": 2, "slug": "org2"}, 2)])
    @pytest.mark.parametrize(
        "role, is_staff, is_allow",
        [
            ("worker", False, False),
        ],
    )
    def test_worker_cannot_create_task_in_project_without_ownership(
        self, org, project_id, role, is_staff, is_allow, is_project_staff, find_users
    ):
        users = find_users(org=org["id"], role=role)
        self._test_users_to_create_task_in_project(
            project_id, users, is_staff, is_allow, is_project_staff, org=org["slug"]
        )

    def test_create_response_matches_get(self, admin_user):
        username = admin_user

        spec = {"name": "test create task", "labels": [{"name": "a"}]}

        response = self._test_create_task_201(username, spec)
        task = json.loads(response.data)

        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.retrieve(task["id"])
            assert DeepDiff(task, json.loads(response.data), ignore_order=True) == {}

    def test_can_create_task_with_skeleton(self, admin_user):
        username = admin_user

        spec = {
            "name": f"test admin1 to create a task with skeleton",
            "labels": [
                {
                    "name": "s1",
                    "color": "#5c5eba",
                    "attributes": [
                        {
                            "name": "color",
                            "mutable": False,
                            "input_type": "select",
                            "default_value": "white",
                            "values": ["white", "black"],
                        }
                    ],
                    "type": "skeleton",
                    "sublabels": [
                        {
                            "name": "1",
                            "color": "#d53957",
                            "attributes": [
                                {
                                    "id": 23,
                                    "name": "attr",
                                    "mutable": False,
                                    "input_type": "select",
                                    "default_value": "val1",
                                    "values": ["val1", "val2"],
                                }
                            ],
                            "type": "points",
                        },
                        {"name": "2", "color": "#4925ec", "attributes": [], "type": "points"},
                        {"name": "3", "color": "#59a8fe", "attributes": [], "type": "points"},
                    ],
                    "svg": '<line x1="36.329429626464844" y1="45.98662185668945" x2="59.07190704345703" y2="23.076923370361328" '
                    'stroke="black" data-type="edge" data-node-from="2" stroke-width="0.5" data-node-to="3"></line>'
                    '<line x1="22.61705780029297" y1="25.75250816345215" x2="36.329429626464844" y2="45.98662185668945" '
                    'stroke="black" data-type="edge" data-node-from="1" stroke-width="0.5" data-node-to="2"></line>'
                    '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="22.61705780029297" cy="25.75250816345215" '
                    'stroke-width="0.1" data-type="element node" data-element-id="1" data-node-id="1" data-label-name="1">'
                    '</circle><circle r="1.5" stroke="black" fill="#b3b3b3" cx="36.329429626464844" cy="45.98662185668945" '
                    'stroke-width="0.1" data-type="element node" data-element-id="2" data-node-id="2" data-label-name="2"></circle>'
                    '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="59.07190704345703" cy="23.076923370361328" '
                    'stroke-width="0.1" data-type="element node" data-element-id="3" data-node-id="3" data-label-name="3"></circle>',
                }
            ],
        }

        self._test_create_task_201(username, spec)

    @pytest.mark.parametrize("assignee", [None, "admin1"])
    def test_can_create_with_assignee(self, admin_user, users_by_name, assignee):
        task_spec = {
            "name": "test task creation with assignee",
            "labels": [{"name": "car"}],
            "assignee_id": users_by_name[assignee]["id"] if assignee else None,
        }

        with make_api_client(admin_user) as api_client:
            (task, _) = api_client.tasks_api.create(task_write_request=task_spec)

            if assignee:
                assert task.assignee.username == assignee
                assert task.assignee_updated_date
            else:
                assert task.assignee is None
                assert task.assignee_updated_date is None


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetData:
    _USERNAME = "user1"

    @pytest.mark.parametrize(
        "content_type, task_id",
        [
            ("image/png", 8),
            ("image/png", 5),
            ("image/x.point-cloud-data", 6),
        ],
    )
    def test_frame_content_type(self, content_type, task_id):
        with make_api_client(self._USERNAME) as api_client:
            (_, response) = api_client.tasks_api.retrieve_data(
                task_id, type="frame", quality="original", number=0
            )
            assert response.status == HTTPStatus.OK
            assert response.headers["Content-Type"] == content_type


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchTaskAnnotations:
    def _test_check_response(self, is_allow, response, data=None):
        if is_allow:
            assert response.status == HTTPStatus.OK
            assert compare_annotations(data, json.loads(response.data)) == {}
        else:
            assert response.status == HTTPStatus.FORBIDDEN

    @pytest.fixture(scope="class")
    def request_data(self, annotations):
        def get_data(tid):
            data = deepcopy(annotations["task"][str(tid)])
            if data["shapes"][0]["type"] == "skeleton":
                data["shapes"][0]["elements"][0].update({"points": [2.0, 3.0, 4.0, 5.0]})
            else:
                data["shapes"][0].update({"points": [2.0, 3.0, 4.0, 5.0, 6.0, 7.0]})
            data["version"] += 1
            return data

        return get_data

    @pytest.mark.parametrize("org", [""])
    @pytest.mark.parametrize(
        "privilege, task_staff, is_allow",
        [
            ("admin", True, True),
            ("admin", False, True),
            ("worker", True, True),
            ("worker", False, False),
            ("user", True, True),
            ("user", False, False),
        ],
    )
    def test_user_update_task_annotations(
        self,
        org,
        privilege,
        task_staff,
        is_allow,
        find_task_staff_user,
        find_users,
        request_data,
        tasks_by_org,
        filter_tasks_with_shapes,
    ):
        users = find_users(privilege=privilege)
        tasks = tasks_by_org[org]
        filtered_tasks = filter_tasks_with_shapes(tasks)
        username, tid = find_task_staff_user(filtered_tasks, users, task_staff, [21])

        data = request_data(tid)
        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.partial_update_annotations(
                id=tid,
                action="update",
                patched_labeled_data_request=deepcopy(data),
                _parse_response=False,
                _check_status=False,
            )

        self._test_check_response(is_allow, response, data)

    @pytest.mark.parametrize("org", [2])
    @pytest.mark.parametrize(
        "role, task_staff, is_allow",
        [
            ("maintainer", False, True),
            ("owner", False, True),
            ("supervisor", False, False),
            ("worker", False, False),
            ("maintainer", True, True),
            ("owner", True, True),
            ("supervisor", True, True),
            ("worker", True, True),
        ],
    )
    def test_member_update_task_annotation(
        self,
        org,
        role,
        task_staff,
        is_allow,
        find_task_staff_user,
        find_users,
        request_data,
        tasks_with_shapes,
    ):
        users = find_users(role=role, org=org)
        tasks = (
            t
            for t in tasks_with_shapes
            if t["organization"] == org
            if t["validation_mode"] != "gt_pool"
        )
        username, tid = find_task_staff_user(tasks, users, task_staff)

        data = request_data(tid)
        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.partial_update_annotations(
                id=tid,
                action="update",
                patched_labeled_data_request=deepcopy(data),
                _parse_response=False,
                _check_status=False,
            )

        self._test_check_response(is_allow, response, data)

    def test_cannot_update_validation_frames_in_honeypot_task(
        self,
        admin_user,
        tasks,
        request_data,
    ):
        task_id = next(t for t in tasks if t["validation_mode"] == "gt_pool" and t["size"] > 0)[
            "id"
        ]

        data = request_data(task_id)
        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.tasks_api.partial_update_annotations(
                id=task_id,
                action="update",
                patched_labeled_data_request=deepcopy(data),
                _parse_response=False,
                _check_status=False,
            )

        assert response.status == HTTPStatus.BAD_REQUEST
        assert b"can only be edited via task import or the GT job" in response.data

    def test_can_update_honeypot_frames_in_honeypot_task(
        self,
        admin_user,
        tasks,
        jobs,
        request_data,
    ):
        task_id = next(t for t in tasks if t["validation_mode"] == "gt_pool" and t["size"] > 0)[
            "id"
        ]
        gt_job = next(j for j in jobs if j["task_id"] == task_id and j["type"] == "ground_truth")

        validation_frames = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
        data = request_data(task_id)
        data["tags"] = [a for a in data["tags"] if a["frame"] not in validation_frames]
        data["shapes"] = [a for a in data["shapes"] if a["frame"] not in validation_frames]
        data["tracks"] = []  # tracks cannot be used in honeypot tasks
        with make_api_client(admin_user) as api_client:
            (_, response) = api_client.tasks_api.partial_update_annotations(
                id=task_id,
                action="update",
                patched_labeled_data_request=deepcopy(data),
                _parse_response=False,
                _check_status=False,
            )

        self._test_check_response(True, response, data)

    def test_remove_first_keyframe(self):
        endpoint = "tasks/8/annotations"
        shapes0 = [
            {"type": "rectangle", "frame": 1, "points": [1, 2, 3, 4]},
            {"type": "rectangle", "frame": 4, "points": [5, 6, 7, 8]},
        ]

        annotations = {"tracks": [{"label_id": 13, "frame": 0, "shapes": shapes0}]}

        response = patch_method("admin1", endpoint, annotations, action="create")
        assert response.status_code == HTTPStatus.OK, response.content

        annotations["tracks"][0]["shapes"] = shapes0[1:]
        response = patch_method("admin1", endpoint, annotations, action="update")
        assert response.status_code == HTTPStatus.OK

    def test_can_split_skeleton_tracks_on_jobs(self, jobs):
        # https://github.com/cvat-ai/cvat/pull/6968
        task_id = 21

        task_jobs = [job for job in jobs if job["task_id"] == task_id]

        frame_ranges = {}
        for job in task_jobs:
            frame_ranges[job["id"]] = set(range(job["start_frame"], job["stop_frame"] + 1))

        # skeleton track that covers few jobs
        annotations = {
            "tracks": [
                {
                    "frame": 0,
                    "label_id": 58,
                    "shapes": [{"type": "skeleton", "frame": 0, "points": []}],
                    "elements": [
                        {
                            "label_id": 59,
                            "frame": 0,
                            "shapes": [
                                # https://github.com/cvat-ai/cvat/issues/7498
                                # https://github.com/cvat-ai/cvat/pull/7615
                                # This shape covers frame 0 to 7,
                                # We need to check if frame 5 is generated correctly for job#1
                                {"type": "points", "frame": 0, "points": [1.0, 2.0]},
                                {"type": "points", "frame": 7, "points": [2.0, 4.0]},
                            ],
                        },
                    ],
                }
            ]
        }

        # clear task annotations
        response = delete_method("admin1", f"tasks/{task_id}/annotations")
        assert response.status_code == 204, f"Cannot delete task's annotations: {response.content}"

        # create skeleton track that covers few jobs
        response = patch_method(
            "admin1", f"tasks/{task_id}/annotations", annotations, action="create"
        )
        assert response.status_code == 200, f"Cannot update task's annotations: {response.content}"

        # check that server splitted skeleton track's elements on jobs correctly
        for job_id, job_frame_range in frame_ranges.items():
            response = get_method("admin1", f"jobs/{job_id}/annotations")
            assert response.status_code == 200, f"Cannot get job's annotations: {response.content}"

            job_annotations = response.json()
            assert len(job_annotations["tracks"]) == 1, "Expected to see only one track"

            track = job_annotations["tracks"][0]
            assert track.get("elements", []), "Expected to see track with elements"

            def interpolate(frame):
                # simple interpolate from ([1, 2], 1) to ([2, 4], 7)
                return [(2.0 - 1.0) / 7 * (frame - 0) + 1.0, (4.0 - 2.0) / 7 * (frame - 0) + 2.0]

            for element in track["elements"]:
                element_frames = set(shape["frame"] for shape in element["shapes"])
                assert all(
                    [
                        not DeepDiff(
                            interpolate(shape["frame"]), shape["points"], significant_digits=2
                        )
                        for shape in element["shapes"]
                        if shape["frame"] >= 0 and shape["frame"] <= 7
                    ]
                )
                assert len(element["shapes"]) == 2
                assert element_frames <= job_frame_range, "Track shapes get out of job frame range"


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
class TestGetTaskDataset:

    @staticmethod
    def _test_can_export_dataset(
        username: str,
        task_id: int,
        *,
        local_download: bool = True,
        **kwargs,
    ) -> Optional[bytes]:
        dataset = export_task_dataset(username, save_images=True, id=task_id, **kwargs)
        if local_download:
            assert zipfile.is_zipfile(io.BytesIO(dataset))
        else:
            assert dataset is None

        return dataset

    @pytest.mark.parametrize("tid", [21])
    @pytest.mark.parametrize(
        "format_name", ["CVAT for images 1.1", "CVAT for video 1.1", "COCO Keypoints 1.0"]
    )
    def test_can_export_task_with_several_jobs(
        self,
        admin_user,
        tid,
        format_name,
    ):
        self._test_can_export_dataset(
            admin_user,
            tid,
            format=format_name,
        )

    @pytest.mark.parametrize("tid", [8])
    def test_can_export_task_to_coco_format(
        self,
        admin_user: str,
        tid: int,
    ):
        # these annotations contains incorrect frame numbers
        # in order to check that server handle such cases
        annotations = {
            "version": 0,
            "tags": [],
            "shapes": [],
            "tracks": [
                {
                    "label_id": 63,
                    "frame": 1,
                    "group": 0,
                    "source": "manual",
                    "shapes": [
                        {
                            "type": "skeleton",
                            "frame": 1,
                            "occluded": False,
                            "outside": False,
                            "z_order": 0,
                            "rotation": 0,
                            "points": [],
                            "attributes": [],
                        }
                    ],
                    "attributes": [],
                    "elements": [
                        {
                            "label_id": 64,
                            "frame": 0,
                            "group": 0,
                            "source": "manual",
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 1,
                                    "occluded": False,
                                    "outside": True,
                                    "z_order": 0,
                                    "rotation": 0,
                                    "points": [74.14935096036425, 79.09960455479086],
                                    "attributes": [],
                                },
                                {
                                    "type": "points",
                                    "frame": 7,
                                    "occluded": False,
                                    "outside": False,
                                    "z_order": 0,
                                    "rotation": 0,
                                    "points": [74.14935096036425, 79.09960455479086],
                                    "attributes": [],
                                },
                            ],
                            "attributes": [],
                        },
                        {
                            "label_id": 65,
                            "frame": 0,
                            "group": 0,
                            "source": "manual",
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "occluded": False,
                                    "outside": False,
                                    "z_order": 0,
                                    "rotation": 0,
                                    "points": [285.07319976630424, 353.51583641966175],
                                    "attributes": [],
                                }
                            ],
                            "attributes": [],
                        },
                    ],
                }
            ],
        }
        response = patch_method(
            admin_user, f"tasks/{tid}/annotations", annotations, action="update"
        )
        assert response.status_code == HTTPStatus.OK

        # check that we can export task dataset
        self._test_can_export_dataset(
            admin_user,
            tid,
            format="COCO Keypoints 1.0",
        )

        # check that server saved track annotations correctly
        response = get_method(admin_user, f"tasks/{tid}/annotations")
        assert response.status_code == HTTPStatus.OK

        annotations = response.json()
        assert annotations["tracks"][0]["frame"] == 0
        assert annotations["tracks"][0]["shapes"][0]["frame"] == 0
        assert annotations["tracks"][0]["elements"][0]["shapes"][0]["frame"] == 0

    @pytest.mark.usefixtures("restore_db_per_function")
    @pytest.mark.usefixtures("restore_redis_ondisk_per_function")
    def test_can_download_task_with_special_chars_in_name(
        self,
        admin_user: str,
    ):
        # Control characters in filenames may conflict with the Content-Disposition header
        # value restrictions, as it needs to include the downloaded file name.

        task_spec = {
            "name": "test_special_chars_{}_in_name".format("".join(chr(c) for c in range(1, 127))),
            "labels": [{"name": "cat"}],
        }

        task_data = {
            "image_quality": 75,
            "client_files": generate_image_files(1),
        }

        task_id, _ = create_task(admin_user, task_spec, task_data)

        dataset = self._test_can_export_dataset(admin_user, task_id)
        assert zipfile.is_zipfile(io.BytesIO(dataset))

    @pytest.mark.usefixtures("restore_db_per_function")
    def test_export_dataset_after_deleting_related_cloud_storage(
        self,
        admin_user: str,
        tasks,
    ):
        related_field = "target_storage"

        task = next(
            t for t in tasks if t[related_field] and t[related_field]["location"] == "cloud_storage"
        )
        task_id = task["id"]
        cloud_storage_id = task[related_field]["cloud_storage_id"]

        with make_api_client(admin_user) as api_client:
            _, response = api_client.cloudstorages_api.destroy(cloud_storage_id)
            assert response.status == HTTPStatus.NO_CONTENT

            result, response = api_client.tasks_api.retrieve(task_id)
            assert not result[related_field]

            self._test_can_export_dataset(admin_user, task["id"])

    @pytest.mark.parametrize(
        "export_format, default_subset_name, subset_path_template",
        [
            ("Datumaro 1.0", "", "images/{subset}"),
            ("YOLO 1.1", "train", "obj_{subset}_data"),
            ("Ultralytics YOLO Detection 1.0", "train", "images/{subset}"),
        ],
    )
    def test_uses_subset_name(
        self,
        admin_user,
        filter_tasks,
        export_format,
        default_subset_name,
        subset_path_template,
    ):
        tasks = filter_tasks(exclude_target_storage__location="cloud_storage")
        group_key_func = itemgetter("subset")
        subsets_and_tasks = [
            (subset, next(group))
            for subset, group in itertools.groupby(
                sorted(tasks, key=group_key_func),
                key=group_key_func,
            )
        ]
        for subset_name, task in subsets_and_tasks:
            dataset = self._test_can_export_dataset(
                admin_user,
                task["id"],
                format=export_format,
            )
            with zipfile.ZipFile(io.BytesIO(dataset)) as zip_file:
                subset_path = subset_path_template.format(subset=subset_name or default_subset_name)
                assert any(
                    subset_path in path for path in zip_file.namelist()
                ), f"No {subset_path} in {zip_file.namelist()}"

    @pytest.mark.parametrize(
        "dimension, mode", [("2d", "annotation"), ("2d", "interpolation"), ("3d", "annotation")]
    )
    def test_datumaro_export_without_annotations_includes_image_info(
        self, admin_user, tasks, mode, dimension
    ):
        task = next(
            t for t in tasks if t.get("size") if t["mode"] == mode if t["dimension"] == dimension
        )

        with make_api_client(admin_user) as api_client:
            dataset_file = io.BytesIO(
                export_dataset(
                    api_client.tasks_api,
                    id=task["id"],
                    format=DATUMARO_FORMAT_FOR_DIMENSION[dimension],
                    save_images=False,
                )
            )

        with zipfile.ZipFile(dataset_file) as zip_file:
            annotations = json.loads(zip_file.read("annotations/default.json"))

        assert annotations["items"]
        for item in annotations["items"]:
            assert "media" not in item

            if dimension == "2d":
                assert osp.splitext(item["image"]["path"])[0] == item["id"]
                assert not Path(item["image"]["path"]).is_absolute()
                assert tuple(item["image"]["size"]) > (0, 0)
            elif dimension == "3d":
                assert osp.splitext(osp.basename(item["point_cloud"]["path"]))[0] == item["id"]
                assert not Path(item["point_cloud"]["path"]).is_absolute()
                for related_image in item["related_images"]:
                    assert not Path(related_image["path"]).is_absolute()
                    if "size" in related_image:
                        assert tuple(related_image["size"]) > (0, 0)


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_cvat_data_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestPostTaskData:
    _USERNAME = "admin1"

    def _test_cannot_create_task(self, username, spec, data, **kwargs):
        with make_api_client(username) as api_client:
            (task, response) = api_client.tasks_api.create(spec, **kwargs)
            assert response.status == HTTPStatus.CREATED

            (result, response) = api_client.tasks_api.create_data(
                task.id, data_request=deepcopy(data), _content_type="application/json", **kwargs
            )
            assert response.status == HTTPStatus.ACCEPTED

            request_details = wait_until_task_is_created(api_client.requests_api, result.rq_id)
            assert request_details.status.value == "failed"

        return request_details

    def test_can_create_task_with_defined_start_and_stop_frames(self):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with defined start and stop frames",
            "labels": [
                {
                    "name": "car",
                    "color": "#ff00ff",
                    "attributes": [
                        {
                            "name": "a",
                            "mutable": True,
                            "input_type": "number",
                            "default_value": "5",
                            "values": ["4", "5", "6"],
                        }
                    ],
                }
            ],
        }

        task_data = {
            "image_quality": 75,
            "start_frame": 2,
            "stop_frame": 5,
            "client_files": generate_image_files(7),
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check task size
        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            assert task.size == 4

    def test_default_overlap_for_small_segment_size(self):
        task_spec = {
            "name": f"test {self._USERNAME} with default overlap and small segment_size",
            "labels": [{"name": "car"}],
            "segment_size": 5,
        }

        task_data = {
            "image_quality": 75,
            "client_files": [generate_video_file(8)],
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check task size
        with make_api_client(self._USERNAME) as api_client:
            paginated_job_list, _ = api_client.jobs_api.list(task_id=task_id)

            jobs = paginated_job_list.results
            jobs.sort(key=lambda job: job.start_frame)

            assert len(jobs) == 2
            # overlap should be 2 frames (frames 3 & 4)
            assert jobs[0].start_frame == 0
            assert jobs[0].stop_frame == 4
            assert jobs[1].start_frame == 3
            assert jobs[1].stop_frame == 7

    @pytest.mark.parametrize(
        "size,expected_segments",
        [
            (2, [(0, 1)]),
            (3, [(0, 2)]),
            (4, [(0, 2), (2, 3)]),
            (5, [(0, 2), (2, 4)]),
            (6, [(0, 2), (2, 4), (4, 5)]),
        ],
    )
    def test_task_segmentation(self, size, expected_segments):
        task_spec = {
            "name": f"test {self._USERNAME} to check segmentation into jobs",
            "labels": [{"name": "car"}],
            "segment_size": 3,
            "overlap": 1,
        }

        task_data = {
            "image_quality": 75,
            "client_files": generate_image_files(size),
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check task size
        with make_api_client(self._USERNAME) as api_client:
            paginated_job_list, _ = api_client.jobs_api.list(task_id=task_id)

            jobs = paginated_job_list.results
            jobs.sort(key=lambda job: job.start_frame)

            assert [(j.start_frame, j.stop_frame) for j in jobs] == expected_segments

    def test_can_create_task_with_exif_rotated_images(self):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with exif rotated images",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        image_files = ["images/exif_rotated/left.jpg", "images/exif_rotated/right.jpg"]
        task_data = {
            "server_files": image_files,
            "image_quality": 70,
            "segment_size": 500,
            "use_cache": True,
            "sorting_method": "natural",
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check that the frames have correct width and height
        for chunk_quality in ["original", "compressed"]:
            with make_api_client(self._USERNAME) as api_client:
                _, response = api_client.tasks_api.retrieve_data(
                    task_id, number=0, type="chunk", quality=chunk_quality
                )
                data_meta, _ = api_client.tasks_api.retrieve_data_meta(task_id)

                with zipfile.ZipFile(io.BytesIO(response.data)) as zip_file:
                    for name, frame_meta in zip(zip_file.namelist(), data_meta.frames):
                        with zip_file.open(name) as zipped_img:
                            im = Image.open(zipped_img)
                            # original is 480x640 with 90/-90 degrees rotation
                            assert frame_meta.height == 640 and frame_meta.width == 480
                            assert im.height == 640 and im.width == 480
                            assert im.getexif().get(274, 1) == 1

    def test_can_create_task_with_big_images(self):
        # Checks for regressions about the issue
        # https://github.com/cvat-ai/cvat/issues/6878
        # In the case of big files (>2.5 MB by default),
        # uploaded files could be write-appended twice,
        # leading to bigger raw file sizes than expected.

        task_spec = {
            "name": f"test {self._USERNAME} to create a task with big images",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        # We need a big file to reproduce the problem
        image_file = generate_image_file("big_image.bmp", size=(4000, 4000), color=(100, 200, 30))
        image_bytes = image_file.getvalue()
        file_size = len(image_bytes)
        assert 10 * 2**20 < file_size

        task_data = {
            "client_files": [image_file],
            "image_quality": 70,
            "use_cache": False,
            "use_zip_chunks": True,
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check that the original chunk image have the original size
        # this is less accurate than checking the uploaded file directly, but faster
        with make_api_client(self._USERNAME) as api_client:
            _, response = api_client.tasks_api.retrieve_data(
                task_id, number=0, quality="original", type="chunk", _parse_response=False
            )
            chunk_file = io.BytesIO(response.data)

        with zipfile.ZipFile(chunk_file) as chunk_zip:
            infos = chunk_zip.infolist()
            assert len(infos) == 1
            assert infos[0].file_size == file_size

            chunk_image = chunk_zip.read(infos[0])
            assert chunk_image == image_bytes

    def test_can_create_task_with_exif_rotated_tif_image(self):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with exif rotated tif image",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        image_files = ["images/exif_rotated/tif_left.tif"]
        task_data = {
            "server_files": image_files,
            "image_quality": 70,
            "segment_size": 500,
            "use_cache": False,
            "sorting_method": "natural",
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        for chunk_quality in ["original", "compressed"]:
            # check that the frame has correct width and height
            with make_api_client(self._USERNAME) as api_client:
                _, response = api_client.tasks_api.retrieve_data(
                    task_id, number=0, type="chunk", quality=chunk_quality
                )

                with zipfile.ZipFile(io.BytesIO(response.data)) as zip_file:
                    assert len(zip_file.namelist()) == 1
                    name = zip_file.namelist()[0]
                    assert name == "000000.tif" if chunk_quality == "original" else "000000.jpeg"
                    with zip_file.open(name) as zipped_img:
                        im = Image.open(zipped_img)
                        # raw image is horizontal 100x150 with -90 degrees rotation
                        assert im.height == 150 and im.width == 100
                        assert im.getexif().get(274, 1) == 1

    def test_can_create_task_with_sorting_method_natural(self):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with a custom sorting method",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        image_files = generate_image_files(15)

        task_data = {
            "client_files": image_files[5:] + image_files[:5],  # perturb the order
            "image_quality": 70,
            "sorting_method": "natural",
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        # check that the frames were sorted again
        with make_api_client(self._USERNAME) as api_client:
            data_meta, _ = api_client.tasks_api.retrieve_data_meta(task_id)

            # generate_image_files produces files that are already naturally sorted
            for image_file, frame in zip(image_files, data_meta.frames):
                assert image_file.name == frame.name

    def test_can_create_task_with_video_without_keyframes(self):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with a video without keyframes",
            "labels": [
                {
                    "name": "label1",
                }
            ],
        }

        task_data = {
            "server_files": [osp.join("videos", "video_without_valid_keyframes.mp4")],
            "image_quality": 70,
        }

        task_id, _ = create_task(self._USERNAME, task_spec, task_data)

        with make_api_client(self._USERNAME) as api_client:
            (_, response) = api_client.tasks_api.retrieve(task_id)
            assert response.status == HTTPStatus.OK

    @pytest.mark.parametrize("data_source", ["client_files", "server_files"])
    def test_can_create_task_with_sorting_method_predefined(self, data_source):
        task_spec = {
            "name": f"test {self._USERNAME} to create a task with a custom sorting method",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        if data_source == "client_files":
            image_files = generate_image_files(15)

            # shuffle to check for occasional sorting, e.g. in the DB
            image_files = image_files[7:] + image_files[5:7] + image_files[:5]
        elif data_source == "server_files":
            # Files from the test file share
            image_files = ["images/image_3.jpg", "images/image_1.jpg", "images/image_2.jpg"]
        else:
            assert False

        task_data = {
            data_source: image_files,
            "image_quality": 70,
            "sorting_method": "predefined",
        }

        (task_id, _) = create_task(self._USERNAME, task_spec, task_data)

        # check that the frames were sorted again
        with make_api_client(self._USERNAME) as api_client:
            (data_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)

            for image_file, frame in zip(image_files, data_meta.frames):
                if isinstance(image_file, str):
                    image_name = image_file
                else:
                    image_name = image_file.name

                assert image_name == frame.name

    def test_can_get_annotations_from_new_task_with_skeletons(self):
        spec = {
            "name": f"test admin1 to create a task with skeleton",
            "labels": [
                {
                    "name": "s1",
                    "color": "#5c5eba",
                    "attributes": [],
                    "type": "skeleton",
                    "sublabels": [
                        {"name": "1", "color": "#d12345", "attributes": [], "type": "points"},
                        {"name": "2", "color": "#350dea", "attributes": [], "type": "points"},
                    ],
                    "svg": '<line x1="19.464284896850586" y1="21.922269821166992" x2="54.08613586425781" y2="43.60293960571289" '
                    'stroke="black" data-type="edge" data-node-from="1" stroke-width="0.5" data-node-to="2"></line>'
                    '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="19.464284896850586" cy="21.922269821166992" '
                    'stroke-width="0.1" data-type="element node" data-element-id="1" data-node-id="1" data-label-id="103"></circle>'
                    '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="54.08613586425781" cy="43.60293960571289" '
                    'stroke-width="0.1" data-type="element node" data-element-id="2" data-node-id="2" data-label-id="104"></circle>',
                }
            ],
        }

        task_data = {
            "image_quality": 75,
            "client_files": generate_image_files(3),
        }

        task_id, _ = create_task(self._USERNAME, spec, task_data)

        response = get_method(self._USERNAME, "labels", task_id=f"{task_id}")
        label_ids = {}
        for root_label in response.json()["results"]:
            for label in [root_label] + root_label["sublabels"]:
                label_ids.setdefault(label["type"], []).append(label["id"])

        response = get_method(self._USERNAME, "jobs", task_id=f"{task_id}")
        job_id = response.json()["results"][0]["id"]
        patch_data = {
            "shapes": [
                {
                    "type": "skeleton",
                    "occluded": False,
                    "outside": False,
                    "z_order": 0,
                    "rotation": 0,
                    "points": [],
                    "frame": 0,
                    "label_id": label_ids["skeleton"][0],
                    "group": 0,
                    "source": "manual",
                    "attributes": [],
                    "elements": [
                        {
                            "type": "points",
                            "occluded": False,
                            "outside": False,
                            "z_order": 0,
                            "rotation": 0,
                            "points": [131.63947368421032, 165.0868421052637],
                            "frame": 0,
                            "label_id": label_ids["points"][0],
                            "group": 0,
                            "source": "manual",
                            "attributes": [],
                        },
                        {
                            "type": "points",
                            "occluded": False,
                            "outside": False,
                            "z_order": 0,
                            "rotation": 0,
                            "points": [354.98157894736823, 304.2710526315795],
                            "frame": 0,
                            "label_id": label_ids["points"][1],
                            "group": 0,
                            "source": "manual",
                            "attributes": [],
                        },
                    ],
                }
            ],
            "tracks": [
                {
                    "frame": 0,
                    "label_id": label_ids["skeleton"][0],
                    "group": 0,
                    "source": "manual",
                    "shapes": [
                        {
                            "type": "skeleton",
                            "occluded": False,
                            "outside": False,
                            "z_order": 0,
                            "rotation": 0,
                            "points": [],
                            "frame": 0,
                            "attributes": [],
                        }
                    ],
                    "attributes": [],
                    "elements": [
                        {
                            "frame": 0,
                            "label_id": label_ids["points"][0],
                            "group": 0,
                            "source": "manual",
                            "shapes": [
                                {
                                    "type": "points",
                                    "occluded": False,
                                    "outside": False,
                                    "z_order": 0,
                                    "rotation": 0,
                                    "points": [295.6394736842103, 472.5868421052637],
                                    "frame": 0,
                                    "attributes": [],
                                }
                            ],
                            "attributes": [],
                        },
                        {
                            "frame": 0,
                            "label_id": label_ids["points"][1],
                            "group": 0,
                            "source": "manual",
                            "shapes": [
                                {
                                    "type": "points",
                                    "occluded": False,
                                    "outside": False,
                                    "z_order": 0,
                                    "rotation": 0,
                                    "points": [619.3236842105262, 846.9815789473689],
                                    "frame": 0,
                                    "attributes": [],
                                }
                            ],
                            "attributes": [],
                        },
                    ],
                }
            ],
            "tags": [],
            "version": 0,
        }

        response = patch_method(
            self._USERNAME, f"jobs/{job_id}/annotations", patch_data, action="create"
        )
        response = get_method(self._USERNAME, f"jobs/{job_id}/annotations")
        assert response.status_code == HTTPStatus.OK

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "use_cache, cloud_storage_id, manifest, use_bucket_content, org",
        [
            (True, 1, "manifest.jsonl", False, ""),  # public bucket
            (True, 2, "sub/manifest.jsonl", True, "org2"),  # private bucket
            (True, 2, "sub/manifest.jsonl", True, "org2"),  # private bucket
            (True, 1, None, False, ""),
            (True, 2, None, True, "org2"),
            (True, 2, None, True, "org2"),
            (False, 1, None, False, ""),
            (False, 2, None, True, "org2"),
            (False, 2, None, True, "org2"),
        ],
    )
    def test_create_task_with_cloud_storage_files(
        self,
        use_cache: bool,
        cloud_storage_id: int,
        manifest: str,
        use_bucket_content: bool,
        org: str,
    ):
        if use_bucket_content:
            cloud_storage_content = get_cloud_storage_content(
                self._USERNAME, cloud_storage_id, manifest
            )
        else:
            cloud_storage_content = ["image_case_65_1.png", "image_case_65_2.png"]
        if manifest:
            cloud_storage_content.append(manifest)

        task_spec = {
            "name": f"Task with files from cloud storage {cloud_storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        data_spec = {
            "image_quality": 75,
            "use_cache": use_cache,
            "cloud_storage_id": cloud_storage_id,
            "server_files": cloud_storage_content,
        }

        kwargs = {"org": org} if org else {}
        create_task(self._USERNAME, task_spec, data_spec, **kwargs)

    def _create_task_with_cloud_data(
        self,
        request,
        cloud_storage: Any,
        use_manifest: bool,
        server_files: list[str],
        use_cache: bool = True,
        sorting_method: str = "lexicographical",
        data_type: str = "image",
        video_frame_count: int = 10,
        server_files_exclude: Optional[list[str]] = None,
        org: str = "",
        filenames: Optional[list[str]] = None,
        task_spec_kwargs: Optional[dict[str, Any]] = None,
        data_spec_kwargs: Optional[dict[str, Any]] = None,
    ) -> tuple[int, Any]:
        s3_client = s3.make_client(bucket=cloud_storage["resource"])
        if data_type == "video":
            video = generate_video_file(video_frame_count)
            s3_client.create_file(
                data=video,
                filename=f"test/video/{video.name}",
            )
            request.addfinalizer(
                partial(
                    s3_client.remove_file,
                    filename=f"test/video/{video.name}",
                )
            )
        else:
            images = generate_image_files(
                3,
                sizes=[(100, 50) if i % 2 else (50, 100) for i in range(3)],
                **({"prefixes": ["img_"] * 3} if not filenames else {"filenames": filenames}),
            )

            for image in images:
                for i in range(2):
                    image.seek(0)
                    s3_client.create_file(
                        data=image,
                        filename=f"test/sub_{i}/{image.name}",
                    )
                    request.addfinalizer(
                        partial(
                            s3_client.remove_file,
                            filename=f"test/sub_{i}/{image.name}",
                        )
                    )

        if use_manifest:
            with TemporaryDirectory() as tmp_dir:
                manifest_root_path = f"{tmp_dir}/test/"
                for i in range(2):
                    path_with_sub_folders = f"{tmp_dir}/test/sub_{i}/"
                    os.makedirs(path_with_sub_folders)
                    for image in images:
                        with open(osp.join(path_with_sub_folders, image.name), "wb") as f:
                            f.write(image.getvalue())

                generate_manifest(manifest_root_path)

                with open(osp.join(manifest_root_path, "manifest.jsonl"), mode="rb") as m_file:
                    s3_client.create_file(
                        data=m_file.read(),
                        filename="test/manifest.jsonl",
                    )
                    request.addfinalizer(
                        partial(
                            s3_client.remove_file,
                            filename="test/manifest.jsonl",
                        )
                    )
        task_spec = {
            "name": f"Task created from directories from cloud storage {cloud_storage['id']}",
            "labels": [
                {
                    "name": "car",
                }
            ],
            **(task_spec_kwargs or {}),
        }

        data_spec = {
            "image_quality": 75,
            "use_cache": use_cache,
            "cloud_storage_id": cloud_storage["id"],
            "server_files": (
                server_files if not use_manifest else server_files + ["test/manifest.jsonl"]
            ),
            "sorting_method": sorting_method,
            **(data_spec_kwargs or {}),
        }

        if server_files_exclude:
            data_spec["server_files_exclude"] = server_files_exclude

        return create_task(self._USERNAME, task_spec, data_spec, org=org)

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("cloud_storage_id", [2])
    @pytest.mark.parametrize(
        "use_cache, use_manifest, server_files, server_files_exclude, task_size",
        [
            (True, False, ["test/"], None, 6),
            (True, False, ["test/sub_0/", "test/sub_1/"], None, 6),
            (True, False, ["test/"], ["test/sub_0/", "test/sub_1/img_1.jpeg"], 2),
            (True, True, ["test/"], None, 6),
            (True, True, ["test/sub_0/", "test/sub_1/"], None, 6),
            (True, True, ["test/"], ["test/sub_0/", "test/sub_1/img_1.jpeg"], 2),
            (False, False, ["test/"], None, 6),
            (False, False, ["test/sub_0/", "test/sub_1/"], None, 6),
            (False, False, ["test/"], ["test/sub_0/", "test/sub_1/img_1.jpeg"], 2),
        ],
    )
    @pytest.mark.parametrize("org", [""])
    def test_create_task_with_cloud_storage_directories_and_excluded_files(
        self,
        cloud_storage_id: int,
        use_cache: bool,
        use_manifest: bool,
        server_files: list[str],
        server_files_exclude: Optional[list[str]],
        task_size: int,
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        task_id, _ = self._create_task_with_cloud_data(
            request,
            cloud_storage,
            use_manifest,
            server_files,
            use_cache=use_cache,
            server_files_exclude=server_files_exclude,
            org=org,
        )

        with make_api_client(self._USERNAME) as api_client:
            (task, response) = api_client.tasks_api.retrieve(task_id)
            assert response.status == HTTPStatus.OK
            assert task.size == task_size

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("cloud_storage_id", [2])
    @pytest.mark.parametrize("use_manifest", [True, False])
    @pytest.mark.parametrize(
        "server_files, expected_result",
        [
            (
                ["test/sub_1/", "test/sub_0/"],
                [
                    "test/sub_1/img_0.jpeg",
                    "test/sub_1/img_1.jpeg",
                    "test/sub_1/img_2.jpeg",
                    "test/sub_0/img_0.jpeg",
                    "test/sub_0/img_1.jpeg",
                    "test/sub_0/img_2.jpeg",
                ],
            )
        ],
    )
    @pytest.mark.parametrize("org", [""])
    def test_create_task_with_cloud_storage_directories_and_predefined_sorting(
        self,
        cloud_storage_id: int,
        use_manifest: bool,
        server_files: list[str],
        expected_result: list[str],
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        task_id, _ = self._create_task_with_cloud_data(
            request, cloud_storage, use_manifest, server_files, sorting_method="predefined", org=org
        )

        with make_api_client(self._USERNAME) as api_client:
            (_, response) = api_client.tasks_api.retrieve(task_id)
            assert response.status == HTTPStatus.OK

            # check sequence of frames
            (data_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            assert expected_result == list(map(lambda x: x.name, data_meta.frames))

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "storage_id, manifest",
        [
            (1, "manifest.jsonl"),  # public bucket
            (2, "sub/manifest.jsonl"),  # private bucket
        ],
    )
    @pytest.mark.parametrize(
        "spec, field",
        [
            ("spec", "source_storage"),
            ("spec", "target_storage"),
            ("data", "cloud_storage_id"),
        ],
    )
    def test_user_cannot_create_task_with_cloud_storage_without_access(
        self, storage_id, spec, field, manifest, regular_lonely_user
    ):
        user = regular_lonely_user

        task_spec = {
            "name": f"Task with files from foreign cloud storage {storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        data_spec = {
            "image_quality": 75,
            "use_cache": True,
        }

        if spec == "spec":
            task_spec.update(
                {
                    field: {
                        "location": "cloud_storage",
                        "cloud_storage_id": storage_id,
                    }
                }
            )
            data_spec["server_files"] = ["images/image_1.jpg"]

        elif spec == "data":
            data_spec.update(
                {
                    field: storage_id,
                    "filename_pattern": "*",
                    "server_files": [manifest],
                }
            )
        else:
            assert False

        with pytest.raises(exceptions.ApiException) as capture:
            create_task(user, task_spec, data_spec)

        assert capture.value.status == HTTPStatus.FORBIDDEN

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("cloud_storage_id", [1])
    @pytest.mark.parametrize(
        "manifest, filename_pattern, sub_dir, task_size",
        [
            ("manifest.jsonl", "*", True, 3),  # public bucket
            ("manifest.jsonl", "test/*", True, 3),
            ("manifest.jsonl", "test/sub*1.jpeg", True, 1),
            ("manifest.jsonl", "*image*.jpeg", True, 3),
            ("manifest.jsonl", "wrong_pattern", True, 0),
            ("abc_manifest.jsonl", "[a-c]*.jpeg", False, 2),
            ("abc_manifest.jsonl", "[d]*.jpeg", False, 1),
            ("abc_manifest.jsonl", "[e-z]*.jpeg", False, 0),
            (None, "*", True, 5),
            (None, "test/*", True, 3),
            (None, "test/sub*1.jpeg", True, 1),
            (None, "*image*.jpeg", True, 3),
            (None, "wrong_pattern", True, 0),
            (None, "[a-c]*.jpeg", False, 2),
            (None, "[d]*.jpeg", False, 1),
            (None, "[e-z]*.jpeg", False, 0),
        ],
    )
    def test_create_task_with_file_pattern(
        self,
        cloud_storage_id,
        manifest,
        filename_pattern,
        sub_dir,
        task_size,
        cloud_storages,
        request,
    ):
        # prepare dataset on the bucket
        prefixes = ("test_image_",) * 3 if sub_dir else ("a_", "b_", "d_")
        images = generate_image_files(3, prefixes=prefixes)
        s3_client = s3.make_client()

        cloud_storage = cloud_storages[cloud_storage_id]

        for image in images:
            s3_client.create_file(
                data=image,
                bucket=cloud_storage["resource"],
                filename=f"{'test/sub/' if sub_dir else ''}{image.name}",
            )
            request.addfinalizer(
                partial(
                    s3_client.remove_file,
                    bucket=cloud_storage["resource"],
                    filename=f"{'test/sub/' if sub_dir else ''}{image.name}",
                )
            )

        if manifest:
            with TemporaryDirectory() as tmp_dir:
                for image in images:
                    with open(osp.join(tmp_dir, image.name), "wb") as f:
                        f.write(image.getvalue())

                generate_manifest(tmp_dir)

                with open(osp.join(tmp_dir, "manifest.jsonl"), mode="rb") as m_file:
                    s3_client.create_file(
                        data=m_file.read(),
                        bucket=cloud_storage["resource"],
                        filename=f"test/sub/{manifest}" if sub_dir else manifest,
                    )
                    request.addfinalizer(
                        partial(
                            s3_client.remove_file,
                            bucket=cloud_storage["resource"],
                            filename=f"test/sub/{manifest}" if sub_dir else manifest,
                        )
                    )

        task_spec = {
            "name": f"Task with files from cloud storage {cloud_storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        data_spec = {
            "image_quality": 75,
            "use_cache": True,
            "cloud_storage_id": cloud_storage_id,
            "filename_pattern": filename_pattern,
        }
        if manifest:
            data_spec["server_files"] = [f"test/sub/{manifest}" if sub_dir else manifest]

        if task_size:
            task_id, _ = create_task(self._USERNAME, task_spec, data_spec)

            with make_api_client(self._USERNAME) as api_client:
                (task, response) = api_client.tasks_api.retrieve(task_id)
                assert response.status == HTTPStatus.OK
                assert task.size == task_size
        else:
            rq_job_details = self._test_cannot_create_task(self._USERNAME, task_spec, data_spec)
            assert "No media data found" in rq_job_details.message

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("use_manifest", [True, False])
    @pytest.mark.parametrize("use_cache", [True, False])
    @pytest.mark.parametrize(
        "sorting_method", ["natural", "predefined", "lexicographical", "random"]
    )
    @pytest.mark.parametrize(
        "cloud_storage_id, org",
        [
            (1, ""),
        ],
    )
    def test_create_task_with_cloud_storage_and_retrieve_data(
        self,
        use_manifest: bool,
        use_cache: bool,
        sorting_method: str,
        cloud_storage_id: int,
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        task_id, _ = self._create_task_with_cloud_data(
            request=request,
            cloud_storage=cloud_storage,
            # manifest file should not be uploaded if random sorting is used or if cache is not used
            use_manifest=use_manifest and use_cache and (sorting_method != "random"),
            use_cache=use_cache,
            server_files=[f"test/sub_{i}/img_{j}.jpeg" for i in range(2) for j in range(3)],
            org=org,
            sorting_method=sorting_method,
        )

        with make_api_client(self._USERNAME) as api_client:
            (_, response) = api_client.tasks_api.retrieve_data(
                task_id, type="chunk", quality="compressed", number=0
            )
            assert response.status == HTTPStatus.OK

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "filenames, sorting_method",
        [
            (["img_1.jpeg", "img_2.jpeg", "img_10.jpeg"], "natural"),
            (["img_10.jpeg", "img_1.jpeg", "img_2.jpeg"], "predefined"),
            (["img_1.jpeg", "img_10.jpeg", "img_2.jpeg"], "lexicographical"),
        ],
    )
    @pytest.mark.parametrize(
        "cloud_storage_id, org",
        [
            (1, ""),
        ],
    )
    def test_create_task_with_cloud_storage_and_check_data_sorting(
        self,
        filenames: list[str],
        sorting_method: str,
        cloud_storage_id: int,
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]

        task_id, _ = self._create_task_with_cloud_data(
            request=request,
            cloud_storage=cloud_storage,
            use_manifest=False,
            use_cache=True,
            server_files=["test/sub_0/" + f for f in filenames],
            org=org,
            sorting_method=sorting_method,
            filenames=filenames,
        )

        with make_api_client(self._USERNAME) as api_client:
            data_meta, _ = api_client.tasks_api.retrieve_data_meta(task_id)

            for image_name, frame in zip(filenames, data_meta.frames):
                assert frame.name.rsplit("/", maxsplit=1)[1] == image_name

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "cloud_storage_id, org",
        [
            (1, ""),
        ],
    )
    def test_create_task_with_cloud_storage_and_check_retrieve_data_meta(
        self,
        cloud_storage_id: int,
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]

        data_spec = {
            "start_frame": 2,
            "stop_frame": 6,
            "frame_filter": "step=2",
        }

        task_id, _ = self._create_task_with_cloud_data(
            request=request,
            cloud_storage=cloud_storage,
            use_manifest=False,
            use_cache=False,
            server_files=["test/video/video.avi"],
            org=org,
            data_spec_kwargs=data_spec,
            data_type="video",
        )

        with make_api_client(self._USERNAME) as api_client:
            data_meta, _ = api_client.tasks_api.retrieve_data_meta(task_id)

        assert data_meta.start_frame == 2
        assert data_meta.stop_frame == 6
        assert data_meta.size == 3

    def test_can_specify_file_job_mapping(self):
        task_spec = {
            "name": f"test file-job mapping",
            "labels": [{"name": "car"}],
        }

        files = generate_image_files(7)
        filenames = [osp.basename(f.name) for f in files]
        expected_segments = [
            filenames[0:1],
            filenames[1:5][::-1],  # a reversed fragment
            filenames[5:7],
        ]

        data_spec = {
            "image_quality": 75,
            "client_files": files,
            "job_file_mapping": expected_segments,
        }

        task_id, _ = create_task(
            self._USERNAME, task_spec, data_spec, content_type="application/json"
        )

        with make_api_client(self._USERNAME) as api_client:
            jobs: list[models.JobRead] = get_paginated_collection(
                api_client.jobs_api.list_endpoint, task_id=task_id, sort="id"
            )
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(id=task_id)

            assert [f.name for f in task_meta.frames] == list(
                chain.from_iterable(expected_segments)
            )

            start_frame = 0
            for i, job in enumerate(jobs):
                expected_size = len(expected_segments[i])
                stop_frame = start_frame + expected_size - 1
                assert job.start_frame == start_frame
                assert job.stop_frame == stop_frame

                start_frame = stop_frame + 1

    def test_cannot_create_task_with_same_labels(self):
        task_spec = {
            "name": "test cannot create task with same labels",
            "labels": [{"name": "l1"}, {"name": "l1"}],
        }
        response = post_method(self._USERNAME, "tasks", task_spec)
        assert response.status_code == HTTPStatus.BAD_REQUEST

        response = get_method(self._USERNAME, "tasks")
        assert response.status_code == HTTPStatus.OK

    def test_cannot_create_task_with_same_skeleton_sublabels(self):
        task_spec = {
            "name": "test cannot create task with same skeleton sublabels",
            "labels": [
                {"name": "s1", "type": "skeleton", "sublabels": [{"name": "1"}, {"name": "1"}]}
            ],
        }
        response = post_method(self._USERNAME, "tasks", task_spec)
        assert response.status_code == HTTPStatus.BAD_REQUEST

        response = get_method(self._USERNAME, "tasks")
        assert response.status_code == HTTPStatus.OK

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("cloud_storage_id", [2])
    @pytest.mark.parametrize("use_manifest", [True, False])
    @pytest.mark.parametrize("server_files", [["test/"]])
    @pytest.mark.parametrize(
        "default_prefix, expected_task_size",
        [
            (
                "test/sub_1/img_0",
                1,
            ),
            (
                "test/sub_1/",
                3,
            ),
        ],
    )
    @pytest.mark.parametrize("org", [""])
    def test_create_task_with_cloud_storage_directories_and_default_bucket_prefix(
        self,
        cloud_storage_id: int,
        use_manifest: bool,
        server_files: list[str],
        default_prefix: str,
        expected_task_size: int,
        org: str,
        cloud_storages,
        request,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]

        with make_api_client(self._USERNAME) as api_client:
            (_, response) = api_client.cloudstorages_api.partial_update(
                cloud_storage_id,
                patched_cloud_storage_write_request={
                    "specific_attributes": f'{cloud_storage["specific_attributes"]}&prefix={default_prefix}'
                },
            )
            assert response.status == HTTPStatus.OK

        task_id, _ = self._create_task_with_cloud_data(
            request, cloud_storage, use_manifest, server_files, org=org
        )

        with make_api_client(self._USERNAME) as api_client:
            (task, response) = api_client.tasks_api.retrieve(task_id)
            assert response.status == HTTPStatus.OK
            assert task.size == expected_task_size

    @parametrize(
        "frame_selection_method, method_params, per_job_count_param",
        map(
            lambda e: (*e[0], e[1]),
            product(
                [
                    *tuple(product(["random_uniform"], [{"frame_count"}, {"frame_share"}])),
                    ("manual", {}),
                ],
                ["frames_per_job_count", "frames_per_job_share"],
            ),
        ),
    )
    def test_can_create_task_with_honeypots(
        self,
        fxt_test_name,
        frame_selection_method: str,
        method_params: set[str],
        per_job_count_param: str,
    ):
        base_segment_size = 4
        total_frame_count = 15
        validation_frames_count = 5
        validation_per_job_count = 2
        regular_frame_count = total_frame_count - validation_frames_count
        resulting_task_size = (
            regular_frame_count
            + validation_per_job_count * math.ceil(regular_frame_count / base_segment_size)
            + validation_frames_count
        )

        image_files = generate_image_files(total_frame_count)

        validation_params = {"mode": "gt_pool", "frame_selection_method": frame_selection_method}

        if per_job_count_param == "frames_per_job_count":
            validation_params[per_job_count_param] = validation_per_job_count
        elif per_job_count_param == "frames_per_job_share":
            validation_params[per_job_count_param] = validation_per_job_count / base_segment_size
        else:
            assert False

        if frame_selection_method == "random_uniform":
            validation_params["random_seed"] = 42

            for method_param in method_params:
                if method_param == "frame_count":
                    validation_params[method_param] = validation_frames_count
                elif method_param == "frame_share":
                    validation_params[method_param] = validation_frames_count / total_frame_count
                else:
                    assert False
        elif frame_selection_method == "manual":
            rng = np.random.Generator(np.random.MT19937(seed=42))
            validation_params["frames"] = rng.choice(
                [f.name for f in image_files], validation_frames_count, replace=False
            ).tolist()
        else:
            assert False

        task_params = {
            "name": fxt_test_name,
            "labels": [{"name": "a"}],
            "segment_size": base_segment_size,
        }

        data_params = {
            "image_quality": 70,
            "client_files": image_files,
            "sorting_method": "random",
            "validation_params": validation_params,
        }

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)

        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            annotation_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="annotation"
                )
            ]
            gt_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
                )
            ]

            assert len(gt_job_metas) == 1

        assert task.segment_size == 0  # means "custom segments"
        assert task.size == resulting_task_size
        assert task_meta.size == resulting_task_size

        # validation frames (pool frames) must be appended in the end of the task, in the GT job
        validation_frames = set(f.name for f in task_meta.frames[-validation_frames_count:])
        if frame_selection_method == "manual":
            assert sorted(validation_frames) == sorted(validation_params["frames"])
            assert sorted(f.name for f in gt_job_metas[0].frames) == sorted(
                validation_params["frames"]
            )

        annotation_job_frame_counts = Counter(
            f.name for f in task_meta.frames[:-validation_frames_count]
        )

        regular_frame_counts = {
            k: v for k, v in annotation_job_frame_counts.items() if k not in validation_frames
        }
        # regular frames must not repeat
        assert regular_frame_counts == {
            f.name: 1 for f in image_files if f.name not in validation_frames
        }

        # only validation frames can repeat
        assert set(fn for fn, count in annotation_job_frame_counts.items() if count != 1).issubset(
            validation_frames
        )

        if frame_selection_method == "random_uniform":
            # Test distribution
            validation_frame_counts = {
                f: annotation_job_frame_counts.get(f, 0) + 1 for f in validation_frames
            }
            assert max(validation_frame_counts.values()) <= 1 + min(
                validation_frame_counts.values()
            )

        # each job must have the specified number of validation frames
        for job_meta in annotation_job_metas:
            assert (
                len(set(f.name for f in job_meta.frames if f.name in validation_frames))
                == validation_per_job_count
            )

    @pytest.mark.parametrize("random_seed", [1, 2, 5])
    def test_can_create_task_with_honeypots_random_seed_guarantees_the_same_layout(
        self, fxt_test_name, random_seed: int
    ):
        base_segment_size = 4
        total_frame_count = 15
        validation_frames_count = 5
        validation_per_job_count = 2

        image_files = generate_image_files(total_frame_count)

        validation_params = {
            "mode": "gt_pool",
            "frame_selection_method": "random_uniform",
            "frame_count": validation_frames_count,
            "frames_per_job_count": validation_per_job_count,
            "random_seed": random_seed,
        }

        task_params = {
            "name": fxt_test_name,
            "labels": [{"name": "a"}],
            "segment_size": base_segment_size,
        }

        data_params = {
            "image_quality": 70,
            "client_files": image_files,
            "sorting_method": "random",
            "validation_params": validation_params,
        }

        def _create_task():
            with make_api_client(self._USERNAME) as api_client:
                task_id, _ = create_task(
                    self._USERNAME, spec=deepcopy(task_params), data=deepcopy(data_params)
                )
                task_meta = json.loads(api_client.tasks_api.retrieve_data_meta(task_id)[1].data)
                task_validation_layout = json.loads(
                    api_client.tasks_api.retrieve_validation_layout(task_id)[1].data
                )
                return task_meta, task_validation_layout

        task1_meta, task1_validation_layout = _create_task()
        task2_meta, task2_validation_layout = _create_task()

        assert (
            DeepDiff(
                task1_meta,
                task2_meta,
                ignore_order=False,
                exclude_regex_paths=[r"root\['chunks_updated_date'\]"],  # must be different
            )
            == {}
        )
        assert DeepDiff(task1_validation_layout, task2_validation_layout, ignore_order=False) == {}

    @parametrize(
        "frame_selection_method, method_params",
        [
            *tuple(product(["random_uniform"], [{"frame_count"}, {"frame_share"}])),
            *tuple(
                product(["random_per_job"], [{"frames_per_job_count"}, {"frames_per_job_share"}])
            ),
            ("manual", {}),
        ],
        idgen=lambda **args: "-".join([args["frame_selection_method"], *args["method_params"]]),
    )
    def test_can_create_task_with_gt_job_from_images(
        self,
        request: pytest.FixtureRequest,
        frame_selection_method: str,
        method_params: set[str],
    ):
        segment_size = 4
        total_frame_count = 15
        resulting_task_size = total_frame_count

        image_files = generate_image_files(total_frame_count)

        validation_params = {"mode": "gt", "frame_selection_method": frame_selection_method}

        if "random" in frame_selection_method:
            validation_params["random_seed"] = 42

        if frame_selection_method == "random_uniform":
            validation_frames_count = 5

            for method_param in method_params:
                if method_param == "frame_count":
                    validation_params[method_param] = validation_frames_count
                elif method_param == "frame_share":
                    validation_params[method_param] = validation_frames_count / total_frame_count
                else:
                    assert False
        elif frame_selection_method == "random_per_job":
            validation_per_job_count = 2
            validation_frames_count = validation_per_job_count * math.ceil(
                total_frame_count / segment_size
            )

            for method_param in method_params:
                if method_param == "frames_per_job_count":
                    validation_params[method_param] = validation_per_job_count
                elif method_param == "frames_per_job_share":
                    validation_params[method_param] = validation_per_job_count / segment_size
                else:
                    assert False
        elif frame_selection_method == "manual":
            validation_frames_count = 5

            rng = np.random.Generator(np.random.MT19937(seed=42))
            validation_params["frames"] = rng.choice(
                [f.name for f in image_files], validation_frames_count, replace=False
            ).tolist()
        else:
            assert False

        task_params = {
            "name": request.node.name,
            "labels": [{"name": "a"}],
            "segment_size": segment_size,
        }

        data_params = {
            "image_quality": 70,
            "client_files": image_files,
            "validation_params": validation_params,
        }

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)

        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            annotation_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="annotation"
                )
            ]
            gt_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
                )
            ]

            assert len(gt_job_metas) == 1

            if frame_selection_method in ("random_uniform", "manual"):
                assert gt_job_metas[0].size == validation_frames_count
            elif frame_selection_method == "random_per_job":
                assert gt_job_metas[0].size == (
                    resulting_task_size // segment_size * validation_per_job_count
                    + min(resulting_task_size % segment_size, validation_per_job_count)
                )
            else:
                assert False

        assert task.segment_size == segment_size
        assert task.size == resulting_task_size
        assert task_meta.size == resulting_task_size

        validation_frames = [
            gt_job_metas[0].frames[rel_frame_id].name
            for rel_frame_id, abs_frame_id in enumerate(
                range(
                    gt_job_metas[0].start_frame,
                    gt_job_metas[0].stop_frame + 1,
                    int((gt_job_metas[0].frame_filter or "step=1").split("=")[1]),
                )
            )
            if abs_frame_id in gt_job_metas[0].included_frames
        ]
        if frame_selection_method == "manual":
            assert sorted(validation_params["frames"]) == sorted(validation_frames)

        assert len(validation_frames) == validation_frames_count

        # frames must not repeat
        assert sorted(f.name for f in image_files) == sorted(f.name for f in task_meta.frames)

        if frame_selection_method == "random_per_job":
            # each job must have the specified number of validation frames
            for job_meta in annotation_job_metas:
                assert (
                    len([f.name for f in job_meta.frames if f.name in validation_frames])
                    == validation_per_job_count
                )

    @parametrize(
        "frame_selection_method, method_params",
        [
            *tuple(product(["random_uniform"], [{"frame_count"}, {"frame_share"}])),
            *tuple(
                product(["random_per_job"], [{"frames_per_job_count"}, {"frames_per_job_share"}])
            ),
        ],
        idgen=lambda **args: "-".join([args["frame_selection_method"], *args["method_params"]]),
    )
    def test_can_create_task_with_gt_job_from_video(
        self,
        request: pytest.FixtureRequest,
        frame_selection_method: str,
        method_params: set[str],
    ):
        segment_size = 4
        total_frame_count = 15
        resulting_task_size = total_frame_count

        video_file = generate_video_file(total_frame_count)

        validation_params = {"mode": "gt", "frame_selection_method": frame_selection_method}

        if "random" in frame_selection_method:
            validation_params["random_seed"] = 42

        if frame_selection_method == "random_uniform":
            validation_frames_count = 5

            for method_param in method_params:
                if method_param == "frame_count":
                    validation_params[method_param] = validation_frames_count
                elif method_param == "frame_share":
                    validation_params[method_param] = validation_frames_count / total_frame_count
                else:
                    assert False
        elif frame_selection_method == "random_per_job":
            validation_per_job_count = 2

            for method_param in method_params:
                if method_param == "frames_per_job_count":
                    validation_params[method_param] = validation_per_job_count
                elif method_param == "frames_per_job_share":
                    validation_params[method_param] = validation_per_job_count / segment_size
                else:
                    assert False

        task_params = {
            "name": request.node.name,
            "labels": [{"name": "a"}],
            "segment_size": segment_size,
        }

        data_params = {
            "image_quality": 70,
            "client_files": [video_file],
            "validation_params": validation_params,
        }

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)

        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            annotation_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="annotation"
                )
            ]
            gt_job_metas = [
                api_client.jobs_api.retrieve_data_meta(job.id)[0]
                for job in get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
                )
            ]

            assert len(gt_job_metas) == 1

            if frame_selection_method == "random_uniform":
                assert gt_job_metas[0].size == validation_frames_count
            elif frame_selection_method == "random_per_job":
                assert gt_job_metas[0].size == (
                    resulting_task_size // segment_size * validation_per_job_count
                    + min(resulting_task_size % segment_size, validation_per_job_count)
                )
            else:
                assert False

        assert task.segment_size == segment_size
        assert task.size == resulting_task_size
        assert task_meta.size == resulting_task_size

        frame_step = parse_frame_step(gt_job_metas[0].frame_filter)
        validation_frames = [
            abs_frame_id
            for abs_frame_id in range(
                gt_job_metas[0].start_frame,
                gt_job_metas[0].stop_frame + 1,
                frame_step,
            )
            if abs_frame_id in gt_job_metas[0].included_frames
        ]

        if frame_selection_method == "random_per_job":
            # each job must have the specified number of validation frames
            for job_meta in annotation_job_metas:
                assert (
                    len(
                        set(
                            range(job_meta.start_frame, job_meta.stop_frame + 1, frame_step)
                        ).intersection(validation_frames)
                    )
                    == validation_per_job_count
                )
        else:
            assert len(validation_frames) == validation_frames_count

    @pytest.mark.with_external_services
    @pytest.mark.parametrize("cloud_storage_id", [2])
    @pytest.mark.parametrize(
        "validation_mode",
        [
            models.ValidationMode("gt"),
            models.ValidationMode("gt_pool"),
        ],
    )
    def test_can_create_task_with_validation_and_cloud_data(
        self,
        cloud_storage_id: int,
        validation_mode: models.ValidationMode,
        request: pytest.FixtureRequest,
        admin_user: str,
        cloud_storages: Iterable,
    ):
        cloud_storage = cloud_storages[cloud_storage_id]
        server_files = [f"test/sub_0/img_{i}.jpeg" for i in range(3)]
        validation_frames = ["test/sub_0/img_1.jpeg"]

        (task_id, _) = self._create_task_with_cloud_data(
            request,
            cloud_storage,
            use_manifest=False,
            server_files=server_files,
            sorting_method=models.SortingMethod(
                "random"
            ),  # only random sorting can be used with gt_pool
            data_spec_kwargs={
                "validation_params": models.DataRequestValidationParams._from_openapi_data(
                    mode=validation_mode,
                    frames=validation_frames,
                    frame_selection_method=models.FrameSelectionMethod("manual"),
                    frames_per_job_count=1,
                )
            },
            task_spec_kwargs={
                # in case of gt_pool: each regular job will contain 1 regular and 1 validation frames,
                # (number of validation frames is not included into segment_size)
                "segment_size": 1,
            },
        )

        with make_api_client(admin_user) as api_client:
            # check that GT job was created
            (paginated_jobs, _) = api_client.jobs_api.list(task_id=task_id, type="ground_truth")
            assert 1 == len(paginated_jobs["results"])

            (paginated_jobs, _) = api_client.jobs_api.list(task_id=task_id, type="annotation")
            jobs_count = (
                len(server_files) - len(validation_frames)
                if validation_mode == models.ValidationMode("gt_pool")
                else len(server_files)
            )
            assert jobs_count == len(paginated_jobs["results"])
            # check that the returned meta of images corresponds to the chunk data
            # Note: meta is based on the order of images from database
            # while chunk with CS data is based on the order of images in a manifest
            for job in paginated_jobs["results"]:
                (job_meta, _) = api_client.jobs_api.retrieve_data_meta(job["id"])
                (_, response) = api_client.jobs_api.retrieve_data(
                    job["id"], type="chunk", quality="compressed", index=0
                )
                chunk_file = io.BytesIO(response.data)
                assert zipfile.is_zipfile(chunk_file)

                with zipfile.ZipFile(chunk_file, "r") as chunk_archive:
                    chunk_images = {
                        int(os.path.splitext(name)[0]): np.array(
                            Image.open(io.BytesIO(chunk_archive.read(name)))
                        )
                        for name in chunk_archive.namelist()
                    }
                    chunk_images = dict(sorted(chunk_images.items(), key=lambda e: e[0]))

                    for img, img_meta in zip(chunk_images.values(), job_meta.frames):
                        assert (img.shape[0], img.shape[1]) == (img_meta.height, img_meta.width)

    def test_can_create_task_with_consensus(self, request: pytest.FixtureRequest):
        segment_size = 2
        regular_job_count = 2
        replication = 2
        images = generate_image_files(segment_size * regular_job_count)
        resulting_task_size = len(images)

        task_params = {
            "name": request.node.name,
            "labels": [{"name": "a"}],
            "segment_size": segment_size,
            "consensus_replicas": replication,
        }

        data_params = {
            "image_quality": 70,
            "client_files": images,
        }

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)

        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
            jobs = get_paginated_collection(api_client.jobs_api.list_endpoint, task_id=task_id)

            annotation_job_metas = {
                job.id: json.loads(api_client.jobs_api.retrieve_data_meta(job.id)[1].data)
                for job in jobs
                if job.type == "annotation"
            }
            consensus_job_metas = {
                job.id: json.loads(api_client.jobs_api.retrieve_data_meta(job.id)[1].data)
                for job in jobs
                if job.type == "consensus_replica"
            }

        assert task.segment_size == segment_size
        assert task.size == resulting_task_size
        assert task_meta.size == resulting_task_size

        assert len(jobs) == regular_job_count * (1 + replication)
        assert len(annotation_job_metas) == regular_job_count
        assert len(consensus_job_metas) == regular_job_count * replication

        for annotation_job in (j for j in jobs if j.type == "annotation"):
            assert annotation_job_metas[annotation_job.id]["size"] == segment_size

            job_replicas = [j for j in jobs if j.parent_job_id == annotation_job.id]
            assert len(job_replicas) == replication

            for replica in job_replicas:
                assert (
                    DeepDiff(
                        consensus_job_metas[replica.id], annotation_job_metas[annotation_job.id]
                    )
                    == {}
                )


class _SourceDataType(Enum):
    images = "images"
    video = "video"


class _TaskSpec(models.ITaskWriteRequest, models.IDataRequest, metaclass=ABCMeta):
    size: int
    frame_step: int
    source_data_type: _SourceDataType

    @abstractmethod
    def read_frame(self, i: int) -> Image.Image: ...


@attrs.define
class _TaskSpecBase(_TaskSpec):
    _params: Union[dict, models.TaskWriteRequest]
    _data_params: Union[dict, models.DataRequest]
    size: int = attrs.field(kw_only=True)

    @property
    def frame_step(self) -> int:
        return parse_frame_step(getattr(self, "frame_filter", ""))

    def __getattr__(self, k: str) -> Any:
        notfound = object()

        for params in [self._params, self._data_params]:
            if isinstance(params, dict):
                v = params.get(k, notfound)
            else:
                v = getattr(params, k, notfound)

            if v is not notfound:
                return v

        raise AttributeError(k)


@attrs.define
class _ImagesTaskSpec(_TaskSpecBase):
    source_data_type: ClassVar[_SourceDataType] = _SourceDataType.images

    _get_frame: Callable[[int], bytes] = attrs.field(kw_only=True)

    def read_frame(self, i: int) -> Image.Image:
        return Image.open(io.BytesIO(self._get_frame(i)))


@attrs.define
class _VideoTaskSpec(_TaskSpecBase):
    source_data_type: ClassVar[_SourceDataType] = _SourceDataType.video

    _get_video_file: Callable[[], io.IOBase] = attrs.field(kw_only=True)

    def read_frame(self, i: int) -> Image.Image:
        with closing(read_video_file(self._get_video_file())) as reader:
            for _ in range(i + 1):
                frame = next(reader)

            return frame


class _TestTasksBase:
    _USERNAME = "admin1"

    def _image_task_fxt_base(
        self,
        request: pytest.FixtureRequest,
        *,
        frame_count: Optional[int] = 10,
        image_files: Optional[Sequence[io.BytesIO]] = None,
        start_frame: Optional[int] = None,
        stop_frame: Optional[int] = None,
        step: Optional[int] = None,
        segment_size: Optional[int] = None,
        server_files: Optional[Sequence[str]] = None,
        cloud_storage_id: Optional[int] = None,
        job_replication: Optional[int] = None,
        **data_kwargs,
    ) -> Generator[tuple[_ImagesTaskSpec, int], None, None]:
        task_params = {
            "name": f"{request.node.name}[{request.fixturename}]",
            "labels": [{"name": "a"}],
            **({"segment_size": segment_size} if segment_size else {}),
            **({"consensus_replicas": job_replication} if job_replication else {}),
        }

        if server_files is not None:
            assert (
                image_files is not None
            ), "'server_files' must be used together with 'image_files'"
        else:
            assert bool(image_files) ^ bool(
                frame_count
            ), "Expected only one of 'image_files' and 'frame_count'"
            if not image_files:
                image_files = generate_image_files(frame_count)

        images_data = [f.getvalue() for f in image_files]

        resulting_task_size = len(
            range(start_frame or 0, (stop_frame or len(images_data) - 1) + 1, step or 1)
        )

        data_params = {
            "image_quality": 70,
            "sorting_method": "natural",
            "chunk_size": max(1, (segment_size or resulting_task_size) // 2),
            **(
                {
                    "server_files": server_files,
                    "cloud_storage_id": cloud_storage_id,
                }
                if server_files
                else {"client_files": image_files}
            ),
        }
        data_params.update(data_kwargs)

        if start_frame is not None:
            data_params["start_frame"] = start_frame

        if stop_frame is not None:
            data_params["stop_frame"] = stop_frame

        if step is not None:
            data_params["frame_filter"] = f"step={step}"

        def get_frame(i: int) -> bytes:
            return images_data[i]

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)
        yield _ImagesTaskSpec(
            models.TaskWriteRequest._from_openapi_data(**task_params),
            models.DataRequest._from_openapi_data(**data_params),
            get_frame=get_frame,
            size=resulting_task_size,
        ), task_id

    @pytest.fixture(scope="class")
    def fxt_uploaded_images_task(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_fxt_base(request=request)

    @pytest.fixture(scope="class")
    def fxt_uploaded_images_task_with_segments(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_fxt_base(request=request, segment_size=4)

    @fixture(scope="class")
    @parametrize("step", [2, 5])
    @parametrize("stop_frame", [15, 26])
    @parametrize("start_frame", [3, 7])
    def fxt_uploaded_images_task_with_segments_start_stop_step(
        self, request: pytest.FixtureRequest, start_frame: int, stop_frame: Optional[int], step: int
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_fxt_base(
            request=request,
            frame_count=30,
            segment_size=4,
            start_frame=start_frame,
            stop_frame=stop_frame,
            step=step,
        )

    @pytest.fixture(scope="class")
    def fxt_uploaded_images_task_with_segments_and_consensus(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_fxt_base(request=request, segment_size=4, job_replication=2)

    def _image_task_with_honeypots_and_segments_base(
        self,
        request: pytest.FixtureRequest,
        *,
        start_frame: Optional[int] = None,
        step: Optional[int] = None,
        random_seed: int = 42,
        image_files: Optional[Sequence[io.BytesIO]] = None,
        server_files: Optional[Sequence[str]] = None,
        cloud_storage_id: Optional[int] = None,
        **kwargs,
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        validation_params = models.DataRequestValidationParams._from_openapi_data(
            mode="gt_pool",
            frame_selection_method="random_uniform",
            random_seed=random_seed,
            frame_count=5,
            frames_per_job_count=2,
        )

        used_frames_count = 15
        total_frame_count = (start_frame or 0) + used_frames_count * (step or 1)
        base_segment_size = 4
        regular_frame_count = used_frames_count - validation_params.frame_count
        final_segment_size = base_segment_size + validation_params.frames_per_job_count
        final_task_size = (
            regular_frame_count
            + validation_params.frames_per_job_count
            * math.ceil(regular_frame_count / base_segment_size)
            + validation_params.frame_count
        )

        if image_files:
            if len(image_files) != total_frame_count:
                raise ValueError(
                    f"If provided, image_files must contain {total_frame_count} images"
                )
        else:
            image_files = generate_image_files(total_frame_count)

        with closing(
            self._image_task_fxt_base(
                request=request,
                frame_count=None,
                image_files=image_files,
                segment_size=base_segment_size,
                sorting_method="random",
                start_frame=start_frame,
                step=step,
                validation_params=validation_params,
                server_files=server_files,
                cloud_storage_id=cloud_storage_id,
                **kwargs,
            )
        ) as task_gen:
            for task_spec, task_id in task_gen:
                # Get the actual frame order after the task is created
                with make_api_client(self._USERNAME) as api_client:
                    (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)
                    frame_map = [
                        next(i for i, f in enumerate(image_files) if f.name == frame_info.name)
                        for frame_info in task_meta.frames
                    ]

                _get_frame = task_spec._get_frame
                task_spec._get_frame = lambda i: _get_frame(frame_map[i])

                task_spec.size = final_task_size
                task_spec._params.segment_size = final_segment_size

                # These parameters are not applicable to the resulting task,
                # they are only effective during task creation
                if start_frame or step:
                    task_spec._data_params.start_frame = 0
                    task_spec._data_params.stop_frame = task_spec.size
                    task_spec._data_params.frame_filter = ""

                yield task_spec, task_id

    @fixture(scope="class")
    def fxt_uploaded_images_task_with_honeypots_and_segments(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_with_honeypots_and_segments_base(request)

    @fixture(scope="class")
    @parametrize("start_frame, step", [(2, 3)])
    def fxt_uploaded_images_task_with_honeypots_and_segments_start_step(
        self, request: pytest.FixtureRequest, start_frame: Optional[int], step: Optional[int]
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._image_task_with_honeypots_and_segments_base(
            request, start_frame=start_frame, step=step
        )

    def _images_task_with_honeypots_and_changed_real_frames_base(
        self, request: pytest.FixtureRequest, **kwargs
    ):
        with closing(
            self._image_task_with_honeypots_and_segments_base(
                request, start_frame=2, step=3, **kwargs
            )
        ) as gen_iter:
            task_spec, task_id = next(gen_iter)

            with make_api_client(self._USERNAME) as api_client:
                validation_layout, _ = api_client.tasks_api.retrieve_validation_layout(task_id)
                validation_frames = validation_layout.validation_frames

                new_honeypot_real_frames = [
                    validation_frames[(validation_frames.index(f) + 1) % len(validation_frames)]
                    for f in validation_layout.honeypot_real_frames
                ]
                api_client.tasks_api.partial_update_validation_layout(
                    task_id,
                    patched_task_validation_layout_write_request=(
                        models.PatchedTaskValidationLayoutWriteRequest(
                            frame_selection_method="manual",
                            honeypot_real_frames=new_honeypot_real_frames,
                        )
                    ),
                )

                # Get the new frame order
                frame_map = dict(zip(validation_layout.honeypot_frames, new_honeypot_real_frames))

                _get_frame = task_spec._get_frame
                task_spec._get_frame = lambda i: _get_frame(frame_map.get(i, i))

            yield task_spec, task_id

    @fixture(scope="class")
    @parametrize("random_seed", [1, 2, 5])
    def fxt_uploaded_images_task_with_honeypots_and_changed_real_frames(
        self, request: pytest.FixtureRequest, random_seed: int
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._images_task_with_honeypots_and_changed_real_frames_base(
            request, random_seed=random_seed
        )

    @fixture(scope="class")
    @parametrize(
        "cloud_storage_id",
        [pytest.param(2, marks=[pytest.mark.with_external_services, pytest.mark.timeout(60)])],
    )
    def fxt_cloud_images_task_with_honeypots_and_changed_real_frames(
        self, request: pytest.FixtureRequest, cloud_storages, cloud_storage_id: int
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        cloud_storage = cloud_storages[cloud_storage_id]
        s3_client = s3.make_client(bucket=cloud_storage["resource"])

        image_files = generate_image_files(47)

        for image in image_files:
            image.name = f"test/{image.name}"
            image.seek(0)

            s3_client.create_file(data=image, filename=image.name)
            request.addfinalizer(partial(s3_client.remove_file, filename=image.name))

        server_files = [f.name for f in image_files]

        for image in image_files:
            image.seek(0)

        yield from self._images_task_with_honeypots_and_changed_real_frames_base(
            request,
            image_files=image_files,
            server_files=server_files,
            cloud_storage_id=cloud_storage_id,
            # FIXME: random sorting with frame filter and cloud images (and, optionally, honeypots)
            # doesn't work with static cache
            # https://github.com/cvat-ai/cvat/issues/9021
            use_cache=True,
        )

    def _uploaded_images_task_with_gt_and_segments_base(
        self,
        request: pytest.FixtureRequest,
        *,
        start_frame: Optional[int] = None,
        step: Optional[int] = None,
        frame_selection_method: str = "random_uniform",
        job_replication: Optional[int] = None,
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        used_frames_count = 16
        total_frame_count = (start_frame or 0) + used_frames_count * (step or 1)
        segment_size = 5
        image_files = generate_image_files(total_frame_count)

        validation_params_kwargs = {"frame_selection_method": frame_selection_method}

        if "random" in frame_selection_method:
            validation_params_kwargs["random_seed"] = 42

        if frame_selection_method == "random_uniform":
            validation_frames_count = 10
            validation_params_kwargs["frame_count"] = validation_frames_count
        elif frame_selection_method == "random_per_job":
            frames_per_job_count = 3
            validation_params_kwargs["frames_per_job_count"] = frames_per_job_count
            validation_frames_count = used_frames_count // segment_size + min(
                used_frames_count % segment_size, frames_per_job_count
            )
        elif frame_selection_method == "manual":
            validation_frames_count = 10

            valid_frame_ids = range(
                (start_frame or 0), (start_frame or 0) + used_frames_count * (step or 1), step or 1
            )
            rng = np.random.Generator(np.random.MT19937(seed=42))
            validation_params_kwargs["frames"] = rng.choice(
                [f.name for i, f in enumerate(image_files) if i in valid_frame_ids],
                validation_frames_count,
                replace=False,
            ).tolist()
        else:
            raise NotImplementedError

        validation_params = models.DataRequestValidationParams._from_openapi_data(
            mode="gt",
            **validation_params_kwargs,
        )

        yield from self._image_task_fxt_base(
            request=request,
            frame_count=None,
            image_files=image_files,
            segment_size=segment_size,
            sorting_method="natural",
            start_frame=start_frame,
            step=step,
            validation_params=validation_params,
            job_replication=job_replication,
        )

    @pytest.fixture(scope="class")
    def fxt_uploaded_images_task_with_gt_and_segments_and_consensus(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._uploaded_images_task_with_gt_and_segments_base(
            request=request, job_replication=2
        )

    @fixture(scope="class")
    @parametrize("start_frame, step", [(2, 3)])
    @parametrize("frame_selection_method", ["random_uniform", "random_per_job", "manual"])
    def fxt_uploaded_images_task_with_gt_and_segments_start_step(
        self,
        request: pytest.FixtureRequest,
        start_frame: Optional[int],
        step: Optional[int],
        frame_selection_method: str,
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._uploaded_images_task_with_gt_and_segments_base(
            request,
            start_frame=start_frame,
            step=step,
            frame_selection_method=frame_selection_method,
        )

    def _uploaded_video_task_fxt_base(
        self,
        request: pytest.FixtureRequest,
        *,
        frame_count: int = 10,
        segment_size: Optional[int] = None,
        start_frame: Optional[int] = None,
        stop_frame: Optional[int] = None,
        step: Optional[int] = None,
    ) -> Generator[tuple[_VideoTaskSpec, int], None, None]:
        task_params = {
            "name": f"{request.node.name}[{request.fixturename}]",
            "labels": [{"name": "a"}],
        }
        if segment_size:
            task_params["segment_size"] = segment_size

        resulting_task_size = len(
            range(start_frame or 0, (stop_frame or frame_count - 1) + 1, step or 1)
        )

        video_file = generate_video_file(frame_count)
        video_data = video_file.getvalue()
        data_params = {
            "image_quality": 70,
            "client_files": [video_file],
            "chunk_size": max(1, (segment_size or resulting_task_size) // 2),
        }

        if start_frame is not None:
            data_params["start_frame"] = start_frame

        if stop_frame is not None:
            data_params["stop_frame"] = stop_frame

        if step is not None:
            data_params["frame_filter"] = f"step={step}"

        def get_video_file() -> io.BytesIO:
            return io.BytesIO(video_data)

        task_id, _ = create_task(self._USERNAME, spec=task_params, data=data_params)
        yield _VideoTaskSpec(
            models.TaskWriteRequest._from_openapi_data(**task_params),
            models.DataRequest._from_openapi_data(**data_params),
            get_video_file=get_video_file,
            size=resulting_task_size,
        ), task_id

    @pytest.fixture(scope="class")
    def fxt_uploaded_video_task(
        self,
        request: pytest.FixtureRequest,
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._uploaded_video_task_fxt_base(request=request)

    @pytest.fixture(scope="class")
    def fxt_uploaded_video_task_with_segments(
        self, request: pytest.FixtureRequest
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._uploaded_video_task_fxt_base(request=request, segment_size=4)

    @fixture(scope="class")
    @parametrize("step", [2, 5])
    @parametrize("stop_frame", [15, 26])
    @parametrize("start_frame", [3, 7])
    def fxt_uploaded_video_task_with_segments_start_stop_step(
        self, request: pytest.FixtureRequest, start_frame: int, stop_frame: Optional[int], step: int
    ) -> Generator[tuple[_TaskSpec, int], None, None]:
        yield from self._uploaded_video_task_fxt_base(
            request=request,
            frame_count=30,
            segment_size=4,
            start_frame=start_frame,
            stop_frame=stop_frame,
            step=step,
        )

    def _compute_annotation_segment_params(self, task_spec: _TaskSpec) -> list[tuple[int, int]]:
        segment_params = []
        frame_step = task_spec.frame_step
        segment_size = getattr(task_spec, "segment_size", 0) or task_spec.size * frame_step
        start_frame = getattr(task_spec, "start_frame", 0)
        stop_frame = getattr(task_spec, "stop_frame", None) or (
            start_frame + (task_spec.size - 1) * frame_step
        )
        end_frame = calc_end_frame(start_frame, stop_frame, frame_step)

        validation_params = getattr(task_spec, "validation_params", None)
        if validation_params and validation_params.mode.value == "gt_pool":
            end_frame = min(
                end_frame, (task_spec.size - validation_params.frame_count) * frame_step
            )
            segment_size = min(segment_size, end_frame - 1)

        overlap = min(
            (
                getattr(task_spec, "overlap", None) or 0
                if task_spec.source_data_type == _SourceDataType.images
                else 5
            ),
            segment_size // 2,
        )
        segment_start = start_frame
        while segment_start < end_frame:
            if start_frame < segment_start:
                segment_start -= overlap * frame_step

            segment_end = segment_start + frame_step * segment_size

            segment_params.append((segment_start, min(segment_end, end_frame) - frame_step))
            segment_start = segment_end

        return segment_params

    @staticmethod
    def _compare_images(
        expected: Image.Image, actual: Image.Image, *, must_be_identical: bool = True
    ):
        expected_pixels = np.array(expected)
        chunk_frame_pixels = np.array(actual)
        assert expected_pixels.shape == chunk_frame_pixels.shape

        if not must_be_identical:
            # video chunks can have slightly changed colors, due to codec specifics
            # compressed images can also be distorted
            assert np.allclose(chunk_frame_pixels, expected_pixels, atol=2)
        else:
            assert np.array_equal(chunk_frame_pixels, expected_pixels)

    def _get_job_abs_frame_set(self, job_meta: models.DataMetaRead) -> Sequence[int]:
        if job_meta.included_frames:
            return job_meta.included_frames
        else:
            return range(
                job_meta.start_frame,
                job_meta.stop_frame + 1,
                parse_frame_step(job_meta.frame_filter),
            )

    _tasks_with_honeypots_cases = [
        fixture_ref("fxt_uploaded_images_task_with_honeypots_and_segments"),
        fixture_ref("fxt_uploaded_images_task_with_honeypots_and_segments_start_step"),
        fixture_ref("fxt_uploaded_images_task_with_honeypots_and_changed_real_frames"),
        fixture_ref("fxt_cloud_images_task_with_honeypots_and_changed_real_frames"),
    ]

    _tasks_with_simple_gt_job_cases = [
        fixture_ref("fxt_uploaded_images_task_with_gt_and_segments_start_step"),
        fixture_ref("fxt_uploaded_images_task_with_gt_and_segments_and_consensus"),
    ]

    _tasks_with_consensus_cases = [
        fixture_ref("fxt_uploaded_images_task_with_segments_and_consensus"),
        fixture_ref("fxt_uploaded_images_task_with_gt_and_segments_and_consensus"),
    ]

    # Keep in mind that these fixtures are generated eagerly
    # (before each depending test or group of tests),
    # e.g. a failing task creation in one the fixtures will fail all the depending tests cases.
    _all_task_cases = unique(
        [
            fixture_ref("fxt_uploaded_images_task"),
            fixture_ref("fxt_uploaded_images_task_with_segments"),
            fixture_ref("fxt_uploaded_images_task_with_segments_start_stop_step"),
            fixture_ref("fxt_uploaded_video_task"),
            fixture_ref("fxt_uploaded_video_task_with_segments"),
            fixture_ref("fxt_uploaded_video_task_with_segments_start_stop_step"),
        ]
        + _tasks_with_honeypots_cases
        + _tasks_with_simple_gt_job_cases
        + _tasks_with_consensus_cases,
        key=lambda fxt_ref: fxt_ref.fixture,
    )


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_cvat_data_per_class")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestTaskData(_TestTasksBase):
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    def test_can_get_task_meta(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)

            assert task_meta.size == task_spec.size
            assert task_meta.start_frame == getattr(task_spec, "start_frame", 0)
            assert task_meta.stop_frame == getattr(task_spec, "stop_frame", None) or task_spec.size
            assert task_meta.frame_filter == getattr(task_spec, "frame_filter", "")

            task_frame_set = set(
                range(task_meta.start_frame, task_meta.stop_frame + 1, task_spec.frame_step)
            )
            assert len(task_frame_set) == task_meta.size

            if getattr(task_spec, "chunk_size", None):
                assert task_meta.chunk_size == task_spec.chunk_size

            if task_spec.source_data_type == _SourceDataType.video:
                assert len(task_meta.frames) == 1
            else:
                assert len(task_meta.frames) == task_meta.size

    @pytest.mark.timeout(
        # This test has to check all the task frames availability, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    def test_can_get_task_frames(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)

            for quality, abs_frame_id in product(
                ["original", "compressed"],
                range(task_meta.start_frame, task_meta.stop_frame + 1, task_spec.frame_step),
            ):
                rel_frame_id = (
                    abs_frame_id - getattr(task_spec, "start_frame", 0)
                ) // task_spec.frame_step
                (_, response) = api_client.tasks_api.retrieve_data(
                    task_id,
                    type="frame",
                    quality=quality,
                    number=rel_frame_id,
                    _parse_response=False,
                )

                if task_spec.source_data_type == _SourceDataType.video:
                    frame_size = (task_meta.frames[0].width, task_meta.frames[0].height)
                else:
                    frame_size = (
                        task_meta.frames[rel_frame_id].width,
                        task_meta.frames[rel_frame_id].height,
                    )

                frame = Image.open(io.BytesIO(response.data))
                assert frame_size == frame.size

                self._compare_images(
                    task_spec.read_frame(abs_frame_id),
                    frame,
                    must_be_identical=(
                        task_spec.source_data_type == _SourceDataType.images
                        and quality == "original"
                    ),
                )

    @pytest.mark.timeout(
        # This test has to check all the task chunks availability, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    def test_can_get_task_chunks(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            (task, _) = api_client.tasks_api.retrieve(task_id)
            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)

            if task_spec.source_data_type == _SourceDataType.images:
                assert task.data_original_chunk_type == "imageset"
                assert task.data_compressed_chunk_type == "imageset"
            elif task_spec.source_data_type == _SourceDataType.video:
                assert task.data_original_chunk_type == "video"

                if getattr(task_spec, "use_zip_chunks", False):
                    assert task.data_compressed_chunk_type == "imageset"
                else:
                    assert task.data_compressed_chunk_type == "video"
            else:
                assert False

            task_abs_frames = range(
                task_meta.start_frame, task_meta.stop_frame + 1, task_spec.frame_step
            )
            task_chunk_frames = [
                (chunk_number, list(chunk_frames))
                for chunk_number, chunk_frames in groupby(
                    task_abs_frames,
                    key=lambda abs_frame: (
                        (abs_frame - task_meta.start_frame) // task_spec.frame_step
                    )
                    // task_meta.chunk_size,
                )
            ]
            for quality, (chunk_id, expected_chunk_frame_ids) in product(
                ["original", "compressed"], task_chunk_frames
            ):
                (_, response) = api_client.tasks_api.retrieve_data(
                    task_id, type="chunk", quality=quality, number=chunk_id, _parse_response=False
                )

                chunk_file = io.BytesIO(response.data)
                if zipfile.is_zipfile(chunk_file):
                    with zipfile.ZipFile(chunk_file, "r") as chunk_archive:
                        chunk_images = {
                            int(os.path.splitext(name)[0]): np.array(
                                Image.open(io.BytesIO(chunk_archive.read(name)))
                            )
                            for name in chunk_archive.namelist()
                        }
                        chunk_images = dict(sorted(chunk_images.items(), key=lambda e: e[0]))
                else:
                    chunk_images = dict(enumerate(read_video_file(chunk_file)))

                assert sorted(chunk_images.keys()) == list(range(len(expected_chunk_frame_ids)))

                for chunk_frame, abs_frame_id in zip(chunk_images, expected_chunk_frame_ids):
                    self._compare_images(
                        task_spec.read_frame(abs_frame_id),
                        chunk_images[chunk_frame],
                        must_be_identical=(
                            task_spec.source_data_type == _SourceDataType.images
                            and quality == "original"
                        ),
                    )

    @pytest.mark.timeout(
        # This test has to check all the task meta availability, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    def test_can_get_annotation_job_meta(self, task_spec: _TaskSpec, task_id: int):
        segment_params = self._compute_annotation_segment_params(task_spec)

        with make_api_client(self._USERNAME) as api_client:
            jobs = sorted(
                get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="annotation"
                ),
                key=lambda j: j.start_frame,
            )
            assert len(jobs) == len(segment_params)

            for (segment_start, segment_stop), job in zip(segment_params, jobs):
                (job_meta, _) = api_client.jobs_api.retrieve_data_meta(job.id)

                assert (job_meta.start_frame, job_meta.stop_frame) == (segment_start, segment_stop)
                assert job_meta.frame_filter == getattr(task_spec, "frame_filter", "")

                segment_size = math.ceil((segment_stop - segment_start + 1) / task_spec.frame_step)
                assert job_meta.size == segment_size

                job_abs_frame_set = self._get_job_abs_frame_set(job_meta)
                assert len(job_abs_frame_set) == job_meta.size
                assert set(job_abs_frame_set).issubset(
                    range(
                        job_meta.start_frame,
                        job_meta.stop_frame + 1,
                        parse_frame_step(job_meta.frame_filter),
                    )
                )

                if getattr(task_spec, "chunk_size", None):
                    assert job_meta.chunk_size == task_spec.chunk_size

                if task_spec.source_data_type == _SourceDataType.video:
                    assert len(job_meta.frames) == 1
                else:
                    assert len(job_meta.frames) == job_meta.size

    @parametrize("task_spec, task_id", _TestTasksBase._tasks_with_simple_gt_job_cases)
    def test_can_get_simple_gt_job_meta(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            jobs = sorted(
                get_paginated_collection(
                    api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
                ),
                key=lambda j: j.start_frame,
            )
            assert len(jobs) == 1

            gt_job = jobs[0]
            (job_meta, _) = api_client.jobs_api.retrieve_data_meta(gt_job.id)

            task_start_frame = getattr(task_spec, "start_frame", 0)
            assert (job_meta.start_frame, job_meta.stop_frame) == (
                task_start_frame,
                task_start_frame + (task_spec.size - 1) * task_spec.frame_step,
            )
            assert job_meta.frame_filter == getattr(task_spec, "frame_filter", "")

            frame_selection_method = task_spec.validation_params.frame_selection_method.value
            if frame_selection_method == "random_uniform":
                validation_frames_count = task_spec.validation_params.frame_count
            elif frame_selection_method == "random_per_job":
                frames_per_job_count = task_spec.validation_params.frames_per_job_count
                validation_frames_count = (
                    task_spec.size // task_spec.segment_size * frames_per_job_count
                    + min(task_spec.size % task_spec.segment_size, frames_per_job_count)
                )
            elif frame_selection_method == "manual":
                validation_frames_count = len(task_spec.validation_params.frames)
            else:
                raise NotImplementedError(frame_selection_method)

            assert job_meta.size == validation_frames_count

            job_abs_frame_set = self._get_job_abs_frame_set(job_meta)
            assert len(job_abs_frame_set) == job_meta.size
            assert set(job_abs_frame_set).issubset(
                range(
                    job_meta.start_frame,
                    job_meta.stop_frame + 1,
                    parse_frame_step(job_meta.frame_filter),
                )
            )

            if getattr(task_spec, "chunk_size", None):
                assert job_meta.chunk_size == task_spec.chunk_size

            if task_spec.source_data_type == _SourceDataType.video:
                assert len(job_meta.frames) == 1
            else:
                # there are placeholders on the non-included places
                assert len(job_meta.frames) == task_spec.size

    @parametrize("task_spec, task_id", _TestTasksBase._tasks_with_honeypots_cases)
    def test_can_get_honeypot_gt_job_meta(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            gt_jobs = get_paginated_collection(
                api_client.jobs_api.list_endpoint, task_id=task_id, type="ground_truth"
            )
            assert len(gt_jobs) == 1

            gt_job = gt_jobs[0]
            segment_start = task_spec.size - task_spec.validation_params.frame_count
            segment_stop = task_spec.size - 1

            (job_meta, _) = api_client.jobs_api.retrieve_data_meta(gt_job.id)

            assert (job_meta.start_frame, job_meta.stop_frame) == (segment_start, segment_stop)
            assert job_meta.frame_filter == getattr(task_spec, "frame_filter", "")

            segment_size = math.ceil((segment_stop - segment_start + 1) / task_spec.frame_step)
            assert job_meta.size == segment_size

            task_frame_set = set(
                range(job_meta.start_frame, job_meta.stop_frame + 1, task_spec.frame_step)
            )
            assert len(task_frame_set) == job_meta.size

            if getattr(task_spec, "chunk_size", None):
                assert job_meta.chunk_size == task_spec.chunk_size

            if task_spec.source_data_type == _SourceDataType.video:
                assert len(job_meta.frames) == 1
            else:
                assert len(job_meta.frames) == job_meta.size

    @pytest.mark.timeout(
        # This test has to check the job meta for all jobs, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._tasks_with_consensus_cases)
    def test_can_get_consensus_replica_job_meta(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            jobs = sorted(
                get_paginated_collection(api_client.jobs_api.list_endpoint, task_id=task_id),
                key=lambda j: j.start_frame,
            )

            # Only annotation jobs can have replicas
            annotation_jobs = [j for j in jobs if j.type == "annotation"]
            assert (
                len([j for j in jobs if j.type == "consensus_replica"])
                == len(annotation_jobs) * task_spec.consensus_replicas
            )

            for job in annotation_jobs:
                annotation_job_meta = json.loads(
                    api_client.jobs_api.retrieve_data_meta(job.id)[1].data
                )

                replicas = [
                    j for j in jobs if j.type == "consensus_replica" if j.parent_job_id == job.id
                ]
                assert len(replicas) == task_spec.consensus_replicas

                for replica_job in replicas:
                    replica_job_meta = json.loads(
                        api_client.jobs_api.retrieve_data_meta(replica_job.id)[1].data
                    )
                    assert DeepDiff(annotation_job_meta, replica_job_meta) == {}

    @pytest.mark.timeout(
        # This test has to check all the job frames availability, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    def test_can_get_job_frames(self, task_spec: _TaskSpec, task_id: int):
        with make_api_client(self._USERNAME) as api_client:
            jobs = sorted(
                get_paginated_collection(api_client.jobs_api.list_endpoint, task_id=task_id),
                key=lambda j: j.start_frame,
            )
            for job in jobs:
                (job_meta, _) = api_client.jobs_api.retrieve_data_meta(job.id)
                job_abs_frames = self._get_job_abs_frame_set(job_meta)

                for quality, (frame_pos, abs_frame_id) in product(
                    ["original", "compressed"],
                    enumerate(job_abs_frames),
                ):
                    rel_frame_id = (
                        abs_frame_id - getattr(task_spec, "start_frame", 0)
                    ) // task_spec.frame_step
                    (_, response) = api_client.jobs_api.retrieve_data(
                        job.id,
                        type="frame",
                        quality=quality,
                        number=rel_frame_id,
                        _parse_response=False,
                    )

                    if task_spec.source_data_type == _SourceDataType.video:
                        frame_size = (job_meta.frames[0].width, job_meta.frames[0].height)
                    else:
                        frame_size = (
                            job_meta.frames[frame_pos].width,
                            job_meta.frames[frame_pos].height,
                        )

                    frame = Image.open(io.BytesIO(response.data))
                    assert frame_size == frame.size

                    self._compare_images(
                        task_spec.read_frame(abs_frame_id),
                        frame,
                        must_be_identical=(
                            task_spec.source_data_type == _SourceDataType.images
                            and quality == "original"
                        ),
                    )

    @pytest.mark.timeout(
        # This test has to check all the job chunks availability, it can make many requests
        timeout=300
    )
    @parametrize("task_spec, task_id", _TestTasksBase._all_task_cases)
    @parametrize("indexing", ["absolute", "relative"])
    def test_can_get_job_chunks(self, task_spec: _TaskSpec, task_id: int, indexing: str):
        _placeholder_image = Image.fromarray(np.zeros((1, 1, 3), dtype=np.uint8))

        with make_api_client(self._USERNAME) as api_client:
            jobs = sorted(
                get_paginated_collection(api_client.jobs_api.list_endpoint, task_id=task_id),
                key=lambda j: j.start_frame,
            )

            (task_meta, _) = api_client.tasks_api.retrieve_data_meta(task_id)

            for job in jobs:
                (job_meta, _) = api_client.jobs_api.retrieve_data_meta(job.id)

                if job_meta.included_frames:
                    assert len(job_meta.included_frames) == job_meta.size

                if task_spec.source_data_type == _SourceDataType.images:
                    assert job.data_original_chunk_type == "imageset"
                    assert job.data_compressed_chunk_type == "imageset"
                elif task_spec.source_data_type == _SourceDataType.video:
                    assert job.data_original_chunk_type == "video"

                    if getattr(task_spec, "use_zip_chunks", False):
                        assert job.data_compressed_chunk_type == "imageset"
                    else:
                        assert job.data_compressed_chunk_type == "video"
                else:
                    assert False

                if indexing == "absolute":
                    chunk_count = math.ceil(task_meta.size / task_meta.chunk_size)

                    def get_task_chunk_abs_frame_ids(chunk_id: int) -> Sequence[int]:
                        return range(
                            task_meta.start_frame
                            + chunk_id * task_meta.chunk_size * task_spec.frame_step,
                            task_meta.start_frame
                            + min((chunk_id + 1) * task_meta.chunk_size, task_meta.size)
                            * task_spec.frame_step,
                            task_spec.frame_step,
                        )

                    def get_job_frame_ids() -> Sequence[int]:
                        return range(
                            job_meta.start_frame, job_meta.stop_frame + 1, task_spec.frame_step
                        )

                    def get_expected_chunk_abs_frame_ids(chunk_id: int):
                        return sorted(
                            set(get_task_chunk_abs_frame_ids(chunk_id)) & set(get_job_frame_ids())
                        )

                    job_chunk_ids = (
                        task_chunk_id
                        for task_chunk_id in range(chunk_count)
                        if get_expected_chunk_abs_frame_ids(task_chunk_id)
                    )
                else:
                    chunk_count = math.ceil(job_meta.size / job_meta.chunk_size)
                    job_chunk_ids = range(chunk_count)

                    def get_expected_chunk_abs_frame_ids(chunk_id: int):
                        job_abs_frames = self._get_job_abs_frame_set(job_meta)
                        return job_abs_frames[
                            chunk_id * job_meta.chunk_size : (chunk_id + 1) * job_meta.chunk_size
                        ]

                for quality, chunk_id in product(["original", "compressed"], job_chunk_ids):
                    expected_chunk_abs_frame_ids = get_expected_chunk_abs_frame_ids(chunk_id)

                    kwargs = {}
                    if indexing == "absolute":
                        kwargs["number"] = chunk_id
                    elif indexing == "relative":
                        kwargs["index"] = chunk_id
                    else:
                        assert False

                    (_, response) = api_client.jobs_api.retrieve_data(
                        job.id,
                        type="chunk",
                        quality=quality,
                        **kwargs,
                        _parse_response=False,
                    )

                    chunk_file = io.BytesIO(response.data)
                    if zipfile.is_zipfile(chunk_file):
                        with zipfile.ZipFile(chunk_file, "r") as chunk_archive:
                            chunk_images = {
                                int(os.path.splitext(name)[0]): np.array(
                                    Image.open(io.BytesIO(chunk_archive.read(name)))
                                )
                                for name in chunk_archive.namelist()
                            }
                            chunk_images = dict(sorted(chunk_images.items(), key=lambda e: e[0]))
                    else:
                        chunk_images = dict(enumerate(read_video_file(chunk_file)))

                    assert sorted(chunk_images.keys()) == list(
                        range(len(expected_chunk_abs_frame_ids))
                    )

                    for chunk_frame, abs_frame_id in zip(
                        chunk_images, expected_chunk_abs_frame_ids
                    ):
                        if (
                            indexing == "absolute"
                            and job_meta.included_frames
                            and abs_frame_id not in job_meta.included_frames
                        ):
                            expected_image = _placeholder_image
                        else:
                            expected_image = task_spec.read_frame(abs_frame_id)

                        self._compare_images(
                            expected_image,
                            chunk_images[chunk_frame],
                            must_be_identical=(
                                task_spec.source_data_type == _SourceDataType.images
                                and quality == "original"
                            ),
                        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchTaskLabel:
    def _get_task_labels(self, pid, user, **kwargs) -> list[models.Label]:
        kwargs.setdefault("return_json", True)
        with make_api_client(user) as api_client:
            return get_paginated_collection(
                api_client.labels_api.list_endpoint, task_id=pid, **kwargs
            )

    def test_can_delete_label(self, tasks_wlc, labels, admin_user):
        task = [t for t in tasks_wlc if t["project_id"] is None and t["labels"]["count"] > 0][0]
        label = deepcopy([l for l in labels if l.get("task_id") == task["id"]][0])
        label_payload = {"id": label["id"], "deleted": True}

        prev_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [label_payload]})
        curr_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK, response.content
        assert curr_lc == prev_lc - 1

    def test_can_delete_skeleton_label(self, tasks, labels, admin_user):
        task = next(
            t
            for t in tasks
            if any(
                label
                for label in labels
                if label.get("task_id") == t["id"]
                if label["type"] == "skeleton"
            )
        )
        task_labels = deepcopy([l for l in labels if l.get("task_id") == task["id"]])
        label = next(l for l in task_labels if l["type"] == "skeleton")
        task_labels.remove(label)
        label_payload = {"id": label["id"], "deleted": True}

        prev_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [label_payload]})
        curr_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc - 1

        resulting_labels = self._get_task_labels(task["id"], admin_user)
        assert DeepDiff(resulting_labels, task_labels, ignore_order=True) == {}

    def test_can_rename_label(self, tasks_wlc, labels, admin_user):
        task = [t for t in tasks_wlc if t["project_id"] is None and t["labels"]["count"] > 0][0]
        task_labels = deepcopy([l for l in labels if l.get("task_id") == task["id"]])
        task_labels[0].update({"name": "new name"})

        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [task_labels[0]]})
        assert response.status_code == HTTPStatus.OK

        resulting_labels = self._get_task_labels(task["id"], admin_user)
        assert DeepDiff(resulting_labels, task_labels, ignore_order=True) == {}

    def test_cannot_rename_label_to_duplicate_name(self, tasks_wlc, labels, admin_user):
        task = [t for t in tasks_wlc if t["project_id"] is None and t["labels"]["count"] > 1][0]
        task_labels = deepcopy([l for l in labels if l.get("task_id") == task["id"]])
        task_labels[0].update({"name": task_labels[1]["name"]})

        label_payload = {"id": task_labels[0]["id"], "name": task_labels[0]["name"]}

        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [label_payload]})
        assert response.status_code == HTTPStatus.BAD_REQUEST
        assert "All label names must be unique" in response.text

    def test_cannot_add_foreign_label(self, tasks, labels, admin_user):
        task = [t for t in tasks if t["project_id"] is None][0]
        new_label = deepcopy(
            [
                l
                for l in labels
                if l.get("task_id") != task["id"]
                if not l.get("project_id") or l.get("project_id") != task.get("project_id")
            ][0]
        )

        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [new_label]})
        assert response.status_code == HTTPStatus.NOT_FOUND
        assert f"Not found label with id #{new_label['id']} to change" in response.text

    def test_admin_can_add_label(self, tasks, admin_user):
        task = [t for t in tasks if t["project_id"] is None][0]
        new_label = {"name": "new name"}

        prev_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [new_label]})
        curr_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_non_task_staff_privileged_org_members_can_add_label(
        self,
        find_users,
        tasks,
        is_task_staff,
        is_org_member,
        role,
    ):
        users = find_users(role=role, exclude_privilege="admin")

        user, task = next(
            (user, task)
            for user, task in product(users, tasks)
            if not is_task_staff(user["id"], task["id"])
            and task["organization"]
            and is_org_member(user["id"], task["organization"] and task["project_id"] is None)
        )

        new_label = {"name": "new name"}
        prev_lc = get_method(user["username"], "labels", task_id=task["id"]).json()["count"]

        response = patch_method(
            user["username"],
            f'tasks/{task["id"]}',
            {"labels": [new_label]},
        )
        curr_lc = get_method(user["username"], "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    @pytest.mark.parametrize("role", ["supervisor", "worker"])
    def test_non_task_staff_org_members_cannot_add_label(
        self,
        find_users,
        tasks,
        is_task_staff,
        is_org_member,
        role,
    ):
        users = find_users(exclude_privilege="admin")

        user, task = next(
            (user, task)
            for user, task in product(users, tasks)
            if not is_task_staff(user["id"], task["id"])
            and task["organization"]
            and is_org_member(user["id"], task["organization"], role=role)
        )

        new_label = {"name": "new name"}
        response = patch_method(
            user["username"],
            f'tasks/{task["id"]}',
            {"labels": [new_label]},
        )
        assert response.status_code == HTTPStatus.FORBIDDEN

    # TODO: add supervisor too, but this leads to a test-side problem with DB restoring
    @pytest.mark.parametrize("role", ["worker"])
    def test_task_staff_org_members_can_add_label(
        self, find_users, tasks, is_task_staff, is_org_member, labels, role
    ):
        users = find_users(role=role, exclude_privilege="admin")

        user, task = next(
            (user, task)
            for user, task in product(users, tasks)
            if is_task_staff(user["id"], task["id"])
            and task["organization"]
            and is_org_member(user["id"], task["organization"])
            and any(label.get("task_id") == task["id"] for label in labels)
        )

        prev_lc = get_method(user["username"], "labels", task_id=task["id"]).json()["count"]
        new_label = {"name": "new name"}
        response = patch_method(
            user["username"],
            f'tasks/{task["id"]}',
            {"labels": [new_label]},
        )
        curr_lc = get_method(user["username"], "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1

    def test_admin_can_add_skeleton(self, tasks, admin_user):
        task = [t for t in tasks if t["project_id"] is None][0]
        new_skeleton = {
            "name": "new skeleton",
            "type": "skeleton",
            "sublabels": [
                {
                    "name": "1",
                    "type": "points",
                }
            ],
            "svg": '<circle r="1.5" stroke="black" fill="#b3b3b3" cx="48.794559478759766" '
            'cy="36.98698806762695" stroke-width="0.1" data-type="element node" '
            'data-element-id="1" data-node-id="1" data-label-name="597501"></circle>',
        }

        prev_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        response = patch_method(admin_user, f'tasks/{task["id"]}', {"labels": [new_skeleton]})
        curr_lc = get_method(admin_user, "labels", task_id=task["id"]).json()["count"]
        assert response.status_code == HTTPStatus.OK
        assert curr_lc == prev_lc + 1


@pytest.mark.usefixtures("restore_db_per_function")
@pytest.mark.usefixtures("restore_cvat_data_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestWorkWithTask:
    _USERNAME = "admin1"

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "cloud_storage_id, manifest",
        [(1, "manifest.jsonl")],  # public bucket
    )
    def test_work_with_task_containing_non_stable_cloud_storage_files(
        self, cloud_storage_id, manifest, cloud_storages, request
    ):
        image_name = "image_case_65_1.png"
        cloud_storage_content = [image_name, manifest]

        task_spec = {
            "name": f"Task with mythical file from cloud storage {cloud_storage_id}",
            "labels": [{"name": "car"}],
        }

        data_spec = {
            "image_quality": 75,
            "use_cache": True,
            "cloud_storage_id": cloud_storage_id,
            "server_files": cloud_storage_content,
        }

        task_id, _ = create_task(self._USERNAME, task_spec, data_spec)

        # save image from the "public" bucket and remove it temporary
        bucket_name = cloud_storages[cloud_storage_id]["resource"]
        s3_client = s3.make_client(bucket=bucket_name)

        image = s3_client.download_fileobj(image_name)
        s3_client.remove_file(image_name)
        request.addfinalizer(partial(s3_client.create_file, filename=image_name, data=image))

        with make_api_client(self._USERNAME) as api_client:
            try:
                api_client.tasks_api.retrieve_data(
                    task_id, number=0, quality="original", type="frame"
                )
                raise AssertionError("Frame should not exist")
            except AssertionError:
                raise
            except Exception as ex:
                assert ex.status == HTTPStatus.NOT_FOUND
                assert image_name in ex.body


@pytest.mark.usefixtures("restore_redis_inmem_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_per_class")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
class TestTaskBackups:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        restore_db_per_function,
        restore_cvat_data_per_function,
        tmp_path: Path,
        admin_user: str,
    ):
        self.tmp_dir = tmp_path

        self.user = admin_user

        with make_sdk_client(self.user) as client:
            self.client = client

    def _test_can_export_backup(self, task_id: int):
        task = self.client.tasks.retrieve(task_id)

        filename = self.tmp_dir / f"task_{task.id}_backup.zip"
        task.download_backup(filename)

        assert filename.is_file()
        assert filename.stat().st_size > 0

    @pytest.mark.parametrize("mode", ["annotation", "interpolation"])
    def test_can_export_backup(self, tasks, mode):
        task_id = next(t for t in tasks if t["mode"] == mode and not t["validation_mode"])["id"]
        self._test_can_export_backup(task_id)

    def test_can_export_backup_for_consensus_task(self, tasks):
        task_id = next(t for t in tasks if t["consensus_enabled"])["id"]
        self._test_can_export_backup(task_id)

    def test_can_export_backup_for_honeypot_task(self, tasks):
        task_id = next(t for t in tasks if t["validation_mode"] == "gt_pool")["id"]
        self._test_can_export_backup(task_id)

    def test_cannot_export_backup_for_task_without_data(self, tasks):
        task_id = next(t for t in tasks if t["jobs"]["count"] == 0)["id"]

        with pytest.raises(ApiException) as exc:
            self._test_can_export_backup(task_id)

            assert exc.status == HTTPStatus.BAD_REQUEST
            assert "Backup of a task without data is not allowed" == exc.body.encode()

    @pytest.mark.with_external_services
    def test_can_export_and_import_backup_task_with_cloud_storage(self, tasks):
        cloud_storage_content = ["image_case_65_1.png", "image_case_65_2.png"]
        task_spec = {
            "name": "Task with files from cloud storage",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }
        data_spec = {
            "image_quality": 75,
            "use_cache": False,
            "cloud_storage_id": 1,
            "server_files": cloud_storage_content,
        }
        task_id, _ = create_task(self.user, task_spec, data_spec)

        task = self.client.tasks.retrieve(task_id)

        filename = self.tmp_dir / f"cloud_task_{task.id}_backup.zip"
        task.download_backup(filename)

        assert filename.is_file()
        assert filename.stat().st_size > 0
        self._test_can_restore_task_from_backup(task_id)

    @pytest.mark.parametrize("mode", ["annotation", "interpolation"])
    def test_can_import_backup(self, tasks, mode):
        task_id = next(t for t in tasks if t["mode"] == mode)["id"]
        self._test_can_restore_task_from_backup(task_id)

    def test_can_import_backup_with_honeypot_task(self, tasks):
        task_id = next(t for t in tasks if t["validation_mode"] == "gt_pool")["id"]
        self._test_can_restore_task_from_backup(task_id)

    def test_can_import_backup_with_consensus_task(self, tasks):
        task_id = next(t for t in tasks if t["consensus_enabled"])["id"]
        self._test_can_restore_task_from_backup(task_id)

    @pytest.mark.parametrize("mode", ["annotation", "interpolation"])
    def test_can_import_backup_for_task_in_nondefault_state(self, tasks, mode):
        # Reproduces the problem with empty 'mode' in a restored task,
        # described in the reproduction steps https://github.com/cvat-ai/cvat/issues/5668

        task_json = next(t for t in tasks if t["mode"] == mode and t["jobs"]["count"])

        task = self.client.tasks.retrieve(task_json["id"])
        jobs = task.get_jobs()
        for j in jobs:
            j.update({"stage": "validation"})

        self._test_can_restore_task_from_backup(task_json["id"])

    def test_can_import_backup_with_gt_job(self, tasks, jobs, job_has_annotations):
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            if job_has_annotations(j["id"])
            if tasks[j["task_id"]]["validation_mode"] == "gt"
            if tasks[j["task_id"]]["size"]
        )
        task = tasks[gt_job["task_id"]]

        self._test_can_restore_task_from_backup(task["id"])

    def _test_can_restore_task_from_backup(self, task_id: int):
        old_task = self.client.tasks.retrieve(task_id)
        (_, response) = self.client.api_client.tasks_api.retrieve(task_id)
        task_json = json.loads(response.data)

        filename = self.tmp_dir / f"task_{old_task.id}_backup.zip"
        old_task.download_backup(filename)

        new_task = self.client.tasks.create_from_backup(filename)

        old_meta = json.loads(old_task.api.retrieve_data_meta(old_task.id)[1].data)
        new_meta = json.loads(new_task.api.retrieve_data_meta(new_task.id)[1].data)
        assert (
            DeepDiff(
                old_meta,
                new_meta,
                ignore_order=True,
                exclude_regex_paths=[r"root\['chunks_updated_date'\]"],  # must be different
            )
            == {}
        )

        old_jobs = sorted(old_task.get_jobs(), key=lambda j: (j.start_frame, j.type))
        new_jobs = sorted(new_task.get_jobs(), key=lambda j: (j.start_frame, j.type))
        assert len(old_jobs) == len(new_jobs)

        for old_job, new_job in zip(old_jobs, new_jobs):
            old_job_meta = json.loads(old_job.api.retrieve_data_meta(old_job.id)[1].data)
            new_job_meta = json.loads(new_job.api.retrieve_data_meta(new_job.id)[1].data)
            assert (
                DeepDiff(
                    old_job_meta,
                    new_job_meta,
                    ignore_order=True,
                    exclude_regex_paths=[r"root\['chunks_updated_date'\]"],  # must be different
                )
                == {}
            )

            old_job_annotations = json.loads(old_job.api.retrieve_annotations(old_job.id)[1].data)
            new_job_annotations = json.loads(new_job.api.retrieve_annotations(new_job.id)[1].data)
            assert compare_annotations(old_job_annotations, new_job_annotations) == {}

        (_, response) = self.client.api_client.tasks_api.retrieve(new_task.id)
        restored_task_json = json.loads(response.data)

        assert restored_task_json["assignee"] is None
        assert restored_task_json["owner"]["username"] == self.user
        assert restored_task_json["id"] != task_json["id"]
        assert restored_task_json["data"] != task_json["data"]
        assert restored_task_json["organization"] is None
        assert restored_task_json["data_compressed_chunk_type"] in ["imageset", "video"]
        if task_json["jobs"]["count"] == 1:
            assert restored_task_json["overlap"] == 0
        else:
            assert restored_task_json["overlap"] == task_json["overlap"]
        assert restored_task_json["jobs"]["completed"] == 0
        assert restored_task_json["jobs"]["validation"] == 0
        assert restored_task_json["source_storage"] is None
        assert restored_task_json["target_storage"] is None
        assert restored_task_json["project_id"] is None

        assert (
            DeepDiff(
                task_json,
                restored_task_json,
                ignore_order=True,
                exclude_regex_paths=[
                    r"root\['id'\]",  # id, must be different
                    r"root\['created_date'\]",  # must be different
                    r"root\['updated_date'\]",  # must be different
                    r"root\['assignee'\]",  # id, depends on the situation
                    r"root\['owner'\]",  # id, depends on the situation
                    r"root\['data'\]",  # id, must be different
                    r"root\['organization'\]",  # depends on the task setup
                    r"root\['project_id'\]",  # should be dropped
                    r"root(\['.*'\])*\['url'\]",  # depends on the task id
                    r"root\['data_compressed_chunk_type'\]",  # depends on the server configuration
                    r"root\['source_storage'\]",  # should be dropped
                    r"root\['target_storage'\]",  # should be dropped
                    r"root\['jobs'\]\['completed'\]",  # job statuses should be renewed
                    r"root\['jobs'\]\['validation'\]",  # job statuses should be renewed
                    r"root\['status'\]",  # task status should be renewed
                    # depends on the actual job configuration,
                    # unlike to what is obtained from the regular task creation,
                    # where the requested number is recorded
                    r"root\['overlap'\]",
                ],
            )
            == {}
        )

        old_task_annotations = json.loads(old_task.api.retrieve_annotations(old_task.id)[1].data)
        new_task_annotations = json.loads(new_task.api.retrieve_annotations(new_task.id)[1].data)
        assert compare_annotations(old_task_annotations, new_task_annotations) == {}


@pytest.mark.usefixtures("restore_db_per_function")
class TestWorkWithSimpleGtJobTasks:
    @fixture
    def fxt_task_with_gt_job(
        self, tasks, jobs, job_has_annotations
    ) -> Generator[dict[str, Any], None, None]:
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            if job_has_annotations(j["id"])
            if tasks[j["task_id"]]["validation_mode"] == "gt"
            if tasks[j["task_id"]]["size"]
        )

        task = tasks[gt_job["task_id"]]

        annotation_jobs = sorted(
            [j for j in jobs if j["task_id"] == task["id"] if j["id"] != gt_job["id"]],
            key=lambda j: j["start_frame"],
        )

        yield task, gt_job, annotation_jobs

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_gt_job)])
    def test_gt_job_annotations_are_not_present_in_task_annotation_export(
        self, tmp_path, admin_user, task, gt_job, annotation_jobs
    ):
        with make_sdk_client(admin_user) as client:
            for j in annotation_jobs:
                client.jobs.retrieve(j["id"]).remove_annotations()

            task_obj = client.tasks.retrieve(task["id"])
            task_raw_annotations = task_obj.get_annotations()

            # It's quite hard to parse the dataset files, just import the data back instead
            dataset_format = "CVAT for images 1.1"

            dataset_file = tmp_path / "dataset.zip"
            task_obj.export_dataset(dataset_format, dataset_file, include_images=True)
            task_obj.import_annotations("CVAT 1.1", dataset_file)
            task_dataset_file_annotations = task_obj.get_annotations()

            annotations_file = tmp_path / "annotations.zip"
            task_obj.export_dataset(dataset_format, annotations_file, include_images=False)
            task_obj.import_annotations("CVAT 1.1", annotations_file)
            task_annotations_file_annotations = task_obj.get_annotations()

        for annotation_source in [
            task_raw_annotations,
            task_dataset_file_annotations,
            task_annotations_file_annotations,
        ]:
            assert not annotation_source.tags
            assert not annotation_source.shapes
            assert not annotation_source.tracks

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_gt_job)])
    def test_can_exclude_and_restore_gt_frames_via_gt_job_meta(
        self, admin_user, task, gt_job, annotation_jobs
    ):
        with make_api_client(admin_user) as api_client:
            task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
            gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            frame_step = parse_frame_step(task_meta.frame_filter)

            for deleted_gt_frames in [
                [i]
                for i in range(gt_job_meta["start_frame"], gt_job["stop_frame"] + 1)
                if gt_job_meta.start_frame + i * frame_step in gt_job_meta.included_frames
            ] + [[]]:
                updated_gt_job_meta, _ = api_client.jobs_api.partial_update_data_meta(
                    gt_job["id"],
                    patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                        deleted_frames=deleted_gt_frames
                    ),
                )

                assert updated_gt_job_meta.deleted_frames == deleted_gt_frames

                # the excluded GT frames must be excluded only from the GT job
                updated_task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
                assert updated_task_meta.deleted_frames == []

                for j in annotation_jobs:
                    updated_job_meta, _ = api_client.jobs_api.retrieve_data_meta(j["id"])
                    assert updated_job_meta.deleted_frames == []

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_gt_job)])
    def test_can_delete_gt_frames_by_changing_job_meta_in_owning_annotation_job(
        self, admin_user, task, gt_job, annotation_jobs
    ):
        with make_api_client(admin_user) as api_client:
            task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
            gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            frame_step = parse_frame_step(task_meta.frame_filter)

            gt_frames = [
                (f - gt_job_meta.start_frame) // frame_step for f in gt_job_meta.included_frames
            ]
            deleted_gt_frame = gt_frames[0]

            annotation_job = next(
                j
                for j in annotation_jobs
                if j["start_frame"] <= deleted_gt_frame <= j["stop_frame"]
            )
            api_client.jobs_api.partial_update_data_meta(
                annotation_job["id"],
                patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                    deleted_frames=[deleted_gt_frame]
                ),
            )

            # in this case deleted frames are deleted everywhere
            updated_gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            assert updated_gt_job_meta.deleted_frames == [deleted_gt_frame]

            updated_task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
            assert updated_task_meta.deleted_frames == [deleted_gt_frame]


@pytest.mark.usefixtures("restore_db_per_function")
class TestWorkWithHoneypotTasks:
    @fixture
    def fxt_task_with_honeypots(
        self, tasks, jobs, job_has_annotations
    ) -> Generator[dict[str, Any], None, None]:
        gt_job = next(
            j
            for j in jobs
            if j["type"] == "ground_truth"
            if j["frame_count"] >= 4
            if job_has_annotations(j["id"])
            if tasks[j["task_id"]]["validation_mode"] == "gt_pool"
            if tasks[j["task_id"]]["size"]
        )

        task = tasks[gt_job["task_id"]]

        annotation_jobs = sorted(
            [j for j in jobs if j["task_id"] == task["id"] if j["id"] != gt_job["id"]],
            key=lambda j: j["start_frame"],
        )

        yield task, gt_job, annotation_jobs

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    def test_gt_job_annotations_are_present_in_task_annotation_export(
        self, tmp_path, admin_user, task, gt_job, annotation_jobs
    ):
        with make_sdk_client(admin_user) as client:
            for j in annotation_jobs:
                client.jobs.retrieve(j["id"]).remove_annotations()

            task_obj = client.tasks.retrieve(task["id"])
            task_raw_annotations = json.loads(task_obj.api.retrieve_annotations(task["id"])[1].data)

            # It's quite hard to parse the dataset files, just import the data back instead
            dataset_format = "CVAT for images 1.1"

            dataset_file = tmp_path / "dataset.zip"
            task_obj.export_dataset(dataset_format, dataset_file, include_images=True)
            task_obj.import_annotations("CVAT 1.1", dataset_file)
            task_dataset_file_annotations = json.loads(
                task_obj.api.retrieve_annotations(task["id"])[1].data
            )

            annotations_file = tmp_path / "annotations.zip"
            task_obj.export_dataset(dataset_format, annotations_file, include_images=False)
            task_obj.import_annotations("CVAT 1.1", annotations_file)
            task_annotations_file_annotations = json.loads(
                task_obj.api.retrieve_annotations(task["id"])[1].data
            )

        # there will be other annotations after uploading into a honeypot task,
        # we need to compare only the validation frames in this test
        validation_frames = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
        for anns in [
            task_raw_annotations,
            task_dataset_file_annotations,
            task_annotations_file_annotations,
        ]:
            anns["tags"] = [t for t in anns["tags"] if t["frame"] in validation_frames]
            anns["shapes"] = [t for t in anns["shapes"] if t["frame"] in validation_frames]

        assert task_raw_annotations["tags"] or task_raw_annotations["shapes"]
        assert not task_raw_annotations["tracks"]  # tracks are prohibited in such tasks
        assert compare_annotations(task_raw_annotations, task_dataset_file_annotations) == {}
        assert compare_annotations(task_raw_annotations, task_annotations_file_annotations) == {}

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @pytest.mark.parametrize("dataset_format", ["CVAT for images 1.1", "Datumaro 1.0"])
    def test_placeholder_frames_are_not_present_in_task_annotation_export(
        self, tmp_path, admin_user, task, gt_job, annotation_jobs, dataset_format
    ):
        with make_sdk_client(admin_user) as client:
            for j in annotation_jobs:
                client.jobs.retrieve(j["id"]).remove_annotations()

            task_obj = client.tasks.retrieve(task["id"])

            dataset_file = tmp_path / "dataset.zip"
            task_obj.export_dataset(dataset_format, dataset_file, include_images=True)

            task_meta = task_obj.get_meta()

        task_frame_names = [frame.name for frame in task_meta.frames]
        gt_frame_ids = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
        gt_frame_names = [task_frame_names[i] for i in gt_frame_ids]

        frame_step = parse_frame_step(task_meta.frame_filter)
        expected_frames = [
            (task_meta.start_frame + frame * frame_step, name)
            for frame, name in enumerate(task_frame_names)
            if frame in gt_frame_ids or name not in gt_frame_names
        ]

        with zipfile.ZipFile(dataset_file, "r") as archive:
            if dataset_format == "CVAT for images 1.1":
                annotations = archive.read("annotations.xml").decode()
                matches = re.findall(r'<image id="(\d+)" name="([^"]+)"', annotations, re.MULTILINE)
                assert sorted((int(match[0]), match[1]) for match in matches) == sorted(
                    expected_frames
                )
            elif dataset_format == "Datumaro 1.0":
                with archive.open("annotations/default.json", "r") as annotation_file:
                    annotations = json.load(annotation_file)

                assert sorted(
                    (int(item["attr"]["frame"]), item["id"]) for item in annotations["items"]
                ) == sorted((frame, os.path.splitext(name)[0]) for frame, name in expected_frames)
            else:
                assert False

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @parametrize("method", ["gt_job_meta", "task_validation_layout"])
    def test_can_exclude_and_restore_gt_frames(
        self, admin_user, task, gt_job, annotation_jobs, method: str
    ):
        with make_api_client(admin_user) as api_client:
            task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
            task_frames = [f.name for f in task_meta.frames]

            for deleted_gt_frames in [
                [v] for v in range(gt_job["start_frame"], gt_job["stop_frame"] + 1)[:2]
            ] + [[]]:
                if method == "gt_job_meta":
                    api_client.jobs_api.partial_update_data_meta(
                        gt_job["id"],
                        patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                            deleted_frames=deleted_gt_frames
                        ),
                    )
                elif method == "task_validation_layout":
                    api_client.tasks_api.partial_update_validation_layout(
                        task["id"],
                        patched_task_validation_layout_write_request=(
                            models.PatchedTaskValidationLayoutWriteRequest(
                                disabled_frames=deleted_gt_frames
                            )
                        ),
                    )
                else:
                    assert False

                updated_validation_layout, _ = api_client.tasks_api.retrieve_validation_layout(
                    task["id"]
                )
                assert updated_validation_layout.disabled_frames == deleted_gt_frames

                updated_gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
                assert updated_gt_job_meta.deleted_frames == deleted_gt_frames

                # the excluded GT frames must be excluded from all the jobs with the same frame
                deleted_frame_names = [task_frames[i] for i in deleted_gt_frames]
                updated_task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
                assert (
                    sorted(i for i, f in enumerate(task_frames) if f in deleted_frame_names)
                    == updated_task_meta.deleted_frames
                )

                for j in annotation_jobs:
                    deleted_job_frames = [
                        i
                        for i in updated_task_meta.deleted_frames
                        if j["start_frame"] <= i <= j["stop_frame"]
                    ]

                    updated_job_meta, _ = api_client.jobs_api.retrieve_data_meta(j["id"])
                    assert deleted_job_frames == updated_job_meta.deleted_frames

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    def test_can_delete_honeypot_frames_by_changing_job_meta_in_annotation_job(
        self, admin_user, task, gt_job, annotation_jobs
    ):
        with make_api_client(admin_user) as api_client:
            task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])

            task_frame_names = [frame.name for frame in task_meta.frames]
            gt_frame_ids = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
            gt_frame_names = [task_frame_names[i] for i in gt_frame_ids]

            honeypot_frame_ids = [
                i for i, f in enumerate(task_meta.frames) if f.name in gt_frame_names
            ]
            deleted_honeypot_frame = honeypot_frame_ids[0]

            annotation_job_with_honeypot = next(
                j
                for j in annotation_jobs
                if j["start_frame"] <= deleted_honeypot_frame <= j["stop_frame"]
            )
            api_client.jobs_api.partial_update_data_meta(
                annotation_job_with_honeypot["id"],
                patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                    deleted_frames=[deleted_honeypot_frame]
                ),
            )

            updated_gt_job_meta, _ = api_client.jobs_api.retrieve_data_meta(gt_job["id"])
            assert updated_gt_job_meta.deleted_frames == []  # must not be affected

            updated_task_meta, _ = api_client.tasks_api.retrieve_data_meta(task["id"])
            assert updated_task_meta.deleted_frames == [deleted_honeypot_frame]  # must be affected

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    def test_can_restore_gt_frames_via_task_meta_only_if_all_frames_are_restored(
        self, admin_user, task, gt_job, annotation_jobs
    ):
        assert gt_job["stop_frame"] - gt_job["start_frame"] + 1 >= 2

        with make_api_client(admin_user) as api_client:
            api_client.jobs_api.partial_update_data_meta(
                gt_job["id"],
                patched_job_data_meta_write_request=models.PatchedJobDataMetaWriteRequest(
                    deleted_frames=[gt_job["start_frame"]]
                ),
            )

            _, response = api_client.tasks_api.partial_update_data_meta(
                task["id"],
                patched_data_meta_write_request=models.PatchedDataMetaWriteRequest(
                    deleted_frames=[gt_job["start_frame"], gt_job["start_frame"] + 1]
                ),
                _parse_response=False,
                _check_status=False,
            )
            assert response.status == HTTPStatus.BAD_REQUEST
            assert b"GT frames can only be deleted" in response.data

            updated_task_meta, _ = api_client.tasks_api.partial_update_data_meta(
                task["id"],
                patched_data_meta_write_request=models.PatchedDataMetaWriteRequest(
                    deleted_frames=[]
                ),
            )
            assert updated_task_meta.deleted_frames == []

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @parametrize("frame_selection_method", ["manual", "random_uniform"])
    def test_can_change_honeypot_frames_in_task(
        self, admin_user, task, gt_job, annotation_jobs, frame_selection_method: str
    ):
        assert gt_job["stop_frame"] - gt_job["start_frame"] + 1 >= 2

        with make_api_client(admin_user) as api_client:
            gt_frame_set = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
            old_validation_layout = json.loads(
                api_client.tasks_api.retrieve_validation_layout(task["id"])[1].data
            )

            api_client.tasks_api.partial_update_validation_layout(
                task["id"],
                patched_task_validation_layout_write_request=models.PatchedTaskValidationLayoutWriteRequest(
                    frame_selection_method="manual",
                    honeypot_real_frames=old_validation_layout["honeypot_count"]
                    * [gt_frame_set[0]],
                ),
            )

            params = {"frame_selection_method": frame_selection_method}

            if frame_selection_method == "manual":
                requested_honeypot_real_frames = [
                    gt_frame_set[(old_real_frame + 1) % len(gt_frame_set)]
                    for old_real_frame in old_validation_layout["honeypot_real_frames"]
                ]

                params["honeypot_real_frames"] = requested_honeypot_real_frames

            new_validation_layout = json.loads(
                api_client.tasks_api.partial_update_validation_layout(
                    task["id"],
                    patched_task_validation_layout_write_request=(
                        models.PatchedTaskValidationLayoutWriteRequest(**params)
                    ),
                )[1].data
            )

            new_honeypot_real_frames = new_validation_layout["honeypot_real_frames"]

            assert old_validation_layout["honeypot_count"] == len(new_honeypot_real_frames)
            assert all(f in gt_frame_set for f in new_honeypot_real_frames)

            if frame_selection_method == "manual":
                assert new_honeypot_real_frames == requested_honeypot_real_frames
            elif frame_selection_method == "random_uniform":
                # Test distribution
                validation_frame_counts = count_frame_uses(
                    new_honeypot_real_frames,
                    included_frames=new_validation_layout["validation_frames"],
                )
                assert max(validation_frame_counts.values()) <= 1 + min(
                    validation_frame_counts.values()
                )

            assert (
                DeepDiff(
                    old_validation_layout,
                    new_validation_layout,
                    exclude_regex_paths=[r"root\['honeypot_real_frames'\]\[\d+\]"],
                )
                == {}
            )

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @parametrize("frame_selection_method", ["manual", "random_uniform"])
    def test_can_change_honeypot_frames_in_task_can_only_select_from_active_validation_frames(
        self, admin_user, task, gt_job, annotation_jobs, frame_selection_method: str
    ):
        assert gt_job["stop_frame"] - gt_job["start_frame"] + 1 >= 2

        with make_api_client(admin_user) as api_client:
            old_validation_layout = json.loads(
                api_client.tasks_api.retrieve_validation_layout(task["id"])[1].data
            )

            honeypots_per_job = old_validation_layout["frames_per_job_count"]

            gt_frame_set = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
            active_gt_set = gt_frame_set[:honeypots_per_job]

            api_client.tasks_api.partial_update_validation_layout(
                task["id"],
                patched_task_validation_layout_write_request=models.PatchedTaskValidationLayoutWriteRequest(
                    disabled_frames=[f for f in gt_frame_set if f not in active_gt_set],
                    frame_selection_method="manual",
                    honeypot_real_frames=old_validation_layout["honeypot_count"]
                    * [active_gt_set[0]],
                ),
            )

            params = {"frame_selection_method": frame_selection_method}

            if frame_selection_method == "manual":
                requested_honeypot_real_frames = [
                    active_gt_set[(old_real_frame + 1) % len(active_gt_set)]
                    for old_real_frame in old_validation_layout["honeypot_real_frames"]
                ]

                params["honeypot_real_frames"] = requested_honeypot_real_frames

                _, response = api_client.tasks_api.partial_update_validation_layout(
                    task["id"],
                    patched_task_validation_layout_write_request=(
                        models.PatchedTaskValidationLayoutWriteRequest(
                            frame_selection_method="manual",
                            honeypot_real_frames=[
                                next(f for f in gt_frame_set if f not in active_gt_set)
                            ]
                            * old_validation_layout["honeypot_count"],
                        )
                    ),
                    _parse_response=False,
                    _check_status=False,
                )
                assert response.status == HTTPStatus.BAD_REQUEST
                assert b"are disabled. Restore them" in response.data

            new_validation_layout = json.loads(
                api_client.tasks_api.partial_update_validation_layout(
                    task["id"],
                    patched_task_validation_layout_write_request=(
                        models.PatchedTaskValidationLayoutWriteRequest(**params)
                    ),
                )[1].data
            )

            new_honeypot_real_frames = new_validation_layout["honeypot_real_frames"]

            assert old_validation_layout["honeypot_count"] == len(new_honeypot_real_frames)
            assert all([f in active_gt_set for f in new_honeypot_real_frames])

            if frame_selection_method == "manual":
                assert new_honeypot_real_frames == requested_honeypot_real_frames
            else:
                assert all(
                    [
                        honeypots_per_job
                        == len(
                            set(
                                new_honeypot_real_frames[
                                    j * honeypots_per_job : (j + 1) * honeypots_per_job
                                ]
                            )
                        )
                        for j in range(len(annotation_jobs))
                    ]
                ), new_honeypot_real_frames

                # Test distribution
                validation_frame_counts = count_frame_uses(
                    new_honeypot_real_frames, included_frames=active_gt_set
                )
                assert max(validation_frame_counts.values()) <= 1 + min(
                    validation_frame_counts.values()
                )

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @parametrize("frame_selection_method", ["manual", "random_uniform"])
    def test_can_restore_and_change_honeypot_frames_in_task_in_the_same_request(
        self, admin_user, task, gt_job, annotation_jobs, frame_selection_method: str
    ):
        assert gt_job["stop_frame"] - gt_job["start_frame"] + 1 >= 2

        with make_api_client(admin_user) as api_client:
            old_validation_layout = json.loads(
                api_client.tasks_api.retrieve_validation_layout(task["id"])[1].data
            )

            honeypots_per_job = old_validation_layout["frames_per_job_count"]

            gt_frame_set = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)
            active_gt_set = gt_frame_set[:honeypots_per_job]

            api_client.tasks_api.partial_update_validation_layout(
                task["id"],
                patched_task_validation_layout_write_request=models.PatchedTaskValidationLayoutWriteRequest(
                    disabled_frames=[f for f in gt_frame_set if f not in active_gt_set],
                    frame_selection_method="manual",
                    honeypot_real_frames=old_validation_layout["honeypot_count"]
                    * [active_gt_set[0]],
                ),
            )

            active_gt_set = gt_frame_set

            params = {
                "frame_selection_method": frame_selection_method,
                "disabled_frames": [],  # restore all validation frames
            }

            if frame_selection_method == "manual":
                requested_honeypot_real_frames = [
                    active_gt_set[(old_real_frame + 1) % len(active_gt_set)]
                    for old_real_frame in old_validation_layout["honeypot_real_frames"]
                ]

                params["honeypot_real_frames"] = requested_honeypot_real_frames

            new_validation_layout = json.loads(
                api_client.tasks_api.partial_update_validation_layout(
                    task["id"],
                    patched_task_validation_layout_write_request=(
                        models.PatchedTaskValidationLayoutWriteRequest(**params)
                    ),
                )[1].data
            )

            new_honeypot_real_frames = new_validation_layout["honeypot_real_frames"]

            assert old_validation_layout["honeypot_count"] == len(new_honeypot_real_frames)
            assert sorted(new_validation_layout["disabled_frames"]) == sorted(
                params["disabled_frames"]
            )

            if frame_selection_method == "manual":
                assert new_honeypot_real_frames == requested_honeypot_real_frames
            else:
                assert all(
                    [
                        honeypots_per_job
                        == len(
                            set(
                                new_honeypot_real_frames[
                                    j * honeypots_per_job : (j + 1) * honeypots_per_job
                                ]
                            )
                        )
                    ]
                    for j in range(len(annotation_jobs))
                ), new_honeypot_real_frames

                # Test distribution
                validation_frame_counts = count_frame_uses(
                    new_honeypot_real_frames, included_frames=active_gt_set
                )
                assert max(validation_frame_counts.values()) <= 1 + min(
                    validation_frame_counts.values()
                )

    @parametrize("task, gt_job, annotation_jobs", [fixture_ref(fxt_task_with_honeypots)])
    @parametrize("frame_selection_method", ["manual", "random_uniform"])
    def test_can_change_honeypot_frames_in_annotation_jobs(
        self, admin_user, task, gt_job, annotation_jobs, frame_selection_method: str
    ):
        _MAX_RANDOM_ATTEMPTS = 20  # This test can have random outcomes, it's expected

        assert gt_job["stop_frame"] - gt_job["start_frame"] + 1 >= 2

        with make_api_client(admin_user) as api_client:
            gt_frame_set = range(gt_job["start_frame"], gt_job["stop_frame"] + 1)

            for annotation_job in annotation_jobs:
                old_validation_layout = json.loads(
                    api_client.jobs_api.retrieve_validation_layout(annotation_job["id"])[1].data
                )
                old_job_meta, _ = api_client.jobs_api.retrieve_data_meta(annotation_job["id"])

                params = {"frame_selection_method": frame_selection_method}

                if frame_selection_method == "manual":
                    requested_honeypot_real_frames = [
                        gt_frame_set[(gt_frame_set.index(old_real_frame) + 1) % len(gt_frame_set)]
                        for old_real_frame in old_validation_layout["honeypot_real_frames"]
                    ]

                    params["honeypot_real_frames"] = requested_honeypot_real_frames

                attempt = 0
                while attempt < _MAX_RANDOM_ATTEMPTS:
                    new_validation_layout = json.loads(
                        api_client.jobs_api.partial_update_validation_layout(
                            annotation_job["id"],
                            patched_job_validation_layout_write_request=(
                                models.PatchedJobValidationLayoutWriteRequest(**params)
                            ),
                        )[1].data
                    )

                    new_honeypot_real_frames = new_validation_layout["honeypot_real_frames"]

                    if (
                        frame_selection_method == "random_uniform"
                        and new_honeypot_real_frames
                        == old_validation_layout["honeypot_real_frames"]
                    ):
                        attempt += 1
                        # The test is fully random, it's possible to get no changes in the updated
                        # honeypots. Passing a random seed has little sense in this endpoint,
                        # so we retry several times in such a case instead.
                    else:
                        break

                if attempt >= _MAX_RANDOM_ATTEMPTS and frame_selection_method == "random_uniform":
                    # The situation is unlikely if everything works, so we consider it a fail
                    pytest.fail(f"too many attempts ({attempt}) with random honeypot updating")

                assert old_validation_layout["honeypot_count"] == len(new_honeypot_real_frames)
                assert all(f in gt_frame_set for f in new_honeypot_real_frames)

                if frame_selection_method == "manual":
                    assert new_honeypot_real_frames == requested_honeypot_real_frames

                assert (
                    DeepDiff(
                        old_validation_layout,
                        new_validation_layout,
                        exclude_regex_paths=[r"root\['honeypot_real_frames'\]\[\d+\]"],
                    )
                    == {}
                )

                new_job_meta, _ = api_client.jobs_api.retrieve_data_meta(annotation_job["id"])
                assert new_job_meta.chunks_updated_date > old_job_meta.chunks_updated_date


@pytest.mark.usefixtures("restore_db_per_function")
class TestWorkWithConsensusTasks:
    @pytest.mark.parametrize("task_id", [30])
    def test_replica_annotations_are_not_present_in_task_annotations(
        self, admin_user, jobs, annotations, task_id: int
    ):
        task_jobs = [j for j in jobs if j["task_id"] == task_id]
        consensus_jobs = [j for j in task_jobs if j["type"] == "consensus_replica"]

        # Ensure there are annotations in replicas
        assert any(
            len(annotations["job"][str(j["id"])]["tags"])
            + len(annotations["job"][str(j["id"])]["shapes"])
            + len(annotations["job"][str(j["id"])]["tracks"])
            for j in consensus_jobs
        )

        with make_api_client(admin_user) as api_client:
            for annotation_job in task_jobs:
                if annotation_job["type"] != "consensus_replica":
                    api_client.jobs_api.destroy_annotations(annotation_job["id"])

            updated_task_annotations, _ = api_client.tasks_api.retrieve_annotations(task_id)
            assert not updated_task_annotations.tags
            assert not updated_task_annotations.shapes
            assert not updated_task_annotations.tracks

            for consensus_job in consensus_jobs:
                job_annotations = annotations["job"][str(consensus_job["id"])]
                updated_job_annotations, _ = api_client.jobs_api.retrieve_annotations(
                    consensus_job["id"]
                )

                assert len(job_annotations["tags"]) == len(updated_job_annotations.tags)
                assert len(job_annotations["shapes"]) == len(updated_job_annotations.shapes)
                assert len(job_annotations["tracks"]) == len(updated_job_annotations.tracks)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetTaskPreview:
    def _test_task_preview_200(self, username, task_id, **kwargs):
        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.retrieve_preview(task_id, **kwargs)

            assert response.status == HTTPStatus.OK
            (width, height) = Image.open(io.BytesIO(response.data)).size
            assert width > 0 and height > 0

    def _test_task_preview_403(self, username, task_id):
        with make_api_client(username) as api_client:
            (_, response) = api_client.tasks_api.retrieve_preview(
                task_id, _parse_response=False, _check_status=False
            )
            assert response.status == HTTPStatus.FORBIDDEN

    def _test_assigned_users_to_see_task_preview(self, tasks, users, is_task_staff, **kwargs):
        for task in tasks:
            staff_users = [user for user in users if is_task_staff(user["id"], task["id"])]
            assert len(staff_users)

            for user in staff_users:
                self._test_task_preview_200(user["username"], task["id"], **kwargs)

    def _test_assigned_users_cannot_see_task_preview(self, tasks, users, is_task_staff, **kwargs):
        for task in tasks:
            not_staff_users = [user for user in users if not is_task_staff(user["id"], task["id"])]
            assert len(not_staff_users)

            for user in not_staff_users:
                self._test_task_preview_403(user["username"], task["id"], **kwargs)

    @pytest.mark.parametrize("project_id, groups", [(1, "user")])
    def test_task_assigned_to_see_task_preview(
        self, project_id, groups, users, tasks, find_users, is_task_staff
    ):
        users = find_users(privilege=groups)
        tasks = list(filter(lambda x: x["project_id"] == project_id and x["assignee"], tasks))
        assert len(tasks)

        self._test_assigned_users_to_see_task_preview(tasks, users, is_task_staff)

    @pytest.mark.parametrize("org, project_id, role", [({"id": 2, "slug": "org2"}, 2, "worker")])
    def test_org_task_assigneed_to_see_task_preview(
        self, org, project_id, role, users, tasks, find_users, is_task_staff
    ):
        users = find_users(org=org["id"], role=role)
        tasks = list(filter(lambda x: x["project_id"] == project_id and x["assignee"], tasks))
        assert len(tasks)

        self._test_assigned_users_to_see_task_preview(tasks, users, is_task_staff)

    @pytest.mark.parametrize("project_id, groups", [(1, "user")])
    def test_task_unassigned_cannot_see_task_preview(
        self, project_id, groups, users, tasks, find_users, is_task_staff
    ):
        users = find_users(privilege=groups)
        tasks = list(filter(lambda x: x["project_id"] == project_id and x["assignee"], tasks))
        assert len(tasks)

        self._test_assigned_users_cannot_see_task_preview(tasks, users, is_task_staff)


@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
class TestUnequalJobs:
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_function, tmp_path: Path, admin_user: str):
        self.tmp_dir = tmp_path

        self.user = admin_user

        with make_sdk_client(self.user) as client:
            self.client = client

    @pytest.fixture
    def fxt_task_with_unequal_jobs(self):
        task_spec = {
            "name": f"test file-job mapping",
            "labels": [{"name": "car"}],
        }

        files = generate_image_files(7)
        filenames = [osp.basename(f.name) for f in files]
        for file_data in files:
            with open(self.tmp_dir / file_data.name, "wb") as f:
                f.write(file_data.getvalue())

        expected_segments = [
            filenames[0:1],
            filenames[1:5][::-1],  # a reversed fragment
            filenames[5:7],
        ]

        data_spec = {
            "job_file_mapping": expected_segments,
        }

        yield self.client.tasks.create_from_data(
            spec=task_spec,
            resource_type=ResourceType.LOCAL,
            resources=[self.tmp_dir / fn for fn in filenames],
            data_params=data_spec,
        )

    def test_can_export(self, fxt_task_with_unequal_jobs: Task):
        task = fxt_task_with_unequal_jobs

        filename = self.tmp_dir / f"task_{task.id}_coco.zip"
        task.export_dataset("COCO 1.0", filename)

        assert filename.is_file()
        assert filename.stat().st_size > 0

    def test_can_import_annotations(self, fxt_task_with_unequal_jobs: Task):
        task = fxt_task_with_unequal_jobs

        format_name = "COCO 1.0"
        filename = self.tmp_dir / f"task_{task.id}_coco.zip"
        task.export_dataset(format_name, filename)

        task.import_annotations(format_name, filename)

    def test_can_dump_backup(self, fxt_task_with_unequal_jobs: Task):
        task = fxt_task_with_unequal_jobs

        filename = self.tmp_dir / f"task_{task.id}_backup.zip"
        task.download_backup(filename)

        assert filename.is_file()
        assert filename.stat().st_size > 0

    def test_can_import_backup(self, fxt_task_with_unequal_jobs: Task):
        task = fxt_task_with_unequal_jobs

        filename = self.tmp_dir / f"task_{task.id}_backup.zip"
        task.download_backup(filename)

        restored_task = self.client.tasks.create_from_backup(filename)

        old_jobs = task.get_jobs()
        new_jobs = restored_task.get_jobs()
        assert len(old_jobs) == len(new_jobs)

        for old_job, new_job in zip(old_jobs, new_jobs):
            assert old_job.start_frame == new_job.start_frame
            assert old_job.stop_frame == new_job.stop_frame


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchTask:
    @pytest.mark.parametrize("task_id, project_id, user", [(19, 12, "admin1")])
    def test_move_task_to_project_with_attributes(self, task_id, project_id, user):
        response = get_method(user, f"tasks/{task_id}/annotations")
        assert response.status_code == HTTPStatus.OK
        annotations = response.json()

        response = patch_method(user, f"tasks/{task_id}", {"project_id": project_id})
        assert response.status_code == HTTPStatus.OK

        response = get_method(user, f"tasks/{task_id}")
        assert response.status_code == HTTPStatus.OK
        assert response.json().get("project_id") == project_id

        response = get_method(user, f"tasks/{task_id}/annotations")
        assert response.status_code == HTTPStatus.OK
        assert (
            DeepDiff(
                annotations,
                response.json(),
                ignore_order=True,
                exclude_regex_paths=[
                    r"root\['\w+'\]\[\d+\]\['label_id'\]",
                    r"root\['\w+'\]\[\d+\]\['attributes'\]\[\d+\]\['spec_id'\]",
                ],
            )
            == {}
        )

    @pytest.mark.parametrize("task_id, project_id, user", [(20, 13, "admin1")])
    def test_move_task_from_one_project_to_another_with_attributes(self, task_id, project_id, user):
        response = get_method(user, f"tasks/{task_id}/annotations")
        assert response.status_code == HTTPStatus.OK
        annotations = response.json()

        response = patch_method(user, f"tasks/{task_id}", {"project_id": project_id})
        assert response.status_code == HTTPStatus.OK

        response = get_method(user, f"tasks/{task_id}")
        assert response.status_code == HTTPStatus.OK
        assert response.json().get("project_id") == project_id

        response = get_method(user, f"tasks/{task_id}/annotations")
        assert response.status_code == HTTPStatus.OK
        assert compare_annotations(annotations, response.json()) == {}

    @pytest.mark.with_external_services
    @pytest.mark.parametrize(
        "storage_id",
        [
            1,  # public bucket
            2,  # private bucket
        ],
    )
    @pytest.mark.parametrize("field", ["source_storage", "target_storage"])
    def test_user_cannot_update_task_with_cloud_storage_without_access(
        self, storage_id, field, regular_lonely_user
    ):
        user = regular_lonely_user

        task_spec = {
            "name": f"Task with files from foreign cloud storage {storage_id}",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }
        data_spec = {
            "image_quality": 75,
            "use_cache": True,
            "server_files": ["images/image_1.jpg"],
        }
        (task_id, _) = create_task(user, task_spec, data_spec)

        updated_fields = {
            field: {
                "location": "cloud_storage",
                "cloud_storage_id": storage_id,
            }
        }

        with make_api_client(user) as api_client:
            (_, response) = api_client.tasks_api.partial_update(
                task_id,
                patched_task_write_request=updated_fields,
                _parse_response=False,
                _check_status=False,
            )
        assert response.status == HTTPStatus.FORBIDDEN

    def test_malefactor_cannot_obtain_task_details_via_empty_partial_update_request(
        self, regular_lonely_user, tasks
    ):
        task = next(iter(tasks))

        with make_api_client(regular_lonely_user) as api_client:
            with pytest.raises(ForbiddenException):
                api_client.tasks_api.partial_update(task["id"])

    @pytest.mark.parametrize("has_old_assignee", [False, True])
    @pytest.mark.parametrize("new_assignee", [None, "same", "different"])
    def test_can_update_assignee_updated_date_on_assignee_updates(
        self, admin_user, tasks, users, has_old_assignee, new_assignee
    ):
        task = next(t for t in tasks if bool(t.get("assignee")) == has_old_assignee)

        old_assignee_id = (task.get("assignee") or {}).get("id")

        new_assignee_id = None
        if new_assignee == "same":
            new_assignee_id = old_assignee_id
        elif new_assignee == "different":
            new_assignee_id = next(u for u in users if u["id"] != old_assignee_id)["id"]

        with make_api_client(admin_user) as api_client:
            (updated_task, _) = api_client.tasks_api.partial_update(
                task["id"], patched_task_write_request={"assignee_id": new_assignee_id}
            )

            op = operator.eq if new_assignee_id == old_assignee_id else operator.ne

            if isinstance(updated_task.assignee_updated_date, datetime):
                assert op(
                    str(updated_task.assignee_updated_date.isoformat()).replace("+00:00", "Z"),
                    task["assignee_updated_date"],
                )
            else:
                assert op(updated_task.assignee_updated_date, task["assignee_updated_date"])

    @staticmethod
    def _test_patch_linked_storage(
        user: str, task_id: int, *, expected_status: HTTPStatus = HTTPStatus.OK
    ) -> None:
        with make_api_client(user) as api_client:
            for associated_storage in ("source_storage", "target_storage"):
                patch_data = {
                    associated_storage: {
                        "location": "local",
                    }
                }
                (_, response) = api_client.tasks_api.partial_update(
                    task_id,
                    patched_task_write_request=patch_data,
                    _check_status=False,
                    _parse_response=False,
                )
                assert response.status == expected_status, response.status

    @pytest.mark.parametrize(
        "role, is_allow",
        [
            ("owner", True),
            ("maintainer", True),
            ("supervisor", False),
            ("worker", False),
        ],
    )
    def test_update_task_linked_storage_by_org_roles(
        self,
        role: str,
        is_allow: bool,
        tasks,
        find_users,
    ):
        username, task_id = next(
            (
                (user["username"], task["id"])
                for user in find_users(role=role, exclude_privilege="admin")
                for task in tasks
                if task["organization"] == user["org"]
                and not task["project_id"]
                and task["owner"]["id"] != user["id"]
            )
        )

        self._test_patch_linked_storage(
            username,
            task_id,
            expected_status=HTTPStatus.OK if is_allow else HTTPStatus.FORBIDDEN,
        )

    @pytest.mark.parametrize("org", (True, False))
    @pytest.mark.parametrize(
        "is_task_owner, is_task_assignee, is_project_owner, is_project_assignee",
        [tuple(i == j for j in range(4)) for i in range(5)],
    )
    def test_update_task_linked_storage_by_assignee_or_owner(
        self,
        org: bool,
        is_task_owner: bool,
        is_task_assignee: bool,
        is_project_owner: bool,
        is_project_assignee: bool,
        tasks,
        find_users,
        projects,
    ):
        is_allow = is_task_owner or is_project_owner
        has_project = is_project_owner or is_project_assignee

        username: Optional[str] = None
        task_id: Optional[int] = None

        filtered_users = (
            (find_users(role="worker") + find_users(role="supervisor"))
            if org
            else find_users(org=None)
        )

        for task in tasks:
            if task_id is not None:
                break

            if (
                org
                and not task["organization"]
                or not org
                and task["organization"]
                or has_project
                and task["project_id"] is None
                or not has_project
                and task["project_id"]
            ):
                continue

            for user in filtered_users:
                if org and task["organization"] != user["org"]:
                    continue

                is_user_task_owner = task["owner"]["id"] == user["id"]
                is_user_task_assignee = (task["assignee"] or {}).get("id") == user["id"]
                project = projects[task["project_id"]] if task["project_id"] else None
                is_user_project_owner = (project or {}).get("owner", {}).get("id") == user["id"]
                is_user_project_assignee = ((project or {}).get("assignee") or {}).get(
                    "id"
                ) == user["id"]

                if (
                    is_task_owner
                    and is_user_task_owner
                    or is_task_assignee
                    and is_user_task_assignee
                    or is_project_owner
                    and is_user_project_owner
                    or is_project_assignee
                    and is_user_project_assignee
                    or (
                        not any(
                            [
                                is_task_owner,
                                is_task_assignee,
                                is_project_owner,
                                is_project_assignee,
                                is_user_task_owner,
                                is_user_task_assignee,
                                is_user_project_owner,
                                is_project_assignee,
                            ]
                        )
                    )
                ):
                    task_id = task["id"]
                    username = user["username"]
                    break

        assert task_id is not None

        self._test_patch_linked_storage(
            username,
            task_id,
            expected_status=HTTPStatus.OK if is_allow else HTTPStatus.FORBIDDEN,
        )


@pytest.mark.usefixtures("restore_db_per_function")
def test_can_report_correct_completed_jobs_count(tasks_wlc, jobs_wlc, admin_user):
    # Reproduces https://github.com/cvat-ai/cvat/issues/6098
    task = next(
        t
        for t in tasks_wlc
        if t["jobs"]["count"] > 1 and t["jobs"]["completed"] == 0 and t["labels"]["count"] > 1
    )
    task_jobs = [j for j in jobs_wlc if j["task_id"] == task["id"]]

    with make_api_client(admin_user) as api_client:
        api_client.jobs_api.partial_update(
            task_jobs[0]["id"],
            patched_job_write_request=dict(stage="acceptance", state="completed"),
        )

        task, _ = api_client.tasks_api.retrieve(task["id"])
        assert task.jobs.completed == 1


class TestImportTaskAnnotations:
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_function, tmp_path: Path, admin_user: str):
        self.tmp_dir = tmp_path
        self.user = admin_user
        self.export_format = "CVAT for images 1.1"
        self.import_format = "CVAT 1.1"

        with make_sdk_client(self.user) as client:
            self.client = client

    def _check_annotations(self, task_id):
        with make_api_client(self.user) as api_client:
            (_, response) = api_client.tasks_api.retrieve_annotations(id=task_id)
            assert response.status == HTTPStatus.OK
            annotations = json.loads(response.data)["shapes"]
            assert len(annotations) > 0

    def _delete_annotations(self, task_id):
        with make_api_client(self.user) as api_client:
            (_, response) = api_client.tasks_api.destroy_annotations(id=task_id)
            assert response.status == HTTPStatus.NO_CONTENT

    @pytest.mark.skip("Fails sometimes, needs to be fixed")
    @pytest.mark.timeout(70)
    @pytest.mark.parametrize("successful_upload", [True, False])
    def test_can_import_annotations_after_previous_unclear_import(
        self, successful_upload: bool, tasks_with_shapes
    ):
        task_id = tasks_with_shapes[0]["id"]
        self._check_annotations(task_id)

        with NamedTemporaryFile() as f:
            filename = self.tmp_dir / f"task_{task_id}_{Path(f.name).name}_coco.zip"

        task = self.client.tasks.retrieve(task_id)
        task.export_dataset(self.export_format, filename, include_images=False)

        self._delete_annotations(task_id)

        params = {"format": self.import_format, "filename": filename.name}
        url = self.client.api_map.make_endpoint_url(
            self.client.api_client.tasks_api.create_annotations_endpoint.path
        ).format(id=task_id)
        uploader = Uploader(self.client)

        if successful_upload:
            # define time required to upload file with annotations
            start_time = time()
            task.import_annotations(self.import_format, filename)
            required_time = ceil(time() - start_time) * 2
            self._delete_annotations(task_id)

            response = uploader.upload_file(
                url, filename, meta=params, query_params=params, logger=self.client.logger.debug
            )
            rq_id = json.loads(response.data)["rq_id"]
            assert rq_id
        else:
            required_time = 60
            uploader._tus_start_upload(url, query_params=params)
            uploader._upload_file_data_with_tus(
                url,
                filename,
                meta=params,
                logger=self.client.logger.debug,
                pbar=NullProgressReporter(),
            )

        sleep(required_time)
        if successful_upload:
            self._check_annotations(task_id)
            self._delete_annotations(task_id)
        task.import_annotations(self.import_format, filename)
        self._check_annotations(task_id)

    @pytest.mark.skip("Fails sometimes, needs to be fixed")
    @pytest.mark.timeout(70)
    def test_check_import_cache_after_previous_interrupted_upload(self, tasks_with_shapes, request):
        task_id = tasks_with_shapes[0]["id"]
        with NamedTemporaryFile() as f:
            filename = self.tmp_dir / f"task_{task_id}_{Path(f.name).name}_coco.zip"
        task = self.client.tasks.retrieve(task_id)
        task.export_dataset(self.export_format, filename, include_images=False)

        params = {"format": self.import_format, "filename": filename.name}
        url = self.client.api_map.make_endpoint_url(
            self.client.api_client.tasks_api.create_annotations_endpoint.path
        ).format(id=task_id)

        uploader = Uploader(self.client)
        uploader._tus_start_upload(url, query_params=params)
        uploader._upload_file_data_with_tus(
            url,
            filename,
            meta=params,
            logger=self.client.logger.debug,
            pbar=NullProgressReporter(),
        )
        number_of_files = 1
        sleep(30)  # wait when the cleaning job from rq worker will be started
        command = ["/bin/bash", "-c", f"ls data/tasks/{task_id}/tmp | wc -l"]
        for _ in range(12):
            sleep(2)
            result, _ = container_exec_cvat(request, command)
            number_of_files = int(result)
            if not number_of_files:
                break
        assert not number_of_files

    def test_import_annotations_after_deleting_related_cloud_storage(
        self, admin_user: str, tasks_with_shapes
    ):
        related_field = "source_storage"

        task = next(
            t
            for t in tasks_with_shapes
            if t[related_field] and t[related_field]["location"] == "cloud_storage"
        )
        task_id = task["id"]
        cloud_storage_id = task["source_storage"]["cloud_storage_id"]

        # generate temporary destination
        with NamedTemporaryFile(dir=self.tmp_dir, suffix=f"task_{task_id}.zip") as f:
            file_path = Path(f.name)

        task = self.client.tasks.retrieve(task_id)
        self._check_annotations(task_id)

        with make_api_client(admin_user) as api_client:
            _, response = api_client.cloudstorages_api.destroy(cloud_storage_id)
            assert response.status == HTTPStatus.NO_CONTENT

        task = self.client.tasks.retrieve(task_id)
        assert not getattr(task, related_field)

        task.export_dataset(self.export_format, file_path, include_images=False)
        self._delete_annotations(task_id)
        task.import_annotations(self.import_format, file_path)
        self._check_annotations(task_id)

    @pytest.mark.parametrize("dimension", ["2d", "3d"])
    def test_can_import_datumaro_json(self, admin_user, tasks, dimension):
        task = next(
            t
            for t in tasks
            if t.get("size")
            if t["dimension"] == dimension and t.get("validation_mode") != "gt_pool"
        )

        with make_api_client(admin_user) as api_client:
            original_annotations = json.loads(
                api_client.tasks_api.retrieve_annotations(task["id"])[1].data
            )

            dataset_archive = io.BytesIO(
                export_dataset(
                    api_client.tasks_api,
                    id=task["id"],
                    format=DATUMARO_FORMAT_FOR_DIMENSION[dimension],
                    save_images=False,
                )
            )

        with zipfile.ZipFile(dataset_archive) as zip_file:
            annotations = zip_file.read("annotations/default.json")

        with TemporaryDirectory() as tempdir:
            annotations_path = Path(tempdir) / "annotations.json"
            annotations_path.write_bytes(annotations)
            self.client.tasks.retrieve(task["id"]).import_annotations(
                DATUMARO_FORMAT_FOR_DIMENSION[dimension], annotations_path
            )

        with make_api_client(admin_user) as api_client:
            updated_annotations = json.loads(
                api_client.tasks_api.retrieve_annotations(task["id"])[1].data
            )

        assert compare_annotations(original_annotations, updated_annotations) == {}

    @parametrize(
        "format_name, specific_info_included",
        [
            ("COCO 1.0", None),
            ("COCO Keypoints 1.0", None),
            ("CVAT 1.1", True),
            ("LabelMe 3.0", True),
            ("MOT 1.1", True),
            ("MOTS PNG 1.0", False),
            pytest.param("PASCAL VOC 1.1", None, marks=pytest.mark.xfail),
            ("Segmentation mask 1.1", True),
            ("YOLO 1.1", True),
            ("WiderFace 1.0", True),
            ("VGGFace2 1.0", True),
            ("Market-1501 1.0", False),
            ("Kitti Raw Format 1.0", True),
            ("Sly Point Cloud Format 1.0", False),
            ("KITTI 1.0", False),
            ("LFW 1.0", True),
            ("Cityscapes 1.0", True),
            ("Open Images V6 1.0", True),
            ("Datumaro 1.0", True),
            ("Datumaro 3D 1.0", True),
            ("Ultralytics YOLO Oriented Bounding Boxes 1.0", True),
            ("Ultralytics YOLO Detection 1.0", True),
            ("Ultralytics YOLO Pose 1.0", True),
            ("Ultralytics YOLO Segmentation 1.0", True),
        ],
    )
    def test_check_import_error_on_wrong_file_structure(
        self, tasks_with_shapes: Iterable, format_name: str, specific_info_included: Optional[bool]
    ):
        task_id = tasks_with_shapes[0]["id"]

        source_archive_path = self.tmp_dir / "incorrect_archive.zip"

        incorrect_files = ["incorrect_file1.txt", "incorrect_file2.txt"]
        for file in incorrect_files:
            with open(self.tmp_dir / file, "w") as f:
                f.write("Some text")

        with zipfile.ZipFile(source_archive_path, mode="a") as zip_file:
            for path in incorrect_files:
                zip_file.write(self.tmp_dir / path, path)

        task = self.client.tasks.retrieve(task_id)

        with pytest.raises(exceptions.ApiException) as capture:
            task.import_annotations(format_name, source_archive_path)

        error_message = capture.value.body.decode()

        if specific_info_included is None:
            assert "Failed to find dataset" in error_message
            return

        assert "Check [format docs]" in error_message
        expected_msg = (
            "Dataset must contain a file:"
            if specific_info_included
            else "specific requirement information unavailable"
        )
        assert expected_msg in error_message


@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestImportWithComplexFilenames:
    @pytest.fixture(
        autouse=True,
        scope="class",
        # classmethod way may not work in some versions
        # https://github.com/cvat-ai/cvat/actions/runs/5336023573/jobs/9670573955?pr=6350
        name="TestImportWithComplexFilenames.setup_class",
    )
    @classmethod
    def setup_class(
        cls, restore_db_per_class, tmp_path_factory: pytest.TempPathFactory, admin_user: str
    ):
        cls.tmp_dir = tmp_path_factory.mktemp(cls.__class__.__name__)
        cls.user = admin_user
        cls.format_name = "PASCAL VOC 1.1"

        with make_sdk_client(cls.user) as client:
            cls.client = client

        cls._init_tasks()

    @classmethod
    def _create_task_with_annotations(cls, filenames: list[str]):
        images = generate_image_files(len(filenames), filenames=filenames)

        source_archive_path = cls.tmp_dir / "source_data.zip"
        with zipfile.ZipFile(source_archive_path, "w") as zip_file:
            for image in images:
                zip_file.writestr(image.name, image.getvalue())

        task = cls.client.tasks.create_from_data(
            {
                "name": "test_images_with_dots",
                "labels": [{"name": "cat"}, {"name": "dog"}],
            },
            resources=[source_archive_path],
        )

        labels = task.get_labels()
        task.set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=frame_id,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    )
                    for frame_id in range(len(filenames))
                ],
            )
        )

        return task

    @classmethod
    def _init_tasks(cls):
        cls.flat_filenames = [
            "filename0.jpg",
            "file.name1.jpg",
            "fi.le.na.me.2.jpg",
            ".filename3.jpg",
            "..filename..4.jpg",
            "..filename..5.png..jpg",
        ]

        cls.nested_filenames = [
            f"{prefix}/{fn}"
            for prefix, fn in zip(
                [
                    "ab/cd",
                    "ab/cd",
                    "ab",
                    "ab",
                    "cd/ef",
                    "cd/ef",
                    "cd",
                    "",
                ],
                cls.flat_filenames,
            )
        ]

        cls.data = {}
        for (kind, filenames), prefix in product(
            [("flat", cls.flat_filenames), ("nested", cls.nested_filenames)], ["", "pre/fix"]
        ):
            key = kind
            if prefix:
                key += "_prefixed"

            task = cls._create_task_with_annotations(
                [f"{prefix}/{fn}" if prefix else fn for fn in filenames]
            )

            dataset_file = cls.tmp_dir / f"{key}_dataset.zip"
            task.export_dataset(cls.format_name, dataset_file, include_images=False)

            cls.data[key] = (task, dataset_file)

    @pytest.mark.skip("Fails sometimes, needs to be fixed")
    @pytest.mark.parametrize(
        "task_kind, annotation_kind, expect_success",
        [
            ("flat", "flat", True),
            ("flat", "flat_prefixed", False),
            ("flat", "nested", False),
            ("flat", "nested_prefixed", False),
            ("flat_prefixed", "flat", True),  # allow this for better UX
            ("flat_prefixed", "flat_prefixed", True),
            ("flat_prefixed", "nested", False),
            ("flat_prefixed", "nested_prefixed", False),
            ("nested", "flat", False),
            ("nested", "flat_prefixed", False),
            ("nested", "nested", True),
            ("nested", "nested_prefixed", False),
            ("nested_prefixed", "flat", False),
            ("nested_prefixed", "flat_prefixed", False),
            ("nested_prefixed", "nested", True),  # allow this for better UX
            ("nested_prefixed", "nested_prefixed", True),
        ],
    )
    def test_import_annotations(self, task_kind, annotation_kind, expect_success):
        # Tests for regressions about https://github.com/cvat-ai/cvat/issues/6319
        #
        # X annotations must be importable to X prefixed cases
        # with and without dots in filenames.
        #
        # Nested structures can potentially be matched to flat ones and vise-versa,
        # but it's not supported now, as it may lead to some errors in matching.

        task: Task = self.data[task_kind][0]
        dataset_file = self.data[annotation_kind][1]

        if expect_success:
            task.import_annotations(self.format_name, dataset_file)

            assert set(s.frame for s in task.get_annotations().shapes) == set(
                range(len(self.flat_filenames))
            )
        else:
            with pytest.raises(exceptions.ApiException) as capture:
                task.import_annotations(self.format_name, dataset_file)

            assert b"Could not match item id" in capture.value.body

    def delete_annotation_and_import_annotations(
        self, task_id, annotations, format_name, dataset_file
    ):
        task = self.client.tasks.retrieve(task_id)
        labels = task.get_labels()
        sublabels = labels[0].sublabels

        # if the annotations shapes label_id does not exist, the put it in the task
        for shape in annotations["shapes"]:
            if "label_id" not in shape:
                shape["label_id"] = labels[0].id

        for track in annotations["tracks"]:
            if "label_id" not in track:
                track["label_id"] = labels[0].id
            for element_idx, element in enumerate(track["elements"]):
                if "label_id" not in element:
                    element["label_id"] = sublabels[element_idx].id

        response = put_method(
            "admin1", f"tasks/{task_id}/annotations", annotations, action="create"
        )
        assert response.status_code == 200, f"Cannot update task's annotations: {response.content}"

        task.export_dataset(format_name, dataset_file, include_images=False)

        # get the original annotations
        response = get_method("admin1", f"tasks/{task.id}/annotations")
        assert response.status_code == 200, f"Cannot get task's annotations: {response.content}"
        original_annotations = response.json()

        # import the annotations
        task.import_annotations(format_name, dataset_file)

        response = get_method("admin1", f"tasks/{task.id}/annotations")
        assert response.status_code == 200, f"Cannot get task's annotations: {response.content}"
        imported_annotations = response.json()

        return original_annotations, imported_annotations

    def compare_original_and_import_annotations(self, original_annotations, imported_annotations):
        assert compare_annotations(original_annotations, imported_annotations) == {}

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_outside_true(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "outside_true_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 0,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 3,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                            "outside": True,
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check if frame 3 is imported correctly with outside = True
        assert imported_annotations["tracks"][0]["shapes"][1]["outside"]

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_intermediate_keyframe(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "intermediate_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 0,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 3,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check that all the keyframe is imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 2

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_outside_without_keyframe(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "outside_without_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 0,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 3,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "outside": True,
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check that all the keyframe is imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 2

        # check that frame 3 is imported correctly with outside = True
        assert imported_annotations["tracks"][0]["shapes"][1]["outside"]

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_no_keyframe(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "no_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 0,
                            "points": [1.0, 2.0, 3.0, 2.0],
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check if first frame is imported correctly with keyframe = True
        assert len(imported_annotations["tracks"][0]["shapes"]) == 1

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_one_outside(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "one_outside_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 3,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "outside": True,
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # only outside=True shape is imported, means there is no visible shape
        assert len(imported_annotations["tracks"]) == 0

    @pytest.mark.parametrize("format_name", ["Datumaro 1.0", "COCO 1.0", "PASCAL VOC 1.1"])
    def test_export_and_import_tracked_format_with_gap(self, format_name):
        task_id = 14
        dataset_file = self.tmp_dir / (format_name + "with_gap_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {
                            "type": "rectangle",
                            "frame": 0,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 2,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "outside": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 4,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 5,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "outside": True,
                        },
                        {
                            "type": "rectangle",
                            "frame": 6,
                            "points": [1.0, 2.0, 3.0, 2.0],
                            "keyframe": True,
                        },
                    ],
                    "elements": [],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check that all the keyframe is imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 5

        outside_count = sum(
            1 for shape in imported_annotations["tracks"][0]["shapes"] if shape["outside"]
        )
        assert outside_count == 2, "Outside shapes are not imported correctly"

    def test_export_and_import_coco_keypoints_with_outside_true(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "outside_true_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 0, "points": [], "keyframe": True},
                        {
                            "type": "skeleton",
                            "frame": 3,
                            "points": [],
                            "keyframe": True,
                            "outside": True,
                        },
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 3,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                    "outside": True,
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check if frame 3 is imported correctly with outside = True
        assert imported_annotations["tracks"][0]["shapes"][1]["outside"]

    def test_export_and_import_coco_keypoints_with_intermediate_keyframe(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "intermediate_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 0, "points": [], "keyframe": True},
                        {
                            "type": "skeleton",
                            "frame": 3,
                            "points": [],
                            "keyframe": True,
                        },
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 3,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check that all the keyframe is imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 2

    def test_export_and_import_coco_keypoints_with_outside_without_keyframe(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "outside_without_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 0, "points": [], "keyframe": True},
                        {
                            "type": "skeleton",
                            "frame": 3,
                            "points": [],
                            "outside": True,
                        },
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 3,
                                    "points": [1.0, 2.0],
                                    "outside": True,
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check that all the keyframe is imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 2

        # check that frame 3 is imported correctly with outside = True
        assert imported_annotations["tracks"][0]["shapes"][1]["outside"]

    def test_export_and_import_coco_keypoints_with_no_keyframe(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "with_no_keyframe_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 0, "points": []},
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "points": [1.0, 2.0],
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check if first frame is imported correctly with keyframe = True
        assert len(imported_annotations["tracks"][0]["shapes"]) == 1

    def test_export_and_import_coco_keypoints_with_one_outside(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "with_one_outside_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 3, "points": [], "outside": True},
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 3,
                                    "points": [1.0, 2.0],
                                    "outside": True,
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # only outside=True shape is imported, means there is no visible shape
        assert len(imported_annotations["tracks"]) == 0

    def test_export_and_import_coco_keypoints_with_gap(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "with_gap_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "frame": 0, "points": [], "keyframe": True},
                        {"type": "skeleton", "frame": 2, "points": [], "outside": True},
                        {"type": "skeleton", "frame": 4, "points": [], "keyframe": True},
                        {"type": "skeleton", "frame": 5, "points": [], "outside": True},
                        {"type": "skeleton", "frame": 6, "points": [], "keyframe": True},
                    ],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "frame": 0,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 2,
                                    "points": [1.0, 2.0],
                                    "outside": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 4,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 5,
                                    "points": [1.0, 2.0],
                                    "outside": True,
                                },
                                {
                                    "type": "points",
                                    "frame": 6,
                                    "points": [1.0, 2.0],
                                    "keyframe": True,
                                },
                            ],
                        },
                    ],
                }
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        # check if all the keyframes are imported correctly
        assert len(imported_annotations["tracks"][0]["shapes"]) == 5

        outside_count = sum(
            1 for shape in imported_annotations["tracks"][0]["shapes"] if shape["outside"]
        )
        assert outside_count == 2, "Outside shapes are not imported correctly"

    def test_export_and_import_complex_coco_keypoints_annotations(self):
        task_id = 14
        format_name = "COCO Keypoints 1.0"
        dataset_file = self.tmp_dir / (format_name + "complex_annotations_source_data.zip")
        annotations = {
            "shapes": [],
            "tracks": [
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "outside": False, "frame": 0},
                        {"type": "skeleton", "outside": False, "frame": 1},
                        {"type": "skeleton", "outside": False, "frame": 2},
                        {"type": "skeleton", "outside": False, "frame": 4},
                        {"type": "skeleton", "outside": False, "frame": 5},
                    ],
                    "attributes": [],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [256.67, 719.25],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [256.67, 719.25],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [256.67, 719.25],
                                    "frame": 2,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [256.67, 719.25],
                                    "frame": 4,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [256.67, 719.25],
                                    "frame": 5,
                                },
                            ],
                        },
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [318.25, 842.06],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [318.25, 842.06],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [318.25, 842.06],
                                    "frame": 2,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [318.25, 842.06],
                                    "frame": 4,
                                },
                            ],
                        },
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [199.2, 798.71],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [199.2, 798.71],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [199.2, 798.71],
                                    "frame": 2,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [199.2, 798.71],
                                    "frame": 4,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [199.2, 798.71],
                                    "frame": 5,
                                },
                            ],
                        },
                    ],
                },
                {
                    "frame": 0,
                    "group": 0,
                    "shapes": [
                        {"type": "skeleton", "outside": False, "frame": 0},
                        {"type": "skeleton", "outside": True, "frame": 1},
                        {"type": "skeleton", "outside": False, "frame": 3},
                        {"type": "skeleton", "outside": False, "frame": 4},
                        {"type": "skeleton", "outside": False, "frame": 5},
                    ],
                    "attributes": [],
                    "elements": [
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [416.16, 244.31],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [416.16, 244.31],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [416.16, 244.31],
                                    "frame": 3,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [416.16, 244.31],
                                    "frame": 4,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [416.16, 244.31],
                                    "frame": 5,
                                },
                            ],
                        },
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [486.17, 379.65],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [486.17, 379.65],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [486.17, 379.65],
                                    "frame": 3,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [486.17, 379.65],
                                    "frame": 4,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [486.17, 379.65],
                                    "frame": 5,
                                },
                            ],
                        },
                        {
                            "frame": 0,
                            "group": 0,
                            "shapes": [
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [350.83, 331.88],
                                    "frame": 0,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [350.83, 331.88],
                                    "frame": 1,
                                },
                                {
                                    "type": "points",
                                    "outside": True,
                                    "points": [350.83, 331.88],
                                    "frame": 3,
                                },
                                {
                                    "type": "points",
                                    "outside": False,
                                    "points": [350.83, 331.88],
                                    "frame": 5,
                                },
                            ],
                        },
                    ],
                },
            ],
        }

        original_annotations, imported_annotations = self.delete_annotation_and_import_annotations(
            task_id, annotations, format_name, dataset_file
        )

        self.compare_original_and_import_annotations(original_annotations, imported_annotations)

        def check_element_outside_count(track_idx, element_idx, expected_count):
            outside_count = sum(
                1
                for shape in imported_annotations["tracks"][0]["elements"][element_idx]["shapes"]
                if shape["outside"]
            )
            assert (
                outside_count == expected_count
            ), f"Outside shapes for track[{track_idx}]element[{element_idx}] are not imported correctly"

        # check track[0] elements outside count
        check_element_outside_count(0, 0, 1)
        check_element_outside_count(0, 1, 2)
        check_element_outside_count(0, 2, 2)

        # check track[1] elements outside count
        check_element_outside_count(1, 0, 1)
        check_element_outside_count(1, 1, 2)
        check_element_outside_count(1, 2, 2)


@pytest.mark.usefixtures("restore_db_per_class")
@pytest.mark.usefixtures("restore_redis_ondisk_per_function")
@pytest.mark.usefixtures("restore_redis_ondisk_after_class")
@pytest.mark.usefixtures("restore_redis_inmem_per_function")
class TestPatchExportFrames(_TestTasksBase):
    @fixture(scope="class")
    @parametrize("media_type", [_SourceDataType.images, _SourceDataType.video])
    @parametrize("step", [5])
    @parametrize("frame_count", [20])
    @parametrize("start_frame", [None, 3])
    def fxt_uploaded_media_task(
        self,
        request: pytest.FixtureRequest,
        media_type: _SourceDataType,
        step: int,
        frame_count: int,
        start_frame: Optional[int],
    ) -> Generator[tuple[_TaskSpec, Task, str], None, None]:
        args = dict(request=request, frame_count=frame_count, step=step, start_frame=start_frame)

        if media_type == _SourceDataType.images:
            (spec, task_id) = next(self._image_task_fxt_base(**args))
        else:
            (spec, task_id) = next(self._uploaded_video_task_fxt_base(**args))

        with make_sdk_client(self._USERNAME) as client:
            task = client.tasks.retrieve(task_id)

            yield (spec, task, f"CVAT for {media_type.value} 1.1")

    @pytest.mark.usefixtures("restore_redis_ondisk_per_function")
    @parametrize("spec, task, format_name", [fixture_ref(fxt_uploaded_media_task)])
    def test_export_with_non_default_frame_step(
        self, tmp_path: Path, spec: _TaskSpec, task: Task, format_name: str
    ):

        dataset_file = tmp_path / "dataset.zip"
        task.export_dataset(format_name, dataset_file, include_images=True)

        def get_img_index(zinfo: zipfile.ZipInfo) -> int:
            name = PurePosixPath(zinfo.filename)
            if name.suffix.lower() not in (".png", ".jpg", ".jpeg"):
                return -1
            return int(name.stem.rsplit("_", maxsplit=1)[-1])

        # get frames and sort them
        with zipfile.ZipFile(dataset_file) as dataset:
            frames = np.array(
                [png_idx for png_idx in map(get_img_index, dataset.filelist) if png_idx != -1]
            )
            frames.sort()

        task_meta = task.get_meta()
        (src_start_frame, src_stop_frame, src_frame_step) = (
            task_meta["start_frame"],
            task_meta["stop_frame"],
            spec.frame_step,
        )
        src_end_frame = calc_end_frame(src_start_frame, src_stop_frame, src_frame_step)
        assert len(frames) == spec.size == task_meta["size"], "Some frames were lost"
        assert np.all(
            frames == np.arange(src_start_frame, src_end_frame, src_frame_step)
        ), "Some frames are wrong"


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_users.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import typing
from http import HTTPStatus

import pytest
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff

from shared.utils.config import make_api_client

from .utils import CollectionSimpleFilterTestBase


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetUsers:
    def _test_can_see(
        self,
        user,
        data,
        id_: typing.Union[typing.Literal["self"], int, None] = None,
        *,
        exclude_paths="",
        **kwargs,
    ):
        with make_api_client(user) as api_client:
            # TODO: refactor into several functions
            if id_ == "self":
                (_, response) = api_client.users_api.retrieve_self(**kwargs, _parse_response=False)
                assert response.status == HTTPStatus.OK
                response_data = json.loads(response.data)
            elif id_ is None:
                response_data = get_paginated_collection(
                    api_client.users_api.list_endpoint, return_json=True, **kwargs
                )
            else:
                (_, response) = api_client.users_api.retrieve(id_, **kwargs, _parse_response=False)
                assert response.status == HTTPStatus.OK
                response_data = json.loads(response.data)

        assert DeepDiff(data, response_data, ignore_order=True, exclude_paths=exclude_paths) == {}

    def _test_cannot_see(
        self, user, id_: typing.Union[typing.Literal["self"], int, None] = None, **kwargs
    ):
        with make_api_client(user) as api_client:
            # TODO: refactor into several functions
            if id_ == "self":
                (_, response) = api_client.users_api.retrieve_self(
                    **kwargs, _parse_response=False, _check_status=False
                )
            elif id_ is None:
                (_, response) = api_client.users_api.list(
                    **kwargs, _parse_response=False, _check_status=False
                )
            else:
                (_, response) = api_client.users_api.retrieve(
                    id_, **kwargs, _parse_response=False, _check_status=False
                )
            assert response.status == HTTPStatus.FORBIDDEN

    def test_admin_can_see_all_others(self, users):
        exclude_paths = [f"root[{i}]['last_login']" for i in range(len(users))]
        self._test_can_see("admin2", users.raw, exclude_paths=exclude_paths)

    def test_everybody_can_see_self(self, users_by_name):
        for user, data in users_by_name.items():
            self._test_can_see(
                user, data, id_="self", exclude_paths=["root['last_login']", "root['key']"]
            )

    def test_non_members_cannot_see_list_of_members(self):
        self._test_cannot_see("user2", org="org1")

    def test_non_admin_cannot_see_others(self, users):
        non_admins = (v for v in users if not v["is_superuser"])
        user = next(non_admins)["username"]
        user_id = next(non_admins)["id"]

        self._test_cannot_see(user, id_=user_id)

    def test_all_members_can_see_list_of_members(self, find_users, users):
        org_members = [user["username"] for user in find_users(org=1)]
        available_fields = ["url", "id", "username", "first_name", "last_name"]

        data = [
            dict(filter(lambda row: row[0] in available_fields, user.items()))
            for user in users
            if user["username"] in org_members
        ]

        for member in org_members:
            self._test_can_see(member, data, org="org1")


class TestUsersListFilters(CollectionSimpleFilterTestBase):
    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, users):
        self.user = admin_user
        self.samples = users

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.users_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("is_active", "username"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_webhooks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from copy import deepcopy
from http import HTTPStatus
from itertools import product

import pytest
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from deepdiff import DeepDiff

from shared.utils.config import delete_method, get_method, patch_method, post_method

from .utils import CollectionSimpleFilterTestBase


@pytest.mark.usefixtures("restore_db_per_function")
class TestPostWebhooks:
    proj_webhook = {
        "description": "webhook description",
        "content_type": "application/json",
        "enable_ssl": False,
        "events": ["create:task", "delete:task"],
        "is_active": True,
        "project_id": 1,
        "secret": "secret",
        "target_url": "http://example.com",
        "type": "project",
    }

    org_webhook = {
        "description": "webhook description",
        "content_type": "application/json",
        "enable_ssl": False,
        "events": ["create:task", "delete:task"],
        "is_active": True,
        "secret": "secret",
        "target_url": "http://example.com",
        "type": "organization",
    }

    def test_sandbox_admin_can_create_webhook_for_project(self, projects, users):
        admin = next((u for u in users if "admin" in u["groups"]))
        project = [
            p for p in projects if p["owner"]["id"] != admin["id"] and p["organization"] is None
        ][0]

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = project["id"]

        response = post_method(admin["username"], "webhooks", webhook)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    def test_admin_can_create_webhook_for_org(self, users, organizations, is_org_member):
        admins = [u for u in users if "admin" in u["groups"]]
        username, org_id = next(
            (
                (user["username"], org["id"])
                for user in admins
                for org in organizations
                if not is_org_member(user["id"], org["id"])
            )
        )

        webhook = deepcopy(self.org_webhook)

        response = post_method(username, "webhooks", webhook, org_id=org_id)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    def test_admin_can_create_webhook_for_project_in_org(
        self, users, projects_by_org, organizations, is_org_member
    ):
        admins = [u for u in users if "admin" in u["groups"]]
        not_org_members = [
            (u, o) for u, o in product(admins, organizations) if not is_org_member(u["id"], o["id"])
        ]

        username, org_id = next(
            (
                (u["username"], o["id"])
                for u, o in not_org_members
                for p in projects_by_org.get(o["id"], [])
                if p["owner"]["id"] != u["id"]
            )
        )

        webhook = deepcopy(self.org_webhook)

        response = post_method(username, "webhooks", webhook, org_id=org_id)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    @pytest.mark.parametrize("privilege", ["user"])
    def test_sandbox_project_owner_can_create_webhook_for_project(self, privilege, projects, users):
        users = [user for user in users if privilege in user["groups"]]
        username, project_id = next(
            (
                (user["username"], project["id"])
                for user in users
                for project in projects
                if project["owner"]["id"] == user["id"] and project["organization"] is None
            )
        )

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = project_id

        response = post_method(username, "webhooks", webhook)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    @pytest.mark.parametrize("privilege", ["worker", "user"])
    def test_sandbox_project_assignee_cannot_create_webhook_for_project(
        self, privilege, projects, users
    ):
        users = [u for u in users if privilege in u["groups"]]
        projects = [p for p in projects if p["assignee"] is not None]
        username, project_id = next(
            (
                (user["username"], project["id"])
                for user in users
                for project in projects
                if project["assignee"]["id"] == user["id"] and project["organization"] is None
            )
        )

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = project_id

        response = post_method(username, "webhooks", webhook)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_member_can_create_webhook_for_org(self, role, find_users, organizations):
        username, org_id = next(
            (
                (u["username"], o["id"])
                for o in organizations
                for u in find_users(org=o["id"], role=role, exclude_privilege="admin")
            )
        )

        webhook = deepcopy(self.org_webhook)

        response = post_method(username, "webhooks", webhook, org_id=org_id)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_member_can_create_webhook_for_project(
        self, role, find_users, organizations, projects_by_org, is_project_staff
    ):
        username, oid, pid = next(
            (
                (u["username"], o["id"], p["id"])
                for o in organizations
                for u in find_users(org=o["id"], role=role, exclude_privilege="admin")
                for p in projects_by_org.get(o["id"], [])
                if not is_project_staff(u["id"], p["id"])
            )
        )

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = pid

        response = post_method(username, "webhooks", webhook, org_id=oid)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    @pytest.mark.parametrize("role", ["supervisor", "worker"])
    def test_member_cannot_create_webhook_for_org(self, role, find_users, organizations):
        username, org_id = next(
            (
                (u["username"], o["id"])
                for o in organizations
                for u in find_users(org=o["id"], role=role, exclude_privilege="admin")
            )
        )

        webhook = deepcopy(self.org_webhook)

        response = post_method(username, "webhooks", webhook, org_id=org_id)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["supervisor", "worker"])
    def test_member_cannot_create_webhook_for_project(
        self, role, find_users, organizations, projects_by_org, is_project_staff
    ):
        username, oid, pid = next(
            (
                (u["username"], o["id"], p["id"])
                for o in organizations
                for u in find_users(org=o["id"], role=role, exclude_privilege="admin")
                for p in projects_by_org.get(o["id"], [])
                if not is_project_staff(u["id"], p["id"])
            )
        )

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = pid

        response = post_method(username, "webhooks", webhook, org_id=oid)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["supervisor"])
    def test_member_project_owner_can_create_webhook_for_project(
        self, role, find_users, organizations, projects_by_org, is_project_staff
    ):
        username, oid, pid = next(
            (
                (u["username"], o["id"], p["id"])
                for o in organizations
                for u in find_users(org=o["id"], role=role, exclude_privilege="admin")
                for p in projects_by_org.get(o["id"], [])
                if p["owner"]["id"] == u["id"]
            )
        )

        webhook = deepcopy(self.proj_webhook)
        webhook["project_id"] = pid

        response = post_method(username, "webhooks", webhook, org_id=oid)

        assert response.status_code == HTTPStatus.CREATED
        assert "secret" not in response.json()

    def test_non_member_cannot_create_webhook_for_org(
        self, find_users, organizations, is_org_member
    ):
        username, org_id = next(
            (
                (u["username"], o["id"])
                for o in organizations
                for u in find_users(exclude_privilege="admin")
                if not is_org_member(u["id"], o["id"])
            )
        )

        webhook = deepcopy(self.org_webhook)

        response = post_method(username, "webhooks", webhook, org_id=org_id)

        assert response.status_code == HTTPStatus.FORBIDDEN

    def test_can_create_without_unnecessary_fields(self):
        post_data = deepcopy(self.proj_webhook)
        post_data.pop("enable_ssl")
        post_data.pop("content_type")
        post_data.pop("description")
        post_data.pop("is_active")
        post_data.pop("secret")

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.CREATED

    def test_can_create_with_mismatching_project_org_fields(self, projects_by_org):
        # In this case we could either fail or ignore invalid query param
        # Currently, the invalid org id will be ignored and the value
        # will be taken from the project.
        post_data = deepcopy(self.proj_webhook)
        org_id = next(iter(projects_by_org))
        project = projects_by_org[org_id][0]
        post_data["project_id"] = project["id"]
        org_id = next(k for k in projects_by_org if k != org_id)

        response = post_method("admin1", "webhooks", post_data, org_id=org_id)

        assert response.status_code == HTTPStatus.CREATED
        assert response.json()["project_id"] == post_data["project_id"]
        assert response.json()["organization"] == project["organization"]

    def test_cannot_create_without_target_url(self):
        post_data = deepcopy(self.proj_webhook)
        post_data.pop("target_url")

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    def test_cannot_create_without_events_list(self):
        post_data = deepcopy(self.proj_webhook)
        post_data.pop("events")

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    def test_cannot_create_without_type(self):
        post_data = deepcopy(self.proj_webhook)
        post_data.pop("type")

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    def test_cannot_create_without_project_id(self):
        post_data = deepcopy(self.proj_webhook)
        post_data.pop("project_id")

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.INTERNAL_SERVER_ERROR

    def test_cannot_create_organization_webhook_when_project_id_is_not_null(self, organizations):
        post_data = deepcopy(self.proj_webhook)
        post_data["type"] = "organization"
        org_id = organizations.raw[0]["id"]

        response = post_method("admin2", "webhooks", post_data, org_id=org_id)

        assert response.status_code == HTTPStatus.INTERNAL_SERVER_ERROR

    @pytest.mark.skip("Not implemented yet")
    def test_cannot_create_non_unique_webhook(self):
        response = post_method("admin2", "webhooks", self.proj_webhook)

        response = post_method("admin2", "webhooks", self.proj_webhook)

        assert response.status_code == HTTPStatus.INTERNAL_SERVER_ERROR

    def test_cannot_create_for_non_existent_organization(self, organizations):
        post_data = deepcopy(self.proj_webhook)
        post_data["type"] = "organization"
        org_id = max(a["id"] for a in organizations.raw) + 1

        response = post_method("admin2", "webhooks", post_data, org_id=org_id)

        assert response.status_code == HTTPStatus.NOT_FOUND

    def test_cannot_create_for_non_existent_project(self, projects):
        post_data = deepcopy(self.proj_webhook)
        post_data["project_id"] = max(a["id"] for a in projects.raw) + 1

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    def test_cannot_create_with_non_supported_type(self):
        post_data = deepcopy(self.proj_webhook)
        post_data["type"] = "some_type"

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    def test_cannot_create_with_non_supported_content_type(self):
        post_data = deepcopy(self.proj_webhook)
        post_data["content_type"] = ["application/x-www-form-urlencoded"]

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    @pytest.mark.parametrize(
        "event", ["some:event", "create:project", "update:organization", "create:invitation"]
    )
    def test_cannot_create_project_webhook_with_non_supported_event_type(self, event):
        post_data = deepcopy(self.proj_webhook)
        post_data["events"] = [event]

        response = post_method("admin2", "webhooks", post_data)

        assert response.status_code == HTTPStatus.BAD_REQUEST

    @pytest.mark.parametrize("event", ["some:event", "create:organization"])
    def test_cannot_create_organization_webhook_with_non_supported_event_type(
        self, event, organizations
    ):
        post_data = deepcopy(self.proj_webhook)
        post_data["type"] = "organization"
        post_data["events"] = [event]
        org_id = next(iter(organizations))["id"]

        response = post_method("admin2", "webhooks", post_data, org_id=org_id)

        assert response.status_code == HTTPStatus.BAD_REQUEST


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetWebhooks:
    def test_admin_can_get_webhook(self, webhooks, users, projects):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, wid = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in proj_webhooks
                if "admin" in user["groups"]
                and webhook["owner"]["id"] != user["id"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
            )
        )

        response = get_method(username, f"webhooks/{wid}")

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhooks[wid], response.json(), ignore_order=True) == {}

    @pytest.mark.parametrize("privilege", ["user"])
    def test_project_owner_can_get_webhook(self, privilege, webhooks, projects, users):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, wid = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in proj_webhooks
                if privilege in user["groups"]
                and projects[webhook["project_id"]]["owner"]["id"] == user["id"]
            )
        )

        response = get_method(username, f"webhooks/{wid}")

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhooks[wid], response.json(), ignore_order=True) == {}

    @pytest.mark.parametrize("privilege", ["user"])
    def test_webhook_owner_can_get_webhook(self, privilege, webhooks, projects, users):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, wid = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in proj_webhooks
                if privilege in user["groups"] and webhook["owner"]["id"] == user["id"]
            )
        )

        response = get_method(username, f"webhooks/{wid}")

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhooks[wid], response.json(), ignore_order=True) == {}

    @pytest.mark.parametrize("privilege", ["user"])
    def test_not_project_staff_cannot_get_webhook(self, privilege, webhooks, projects, users):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, wid = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in proj_webhooks
                if privilege in user["groups"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
                and webhook["owner"]["id"] != user["id"]
            )
        )

        response = get_method(username, f"webhooks/{wid}")

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["owner", "maintainer"])
    def test_org_staff_can_see_org_webhook(self, role, webhooks, find_users):
        webhook = next((w for w in webhooks if w["type"] == "organization"))
        username = next((u["username"] for u in find_users(role=role, org=webhook["organization"])))

        response = get_method(username, f"webhooks/{webhook['id']}", org_id=webhook["organization"])

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhook, response.json(), ignore_order=True) == {}

    @pytest.mark.parametrize("role", ["owner", "maintainer"])
    def test_org_staff_can_see_project_webhook_in_org(self, role, webhooks, find_users, projects):
        proj_webhooks = [
            w for w in webhooks if w["organization"] is not None and w["type"] == "project"
        ]
        username, webhook = next(
            (
                (user["username"], webhook)
                for webhook in proj_webhooks
                for user in find_users(role=role, org=webhook["organization"])
                if projects[webhook["project_id"]]["owner"]["id"] != user["id"]
                and webhook["owner"]["id"] != user["id"]
            )
        )

        response = get_method(username, f"webhooks/{webhook['id']}", org_id=webhook["organization"])

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhook, response.json(), ignore_order=True) == {}

    @pytest.mark.parametrize("role", ["worker", "supervisor"])
    def test_member_cannot_get_org_webhook(self, role, webhooks, find_users):
        webhook = next((w for w in webhooks if w["type"] == "organization"))
        username = next((u["username"] for u in find_users(role=role, org=webhook["organization"])))

        response = get_method(username, f"webhooks/{webhook['id']}", org_id=webhook["organization"])

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["worker", "supervisor"])
    def test_member_cannot_get_project_webhook_in_org(self, role, webhooks, find_users, projects):
        proj_webhooks = [
            w for w in webhooks if w["organization"] is not None and w["type"] == "project"
        ]
        username, webhook = next(
            (
                (user["username"], webhook)
                for webhook in proj_webhooks
                for user in find_users(role=role, org=webhook["organization"])
                if projects[webhook["project_id"]]["owner"]["id"] != user["id"]
                and webhook["owner"]["id"] != user["id"]
            )
        )

        response = get_method(username, f"webhooks/{webhook['id']}", org_id=webhook["organization"])

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["supervisor"])
    def test_member_can_get_project_webhook_in_org(self, role, webhooks, find_users, projects):
        proj_webhooks = [
            w for w in webhooks if w["organization"] is not None and w["type"] == "project"
        ]
        username, webhook = next(
            (
                (user["username"], webhook)
                for webhook in proj_webhooks
                for user in find_users(role=role, org=webhook["organization"])
                if projects[webhook["project_id"]]["owner"]["id"] == user["id"]
                or webhook["owner"]["id"] == user["id"]
            )
        )

        response = get_method(username, f"webhooks/{webhook['id']}", org_id=webhook["organization"])

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert DeepDiff(webhook, response.json(), ignore_order=True) == {}


class TestWebhooksListFilters(CollectionSimpleFilterTestBase):
    field_lookups = {
        "owner": ["owner", "username"],
    }

    @pytest.fixture(autouse=True)
    def setup(self, restore_db_per_class, admin_user, webhooks):
        self.user = admin_user
        self.samples = webhooks

    def _get_endpoint(self, api_client: ApiClient) -> Endpoint:
        return api_client.webhooks_api.list_endpoint

    @pytest.mark.parametrize(
        "field",
        ("target_url", "owner", "type", "project_id"),
    )
    def test_can_use_simple_filter_for_object_list(self, field):
        return super()._test_can_use_simple_filter_for_object_list(field)


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetListWebhooks:
    def test_can_get_webhooks_list(self, webhooks):
        response = get_method("admin2", "webhooks")

        assert response.status_code == HTTPStatus.OK
        assert all(["secret" not in webhook for webhook in response.json()["results"]])
        assert DeepDiff(webhooks.raw, response.json()["results"], ignore_order=True) == {}

    def test_admin_can_get_webhooks_for_project(self, webhooks):
        pid = next(
            (
                webhook["project_id"]
                for webhook in webhooks
                if webhook["type"] == "project" and webhook["organization"] is None
            )
        )

        expected_response = [
            webhook
            for webhook in webhooks
            if webhook["type"] == "project" and webhook["project_id"] == pid
        ]
        filter_val = '{"and":[{"==":[{"var":"project_id"},%s]}]}' % pid

        response = get_method("admin2", "webhooks", filter=filter_val)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    def test_admin_can_get_webhooks_for_organization(self, webhooks):
        org_id = next(
            (webhook["organization"] for webhook in webhooks if webhook["organization"] is not None)
        )

        expected_response = [webhook for webhook in webhooks if webhook["organization"] == org_id]

        response = get_method("admin2", "webhooks", org_id=org_id)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    def test_admin_can_get_webhooks_for_project_in_org(self, webhooks):
        pid, oid = next(
            (
                (webhook["project_id"], webhook["organization"])
                for webhook in webhooks
                if webhook["type"] == "project" and webhook["organization"] is not None
            )
        )

        expected_response = [
            webhook
            for webhook in webhooks
            if webhook["project_id"] == pid and webhook["organization"] == oid
        ]
        filter_val = '{"and":[{"==":[{"var":"project_id"},%s]}]}' % pid

        response = get_method("admin2", "webhooks", org_id=oid, filter=filter_val)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    @pytest.mark.parametrize("privilege", ["user"])
    def test_user_cannot_get_webhook_list_for_project(
        self, privilege, find_users, webhooks, projects
    ):
        username, pid = next(
            (
                (user["username"], webhook["project_id"])
                for user in find_users(privilege=privilege)
                for webhook in webhooks
                if webhook["type"] == "project"
                and webhook["organization"] is None
                and webhook["owner"]["id"] != user["id"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
            )
        )

        filter_val = '{"and":[{"==":[{"var":"project_id"},%s]}]}' % pid

        response = get_method(username, "webhooks", filter=filter_val)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff([], response.json()["results"], ignore_order=True) == {}

    @pytest.mark.parametrize("privilege", ["user"])
    def test_user_can_get_webhook_list_for_project(self, privilege, find_users, webhooks, projects):
        username, pid = next(
            (
                (user["username"], webhook["project_id"])
                for user in find_users(privilege=privilege)
                for webhook in webhooks
                if webhook["type"] == "project"
                and webhook["organization"] is None
                and projects[webhook["project_id"]]["owner"]["id"] == user["id"]
            )
        )

        expected_response = [
            w for w in webhooks if w["type"] == "project" and w["project_id"] == pid
        ]
        filter_val = '{"and":[{"==":[{"var":"project_id"},%s]}]}' % pid

        response = get_method(username, "webhooks", filter=filter_val)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    def test_non_member_cannot_see_webhook_list_for_org(self, webhooks, users, is_org_member):
        username, org_id = next(
            (
                (user["username"], webhook["organization"])
                for webhook in webhooks
                for user in users
                if webhook["organization"] is not None
                and not is_org_member(user["id"], webhook["organization"])
                and "admin" not in user["groups"]
            )
        )

        response = get_method(username, "webhooks", org_id=org_id)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_org_staff_can_see_all_org_webhooks(self, role, webhooks, organizations, find_users):
        username, org_id = next(
            (
                (user["username"], org["id"])
                for webhook in webhooks
                for org in organizations
                for user in find_users(role=role, org=org["id"])
                if webhook["organization"] == org["id"]
            )
        )

        expected_response = [webhook for webhook in webhooks if webhook["organization"] == org_id]

        response = get_method(username, "webhooks", org_id=org_id)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    @pytest.mark.parametrize("role", ["worker", "supervisor"])
    def test_member_cannot_see_all_org_webhook(
        self, role, webhooks, organizations, find_users, projects
    ):
        username, org_id = next(
            (
                (user["username"], org["id"])
                for webhook in webhooks
                for org in organizations
                for user in find_users(role=role, org=org["id"])
                if webhook["organization"] == org["id"]
            )
        )

        expected_response = [
            webhook
            for webhook in webhooks
            if webhook["organization"] == org_id
            and (
                webhook["owner"]["username"] == username
                or (
                    webhook["project_id"]
                    and projects[webhook["project_id"]]["owner"]["username"] == username
                )
            )
        ]

        response = get_method(username, "webhooks", org_id=org_id)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    @pytest.mark.parametrize("role", ["supervisor"])
    def test_member_can_see_list_of_project_webhooks_in_org(
        self, role, webhooks, organizations, find_users, projects
    ):
        username, org_id = next(
            (
                (user["username"], org["id"])
                for webhook in webhooks
                for org in organizations
                for user in find_users(role=role, org=org["id"])
                if webhook["organization"] == org["id"]
                and webhook["type"] == "project"
                and projects[webhook["project_id"]]["owner"]["id"] == user["id"]
            )
        )

        expected_response = [
            webhook
            for webhook in webhooks
            if webhook["organization"] == org_id
            and webhook["type"] == "project"
            and projects[webhook["project_id"]]["owner"]["username"] == username
        ]

        response = get_method(username, "webhooks", org_id=org_id)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(expected_response, response.json()["results"], ignore_order=True) == {}

    @pytest.mark.parametrize("field_value, query_value", [(1, 1), (None, "")])
    def test_can_filter_by_org_id(self, field_value, query_value, webhooks):
        webhooks = filter(lambda w: w["organization"] == field_value, webhooks)
        response = get_method("admin2", f"webhooks", org_id=query_value)

        assert response.status_code == HTTPStatus.OK
        assert DeepDiff(list(webhooks), response.json()["results"], ignore_order=True) == {}


@pytest.mark.usefixtures("restore_db_per_function")
class TestPatchWebhooks:
    WID = 2

    def test_sandbox_admin_can_update_any_webhook(self, webhooks, find_users):
        username, webhook = next(
            (
                (user["username"], deepcopy(webhook))
                for user in find_users(privilege="admin")
                for webhook in webhooks
                if webhook["owner"]["id"] != user["id"] and webhook["organization"] is None
            )
        )
        patch_data = {
            "target_url": "http://newexample.com",
            "secret": "newsecret",
            "events": ["create:task"],
            "is_active": not webhook["is_active"],
            "enable_ssl": not webhook["enable_ssl"],
        }
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data)

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert (
            DeepDiff(
                webhook,
                response.json(),
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['secret']"],
            )
            == {}
        )

    def test_cannot_update_with_nonexistent_contenttype(self):
        patch_data = {"content_type": "application/x-www-form-urlencoded"}

        response = patch_method("admin2", f"webhooks/{self.WID}", patch_data)
        assert response.status_code == HTTPStatus.BAD_REQUEST

    @pytest.mark.parametrize("privilege", ["user"])
    def test_sandbox_user_can_update_webhook(self, privilege, find_users, webhooks):
        username, webhook = next(
            (
                (user["username"], deepcopy(webhook))
                for user in find_users(privilege=privilege)
                for webhook in webhooks
                if webhook["owner"]["id"] == user["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data)

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert (
            DeepDiff(
                webhook,
                response.json(),
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['secret']"],
            )
            == {}
        )

    @pytest.mark.parametrize("privilege", ["worker", "user"])
    def test_sandbox_user_cannot_update_webhook(self, privilege, find_users, webhooks):
        username, webhook = next(
            (
                (user["username"], deepcopy(webhook))
                for user in find_users(privilege=privilege)
                for webhook in webhooks
                if webhook["owner"]["id"] != user["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data)

        assert response.status_code == HTTPStatus.FORBIDDEN

    def test_admin_can_update_org_webhook(self, find_users, organizations, webhooks, is_org_member):
        org_webhooks = [w for w in webhooks if w["type"] == "organization"]
        admin, oid, webhook = next(
            (
                (u["username"], o["id"], deepcopy(w))
                for u in find_users(privilege="admin")
                for o in organizations
                for w in org_webhooks
                if w["organization"] == o["id"] and not is_org_member(u["id"], o["id"])
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(admin, f"webhooks/{webhook['id']}", patch_data, org_id=oid)

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert (
            DeepDiff(
                webhook,
                response.json(),
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['secret']"],
            )
            == {}
        )

    @pytest.mark.parametrize("role", ["maintainer", "owner"])
    def test_member_can_update_org_webhook(self, role, find_users, organizations, webhooks):
        org_webhooks = [w for w in webhooks if w["type"] == "organization"]
        username, oid, webhook = next(
            (
                (u["username"], o["id"], deepcopy(w))
                for o in organizations
                for u in find_users(role=role, org=o["id"])
                for w in org_webhooks
                if w["organization"] == o["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data, org_id=oid)

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert (
            DeepDiff(
                webhook,
                response.json(),
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['secret']"],
            )
            == {}
        )

    @pytest.mark.parametrize("role", ["worker", "supervisor"])
    def test_member_cannot_update_org_webhook(self, role, find_users, organizations, webhooks):
        org_webhooks = [w for w in webhooks if w["type"] == "organization"]
        username, oid, webhook = next(
            (
                (u["username"], o["id"], deepcopy(w))
                for o in organizations
                for u in find_users(role=role, org=o["id"])
                for w in org_webhooks
                if w["organization"] == o["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data, org_id=oid)

        assert response.status_code == HTTPStatus.FORBIDDEN

    @pytest.mark.parametrize(
        "role, allow",
        [("maintainer", True), ("owner", True), ("supervisor", False), ("worker", False)],
    )
    def test_member_can_update_any_project_webhook_in_org(
        self, role, allow, find_users, organizations, projects_by_org, webhooks, is_project_staff
    ):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, org_id, webhook = next(
            (
                (u["username"], o["id"], deepcopy(w))
                for o in organizations
                for u in find_users(role=role, org=o["id"])
                for w in proj_webhooks
                for p in projects_by_org.get(o["id"], [])
                if w["project_id"] == p["id"]
                and w["organization"] == o["id"]
                and not is_project_staff(u["id"], p["id"])
                and w["owner"]["id"] != u["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data, org_id=org_id)

        if not allow:
            assert response.status_code == HTTPStatus.FORBIDDEN
        else:
            assert response.status_code == HTTPStatus.OK
            assert "secret" not in response.json()
            assert (
                DeepDiff(
                    webhook,
                    response.json(),
                    ignore_order=True,
                    exclude_paths=["root['updated_date']", "root['secret']"],
                )
                == {}
            )

    @pytest.mark.parametrize("role", ["supervisor"])
    def test_member_can_update_project_webhook_in_org(
        self, role, find_users, organizations, projects_by_org, webhooks
    ):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, org_id, webhook = next(
            (
                (u["username"], o["id"], deepcopy(w))
                for o in organizations
                for u in find_users(role=role, org=o["id"])
                for w in proj_webhooks
                for p in projects_by_org.get(o["id"], [])
                if w["project_id"] == p["id"]
                and w["organization"] == o["id"]
                and u["id"] == p["owner"]["id"]
            )
        )

        patch_data = {"target_url": "http://newexample.com"}
        webhook.update(patch_data)

        response = patch_method(username, f"webhooks/{webhook['id']}", patch_data, org_id=org_id)

        assert response.status_code == HTTPStatus.OK
        assert "secret" not in response.json()
        assert (
            DeepDiff(
                webhook,
                response.json(),
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['secret']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestDeleteWebhooks:
    @pytest.mark.parametrize("privilege, allow", [("user", False), ("admin", True)])
    def test_user_can_delete_project_webhook(
        self, privilege, allow, find_users, webhooks, projects
    ):
        users = find_users(privilege=privilege)
        username, webhook_id = next(
            (
                (user["username"], webhook["id"])
                for webhook in webhooks
                for user in users
                if webhook["type"] == "project"
                and webhook["organization"] is None
                and webhook["owner"]["id"] != user["id"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
            )
        )

        if not allow:
            response = delete_method(username, f"webhooks/{webhook_id}")
            assert response.status_code == HTTPStatus.FORBIDDEN
        else:
            response = delete_method(username, f"webhooks/{webhook_id}")
            assert response.status_code == HTTPStatus.NO_CONTENT

            response = get_method(username, f"webhooks/{webhook_id}")
            assert response.status_code == HTTPStatus.NOT_FOUND

    def test_admin_can_delete_project_webhook_in_org(
        self, find_users, webhooks, projects, is_org_member
    ):
        admins = find_users(privilege="admin")
        username, webhook_id = next(
            (
                (user["username"], webhook["id"])
                for user in admins
                for webhook in webhooks
                if webhook["type"] == "project"
                and webhook["organization"] is not None
                and webhook["owner"]["id"] != user["id"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
                and not is_org_member(user["id"], webhook["organization"])
            )
        )

        response = delete_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NO_CONTENT

        response = get_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NOT_FOUND

    def test_admin_can_delete_org_webhook(self, find_users, webhooks, is_org_member):
        admins = find_users(privilege="admin")
        username, webhook_id = next(
            (
                (user["username"], webhook["id"])
                for user in admins
                for webhook in webhooks
                if webhook["type"] == "organization"
                and webhook["organization"] is not None
                and webhook["owner"]["id"] != user["id"]
                and not is_org_member(user["id"], webhook["organization"])
            )
        )

        response = delete_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NO_CONTENT

        response = get_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NOT_FOUND

    @pytest.mark.parametrize("privilege", ["user"])
    def test_project_owner_can_delete_project_webhook(
        self, privilege, find_users, webhooks, projects
    ):
        users = find_users(privilege=privilege)
        username, webhook_id = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in webhooks
                if webhook["type"] == "project"
                and webhook["organization"] is None
                and projects[webhook["project_id"]]["owner"]["id"] == user["id"]
            )
        )

        response = delete_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NO_CONTENT

        response = get_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NOT_FOUND

    @pytest.mark.parametrize("privilege", ["user"])
    def test_webhook_owner_can_delete_project_webhook(
        self, privilege, find_users, webhooks, projects
    ):
        users = find_users(privilege=privilege)
        username, webhook_id = next(
            (
                (user["username"], webhook["id"])
                for user in users
                for webhook in webhooks
                if webhook["type"] == "project"
                and webhook["organization"] is None
                and webhook["owner"]["id"] == user["id"]
            )
        )

        response = delete_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NO_CONTENT

        response = get_method(username, f"webhooks/{webhook_id}")
        assert response.status_code == HTTPStatus.NOT_FOUND

    @pytest.mark.parametrize(
        "role, allow",
        [("owner", True), ("maintainer", True), ("worker", False), ("supervisor", False)],
    )
    def test_member_can_delete_org_webhook(self, role, allow, find_users, organizations, webhooks):
        org_webhooks = [w for w in webhooks if w["type"] == "organization"]
        username, org_id, webhook_id = next(
            (
                (user["username"], org["id"], webhook["id"])
                for org in organizations
                for webhook in org_webhooks
                for user in find_users(role=role, org=org["id"])
                if webhook["organization"] == org["id"]
            )
        )

        if not allow:
            response = delete_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.FORBIDDEN
        else:
            response = delete_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.NO_CONTENT

            response = get_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.NOT_FOUND

    @pytest.mark.parametrize(
        "role, allow",
        [("owner", True), ("maintainer", True), ("worker", False), ("supervisor", False)],
    )
    def test_member_can_delete_project_webhook_in_org(
        self, role, allow, find_users, organizations, projects, webhooks
    ):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, org_id, webhook_id = next(
            (
                (user["username"], webhook["organization"], webhook["id"])
                for org in organizations
                for user in find_users(role=role, org=org["id"])
                for webhook in proj_webhooks
                if webhook["organization"]
                and webhook["organization"] == org["id"]
                and projects[webhook["project_id"]]["owner"]["id"] != user["id"]
                and webhook["owner"]["id"] != user["id"]
            )
        )

        if not allow:
            response = delete_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.FORBIDDEN
        else:
            response = delete_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.NO_CONTENT

            response = get_method(username, f"webhooks/{webhook_id}", org_id=org_id)
            assert response.status_code == HTTPStatus.NOT_FOUND

    @pytest.mark.parametrize("role", ["supervisor"])
    def test_member_webhook_staff_can_delete_project_webhook_in_org(
        self, role, find_users, organizations, projects, webhooks
    ):
        proj_webhooks = [w for w in webhooks if w["type"] == "project"]
        username, org_id, webhook_id = next(
            (
                (user["username"], webhook["organization"], webhook["id"])
                for org in organizations
                for user in find_users(role=role, org=org["id"])
                for webhook in proj_webhooks
                if webhook["organization"]
                and webhook["organization"] == org["id"]
                and (
                    projects[webhook["project_id"]]["owner"]["id"] == user["id"]
                    or webhook["owner"]["id"] == user["id"]
                )
            )
        )

        response = delete_method(username, f"webhooks/{webhook_id}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        response = get_method(username, f"webhooks/{webhook_id}", org_id=org_id)
        assert response.status_code == HTTPStatus.NOT_FOUND


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\test_webhooks_sender.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from http import HTTPStatus
from time import sleep, time

import pytest
from deepdiff import DeepDiff

from shared.fixtures.init import CVAT_ROOT_DIR, _run
from shared.utils.config import delete_method, get_method, patch_method, post_method

# Testing webhook functionality:
#  - webhook_receiver container receive post request and return responses with the same body
#  - CVAT save response body for each delivery
#
# So idea of this testing system is quite simple:
#  1) trigger some webhook
#  2) check that webhook is sent by checking value of `response` field for the last delivery of this webhook

# https://docs.pytest.org/en/7.1.x/example/markers.html#marking-whole-classes-or-modules
pytestmark = [pytest.mark.with_external_services]


def target_url():
    env_data = {}
    with open(CVAT_ROOT_DIR / "tests/python/webhook_receiver/.env", "r") as f:
        for line in f:
            name, value = tuple(line.strip().split("="))
            env_data[name] = value

    container_id = _run(
        "docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' test_webhook_receiver_1"
    )[0].strip()[1:-1]

    return f'http://{container_id}:{env_data["SERVER_PORT"]}/{env_data["PAYLOAD_ENDPOINT"]}'


def webhook_spec(events, project_id=None, webhook_type="organization"):
    # Django URL field doesn't allow to use http://webhooks:2020/payload (using alias)
    # So we forced to use ip address of webhook receiver container
    return {
        "target_url": target_url(),
        "content_type": "application/json",
        "enable_ssl": False,
        "events": events,
        "is_active": True,
        "project_id": project_id,
        "type": webhook_type,
    }


def create_webhook(events, webhook_type, project_id=None, org_id=""):
    assert (webhook_type == "project" and project_id is not None) or (
        webhook_type == "organization" and org_id
    )

    response = post_method(
        "admin1", "webhooks", webhook_spec(events, project_id, webhook_type), org_id=org_id
    )
    assert response.status_code == HTTPStatus.CREATED

    return response.json()


def get_deliveries(webhook_id, expected_count=1, *, timeout: int = 60):
    start_time = time()

    delivery_response = {}
    while True:
        response = get_method("admin1", f"webhooks/{webhook_id}/deliveries")
        assert response.status_code == HTTPStatus.OK

        deliveries = response.json()
        if deliveries["count"] == expected_count:
            delivery_response = json.loads(deliveries["results"][0]["response"])
            break

        if time() - start_time > timeout:
            raise TimeoutError("Failed to get deliveries within the specified time interval")

        sleep(1)

    return deliveries, delivery_response


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookProjectEvents:
    def test_webhook_update_project_name(self):
        response = post_method("admin1", "projects", {"name": "project"})
        assert response.status_code == HTTPStatus.CREATED
        project = response.json()

        events = ["update:project"]
        webhook = create_webhook(events, "project", project_id=project["id"])

        patch_data = {"name": "new_project_name"}
        response = patch_method("admin1", f"projects/{project['id']}", patch_data)
        assert response.status_code == HTTPStatus.OK

        response = get_method("admin1", f"webhooks/{webhook['id']}/deliveries")
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook["id"])

        assert deliveries["count"] == 1

        assert payload["event"] == events[0]
        assert payload["sender"]["username"] == "admin1"
        assert payload["before_update"]["name"] == project["name"]

        project.update(patch_data)
        assert (
            DeepDiff(
                payload["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )

    def test_webhook_create_and_delete_project_in_organization(self, organizations):
        org_id = list(organizations)[0]["id"]
        events = ["create:project", "delete:project"]

        webhook = create_webhook(events, "organization", org_id=org_id)

        response = post_method("admin1", "projects", {"name": "project_name"}, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED
        project = response.json()

        deliveries, create_payload = get_deliveries(webhook["id"])

        assert deliveries["count"] == 1

        response = delete_method("admin1", f"projects/{project['id']}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        deliveries, delete_payload = get_deliveries(webhook["id"], 2)

        assert deliveries["count"] == 2

        assert create_payload["event"] == "create:project"
        assert delete_payload["event"] == "delete:project"
        assert (
            DeepDiff(
                create_payload["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                delete_payload["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookIntersection:
    # Test case description:
    #     few webhooks are triggered by the same event
    # In this case we need to check that CVAT will sent
    # the right number of payloads to the target url

    def test_project_and_organization_webhooks_intersection(self, organizations):
        org_id = list(organizations)[0]["id"]
        post_data = {"name": "project_name"}

        response = post_method("admin1", "projects", post_data, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED

        events = ["update:project"]
        project_id = response.json()["id"]
        webhook_id_1 = create_webhook(events, "organization", org_id=org_id)["id"]
        webhook_id_2 = create_webhook(events, "project", project_id=project_id, org_id=org_id)["id"]

        patch_data = {"name": "new_project_name"}
        response = patch_method("admin1", f"projects/{project_id}", patch_data)
        assert response.status_code == HTTPStatus.OK

        deliveries_1, payload_1 = get_deliveries(webhook_id_1)
        deliveries_2, payload_2 = get_deliveries(webhook_id_2)

        assert deliveries_1["count"] == deliveries_2["count"] == 1

        assert payload_1["project"]["name"] == payload_2["project"]["name"] == patch_data["name"]

        assert (
            payload_1["before_update"]["name"]
            == payload_2["before_update"]["name"]
            == post_data["name"]
        )

        assert payload_1["webhook_id"] == webhook_id_1
        assert payload_2["webhook_id"] == webhook_id_2

        assert deliveries_1["results"][0]["webhook_id"] == webhook_id_1
        assert deliveries_2["results"][0]["webhook_id"] == webhook_id_2

    def test_two_project_webhooks_intersection(self):
        post_data = {"name": "project_name"}
        response = post_method("admin1", "projects", post_data)
        assert response.status_code == HTTPStatus.CREATED

        project_id = response.json()["id"]
        events_1 = ["create:task", "update:issue"]
        events_2 = ["create:task", "create:issue"]
        webhook_id_1 = create_webhook(events_1, "project", project_id=project_id)["id"]
        webhook_id_2 = create_webhook(events_2, "project", project_id=project_id)["id"]

        post_data = {"name": "project_name", "project_id": project_id}
        response = post_method("admin1", "tasks", post_data)
        assert response.status_code == HTTPStatus.CREATED

        deliveries_1, payload_1 = get_deliveries(webhook_id_1)
        deliveries_2, payload_2 = get_deliveries(webhook_id_2)

        assert deliveries_1["count"] == deliveries_2["count"] == 1

        assert payload_1["event"] == payload_2["event"] == "create:task"
        assert payload_1["task"]["name"] == payload_2["task"]["name"] == post_data["name"]

        assert payload_1["webhook_id"] == webhook_id_1
        assert payload_2["webhook_id"] == webhook_id_2

    def test_two_organization_webhook_intersection(self, organizations):
        org_id = list(organizations)[0]["id"]

        events_1 = ["create:project", "update:membership"]
        events_2 = ["create:project", "update:job"]

        webhook_id_1 = create_webhook(events_1, "organization", org_id=org_id)["id"]
        webhook_id_2 = create_webhook(events_2, "organization", org_id=org_id)["id"]

        post_data = {"name": "project_name"}
        response = post_method("admin1", "projects", post_data, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED

        project = response.json()

        deliveries_1, payload_1 = get_deliveries(webhook_id_1)
        deliveries_2, payload_2 = get_deliveries(webhook_id_2)

        assert deliveries_1["count"] == deliveries_2["count"] == 1

        assert payload_1["webhook_id"] == webhook_id_1
        assert payload_2["webhook_id"] == webhook_id_2

        assert (
            DeepDiff(
                payload_1["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                payload_2["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookTaskEvents:
    def test_webhook_update_task_assignee(self, users, tasks):
        task_id, project_id = next(
            (
                (task["id"], task["project_id"])
                for task in tasks
                if task["project_id"] is not None
                and task["organization"] is None
                and task["assignee"] is not None
            )
        )

        assignee_id = next(
            (user["id"] for user in users if user["id"] != tasks[task_id]["assignee"]["id"])
        )

        webhook_id = create_webhook(["update:task"], "project", project_id=project_id)["id"]

        patch_data = {"assignee_id": assignee_id}
        response = patch_method("admin1", f"tasks/{task_id}", patch_data)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id=webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["assignee"]["id"] == tasks[task_id]["assignee"]["id"]
        assert payload["task"]["assignee"]["id"] == assignee_id

    def test_webhook_create_and_delete_task(self, organizations):
        org_id = list(organizations)[0]["id"]
        events = ["create:task", "delete:task"]

        webhook = create_webhook(events, "organization", org_id=org_id)

        post_data = {"name": "task_name", "labels": [{"name": "label_0"}]}
        response = post_method("admin1", "tasks", post_data, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED

        task = response.json()

        deliveries, create_payload = get_deliveries(webhook["id"])

        assert deliveries["count"] == 1

        response = delete_method("admin1", f"tasks/{task['id']}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        deliveries, delete_payload = get_deliveries(webhook["id"], 2)

        assert deliveries["count"] == 2

        assert create_payload["event"] == "create:task"
        assert delete_payload["event"] == "delete:task"

        # These values cannot be computed if the task has no data
        assert create_payload["task"]["jobs"]["completed"] is None
        assert create_payload["task"]["jobs"]["validation"] is None
        assert task["jobs"]["completed"] == 0
        assert task["jobs"]["validation"] == 0
        assert delete_payload["task"]["jobs"]["completed"] == 0
        assert delete_payload["task"]["jobs"]["validation"] == 0

        assert (
            DeepDiff(
                create_payload["task"],
                task,
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['jobs']", "root['labels']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                delete_payload["task"],
                task,
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['jobs']", "root['labels']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookJobEvents:
    def test_webhook_update_job_assignee(self, jobs, tasks, users):
        job = next(
            (
                job
                for job in jobs
                if job["assignee"] is None and tasks[job["task_id"]]["organization"] is not None
            )
        )

        org_id = tasks[job["task_id"]]["organization"]

        webhook_id = create_webhook(["update:job"], "organization", org_id=org_id)["id"]

        patch_data = {"assignee": list(users)[0]["id"]}
        response = patch_method("admin1", f"jobs/{job['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["assignee"] is None
        assert payload["job"]["assignee"]["id"] == patch_data["assignee"]

    def test_webhook_update_job_stage(self, jobs, tasks):
        stages = {"annotation", "validation", "acceptance"}
        job = next((job for job in jobs if tasks[job["task_id"]]["organization"] is not None))

        org_id = tasks[job["task_id"]]["organization"]

        webhook_id = create_webhook(["update:job"], "organization", org_id=org_id)["id"]

        patch_data = {"stage": (stages - {job["stage"]}).pop()}
        response = patch_method("admin1", f"jobs/{job['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)
        assert deliveries["count"] == 1
        assert payload["before_update"]["stage"] == job["stage"]
        assert payload["job"]["stage"] == patch_data["stage"]

    def test_webhook_update_job_state(self, jobs, tasks):
        states = {"new", "in progress", "rejected", "completed"}
        job = next(
            (
                job
                for job in jobs
                if tasks[job["task_id"]]["organization"] is not None
                and job["state"] == "in progress"
            )
        )

        org_id = tasks[job["task_id"]]["organization"]

        webhook_id = create_webhook(["update:job"], "organization", org_id=org_id)["id"]

        patch_data = {"state": (states - {job["state"]}).pop()}
        response = patch_method("admin1", f"jobs/{job['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)
        assert deliveries["count"] == 1
        assert payload["before_update"]["state"] == job["state"]
        assert payload["job"]["state"] == patch_data["state"]


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookIssueEvents:
    def test_webhook_update_issue_resolved(self, issues, jobs, tasks):
        issue = next(
            (
                issue
                for issue in issues
                if tasks[jobs[issue["job"]]["task_id"]]["organization"] is not None
            )
        )

        org_id = tasks[jobs[issue["job"]]["task_id"]]["organization"]

        webhook_id = create_webhook(["update:issue"], "organization", org_id=org_id)["id"]

        patch_data = {"resolved": not issue["resolved"]}
        response = patch_method("admin1", f"issues/{issue['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["resolved"] == issue["resolved"]
        assert payload["issue"]["resolved"] == patch_data["resolved"]

    def test_webhook_update_issue_position(self, issues, jobs, tasks):
        issue = next(
            (
                issue
                for issue in issues
                if tasks[jobs[issue["job"]]["task_id"]]["organization"] is not None
            )
        )

        org_id = tasks[jobs[issue["job"]]["task_id"]]["organization"]

        webhook_id = create_webhook(["update:issue"], "organization", org_id=org_id)["id"]

        patch_data = {"position": [0, 1, 2, 3]}
        response = patch_method("admin1", f"issues/{issue['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["position"] == issue["position"]
        assert payload["issue"]["position"] == patch_data["position"]

    def test_webhook_create_and_delete_issue(self, organizations, jobs, tasks):
        org_id = list(organizations)[0]["id"]
        job_id = next(
            (job["id"] for job in jobs if tasks[job["task_id"]]["organization"] == org_id)
        )
        events = ["create:issue", "delete:issue"]

        webhook = create_webhook(events, "organization", org_id=org_id)

        post_data = {"frame": 0, "position": [0, 1, 2, 3], "job": job_id, "message": "issue_msg"}
        response = post_method("admin1", "issues", post_data, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED

        issue = response.json()

        deliveries, create_payload = get_deliveries(webhook["id"])

        assert deliveries["count"] == 1

        response = delete_method("admin1", f"issues/{issue['id']}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        deliveries, delete_payload = get_deliveries(webhook["id"], 2)

        assert deliveries["count"] == 2

        assert create_payload["event"] == "create:issue"
        assert delete_payload["event"] == "delete:issue"
        assert (
            DeepDiff(
                create_payload["issue"],
                issue,
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['comments']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                delete_payload["issue"],
                issue,
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['comments']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookMembershipEvents:
    def test_webhook_update_membership_role(self, memberships):
        roles = {"worker", "supervisor", "maintainer"}

        membership = next(
            (membership for membership in memberships if membership["role"] != "owner")
        )
        org_id = membership["organization"]

        webhook_id = create_webhook(["update:membership"], "organization", org_id=org_id)["id"]

        patch_data = {"role": (roles - {membership["role"]}).pop()}
        response = patch_method(
            "admin1", f"memberships/{membership['id']}", patch_data, org_id=org_id
        )
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["role"] == membership["role"]
        assert payload["membership"]["role"] == patch_data["role"]

    def test_webhook_delete_membership(self, memberships):
        membership = next(
            (membership for membership in memberships if membership["role"] != "owner")
        )
        org_id = membership["organization"]

        webhook_id = create_webhook(["delete:membership"], "organization", org_id=org_id)["id"]

        response = delete_method("admin1", f"memberships/{membership['id']}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert (
            DeepDiff(
                payload["membership"],
                membership,
                ignore_order=True,
                exclude_paths=["root['updated_date']", "root['invitation']"],
            )
            == {}
        )


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookOrganizationEvents:
    def test_webhook_update_organization_name(self, organizations):
        org_id = list(organizations)[0]["id"]

        webhook_id = create_webhook(["update:organization"], "organization", org_id=org_id)["id"]

        patch_data = {"name": "new_org_name"}
        patch_method("admin1", f"organizations/{org_id}", patch_data, org_id=org_id)

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["name"] == organizations[org_id]["name"]
        assert payload["organization"]["name"] == patch_data["name"]


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookCommentEvents:
    def test_webhook_update_comment_message(self, comments, issues, jobs, tasks):
        org_comments = list(
            (comment, tasks[jobs[issues[comment["issue"]]["job"]]["task_id"]]["organization"])
            for comment in comments
        )

        comment, org_id = next(
            ((comment, org_id) for comment, org_id in org_comments if org_id is not None)
        )

        webhook_id = create_webhook(["update:comment"], "organization", org_id=org_id)["id"]

        patch_data = {"message": "new comment message"}
        response = patch_method("admin1", f"comments/{comment['id']}", patch_data, org_id=org_id)
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook_id)

        assert deliveries["count"] == 1
        assert payload["before_update"]["message"] == comment["message"]

        comment.update(patch_data)
        assert (
            DeepDiff(
                payload["comment"],
                comment,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )

    def test_webhook_create_and_delete_comment(self, issues, jobs, tasks):
        issue = next(
            (
                issue
                for issue in issues
                if tasks[jobs[issue["job"]]["task_id"]]["organization"] is not None
            )
        )

        org_id = tasks[jobs[issue["job"]]["task_id"]]["organization"]

        events = ["create:comment", "delete:comment"]
        webhook_id = create_webhook(events, "organization", org_id=org_id)["id"]

        post_data = {"issue": issue["id"], "message": "new comment message"}
        response = post_method("admin1", f"comments", post_data, org_id=org_id)
        assert response.status_code == HTTPStatus.CREATED

        create_deliveries, create_payload = get_deliveries(webhook_id)

        comment_id = response.json()["id"]
        response = delete_method("admin1", f"comments/{comment_id}", org_id=org_id)
        assert response.status_code == HTTPStatus.NO_CONTENT

        delete_deliveries, delete_payload = get_deliveries(webhook_id, 2)

        assert create_deliveries["count"] == 1
        assert delete_deliveries["count"] == 2

        assert create_payload["event"] == "create:comment"
        assert delete_payload["event"] == "delete:comment"

        assert (
            create_payload["comment"]["message"]
            == delete_payload["comment"]["message"]
            == post_data["message"]
        )


@pytest.mark.usefixtures("restore_db_per_class")
class TestGetWebhookDeliveries:
    def test_not_project_staff_cannot_get_webhook(self, projects, users):
        user, project = next(
            (user, project)
            for user in users
            if "user" in user["groups"]
            for project in projects
            if project["owner"]["id"] != user["id"]
        )

        webhook = create_webhook(["create:task"], "project", project_id=project["id"])
        owner = next(user for user in users if user["id"] == project["owner"]["id"])

        response = post_method(owner["username"], f"webhooks/{webhook['id']}/ping", {})
        assert response.status_code == HTTPStatus.OK

        delivery_id = response.json()["id"]

        response = get_method(user["username"], f"webhooks/{webhook['id']}/deliveries")
        assert response.status_code == HTTPStatus.FORBIDDEN

        response = get_method(
            user["username"], f"webhooks/{webhook['id']}/deliveries/{delivery_id}"
        )
        assert response.status_code == HTTPStatus.FORBIDDEN


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookPing:
    def test_ping_webhook(self, projects):
        project_id = list(projects)[0]["id"]

        webhook = create_webhook(["create:task"], "project", project_id=project_id)

        response = post_method("admin1", f"webhooks/{webhook['id']}/ping", {})
        assert response.status_code == HTTPStatus.OK

        deliveries, payload = get_deliveries(webhook["id"])

        assert deliveries["count"] == 1

        assert (
            DeepDiff(
                payload["webhook"],
                webhook,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )

    def test_not_project_staff_cannot_ping(self, projects, users):
        user, project = next(
            (user, project)
            for user in users
            if "user" in user["groups"]
            for project in projects
            if project["owner"]["id"] != user["id"]
        )

        webhook = create_webhook(["create:task"], "project", project_id=project["id"])

        response = post_method(user["username"], f"webhooks/{webhook['id']}/ping", {})
        assert response.status_code == HTTPStatus.FORBIDDEN


@pytest.mark.usefixtures("restore_db_per_function")
class TestWebhookRedelivery:
    def test_webhook_redelivery(self, projects):
        project = list(projects)[0]

        webhook_id = create_webhook(["update:project"], "project", project_id=project["id"])["id"]

        patch_data = {"name": "new_project_name"}
        response = patch_method("admin1", f"projects/{project['id']}", patch_data)
        assert response.status_code == HTTPStatus.OK

        deliveries_1, payload_1 = get_deliveries(webhook_id)
        delivery_id = deliveries_1["results"][0]["id"]

        response = post_method(
            "admin1", f"webhooks/{webhook_id}/deliveries/{delivery_id}/redelivery", {}
        )
        assert response.status_code == HTTPStatus.OK

        deliveries_2, payload_2 = get_deliveries(webhook_id, 2)

        assert deliveries_1["count"] == 1
        assert deliveries_2["count"] == 2

        assert deliveries_1["results"][0]["redelivery"] is False
        assert deliveries_2["results"][0]["redelivery"] is True

        project.update(patch_data)
        assert (
            DeepDiff(
                payload_1["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )
        assert (
            DeepDiff(
                payload_2["project"],
                project,
                ignore_order=True,
                exclude_paths=["root['updated_date']"],
            )
            == {}
        )

    def test_not_project_staff_cannot_redeliver(self, projects, users):
        user, project = next(
            (user, project)
            for user in users
            if "user" in user["groups"]
            for project in projects
            if project["owner"]["id"] != user["id"]
        )

        webhook = create_webhook(["create:task"], "project", project_id=project["id"])
        owner = next(user for user in users if user["id"] == project["owner"]["id"])

        response = post_method(owner["username"], f"webhooks/{webhook['id']}/ping", {})
        assert response.status_code == HTTPStatus.OK

        delivery_id = response.json()["id"]

        response = post_method(
            user["username"], f"webhooks/{webhook['id']}/deliveries/{delivery_id}/redelivery", {}
        )
        assert response.status_code == HTTPStatus.FORBIDDEN


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\utils.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
from abc import ABCMeta, abstractmethod
from collections.abc import Hashable, Iterator, Sequence
from copy import deepcopy
from http import HTTPStatus
from time import sleep
from typing import Any, Callable, Iterable, Optional, TypeVar, Union

import requests
from cvat_sdk.api_client import apis, models
from cvat_sdk.api_client.api.jobs_api import JobsApi
from cvat_sdk.api_client.api.projects_api import ProjectsApi
from cvat_sdk.api_client.api.tasks_api import TasksApi
from cvat_sdk.api_client.api_client import ApiClient, Endpoint
from cvat_sdk.api_client.exceptions import ForbiddenException
from cvat_sdk.core.helpers import get_paginated_collection
from deepdiff import DeepDiff

from shared.utils.config import make_api_client


def initialize_export(endpoint: Endpoint, *, expect_forbidden: bool = False, **kwargs) -> str:
    (_, response) = endpoint.call_with_http_info(
        **kwargs, _parse_response=False, _check_status=False
    )
    if expect_forbidden:
        assert (
            response.status == HTTPStatus.FORBIDDEN
        ), f"Request should be forbidden, status: {response.status}"
        raise ForbiddenException()

    assert response.status == HTTPStatus.ACCEPTED, f"Status: {response.status}"

    # define background request ID returned in the server response
    rq_id = json.loads(response.data).get("rq_id")
    assert rq_id, "The rq_id parameter was not found in the server response"
    return rq_id


def wait_and_download_v2(
    api_client: ApiClient,
    rq_id: str,
    *,
    max_retries: int = 50,
    interval: float = 0.1,
    download_result: bool = True,
) -> Optional[bytes]:
    for _ in range(max_retries):
        (background_request, response) = api_client.requests_api.retrieve(rq_id)
        assert response.status == HTTPStatus.OK
        if (
            background_request.status.value
            == models.RequestStatus.allowed_values[("value",)]["FINISHED"]
        ):
            break
        sleep(interval)
    else:
        assert False, (
            f"Export process was not finished within allowed time ({interval * max_retries}, sec). "
            + f"Last status was: {background_request.status.value}"
        )

    if not download_result:
        return None

    # return downloaded file in case of local downloading or None otherwise
    if background_request.result_url:
        response = requests.get(
            background_request.result_url,
            auth=(api_client.configuration.username, api_client.configuration.password),
        )
        assert response.status_code == HTTPStatus.OK, f"Status: {response.status_code}"

        return response.content

    return None


def export_v2(
    endpoint: Endpoint,
    *,
    max_retries: int = 50,
    interval: float = 0.1,
    expect_forbidden: bool = False,
    wait_result: bool = True,
    download_result: bool = True,
    **kwargs,
) -> Optional[bytes]:
    """Export datasets|annotations|backups using the second version of export API

    Args:
        endpoint (Endpoint): Export endpoint, will be called only to initialize export process
        max_retries (int, optional): Number of retries when checking process status. Defaults to 30.
        interval (float, optional): Interval in seconds between retries. Defaults to 0.1.
        expect_forbidden (bool, optional): Should export request be forbidden or not. Defaults to False.
        download_result (bool, optional): Download exported file. Defaults to True.

    Returns:
        bytes: The content of the file if downloaded locally.
        None: If `wait_result` or `download_result` were False or the file is downloaded to cloud storage.
    """
    # initialize background process and ensure that the first request returns 403 code if request should be forbidden
    rq_id = initialize_export(endpoint, expect_forbidden=expect_forbidden, **kwargs)

    if not wait_result:
        return None

    # check status of background process
    return wait_and_download_v2(
        endpoint.api_client,
        rq_id,
        max_retries=max_retries,
        interval=interval,
        download_result=download_result,
    )


def export_dataset(
    api: Union[ProjectsApi, TasksApi, JobsApi],
    *,
    save_images: bool,
    max_retries: int = 300,
    interval: float = 0.1,
    format: str = "CVAT for images 1.1",  # pylint: disable=redefined-builtin
    **kwargs,
) -> Optional[bytes]:
    return export_v2(
        api.create_dataset_export_endpoint,
        max_retries=max_retries,
        interval=interval,
        save_images=save_images,
        format=format,
        **kwargs,
    )


# FUTURE-TODO: support username: optional, api_client: optional
# tODO: make func signature more userfrendly
def export_project_dataset(username: str, *args, **kwargs) -> Optional[bytes]:
    with make_api_client(username) as api_client:
        return export_dataset(api_client.projects_api, *args, **kwargs)


def export_task_dataset(username: str, *args, **kwargs) -> Optional[bytes]:
    with make_api_client(username) as api_client:
        return export_dataset(api_client.tasks_api, *args, **kwargs)


def export_job_dataset(username: str, *args, **kwargs) -> Optional[bytes]:
    with make_api_client(username) as api_client:
        return export_dataset(api_client.jobs_api, *args, **kwargs)


def export_backup(
    api: Union[ProjectsApi, TasksApi],
    *,
    max_retries: int = 50,
    interval: float = 0.1,
    **kwargs,
) -> Optional[bytes]:
    endpoint = api.create_backup_export_endpoint
    return export_v2(endpoint, max_retries=max_retries, interval=interval, **kwargs)


def export_project_backup(username: str, *args, **kwargs) -> Optional[bytes]:
    with make_api_client(username) as api_client:
        return export_backup(api_client.projects_api, *args, **kwargs)


def export_task_backup(username: str, *args, **kwargs) -> Optional[bytes]:
    with make_api_client(username) as api_client:
        return export_backup(api_client.tasks_api, *args, **kwargs)


def import_resource(
    endpoint: Endpoint,
    *,
    max_retries: int = 50,
    interval: float = 0.1,
    expect_forbidden: bool = False,
    wait_result: bool = True,
    **kwargs,
) -> None:
    # initialize background process and ensure that the first request returns 403 code if request should be forbidden
    (_, response) = endpoint.call_with_http_info(
        **kwargs,
        _parse_response=False,
        _check_status=False,
        _content_type="multipart/form-data",
    )
    if expect_forbidden:
        assert response.status == HTTPStatus.FORBIDDEN, "Request should be forbidden"
        raise ForbiddenException()

    assert response.status == HTTPStatus.ACCEPTED

    if not wait_result:
        return None

    # define background request ID returned in the server response
    rq_id = json.loads(response.data).get("rq_id")
    assert rq_id, "The rq_id parameter was not found in the server response"

    # check status of background process
    for _ in range(max_retries):
        (background_request, response) = endpoint.api_client.requests_api.retrieve(rq_id)
        assert response.status == HTTPStatus.OK
        if background_request.status.value in (
            models.RequestStatus.allowed_values[("value",)]["FINISHED"],
            models.RequestStatus.allowed_values[("value",)]["FAILED"],
        ):
            break
        sleep(interval)
    else:
        assert False, (
            f"Import process was not finished within allowed time ({interval * max_retries}, sec). "
            + f"Last status was: {background_request.status.value}"
        )


def import_backup(
    api: Union[ProjectsApi, TasksApi],
    *,
    max_retries: int = 50,
    interval: float = 0.1,
    **kwargs,
) -> None:
    endpoint = api.create_backup_endpoint
    return import_resource(endpoint, max_retries=max_retries, interval=interval, **kwargs)


def import_project_backup(username: str, data: dict, **kwargs) -> None:
    with make_api_client(username) as api_client:
        return import_backup(api_client.projects_api, project_file_request=deepcopy(data), **kwargs)


def import_task_backup(username: str, data: dict, **kwargs) -> None:
    with make_api_client(username) as api_client:
        return import_backup(api_client.tasks_api, task_file_request=deepcopy(data), **kwargs)


FieldPath = Sequence[Union[str, Callable]]


class CollectionSimpleFilterTestBase(metaclass=ABCMeta):
    # These fields need to be defined in the subclass
    user: str
    samples: list[dict[str, Any]]
    field_lookups: dict[str, FieldPath] = None
    cmp_ignore_keys: list[str] = ["updated_date"]

    @abstractmethod
    def _get_endpoint(self, api_client: ApiClient) -> Endpoint: ...

    def _retrieve_collection(self, **kwargs) -> list:
        kwargs["return_json"] = True
        with make_api_client(self.user) as api_client:
            return get_paginated_collection(self._get_endpoint(api_client), **kwargs)

    @classmethod
    def _get_field(cls, d: dict[str, Any], path: Union[str, FieldPath]) -> Optional[Any]:
        assert path
        for key in path:
            if isinstance(d, dict):
                assert isinstance(key, str)
                d = d.get(key)
            else:
                if callable(key):
                    assert isinstance(d, str)
                    d = key(d)
                else:
                    d = None

        return d

    def _map_field(self, name: str) -> FieldPath:
        return (self.field_lookups or {}).get(name, [name])

    @classmethod
    def _find_valid_field_value(
        cls, samples: Iterator[dict[str, Any]], field_path: FieldPath
    ) -> Any:
        value = None
        for sample in samples:
            value = cls._get_field(sample, field_path)
            if value:
                break

        assert value, f"Failed to find a sample for the '{'.'.join(field_path)}' field"
        return value

    def _get_field_samples(self, field: str) -> tuple[Any, list[dict[str, Any]]]:
        field_path = self._map_field(field)
        field_value = self._find_valid_field_value(self.samples, field_path)

        gt_objects = filter(lambda p: field_value == self._get_field(p, field_path), self.samples)

        return field_value, gt_objects

    def _compare_results(self, gt_objects, received_objects):
        if self.cmp_ignore_keys:
            ignore_keys = [f"root['{k}']" for k in self.cmp_ignore_keys]
        else:
            ignore_keys = None

        diff = DeepDiff(
            list(gt_objects),
            received_objects,
            exclude_paths=ignore_keys,
            ignore_order=True,
        )

        assert diff == {}, diff

    def _test_can_use_simple_filter_for_object_list(
        self, field: str, field_values: Optional[list[Any]] = None
    ):
        gt_objects = []
        field_path = self._map_field(field)

        if not field_values:
            value, gt_objects = self._get_field_samples(field)
            field_values = [value]

        are_gt_objects_initialized = bool(gt_objects)

        for value in field_values:
            if not are_gt_objects_initialized:
                gt_objects = [
                    sample
                    for sample in self.samples
                    if value == self._get_field(sample, field_path)
                ]
            received_items = self._retrieve_collection(**{field: value})
            self._compare_results(gt_objects, received_items)


def get_attrs(obj: Any, attributes: Sequence[str]) -> tuple[Any, ...]:
    """Returns 1 or more object attributes as a tuple"""
    return (getattr(obj, attr) for attr in attributes)


def build_exclude_paths_expr(ignore_fields: Iterator[str]) -> list[str]:
    exclude_expr_parts = []
    for key in ignore_fields:
        if "." in key:
            key_parts = key.split(".")
            expr = r"root\['{}'\]".format(key_parts[0])
            expr += "".join(r"\[.*\]\['{}'\]".format(part) for part in key_parts[1:])
        else:
            expr = r"root\['{}'\]".format(key)

        exclude_expr_parts.append(expr)

    return exclude_expr_parts


def wait_until_task_is_created(api: apis.RequestsApi, rq_id: str) -> models.Request:
    for _ in range(100):
        (request_details, _) = api.retrieve(rq_id)

        if request_details.status.value in ("finished", "failed"):
            return request_details
        sleep(1)
    raise Exception("Cannot create task")


def create_task(username, spec, data, content_type="application/json", **kwargs):
    with make_api_client(username) as api_client:
        (task, response_) = api_client.tasks_api.create(spec, **kwargs)
        assert response_.status == HTTPStatus.CREATED

        sent_upload_start = False

        data_kwargs = (kwargs or {}).copy()
        data_kwargs.pop("org", None)
        data_kwargs.pop("org_id", None)

        if data.get("client_files") and "json" in content_type:
            (_, response) = api_client.tasks_api.create_data(
                task.id,
                data_request=models.DataRequest(image_quality=data["image_quality"]),
                upload_start=True,
                _content_type=content_type,
                **data_kwargs,
            )
            assert response.status == HTTPStatus.ACCEPTED
            sent_upload_start = True

            # Can't encode binary files in json
            (_, response) = api_client.tasks_api.create_data(
                task.id,
                data_request=models.DataRequest(
                    client_files=data["client_files"],
                    image_quality=data["image_quality"],
                ),
                upload_multiple=True,
                _content_type="multipart/form-data",
                **data_kwargs,
            )
            assert response.status == HTTPStatus.OK

            data = data.copy()
            del data["client_files"]

        last_kwargs = {}
        if sent_upload_start:
            last_kwargs["upload_finish"] = True

        (result, response) = api_client.tasks_api.create_data(
            task.id,
            data_request=deepcopy(data),
            _content_type=content_type,
            **data_kwargs,
            **last_kwargs,
        )
        assert response.status == HTTPStatus.ACCEPTED

        request_details = wait_until_task_is_created(api_client.requests_api, result.rq_id)
        assert request_details.status.value == "finished", request_details.message

    return task.id, response_.headers.get("X-Request-Id")


def compare_annotations(a: dict, b: dict) -> dict:
    def _exclude_cb(obj, path):
        return path.endswith("['elements']") and not obj

    return DeepDiff(
        a,
        b,
        ignore_order=True,
        significant_digits=2,  # annotations are stored with 2 decimal digit precision
        exclude_obj_callback=_exclude_cb,
        exclude_regex_paths=[
            r"root\['version|updated_date'\]",
            r"root(\['\w+'\]\[\d+\])+\['id'\]",
            r"root(\['\w+'\]\[\d+\])+\['label_id'\]",
            r"root(\['\w+'\]\[\d+\])+\['attributes'\]\[\d+\]\['spec_id'\]",
            r"root(\['\w+'\]\[\d+\])+\['source'\]",
        ],
    )


DATUMARO_FORMAT_FOR_DIMENSION = {
    "2d": "Datumaro 1.0",
    "3d": "Datumaro 3D 1.0",
}


def parse_frame_step(frame_filter: str) -> int:
    return int((frame_filter or "step=1").split("=")[1])


def calc_end_frame(start_frame: int, stop_frame: int, frame_step: int) -> int:
    return stop_frame - ((stop_frame - start_frame) % frame_step) + frame_step


_T = TypeVar("_T")


def unique(
    it: Union[Iterator[_T], Iterable[_T]], *, key: Callable[[_T], Hashable] = None
) -> Iterable[_T]:
    return {key(v): v for v in it}.values()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\rest_api\__init__.py =====
# Copyright (C) 2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\common.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT
import io
import zipfile
from pathlib import Path
from typing import Optional, Union

import pytest
from cvat_sdk.core.proxies.jobs import Job
from cvat_sdk.core.proxies.projects import Project
from cvat_sdk.core.proxies.tasks import Task
from cvat_sdk.core.proxies.types import Location

from shared.fixtures.data import CloudStorageAssets
from shared.utils.config import IMPORT_EXPORT_BUCKET_ID
from shared.utils.s3 import S3Client
from shared.utils.s3 import make_client as make_s3_client

from .util import make_pbar

ProjectOrTaskOrJob = Union[Project, Task, Job]


class TestDatasetExport:
    def _test_export_locally(
        self,
        resource: ProjectOrTaskOrJob,
        *,
        format_name: str,
        file_path: Path,
        **export_kwargs,
    ):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)
        resource.export_dataset(format_name, file_path, pbar=pbar, **export_kwargs)
        assert self.stdout.getvalue() == ""
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert file_path.is_file()

    def _test_export_to_cloud_storage(
        self,
        resource: ProjectOrTaskOrJob,
        *,
        format_name: str,
        file_path: Path,
        cs_client: S3Client,
        **export_kwargs,
    ):
        resource.export_dataset(format_name, file_path, **export_kwargs)
        assert self.stdout.getvalue() == ""
        dataset = cs_client.download_fileobj(str(file_path))
        assert zipfile.is_zipfile(io.BytesIO(dataset))

    def _test_can_export_dataset(
        self,
        resource: ProjectOrTaskOrJob,
        *,
        format_name: str,
        file_path: Path,
        include_images: bool,
        location: Optional[Location],
        request: pytest.FixtureRequest,
        cloud_storages: CloudStorageAssets,
    ):
        kwargs = {
            "include_images": include_images,
            "location": location,
        }

        expected_locally = (
            location == Location.LOCAL
            or not location
            and (
                not resource.target_storage
                or resource.target_storage.location.value == Location.LOCAL
            )
        )

        if expected_locally:
            self._test_export_locally(
                resource, format_name=format_name, file_path=file_path, **kwargs
            )
        else:
            bucket = next(cs for cs in cloud_storages if cs["id"] == IMPORT_EXPORT_BUCKET_ID)[
                "resource"
            ]
            s3_client = make_s3_client(bucket=bucket)
            request.addfinalizer(lambda: s3_client.remove_file(filename=str(file_path)))
            self._test_export_to_cloud_storage(
                resource,
                format_name=format_name,
                file_path=file_path,
                cs_client=s3_client,
                **(
                    {"cloud_storage_id": IMPORT_EXPORT_BUCKET_ID}
                    if location == Location.CLOUD_STORAGE
                    else {}
                ),
                **kwargs,
            )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\conftest.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

# Force execution of fixture definitions
from .fixtures import *  # pylint: disable=wildcard-import


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\fixtures.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from pathlib import Path
from zipfile import ZipFile

import pytest
from cvat_sdk import Client
from cvat_sdk.core.proxies.types import Location
from PIL import Image

from shared.utils.config import BASE_URL, IMPORT_EXPORT_BUCKET_ID, USER_PASS
from shared.utils.helpers import generate_image_file

from .util import generate_coco_json


@pytest.fixture
def fxt_client(fxt_logger):
    logger, _ = fxt_logger

    client = Client(BASE_URL, logger=logger)
    api_client = client.api_client
    for k in api_client.configuration.logger:
        api_client.configuration.logger[k] = logger
    client.config.status_check_period = 0.01

    with client:
        yield client


@pytest.fixture
def fxt_image_file(tmp_path: Path):
    img_path = tmp_path / "img.png"
    with img_path.open("wb") as f:
        f.write(generate_image_file(filename=str(img_path), size=(5, 10)).getvalue())

    return img_path


@pytest.fixture
def fxt_coco_file(tmp_path: Path, fxt_image_file: Path):
    img_filename = fxt_image_file
    img_size = Image.open(img_filename).size
    ann_filename = tmp_path / "coco.json"
    generate_coco_json(ann_filename, img_info=(img_filename, *img_size))

    yield ann_filename


@pytest.fixture(scope="class")
def fxt_login(admin_user: str, restore_db_per_class):
    client = Client(BASE_URL)
    client.config.status_check_period = 0.01
    user = admin_user

    with client:
        client.login((user, USER_PASS))
        yield (client, user)


@pytest.fixture
def fxt_camvid_dataset(tmp_path: Path):
    img_path = tmp_path / "img.png"
    with img_path.open("wb") as f:
        f.write(generate_image_file(filename=str(img_path), size=(5, 10)).getvalue())

    annot_path = tmp_path / "annot.png"
    r, g, b = (127, 0, 0)
    annot = generate_image_file(
        filename=str(annot_path),
        size=(5, 10),
        color=(r, g, b),
    ).getvalue()
    with annot_path.open("wb") as f:
        f.write(annot)

    label_colors_path = tmp_path / "label_colors.txt"
    with open(label_colors_path, "w") as f:
        f.write(f"{r} {g} {b} car\n")

    dataset_img_path = "default/img.png"
    dataset_annot_path = "default/annot.png"
    default_txt_path = tmp_path / "default.txt"
    with open(default_txt_path, "w") as f:
        f.write(f"/{dataset_img_path} {dataset_annot_path}")

    dataset_path = tmp_path / "camvid_dataset.zip"
    with ZipFile(dataset_path, "x") as f:
        f.write(img_path, arcname=dataset_img_path)
        f.write(annot_path, arcname=dataset_annot_path)
        f.write(default_txt_path, arcname="default.txt")
        f.write(label_colors_path, arcname="label_colors.txt")

    yield dataset_path


@pytest.fixture
def fxt_coco_dataset(tmp_path: Path, fxt_image_file: Path, fxt_coco_file: Path):
    dataset_path = tmp_path / "coco_dataset.zip"
    with ZipFile(dataset_path, "x") as f:
        f.write(fxt_image_file, arcname="images/" + fxt_image_file.name)
        f.write(fxt_coco_file, arcname="annotations/instances_default.json")

    yield dataset_path


@pytest.fixture
def fxt_new_task(fxt_image_file: Path, fxt_login: tuple[Client, str]):
    client, _ = fxt_login
    task = client.tasks.create_from_data(
        spec={
            "name": "test_task",
            "labels": [{"name": "car"}, {"name": "person"}],
        },
        resources=[fxt_image_file],
        data_params={"image_quality": 80},
    )

    yield task


@pytest.fixture
def fxt_new_task_with_target_storage(fxt_image_file: Path, fxt_login: tuple[Client, str]):
    client, _ = fxt_login
    task = client.tasks.create_from_data(
        spec={
            "name": "test_task",
            "labels": [{"name": "car"}, {"name": "person"}],
            "target_storage": {
                "location": Location.CLOUD_STORAGE,
                "cloud_storage_id": IMPORT_EXPORT_BUCKET_ID,
            },
        },
        resources=[fxt_image_file],
        data_params={"image_quality": 80},
    )

    yield task


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_api_wrappers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import pickle
from copy import deepcopy

from cvat_sdk import models
from deepdiff import DeepDiff


def test_models_do_not_change_input_values():
    # Nested containers may be modified during the model input data parsing.
    # This can lead to subtle memory errors, which are very hard to find.
    original_input_data = {
        "name": "test",
        "labels": [
            {
                "name": "cat",
                "attributes": [
                    {
                        "default_value": "yy",
                        "input_type": "text",
                        "mutable": False,
                        "name": "x",
                        "values": ["yy"],
                    },
                    {
                        "default_value": "1",
                        "input_type": "radio",
                        "mutable": False,
                        "name": "y",
                        "values": ["1", "2"],
                    },
                ],
            }
        ],
    }

    input_data = deepcopy(original_input_data)

    models.TaskWriteRequest(**input_data)

    assert DeepDiff(original_input_data, input_data) == {}


def test_models_do_not_store_input_collections():
    # Avoid depending on input data for collection fields after the model is initialized.
    # This can lead to subtle memory errors and unexpected behavior
    # if the original input data is modified.
    input_data = {
        "name": "test",
        "labels": [
            {
                "name": "cat1",
                "attributes": [
                    {
                        "default_value": "yy",
                        "input_type": "text",
                        "mutable": False,
                        "name": "x",
                        "values": ["yy"],
                    },
                    {
                        "default_value": "1",
                        "input_type": "radio",
                        "mutable": False,
                        "name": "y",
                        "values": ["1", "2"],
                    },
                ],
            },
            {"name": "cat2", "attributes": []},
        ],
    }

    model = models.TaskWriteRequest(**input_data)
    model_data1 = model.to_dict()

    # Modify input value containers
    input_data["labels"][0]["attributes"].clear()
    input_data["labels"][1]["attributes"].append(
        {
            "default_value": "",
            "input_type": "text",
            "mutable": True,
            "name": "z",
        }
    )
    input_data["labels"].append({"name": "dog"})

    model_data2 = model.to_dict()

    assert DeepDiff(model_data1, model_data2) == {}


def test_models_do_not_return_internal_collections():
    # Avoid returning internal data for mutable collection fields.
    # This can lead to subtle memory errors and unexpected behavior
    # if the returned data is modified.
    input_data = {
        "name": "test",
        "labels": [],
    }

    model = models.TaskWriteRequest(**input_data)
    model_data1 = model.to_dict()
    model_data1_original = deepcopy(model_data1)

    # Modify an output value container
    model_data1["labels"].append({"name": "dog"})

    model_data2 = model.to_dict()

    assert DeepDiff(model_data1_original, model_data2) == {}


def test_models_are_pickleable():
    model = models.PatchedLabelRequest(id=5, name="person")
    pickled_model = pickle.dumps(model)
    unpickled_model = pickle.loads(pickled_model)

    assert unpickled_model.id == model.id
    assert unpickled_model.name == model.name


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_attributes.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import pytest
from cvat_sdk import models
from cvat_sdk.attributes import (
    attribute_vals_from_dict,
    attribute_value_validator,
    number_attribute_values,
)


def test_number_attribute_values_can_convert_good_values():
    assert number_attribute_values(0, 0, 1) == ["0", "0", "1"]
    assert number_attribute_values(0, 10, 1) == ["0", "10", "1"]
    assert number_attribute_values(0, 10, 10) == ["0", "10", "10"]
    assert number_attribute_values(0, 10, 5) == ["0", "10", "5"]


def test_number_attribute_values_can_reject_bad_values():
    with pytest.raises(ValueError, match="min_value must be less than or equal to max_value"):
        number_attribute_values(1, 0, 1)

    with pytest.raises(ValueError, match="step must be positive"):
        number_attribute_values(0, 10, 0)

    with pytest.raises(ValueError, match="step must be positive"):
        number_attribute_values(0, 10, -1)

    with pytest.raises(ValueError, match="step must be a divisor of max_value - min_value"):
        number_attribute_values(0, 10, 3)


def test_attribute_value_validator_checkbox():
    validator = attribute_value_validator(
        models.AttributeRequest(name="a", mutable=False, input_type="checkbox", values=[])
    )

    assert validator("true")
    assert validator("false")
    assert not validator("maybe")


def test_attribute_value_validator_number():
    validator = attribute_value_validator(
        models.AttributeRequest(
            name="a", mutable=False, input_type="number", values=["0", "10", "2"]
        )
    )

    assert validator("0")
    assert validator("2")
    assert validator("10")
    assert not validator("1")
    assert not validator("-2")
    assert not validator("12")
    assert not validator("not a number")


@pytest.mark.parametrize(
    ["values", "exc_match"],
    [
        (["0", "1"], "wrong number of values"),
        (["0", "10", "1", "1"], "wrong number of values"),
        (["a", "10", "1"], "values could not be converted to integers"),
        (["0", "a", "1"], "values could not be converted to integers"),
        (["0", "10", "a"], "values could not be converted to integers"),
        (["0", "10", "0"], "step must be positive"),
        (["1", "0", "1"], "min_value must be less than or equal to max_value"),
        (["0", "10", "3"], "step must be a divisor of max_value - min_value"),
    ],
)
def test_attribute_value_validator_number_bad_spec(values, exc_match):
    with pytest.raises(ValueError, match=exc_match):
        attribute_value_validator(
            models.AttributeRequest(name="a", mutable=False, input_type="number", values=values)
        )


@pytest.mark.parametrize("input_type", ["radio", "select"])
def test_attribute_value_validator_radiolike(input_type: str):
    validator = attribute_value_validator(
        models.AttributeRequest(name="a", mutable=False, input_type=input_type, values=["a", "b"])
    )

    assert validator("a")
    assert validator("b")
    assert not validator("c")


@pytest.mark.parametrize("input_type", ["radio", "select"])
def test_attribute_value_validator_radiolike_bad_spec(input_type: str):
    with pytest.raises(ValueError, match="empty list of allowed values"):
        attribute_value_validator(
            models.AttributeRequest(name="a", mutable=False, input_type=input_type, values=[])
        )


def test_attribute_value_validator_text():
    validator = attribute_value_validator(
        models.AttributeRequest(name="a", mutable=False, input_type="text", values=[])
    )

    assert validator("anything")


def test_attribute_vals_from_dict():
    assert attribute_vals_from_dict({}) == []

    attrs = attribute_vals_from_dict({0: "x", 1: 5, 2: True, 3: False})
    assert len(attrs) == 4

    for i, attr in enumerate(attrs):
        assert attr.spec_id == i

    assert attrs[0].value == "x"
    assert attrs[1].value == "5"
    assert attrs[2].value == "true"
    assert attrs[3].value == "false"


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_auto_annotation.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import math
from logging import Logger
from pathlib import Path
from types import SimpleNamespace as namespace

import cvat_sdk.auto_annotation as cvataa
import PIL.Image
import pytest
from cvat_sdk import Client, models
from cvat_sdk.attributes import attribute_vals_from_dict, number_attribute_values
from cvat_sdk.core.proxies.tasks import ResourceType

from shared.utils.helpers import generate_image_file

from .util import make_pbar

try:
    import torchvision.models as torchvision_models
except ModuleNotFoundError:
    torchvision_models = None


@pytest.fixture(autouse=True)
def _common_setup(
    tmp_path: Path,
    fxt_login: tuple[Client, str],
    fxt_logger: tuple[Logger, io.StringIO],
    restore_redis_ondisk_per_function,
    restore_redis_inmem_per_function,
):
    logger = fxt_logger[0]
    client = fxt_login[0]
    client.logger = logger
    client.config.cache_dir = tmp_path / "cache"

    api_client = client.api_client
    for k in api_client.configuration.logger:
        api_client.configuration.logger[k] = logger


class TestDetectionFunctionSpec:
    def _test_bad_spec(self, exc_match: str, **kwargs) -> None:
        with pytest.raises(cvataa.BadFunctionError, match=exc_match):
            cvataa.DetectionFunctionSpec(**kwargs)

    def _test_bad_attributes(self, exc_match: str, *attrs: models.AttributeRequest) -> None:
        self._test_bad_spec(
            exc_match, labels=[cvataa.label_spec("car", 123, attributes=list(attrs))]
        )

        self._test_bad_spec(
            exc_match,
            labels=[
                cvataa.skeleton_label_spec(
                    "car", 123, [cvataa.keypoint_spec("engine", 1234, attributes=list(attrs))]
                ),
            ],
        )

    def test_attribute_without_id(self):
        self._test_bad_attributes(
            "attribute .+ has no ID",
            models.AttributeRequest("brand", mutable=False, input_type="text", values=[]),
        )

    def test_duplicate_attribute_id(self):
        self._test_bad_attributes(
            "same ID as another attribute",
            cvataa.text_attribute_spec("brand", 1),
            cvataa.text_attribute_spec("color", 1),
        )

    def test_invalid_attribute_values(self):
        self._test_bad_attributes(
            "has invalid values",
            cvataa.number_attribute_spec("year", 1, []),
        )

    def test_label_without_id(self):
        self._test_bad_spec(
            "label .+ has no ID",
            labels=[
                models.PatchedLabelRequest(
                    name="car",
                ),
            ],
        )

    def test_duplicate_label_id(self):
        self._test_bad_spec(
            "same ID as another label",
            labels=[
                cvataa.label_spec("car", 123),
                cvataa.label_spec("bicycle", 123),
            ],
        )

    def test_non_skeleton_sublabels(self):
        self._test_bad_spec(
            "should be 'skeleton'",
            labels=[
                cvataa.label_spec(
                    "car",
                    123,
                    sublabels=[models.SublabelRequest("wheel", id=1)],
                ),
            ],
        )

    def test_sublabel_without_id(self):
        self._test_bad_spec(
            "sublabel .+ of label .+ has no ID",
            labels=[
                cvataa.skeleton_label_spec(
                    "car",
                    123,
                    [models.SublabelRequest("wheel")],
                ),
            ],
        )

    def test_duplicate_sublabel_id(self):
        self._test_bad_spec(
            "same ID as another sublabel",
            labels=[
                cvataa.skeleton_label_spec(
                    "cat",
                    123,
                    [
                        cvataa.keypoint_spec("head", 1),
                        cvataa.keypoint_spec("tail", 1),
                    ],
                ),
            ],
        )

    def test_sublabel_wrong_type(self):
        self._test_bad_spec(
            "should be 'points'",
            labels=[
                cvataa.skeleton_label_spec(
                    "cat", 123, [models.SublabelRequest(name="head", id=1, type="any")]
                )
            ],
        )


class TestTaskAutoAnnotation:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
    ):
        self.client = fxt_login[0]
        self.images = [
            generate_image_file("1.png", size=(333, 333), color=(0, 0, 0)),
            generate_image_file("2.png", size=(333, 333), color=(100, 100, 100)),
        ]

        image_dir = tmp_path / "images"
        image_dir.mkdir()

        image_paths = []
        for image in self.images:
            image_path = image_dir / image.name
            image_path.write_bytes(image.getbuffer())
            image_paths.append(image_path)

        self.task = self.client.tasks.create_from_data(
            models.TaskWriteRequest(
                "Auto-annotation test task",
                labels=[
                    models.PatchedLabelRequest(name="person"),
                    models.PatchedLabelRequest(name="person-rect", type="rectangle"),
                    models.PatchedLabelRequest(name="person-mask", type="mask"),
                    models.PatchedLabelRequest(name="person-poly", type="polygon"),
                    models.PatchedLabelRequest(
                        name="cat",
                        type="skeleton",
                        attributes=[
                            models.AttributeRequest(
                                name="color",
                                mutable=False,
                                input_type="select",
                                values=["gray", "calico"],
                            ),
                        ],
                        sublabels=[
                            models.SublabelRequest(
                                name="head",
                                attributes=[
                                    models.AttributeRequest(
                                        name="size",
                                        mutable=False,
                                        input_type="number",
                                        values=["1", "10", "1"],
                                    ),
                                ],
                            ),
                            models.SublabelRequest(name="tail"),
                        ],
                    ),
                ],
            ),
            resource_type=ResourceType.LOCAL,
            resources=image_paths,
        )

        task_labels = self.task.get_labels()
        self.task_labels_by_id = {label.id: label for label in task_labels}
        self.cat_sublabels_by_id = {
            sl.id: sl
            for sl in next(label for label in task_labels if label.name == "cat").sublabels
        }
        self.cat_attributes_by_id = {
            attr.id: attr
            for attr in next(label for label in task_labels if label.name == "cat").attributes
        }
        self.cat_head_attributes_by_id = {
            attr.id: attr
            for attr in next(
                sl for sl in self.cat_sublabels_by_id.values() if sl.name == "head"
            ).attributes
        }

        # The initial annotation is just to check that it gets erased after auto-annotation
        self.task.update_annotations(
            models.PatchedLabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=next(iter(self.task_labels_by_id)),
                        type="rectangle",
                        points=[1.0, 2.0, 3.0, 4.0],
                    ),
                ],
            )
        )

    def test_detection_rectangle(self):
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec("person", 123),
                cvataa.label_spec("bicycle (should be ignored)", 456),
            ],
        )

        def detect(
            context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
        ) -> list[models.LabeledShapeRequest]:
            assert context.frame_name in {"1.png", "2.png"}
            assert image.width == image.height == 333
            return [
                cvataa.rectangle(
                    123,  # person
                    # produce different coordinates for different images
                    [*image.getpixel((0, 0)), 300 + int(context.frame_name[0])],
                ),
                cvataa.shape(
                    456,  # ignored
                    type="points",
                    points=[1, 1],
                ),
            ]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            clear_existing=True,
            allow_unmatched_labels=True,
        )

        annotations = self.task.get_annotations()

        shapes = sorted(annotations.shapes, key=lambda shape: shape.frame)

        assert len(shapes) == 2

        for i, shape in enumerate(shapes):
            assert shape.frame == i
            assert shape.type.value == "rectangle"
            assert self.task_labels_by_id[shape.label_id].name == "person"
            assert shape.points[3] in {301, 302}

        assert shapes[0].points[0] != shapes[1].points[0]
        assert shapes[0].points[3] != shapes[1].points[3]

    def test_detection_skeleton(self):
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.skeleton_label_spec(
                    "cat",
                    123,
                    [
                        cvataa.keypoint_spec("head", 10),
                        cvataa.keypoint_spec("torso (should be ignored)", 20),
                        cvataa.keypoint_spec("tail", 30),
                    ],
                ),
            ],
        )

        def detect(context, image: PIL.Image.Image) -> list[models.LabeledShapeRequest]:
            assert image.width == image.height == 333
            return [
                cvataa.skeleton(
                    123,  # cat
                    [
                        # ignored
                        cvataa.keypoint(20, [20, 20]),
                        # tail
                        cvataa.keypoint(30, [30, 30]),
                        # head
                        cvataa.keypoint(10, [10, 10]),
                    ],
                ),
            ]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            clear_existing=True,
            allow_unmatched_labels=True,
        )

        annotations = self.task.get_annotations()

        shapes = sorted(annotations.shapes, key=lambda shape: shape.frame)

        assert len(shapes) == 2

        for i, shape in enumerate(shapes):
            assert shape.frame == i
            assert shape.type.value == "skeleton"
            assert self.task_labels_by_id[shape.label_id].name == "cat"
            assert len(shape.elements) == 2

            elements = sorted(
                shape.elements, key=lambda s: self.cat_sublabels_by_id[s.label_id].name
            )

            for element in elements:
                assert element.frame == i
                assert element.type.value == "points"

            assert self.cat_sublabels_by_id[elements[0].label_id].name == "head"
            assert elements[0].points == [10, 10]
            assert self.cat_sublabels_by_id[elements[1].label_id].name == "tail"
            assert elements[1].points == [30, 30]

    def test_detection_attributes(self):
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.skeleton_label_spec(
                    "cat",
                    123,
                    [
                        cvataa.keypoint_spec(
                            "head",
                            10,
                            attributes=[
                                cvataa.number_attribute_spec(
                                    "size", 1, number_attribute_values(1, 10, 1)
                                ),
                                cvataa.text_attribute_spec("orientation (should be ignored)", 2),
                            ],
                        ),
                        cvataa.keypoint_spec("tail", 30),
                    ],
                    attributes=[
                        cvataa.select_attribute_spec("color", 1, ["calico", "gray"]),
                        cvataa.text_attribute_spec("name (should be ignored)", 2),
                    ],
                ),
            ],
        )

        def detect(context, image: PIL.Image.Image) -> list[models.LabeledShapeRequest]:
            return [
                cvataa.skeleton(
                    123,  # cat
                    [
                        # head
                        cvataa.keypoint(
                            10,
                            [10, 10],
                            attributes=attribute_vals_from_dict({1: 5, 2: "forward"}),
                        ),
                        # tail
                        cvataa.keypoint(30, [30, 30]),
                    ],
                    attributes=attribute_vals_from_dict({1: "calico", 2: "McFluffy"}),
                ),
            ]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            clear_existing=True,
            allow_unmatched_labels=True,
        )

        annotations = self.task.get_annotations()

        shapes = sorted(annotations.shapes, key=lambda shape: shape.frame)

        assert len(shapes) == 2

        for shape in shapes:
            assert self.task_labels_by_id[shape.label_id].name == "cat"

            assert len(shape.attributes) == 1
            assert self.cat_attributes_by_id[shape.attributes[0].spec_id].name == "color"
            assert shape.attributes[0].value == "calico"

            elements = sorted(
                shape.elements, key=lambda s: self.cat_sublabels_by_id[s.label_id].name
            )

            assert self.cat_sublabels_by_id[elements[0].label_id].name == "head"

            assert len(elements[0].attributes) == 1
            assert self.cat_head_attributes_by_id[elements[0].attributes[0].spec_id].name == "size"
            assert elements[0].attributes[0].value == "5"

    def test_progress_reporting(self):
        spec = cvataa.DetectionFunctionSpec(labels=[])

        def detect(context, image):
            return []

        file = io.StringIO()

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            pbar=make_pbar(file),
        )

        assert "100%" in file.getvalue()

    def test_detection_without_clearing(self):
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec("person", 123),
            ],
        )

        def detect(context, image: PIL.Image.Image) -> list[models.LabeledShapeRequest]:
            return [
                cvataa.rectangle(
                    123,  # person
                    [5, 6, 7, 8],
                    rotation=10,
                ),
            ]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            clear_existing=False,
        )

        annotations = self.task.get_annotations()

        shapes = sorted(annotations.shapes, key=lambda shape: (shape.frame, shape.rotation))

        # original annotation
        assert shapes[0].points == [1, 2, 3, 4]
        assert shapes[0].rotation == 0

        # new annotations
        for i in (1, 2):
            assert shapes[i].points == [5, 6, 7, 8]
            assert shapes[i].rotation == 10

    def test_conf_threshold(self):
        spec = cvataa.DetectionFunctionSpec(labels=[])

        received_threshold = None

        def detect(
            context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
        ) -> list[models.LabeledShapeRequest]:
            nonlocal received_threshold
            received_threshold = context.conf_threshold
            return []

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            conf_threshold=0.75,
        )

        assert received_threshold == 0.75  # python:S1244 NOSONAR

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
        )

        assert received_threshold is None

        for bad_threshold in [-0.1, 1.1]:
            with pytest.raises(ValueError):
                cvataa.annotate_task(
                    self.client,
                    self.task.id,
                    namespace(spec=spec, detect=detect),
                    conf_threshold=bad_threshold,
                )

    def test_conv_mask_to_poly(self):
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec("person", 123),
            ],
        )

        received_cmtp = None

        def detect(context, image: PIL.Image.Image) -> list[models.LabeledShapeRequest]:
            nonlocal received_cmtp
            received_cmtp = context.conv_mask_to_poly
            return [cvataa.mask(123, [1, 0, 0, 0, 0])]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            conv_mask_to_poly=False,
        )

        assert received_cmtp is False

        with pytest.raises(cvataa.BadFunctionError, match=".*conv_mask_to_poly.*"):
            cvataa.annotate_task(
                self.client,
                self.task.id,
                namespace(spec=spec, detect=detect),
                conv_mask_to_poly=True,
            )

        assert received_cmtp is True

    @pytest.mark.parametrize(
        ["label_name", "label_type"],
        [
            ("person", "any"),
            ("person-rect", "any"),
            ("person", "rectangle"),
            ("person-rect", "rectangle"),
        ],
    )
    def test_type_compatibility(self, label_name: str, label_type: str) -> None:
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec(label_name, 123, type=label_type),
            ]
        )

        def detect(context, image: PIL.Image.Image) -> list[models.LabeledShapeRequest]:
            return [cvataa.rectangle(123, [1, 2, 3, 4])]

        cvataa.annotate_task(self.client, self.task.id, namespace(spec=spec, detect=detect))

    @pytest.mark.parametrize(
        ["label_name", "conv_mask_to_poly"],
        [
            ("person-mask", False),
            ("person-poly", True),
        ],
    )
    def test_type_compatibility_cmtp(self, label_name: str, conv_mask_to_poly: bool) -> None:
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec(label_name, 123, type="mask"),
            ]
        )

        def detect(
            context: cvataa.DetectionFunctionContext, image: PIL.Image.Image
        ) -> list[models.LabeledShapeRequest]:
            if context.conv_mask_to_poly:
                return [cvataa.polygon(123, [1, 2, 3, 4, 5, 6])]
            else:
                return [cvataa.mask(123, [1, 0, 0, 0, 0])]

        cvataa.annotate_task(
            self.client,
            self.task.id,
            namespace(spec=spec, detect=detect),
            conv_mask_to_poly=conv_mask_to_poly,
        )

    def _test_spec_dataset_mismatch(
        self, exc_match: str, spec: cvataa.DetectionFunctionSpec, *, conv_mask_to_poly: bool = False
    ) -> None:
        def detect(context, image):
            assert False

        with pytest.raises(cvataa.BadFunctionError, match=exc_match):
            cvataa.annotate_task(
                self.client,
                self.task.id,
                namespace(spec=spec, detect=detect),
                conv_mask_to_poly=conv_mask_to_poly,
            )

    def test_label_not_in_dataset(self):
        self._test_spec_dataset_mismatch(
            "not in dataset",
            cvataa.DetectionFunctionSpec(labels=[cvataa.label_spec("dog", 123)]),
        )

    def test_sublabel_not_in_dataset(self):
        self._test_spec_dataset_mismatch(
            "sublabel .+ not in dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec("cat", 123, [cvataa.keypoint_spec("nose", 1)]),
                ],
            ),
        )

    def test_incompatible_label_type(self):
        self._test_spec_dataset_mismatch(
            "has type 'ellipse' in the function, but 'rectangle' in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.label_spec("person-rect", 123, type="ellipse"),
                ],
            ),
        )

        self._test_spec_dataset_mismatch(
            "has type 'polygon' in the function, but 'mask' in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.label_spec("person-mask", 123, type="mask"),
                ],
            ),
            conv_mask_to_poly=True,
        )

    def test_attribute_not_in_dataset(self):
        self._test_spec_dataset_mismatch(
            "attribute .+ not in dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [],
                        attributes=[cvataa.text_attribute_spec("breed", 1)],
                    ),
                ]
            ),
        )

        self._test_spec_dataset_mismatch(
            "attribute .+ not in dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [
                            cvataa.keypoint_spec(
                                "head",
                                12,
                                attributes=[cvataa.text_attribute_spec("orientation", 1)],
                            ),
                        ],
                    ),
                ]
            ),
        )

    def test_mismatched_attribute_input_type(self):
        self._test_spec_dataset_mismatch(
            "has input type .+ in the function, but .+ in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [],
                        attributes=[cvataa.text_attribute_spec("color", 1)],
                    ),
                ]
            ),
        )

        self._test_spec_dataset_mismatch(
            "has input type .+ in the function, but .+ in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [
                            cvataa.keypoint_spec(
                                "head",
                                12,
                                attributes=[cvataa.text_attribute_spec("size", 1)],
                            )
                        ],
                    ),
                ]
            ),
        )

    def test_mismatched_attribute_values(self):
        self._test_spec_dataset_mismatch(
            "has values .+ in the function, but .+ in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [],
                        attributes=[cvataa.select_attribute_spec("color", 1, ["red", "green"])],
                    ),
                ]
            ),
        )

        self._test_spec_dataset_mismatch(
            "has values .+ in the function, but .+ in the dataset",
            cvataa.DetectionFunctionSpec(
                labels=[
                    cvataa.skeleton_label_spec(
                        "cat",
                        123,
                        [
                            cvataa.keypoint_spec(
                                "head",
                                12,
                                attributes=[
                                    cvataa.number_attribute_spec("size", 1, ["-10", "0", "1"])
                                ],
                            ),
                        ],
                    ),
                ]
            ),
        )

    def _test_bad_function_detect(self, detect, exc_match: str) -> None:
        spec = cvataa.DetectionFunctionSpec(
            labels=[
                cvataa.label_spec("person", 123),
                cvataa.label_spec("person", 124, type="rectangle"),
                cvataa.label_spec("person-rect", 125),
                cvataa.label_spec("person-rect", 126, type="rectangle"),
                cvataa.skeleton_label_spec(
                    "cat",
                    456,
                    [
                        cvataa.keypoint_spec(
                            "head",
                            12,
                            attributes=[
                                cvataa.number_attribute_spec(
                                    "size", 1, number_attribute_values(1, 10, 1)
                                ),
                            ],
                        ),
                        cvataa.keypoint_spec("tail", 34),
                    ],
                    attributes=[cvataa.select_attribute_spec("color", 1, ["gray", "calico"])],
                ),
            ],
        )

        with pytest.raises(cvataa.BadFunctionError, match=exc_match):
            cvataa.annotate_task(self.client, self.task.id, namespace(spec=spec, detect=detect))

    def test_preset_shape_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                models.LabeledShapeRequest(
                    type="rectangle", frame=0, label_id=123, id=1111, points=[1, 2, 3, 4]
                ),
            ],
            "shape with preset id",
        )

    def test_preset_shape_source(self):
        self._test_bad_function_detect(
            lambda context, image: [
                models.LabeledShapeRequest(
                    type="rectangle", frame=0, label_id=123, source="manual", points=[1, 2, 3, 4]
                ),
            ],
            "shape with preset source",
        )

    def test_bad_shape_frame_number(self):
        self._test_bad_function_detect(
            lambda context, image: [
                models.LabeledShapeRequest(
                    type="rectangle",
                    frame=1,
                    label_id=123,
                    points=[1, 2, 3, 4],
                ),
            ],
            "unexpected frame number",
        )

    def test_unknown_label_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.rectangle(111, [1, 2, 3, 4]),
            ],
            "unknown label ID",
        )

    def test_preset_element_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        models.SubLabeledShapeRequest(
                            type="points", frame=0, label_id=12, id=1111, points=[1, 2]
                        ),
                    ],
                ),
            ],
            "element with preset id",
        )

    def test_preset_element_source(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        models.SubLabeledShapeRequest(
                            type="points", frame=0, label_id=12, source="manual", points=[1, 2]
                        ),
                    ],
                ),
            ],
            "element with preset source",
        )

    def test_bad_element_frame_number(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        models.SubLabeledShapeRequest(
                            type="points", frame=1, label_id=12, points=[1, 2]
                        ),
                    ],
                ),
            ],
            "element with unexpected frame number",
        )

    def test_non_points_element(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        models.SubLabeledShapeRequest(
                            type="rectangle", frame=0, label_id=12, points=[1, 2, 3, 4]
                        ),
                    ],
                ),
            ],
            "element type other than 'points'",
        )

    def test_unknown_sublabel_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(456, [cvataa.keypoint(56, [1, 2])]),
            ],
            "unknown sublabel ID",
        )

    def test_multiple_elements_with_same_sublabel(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        cvataa.keypoint(12, [1, 2]),
                        cvataa.keypoint(12, [3, 4]),
                    ],
                ),
            ],
            "multiple elements with same sublabel",
        )

    def test_not_enough_elements(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(456, [cvataa.keypoint(12, [1, 2])]),
            ],
            "with fewer elements than expected",
        )

    def test_non_skeleton_with_elements(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.shape(
                    123,
                    type="rectangle",
                    elements=[cvataa.keypoint(12, [1, 2])],
                ),
            ],
            "non-skeleton shape with elements",
        )

    @pytest.mark.parametrize("label_id", [124, 125, 126])
    def test_incompatible_shape_type(self, label_id: int):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.shape(label_id, type="ellipse"),
            ],
            r"shape of type 'ellipse' \(expected 'rectangle'\)",
        )

    def test_attribute_val_with_unknown_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [cvataa.keypoint(12, [1, 2]), cvataa.keypoint(34, [3, 4])],
                    attributes=attribute_vals_from_dict({2: "gray"}),
                ),
            ],
            "attribute with unknown ID",
        )

        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        cvataa.keypoint(12, [1, 2], attributes=attribute_vals_from_dict({2: 5})),
                        cvataa.keypoint(34, [3, 4]),
                    ],
                ),
            ],
            "attribute with unknown ID",
        )

    def test_multiple_attribute_vals_with_same_id(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [cvataa.keypoint(12, [1, 2]), cvataa.keypoint(34, [3, 4])],
                    attributes=[
                        models.AttributeValRequest(spec_id=1, value="gray"),
                        models.AttributeValRequest(spec_id=1, value="gray"),
                    ],
                ),
            ],
            "multiple attributes with same ID",
        )

        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        cvataa.keypoint(
                            12,
                            [1, 2],
                            attributes=[
                                models.AttributeValRequest(spec_id=1, value="5"),
                                models.AttributeValRequest(spec_id=1, value="5"),
                            ],
                        ),
                        cvataa.keypoint(34, [3, 4]),
                    ],
                ),
            ],
            "multiple attributes with same ID",
        )

    def test_attribute_val_unsuitable_for_spec(self):
        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [cvataa.keypoint(12, [1, 2]), cvataa.keypoint(34, [3, 4])],
                    attributes=attribute_vals_from_dict({1: "purple"}),
                ),
            ],
            "unsuitable for its attribute",
        )

        self._test_bad_function_detect(
            lambda context, image: [
                cvataa.skeleton(
                    456,
                    [
                        cvataa.keypoint(
                            12,
                            [1, 2],
                            attributes=attribute_vals_from_dict({1: -1}),
                        ),
                        cvataa.keypoint(34, [3, 4]),
                    ],
                ),
            ],
            "unsuitable for its attribute",
        )


if torchvision_models is not None:
    import torch
    import torch.nn as nn

    class FakeTorchvisionDetector(nn.Module):
        def __init__(self, label_id: int) -> None:
            super().__init__()
            self._label_id = label_id

        def forward(self, images: list[torch.Tensor]) -> list[dict]:
            assert isinstance(images, list)
            assert all(isinstance(t, torch.Tensor) for t in images)

            return [
                {
                    "boxes": torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]]),
                    "labels": torch.tensor([self._label_id, self._label_id]),
                    "scores": torch.tensor([0.75, 0.74]),
                }
            ]

    def fake_get_detection_model(name: str, weights, test_param):
        assert test_param == "expected_value"

        car_label_id = weights.meta["categories"].index("car")

        return FakeTorchvisionDetector(label_id=car_label_id)

    class FakeTorchvisionInstanceSegmenter(nn.Module):
        def __init__(self, label_id: int) -> None:
            super().__init__()
            self._label_id = label_id

        def forward(self, images: list[torch.Tensor]) -> list[dict]:
            assert isinstance(images, list)
            assert all(isinstance(t, torch.Tensor) for t in images)

            def make_box(im, a1, a2):
                return [im.shape[2] * a1, im.shape[1] * a1, im.shape[2] * a2, im.shape[1] * a2]

            def make_mask(im, a1, a2):
                # creates a rectangular mask with a hole
                mask = torch.full((1, im.shape[1], im.shape[2]), 0.49)
                mask[
                    0,
                    math.ceil(im.shape[1] * a1) : math.floor(im.shape[1] * a2),
                    math.ceil(im.shape[2] * a1) : math.floor(im.shape[2] * a2),
                ] = 0.5
                mask[
                    0,
                    math.ceil(im.shape[1] * a1) + 3 : math.floor(im.shape[1] * a2) - 3,
                    math.ceil(im.shape[2] * a1) + 3 : math.floor(im.shape[2] * a2) - 3,
                ] = 0.49
                return mask

            return [
                {
                    "labels": torch.tensor([self._label_id, self._label_id]),
                    "boxes": torch.tensor(
                        [
                            make_box(im, 1 / 6, 1 / 3),
                            make_box(im, 2 / 3, 5 / 6),
                        ]
                    ),
                    "masks": torch.stack(
                        [
                            make_mask(im, 1 / 6, 1 / 3),
                            make_mask(im, 2 / 3, 5 / 6),
                        ]
                    ),
                    "scores": torch.tensor([0.75, 0.74]),
                }
                for im in images
            ]

    def fake_get_instance_segmentation_model(name: str, weights, test_param):
        assert test_param == "expected_value"

        car_label_id = weights.meta["categories"].index("car")

        return FakeTorchvisionInstanceSegmenter(label_id=car_label_id)

    class FakeTorchvisionKeypointDetector(nn.Module):
        def __init__(self, label_id: int, keypoint_names: list[str]) -> None:
            super().__init__()
            self._label_id = label_id
            self._keypoint_names = keypoint_names

        def forward(self, images: list[torch.Tensor]) -> list[dict]:
            assert isinstance(images, list)
            assert all(isinstance(t, torch.Tensor) for t in images)

            return [
                {
                    "labels": torch.tensor([self._label_id, self._label_id]),
                    "keypoints": torch.tensor(
                        [
                            [
                                [hash(name) % 100, 0, 1 if name.startswith("right_") else 0]
                                for i, name in enumerate(self._keypoint_names)
                            ],
                            [[0, 0, 1] for i, name in enumerate(self._keypoint_names)],
                        ]
                    ),
                    "scores": torch.tensor([0.75, 0.74]),
                }
            ]

    def fake_get_keypoint_detection_model(name: str, weights, test_param):
        assert test_param == "expected_value"

        person_label_id = weights.meta["categories"].index("person")

        return FakeTorchvisionKeypointDetector(
            label_id=person_label_id, keypoint_names=weights.meta["keypoint_names"]
        )


@pytest.mark.skipif(torchvision_models is None, reason="torchvision is not installed")
class TestAutoAnnotationFunctions:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
    ):
        self.client = fxt_login[0]

        self.image_dir = tmp_path / "images"
        self.image_dir.mkdir()

    def _create_task(self, labels):
        self.image = generate_image_file("1.png", size=(100, 100))
        image_path = self.image_dir / self.image.name
        image_path.write_bytes(self.image.getbuffer())

        self.task = self.client.tasks.create_from_data(
            models.TaskWriteRequest("Auto-annotation test task", labels=labels),
            resources=[image_path],
        )

        task_labels = self.task.get_labels()
        self.task_labels_by_id = {label.id: label for label in task_labels}

    def test_torchvision_detection(self, monkeypatch: pytest.MonkeyPatch):
        self._create_task([models.PatchedLabelRequest(name="car", type="rectangle")])

        monkeypatch.setattr(torchvision_models, "get_model", fake_get_detection_model)

        import cvat_sdk.auto_annotation.functions.torchvision_detection as td

        cvataa.annotate_task(
            self.client,
            self.task.id,
            td.create("fasterrcnn_resnet50_fpn_v2", "COCO_V1", test_param="expected_value"),
            allow_unmatched_labels=True,
            conf_threshold=0.75,
        )

        annotations = self.task.get_annotations()

        assert len(annotations.shapes) == 1
        assert self.task_labels_by_id[annotations.shapes[0].label_id].name == "car"
        assert annotations.shapes[0].type.value == "rectangle"
        assert annotations.shapes[0].points == [1, 2, 3, 4]

    def test_torchvision_instance_segmentation(self, monkeypatch: pytest.MonkeyPatch):
        self._create_task([models.PatchedLabelRequest(name="car")])

        monkeypatch.setattr(torchvision_models, "get_model", fake_get_instance_segmentation_model)

        import cvat_sdk.auto_annotation.functions.torchvision_instance_segmentation as tis
        from cvat_sdk.masks import encode_mask

        cvataa.annotate_task(
            self.client,
            self.task.id,
            tis.create("maskrcnn_resnet50_fpn_v2", "COCO_V1", test_param="expected_value"),
            allow_unmatched_labels=True,
            conf_threshold=0.75,
        )

        annotations = self.task.get_annotations()

        assert len(annotations.shapes) == 1
        assert self.task_labels_by_id[annotations.shapes[0].label_id].name == "car"

        expected_bitmap = torch.zeros((100, 100), dtype=torch.bool)
        expected_bitmap[17:33, 17:33] = True
        expected_bitmap[20:30, 20:30] = False

        assert annotations.shapes[0].type.value == "mask"
        assert annotations.shapes[0].points == encode_mask(expected_bitmap, [16, 16, 34, 34])

        cvataa.annotate_task(
            self.client,
            self.task.id,
            tis.create("maskrcnn_resnet50_fpn_v2", "COCO_V1", test_param="expected_value"),
            allow_unmatched_labels=True,
            conf_threshold=0.75,
            conv_mask_to_poly=True,
            clear_existing=True,
        )

        annotations = self.task.get_annotations()

        assert len(annotations.shapes) == 1
        assert self.task_labels_by_id[annotations.shapes[0].label_id].name == "car"
        assert annotations.shapes[0].type.value == "polygon"

        # We shouldn't rely on the exact result of polygon conversion,
        # since it depends on a 3rd-party library. Instead, we'll just
        # check that all points are within the expected area.
        for x, y in zip(*[iter(annotations.shapes[0].points)] * 2):
            assert expected_bitmap[round(y), round(x)]

    def test_torchvision_keypoint_detection(self, monkeypatch: pytest.MonkeyPatch):
        self._create_task(
            [
                models.PatchedLabelRequest(
                    name="person",
                    type="skeleton",
                    sublabels=[
                        models.SublabelRequest(name="left_eye"),
                        models.SublabelRequest(name="right_eye"),
                    ],
                ),
            ]
        )
        person_label = next(
            label for label in self.task_labels_by_id.values() if label.name == "person"
        )
        person_sublabels_by_id = {sl.id: sl for sl in person_label.sublabels}

        monkeypatch.setattr(torchvision_models, "get_model", fake_get_keypoint_detection_model)

        import cvat_sdk.auto_annotation.functions.torchvision_keypoint_detection as tkd

        cvataa.annotate_task(
            self.client,
            self.task.id,
            tkd.create("keypointrcnn_resnet50_fpn", "COCO_V1", test_param="expected_value"),
            allow_unmatched_labels=True,
            conf_threshold=0.75,
        )

        annotations = self.task.get_annotations()

        assert len(annotations.shapes) == 1
        assert self.task_labels_by_id[annotations.shapes[0].label_id].name == "person"
        assert annotations.shapes[0].type.value == "skeleton"
        assert len(annotations.shapes[0].elements) == 2

        elements = sorted(
            annotations.shapes[0].elements,
            key=lambda e: person_sublabels_by_id[e.label_id].name,
        )

        assert person_sublabels_by_id[elements[0].label_id].name == "left_eye"
        assert elements[0].points[0] == hash("left_eye") % 100
        assert elements[0].occluded

        assert person_sublabels_by_id[elements[1].label_id].name == "right_eye"
        assert elements[1].points[0] == hash("right_eye") % 100
        assert not elements[1].occluded


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_client.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from contextlib import ExitStack
from logging import Logger

import packaging.version as pv
import pytest
from cvat_sdk import Client, models
from cvat_sdk.core.client import Config, make_client
from cvat_sdk.core.exceptions import IncompatibleVersionException, InvalidHostException
from cvat_sdk.exceptions import ApiException

from shared.utils.config import BASE_URL, USER_PASS


class TestClientUsecases:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        restore_db_per_function,  # force fixture call order to allow DB setup
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_client: Client,
        fxt_stdout: io.StringIO,
        admin_user: str,
    ):
        _, self.logger_stream = fxt_logger
        self.client = fxt_client
        self.stdout = fxt_stdout
        self.user = admin_user

        yield

    def test_can_login_with_basic_auth(self):
        self.client.login((self.user, USER_PASS))

        assert self.client.has_credentials()

    def test_can_fail_to_login_with_basic_auth(self):
        with pytest.raises(ApiException):
            self.client.login((self.user, USER_PASS + "123"))

    def test_can_logout(self):
        self.client.login((self.user, USER_PASS))

        self.client.logout()

        assert not self.client.has_credentials()

    def test_can_get_server_version(self):
        self.client.login((self.user, USER_PASS))

        version = self.client.get_server_version()

        assert (version.major, version.minor) >= (2, 0)


def test_can_strip_trailing_slash_in_hostname_in_make_client(admin_user: str):
    host, port = BASE_URL.split("://", maxsplit=1)[1].rsplit(":", maxsplit=1)

    with make_client(host=host + "/", port=port, credentials=(admin_user, USER_PASS)) as client:
        assert client.api_map.host == BASE_URL


def test_can_strip_trailing_slash_in_hostname_in_client_ctor(admin_user: str):
    with Client(url=BASE_URL + "/") as client:
        client.login((admin_user, USER_PASS))
        assert client.api_map.host == BASE_URL


def test_can_detect_server_schema_if_not_provided():
    host, port = BASE_URL.split("://", maxsplit=1)[1].rsplit(":", maxsplit=1)
    client = make_client(host=host, port=int(port))
    assert client.api_map.host == "http://" + host + ":" + port


def test_can_fail_to_detect_server_schema_if_not_provided():
    host, port = BASE_URL.split("://", maxsplit=1)[1].rsplit(":", maxsplit=1)
    with pytest.raises(InvalidHostException) as capture:
        make_client(host=host, port=int(port) + 1)

    assert capture.match(r"Failed to detect host schema automatically")


def test_can_reject_invalid_server_schema():
    host, port = BASE_URL.split("://", maxsplit=1)[1].rsplit(":", maxsplit=1)
    with pytest.raises(InvalidHostException) as capture:
        make_client(host="ftp://" + host, port=int(port) + 1)

    assert capture.match(r"Invalid url schema 'ftp'")


@pytest.mark.parametrize("raise_exception", (True, False))
def test_can_warn_on_mismatching_server_version(
    fxt_logger: tuple[Logger, io.StringIO], monkeypatch, raise_exception: bool
):
    logger, logger_stream = fxt_logger

    def mocked_version(_):
        return pv.Version("0")

    monkeypatch.setattr(Client, "get_server_version", mocked_version)

    config = Config()

    with ExitStack() as es:
        if raise_exception:
            config.allow_unsupported_server = False
            es.enter_context(pytest.raises(IncompatibleVersionException))

        Client(url=BASE_URL, logger=logger, config=config)

    assert "Server version '0' is not compatible with SDK version" in logger_stream.getvalue()


@pytest.mark.parametrize("do_check", (True, False))
def test_can_check_server_version_in_ctor(
    fxt_logger: tuple[Logger, io.StringIO], monkeypatch, do_check: bool
):
    logger, logger_stream = fxt_logger

    def mocked_version(_):
        return pv.Version("0")

    monkeypatch.setattr(Client, "get_server_version", mocked_version)

    config = Config()
    config.allow_unsupported_server = False

    with ExitStack() as es:
        if do_check:
            es.enter_context(pytest.raises(IncompatibleVersionException))

        Client(url=BASE_URL, logger=logger, config=config, check_server_version=do_check)

    assert (
        "Server version '0' is not compatible with SDK version" in logger_stream.getvalue()
    ) == do_check


def test_can_check_server_version_in_method(fxt_logger: tuple[Logger, io.StringIO], monkeypatch):
    logger, logger_stream = fxt_logger

    def mocked_version(_):
        return pv.Version("0")

    monkeypatch.setattr(Client, "get_server_version", mocked_version)

    config = Config()
    config.allow_unsupported_server = False
    client = Client(url=BASE_URL, logger=logger, config=config, check_server_version=False)

    with client, pytest.raises(IncompatibleVersionException):
        client.check_server_version()

    assert "Server version '0' is not compatible with SDK version" in logger_stream.getvalue()


@pytest.mark.parametrize(
    "server_version, supported_versions, expect_supported",
    [
        # Currently, it is ~=, as defined in https://peps.python.org/pep-0440/
        ("3.2", ["2.0"], False),
        ("2", ["2.1"], False),
        ("2.1", ["2.1"], True),
        ("2.1a", ["2.1"], False),
        ("2.1.post1", ["2.1"], True),
        ("2.1", ["2.1.pre1"], True),
        ("2.1.1", ["2.1"], True),
        ("2.2", ["2.1"], False),
        ("2.2", ["2.1.0", "2.3"], False),
        ("2.2", ["2.1", "2.2", "2.3"], True),
        ("2.2.post1", ["2.1", "2.2", "2.3"], True),
        ("2.2.pre1", ["2.1", "2.2", "2.3"], False),
        ("2.2", ["2.3"], False),
        ("2.1.0.dev123", ["2.1.post2"], False),
        ("1!1.3", ["2.1"], False),
        ("1!1.3.1", ["2.1", "1!1.3"], True),
        ("1!1.1.dev12", ["1!1.1"], False),
    ],
)
def test_can_check_server_version_compatibility(
    fxt_logger: tuple[Logger, io.StringIO],
    monkeypatch: pytest.MonkeyPatch,
    server_version: str,
    supported_versions: list[str],
    expect_supported: bool,
):
    logger, _ = fxt_logger

    monkeypatch.setattr(Client, "get_server_version", lambda _: pv.Version(server_version))
    monkeypatch.setattr(
        Client, "SUPPORTED_SERVER_VERSIONS", [pv.Version(v) for v in supported_versions]
    )
    config = Config(allow_unsupported_server=False)

    with ExitStack() as es:
        if not expect_supported:
            es.enter_context(pytest.raises(IncompatibleVersionException))

        Client(url=BASE_URL, logger=logger, config=config, check_server_version=True)


@pytest.mark.parametrize("verify", [True, False])
def test_can_control_ssl_verification_with_config(verify: bool):
    config = Config(verify_ssl=verify)

    client = Client(BASE_URL, config=config)

    assert client.api_client.configuration.verify_ssl == verify


def test_organization_contexts(admin_user: str):
    with make_client(BASE_URL, credentials=(admin_user, USER_PASS)) as client:
        assert client.organization_slug is None

        org = client.organizations.create(models.OrganizationWriteRequest(slug="testorg"))

        # create a project in the personal workspace
        client.organization_slug = ""
        personal_project = client.projects.create(models.ProjectWriteRequest(name="Personal"))
        assert personal_project.organization is None

        # create a project in the organization
        client.organization_slug = org.slug
        org_project = client.projects.create(models.ProjectWriteRequest(name="Org"))
        assert org_project.organization == org.id

        # both projects should be visible with no context
        client.organization_slug = None
        client.projects.retrieve(personal_project.id)
        client.projects.retrieve(org_project.id)

        # retrieve personal and org projects by id
        client.organization_slug = ""
        client.projects.retrieve(personal_project.id)
        client.projects.retrieve(org_project.id)

        # org context doesn't make sense for detailed request
        client.organization_slug = org.slug
        client.projects.retrieve(org_project.id)
        client.projects.retrieve(personal_project.id)


@pytest.mark.usefixtures("restore_db_per_function")
def test_organization_filtering(regular_lonely_user: str, fxt_image_file):
    with make_client(BASE_URL, credentials=(regular_lonely_user, USER_PASS)) as client:
        org = client.organizations.create(models.OrganizationWriteRequest(slug="testorg"))

        # create a project and task in sandbox
        client.organization_slug = None
        client.projects.create(models.ProjectWriteRequest(name="personal_project"))
        client.tasks.create_from_data(
            spec={"name": "personal_task", "labels": [{"name": "a"}]}, resources=[fxt_image_file]
        )

        # create a project and task in the organization
        client.organization_slug = org.slug
        client.projects.create(models.ProjectWriteRequest(name="org_project"))
        client.tasks.create_from_data(
            spec={"name": "org_task", "labels": [{"name": "a"}]}, resources=[fxt_image_file]
        )

        # return only non-org objects if org parameter is empty
        client.organization_slug = ""
        projects, tasks, jobs = client.projects.list(), client.tasks.list(), client.jobs.list()

        assert len(projects) == len(tasks) == len(jobs) == 1
        assert projects[0].organization == tasks[0].organization == jobs[0].organization == None

        # return all objects if org parameter wasn't presented
        client.organization_slug = None
        projects, tasks, jobs = client.projects.list(), client.tasks.list(), client.jobs.list()

        assert len(projects) == len(tasks) == len(jobs) == 2
        assert {None, org.id} == set([a.organization for a in (*projects, *tasks, *jobs)])

        # return only org objects if org parameter is presented and not empty
        client.organization_slug = org.slug
        projects, tasks, jobs = client.projects.list(), client.tasks.list(), client.jobs.list()

        assert len(projects) == len(tasks) == len(jobs) == 1
        assert projects[0].organization == tasks[0].organization == jobs[0].organization == org.id


def test_organization_context_manager():
    client = Client(BASE_URL)

    client.organization_slug = "abc"

    with client.organization_context("def"):
        assert client.organization_slug == "def"

    assert client.organization_slug == "abc"


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_datasets.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger
from pathlib import Path

import cvat_sdk.datasets as cvatds
import PIL.Image
import pytest
from cvat_sdk import Client, models
from cvat_sdk.core.proxies.tasks import ResourceType

from shared.utils.helpers import generate_image_files

from .util import restrict_api_requests


@pytest.fixture(autouse=True)
def _common_setup(
    tmp_path: Path,
    fxt_login: tuple[Client, str],
    fxt_logger: tuple[Logger, io.StringIO],
    restore_redis_ondisk_per_function,
    restore_redis_inmem_per_function,
):
    logger = fxt_logger[0]
    client = fxt_login[0]
    client.logger = logger
    client.config.cache_dir = tmp_path / "cache"

    api_client = client.api_client
    for k in api_client.configuration.logger:
        api_client.configuration.logger[k] = logger


class TestTaskDataset:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
    ):
        self.client = fxt_login[0]
        self.images = generate_image_files(10)

        image_dir = tmp_path / "images"
        image_dir.mkdir()

        image_paths = []
        for image in self.images:
            image_path = image_dir / image.name
            image_path.write_bytes(image.getbuffer())
            image_paths.append(image_path)

        self.task = self.client.tasks.create_from_data(
            models.TaskWriteRequest(
                "Dataset layer test task",
                labels=[
                    models.PatchedLabelRequest(name="person"),
                    models.PatchedLabelRequest(name="car"),
                ],
            ),
            resource_type=ResourceType.LOCAL,
            resources=image_paths,
            data_params={"chunk_size": 3},
        )

        self.expected_labels = sorted(self.task.get_labels(), key=lambda l: l.id)

        self.task.update_annotations(
            models.PatchedLabeledDataRequest(
                tags=[
                    models.LabeledImageRequest(frame=8, label_id=self.expected_labels[0].id),
                    models.LabeledImageRequest(frame=8, label_id=self.expected_labels[1].id),
                ],
                shapes=[
                    models.LabeledShapeRequest(
                        frame=6,
                        label_id=self.expected_labels[1].id,
                        type=models.ShapeType("rectangle"),
                        points=[1.0, 2.0, 3.0, 4.0],
                    ),
                ],
            )
        )

    def test_basic(self):
        dataset = cvatds.TaskDataset(self.client, self.task.id)

        # verify that the cache is not empty
        assert list(self.client.config.cache_dir.iterdir())

        for expected_label, actual_label in zip(
            self.expected_labels, sorted(dataset.labels, key=lambda l: l.id)
        ):
            assert expected_label.id == actual_label.id
            assert expected_label.name == actual_label.name

        assert len(dataset.samples) == self.task.size

        for index, sample in enumerate(dataset.samples):
            assert sample.frame_index == index
            assert sample.frame_name == self.images[index].name

            actual_image = sample.media.load_image()
            expected_image = PIL.Image.open(self.images[index])

            assert actual_image == expected_image

        assert not dataset.samples[0].annotations.tags
        assert not dataset.samples[1].annotations.shapes

        assert {tag.label_id for tag in dataset.samples[8].annotations.tags} == {
            label.id for label in self.expected_labels
        }
        assert not dataset.samples[8].annotations.shapes

        assert not dataset.samples[6].annotations.tags
        assert len(dataset.samples[6].annotations.shapes) == 1
        assert dataset.samples[6].annotations.shapes[0].type.value == "rectangle"
        assert dataset.samples[6].annotations.shapes[0].points == [1.0, 2.0, 3.0, 4.0]

    def test_deleted_frame(self):
        self.task.remove_frames_by_ids([1])

        dataset = cvatds.TaskDataset(self.client, self.task.id)

        assert len(dataset.samples) == self.task.size - 1

        # sample #0 is still frame #0
        assert dataset.samples[0].frame_index == 0
        assert dataset.samples[0].media.load_image() == PIL.Image.open(self.images[0])

        # sample #1 is now frame #2
        assert dataset.samples[1].frame_index == 2
        assert dataset.samples[1].media.load_image() == PIL.Image.open(self.images[2])

        # sample #5 is now frame #6
        assert dataset.samples[5].frame_index == 6
        assert dataset.samples[5].media.load_image() == PIL.Image.open(self.images[6])
        assert len(dataset.samples[5].annotations.shapes) == 1

    def test_offline(self, monkeypatch: pytest.MonkeyPatch):
        dataset = cvatds.TaskDataset(
            self.client,
            self.task.id,
            update_policy=cvatds.UpdatePolicy.IF_MISSING_OR_STALE,
        )

        fresh_samples = list(dataset.samples)

        restrict_api_requests(monkeypatch)

        dataset = cvatds.TaskDataset(
            self.client,
            self.task.id,
            update_policy=cvatds.UpdatePolicy.NEVER,
        )

        cached_samples = list(dataset.samples)

        for fresh_sample, cached_sample in zip(fresh_samples, cached_samples):
            assert fresh_sample.frame_index == cached_sample.frame_index
            assert fresh_sample.annotations == cached_sample.annotations
            assert fresh_sample.media.load_image() == cached_sample.media.load_image()

    def test_update(self, monkeypatch: pytest.MonkeyPatch):
        dataset = cvatds.TaskDataset(
            self.client,
            self.task.id,
        )

        # Recreating the dataset should only result in minimal requests.
        restrict_api_requests(
            monkeypatch, allow_paths={f"/api/tasks/{self.task.id}", "/api/labels"}
        )

        dataset = cvatds.TaskDataset(
            self.client,
            self.task.id,
        )

        assert dataset.samples[6].annotations.shapes[0].label_id == self.expected_labels[1].id

        # After an update, the annotations should be redownloaded.
        monkeypatch.undo()

        self.task.update_annotations(
            models.PatchedLabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        id=dataset.samples[6].annotations.shapes[0].id,
                        frame=6,
                        label_id=self.expected_labels[0].id,
                        type=models.ShapeType("rectangle"),
                        points=[1.0, 2.0, 3.0, 4.0],
                    ),
                ]
            )
        )

        dataset = cvatds.TaskDataset(
            self.client,
            self.task.id,
        )

        assert dataset.samples[6].annotations.shapes[0].label_id == self.expected_labels[0].id

    def test_no_annotations(self):
        dataset = cvatds.TaskDataset(self.client, self.task.id, load_annotations=False)

        for index, sample in enumerate(dataset.samples):
            assert sample.frame_index == index
            assert sample.frame_name == self.images[index].name

            actual_image = sample.media.load_image()
            expected_image = PIL.Image.open(self.images[index])

            assert actual_image == expected_image

            assert sample.annotations is None


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_issues_comments.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger
from pathlib import Path

import pytest
from cvat_sdk import Client
from cvat_sdk.api_client import exceptions, models
from cvat_sdk.core.proxies.tasks import ResourceType, Task


class TestIssuesUsecases:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

    @pytest.fixture
    def fxt_new_task(self, fxt_image_file: Path):
        task = self.client.tasks.create_from_data(
            spec={
                "name": "test_task",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
            resource_type=ResourceType.LOCAL,
            resources=[fxt_image_file],
            data_params={"image_quality": 80},
        )

        return task

    def test_can_retrieve_issue(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )

        retrieved_issue = self.client.issues.retrieve(issue.id)

        assert issue.id == retrieved_issue.id
        assert self.stdout.getvalue() == ""

    def test_can_list_issues(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
                assignee=self.client.users.list()[0].id,
            )
        )

        issues = self.client.issues.list()

        assert any(issue.id == j.id for j in issues)
        assert self.stdout.getvalue() == ""

    def test_can_list_comments(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comment = self.client.comments.create(models.CommentWriteRequest(issue.id, message="hi!"))
        issue.fetch()

        comment_ids = {c.id for c in issue.get_comments()}

        assert len(comment_ids) == 2
        assert comment.id in comment_ids
        assert self.stdout.getvalue() == ""

    def test_can_modify_issue(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )

        issue.update(models.PatchedIssueWriteRequest(resolved=True))

        retrieved_issue = self.client.issues.retrieve(issue.id)
        assert retrieved_issue.resolved is True
        assert issue.resolved == retrieved_issue.resolved
        assert self.stdout.getvalue() == ""

    def test_can_remove_issue(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comments = issue.get_comments()

        issue.remove()

        with pytest.raises(exceptions.NotFoundException):
            issue.fetch()
        with pytest.raises(exceptions.NotFoundException):
            self.client.comments.retrieve(comments[0].id)
        assert self.stdout.getvalue() == ""


class TestCommentsUsecases:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

    @pytest.fixture
    def fxt_new_task(self, fxt_image_file: Path):
        task = self.client.tasks.create_from_data(
            spec={
                "name": "test_task",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
            resource_type=ResourceType.LOCAL,
            resources=[fxt_image_file],
            data_params={"image_quality": 80},
        )

        return task

    def test_can_retrieve_comment(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comment = self.client.comments.create(models.CommentWriteRequest(issue.id, message="hi!"))

        retrieved_comment = self.client.comments.retrieve(comment.id)

        assert comment.id == retrieved_comment.id
        assert self.stdout.getvalue() == ""

    def test_can_list_comments(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comment = self.client.comments.create(models.CommentWriteRequest(issue.id, message="hi!"))

        comments = self.client.comments.list()

        assert any(comment.id == c.id for c in comments)
        assert self.stdout.getvalue() == ""

    def test_can_modify_comment(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comment = self.client.comments.create(models.CommentWriteRequest(issue.id, message="hi!"))

        comment.update(models.PatchedCommentWriteRequest(message="bar"))

        retrieved_comment = self.client.comments.retrieve(comment.id)
        assert retrieved_comment.message == "bar"
        assert comment.message == retrieved_comment.message
        assert self.stdout.getvalue() == ""

    def test_can_remove_comment(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )
        comment = self.client.comments.create(models.CommentWriteRequest(issue.id, message="hi!"))

        comment.remove()

        with pytest.raises(exceptions.NotFoundException):
            comment.fetch()
        assert self.stdout.getvalue() == ""


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_jobs.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger
from pathlib import Path
from typing import Optional

import pytest
from cvat_sdk import Client
from cvat_sdk.api_client import models
from cvat_sdk.core.proxies.tasks import Task
from cvat_sdk.core.proxies.types import Location
from PIL import Image
from pytest_cases import fixture_ref, parametrize

from shared.fixtures.data import CloudStorageAssets

from .common import TestDatasetExport
from .util import make_pbar


class TestJobUsecases(TestDatasetExport):
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
        restore_redis_ondisk_per_function,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

    @pytest.fixture
    def fxt_task_with_shapes(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    ),
                ],
            )
        )

        return fxt_new_task

    def test_can_retrieve_job(self, fxt_new_task: Task):
        job_id = fxt_new_task.get_jobs()[0].id

        job = self.client.jobs.retrieve(job_id)

        assert job.id == job_id
        assert self.stdout.getvalue() == ""

    def test_can_list_jobs(self, fxt_new_task: Task):
        task_job_ids = set(j.id for j in fxt_new_task.get_jobs())

        jobs = self.client.jobs.list()

        assert len(task_job_ids) != 0
        assert task_job_ids.issubset(j.id for j in jobs)
        assert self.stdout.getvalue() == ""

    def test_can_update_job_field_directly(self, fxt_new_task: Task):
        job = self.client.jobs.list()[0]
        assert not job.assignee
        new_assignee = self.client.users.list()[0]

        job.update({"assignee": new_assignee.id})

        updated_job = self.client.jobs.retrieve(job.id)
        assert updated_job.assignee.id == new_assignee.id
        assert self.stdout.getvalue() == ""

    def test_can_get_labels(self, fxt_new_task: Task):
        expected_labels = {"car", "person"}

        received_labels = fxt_new_task.get_jobs()[0].get_labels()

        assert {obj.name for obj in received_labels} == expected_labels
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("format_name", ("CVAT for images 1.1",))
    @pytest.mark.parametrize("include_images", (True, False))
    @parametrize(
        "task, location",
        [
            (fixture_ref("fxt_new_task"), None),
            (fixture_ref("fxt_new_task"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_task"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (
                pytest.param(
                    fixture_ref("fxt_new_task_with_target_storage"),
                    None,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (fixture_ref("fxt_new_task_with_target_storage"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_task_with_target_storage"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
        ],
    )
    def test_can_export_dataset(
        self,
        format_name: str,
        include_images: bool,
        task: Task,
        location: Optional[Location],
        request: pytest.FixtureRequest,
        cloud_storages: CloudStorageAssets,
    ):
        job_id = task.get_jobs()[0].id
        job = self.client.jobs.retrieve(job_id)
        file_path = self.tmp_path / f"job_{job.id}-{format_name.lower()}.zip"
        self._test_can_export_dataset(
            job,
            format_name=format_name,
            file_path=file_path,
            include_images=include_images,
            location=location,
            request=request,
            cloud_storages=cloud_storages,
        )

    def test_can_download_preview(self, fxt_new_task: Task):
        frame_encoded = fxt_new_task.get_jobs()[0].get_preview()
        (width, height) = Image.open(frame_encoded).size

        assert width > 0 and height > 0
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    def test_can_download_frame(self, fxt_new_task: Task, quality: str):
        frame_encoded = fxt_new_task.get_jobs()[0].get_frame(0, quality=quality)
        (width, height) = Image.open(frame_encoded).size

        assert width > 0 and height > 0
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    @pytest.mark.parametrize("image_extension", (None, "bmp"))
    def test_can_download_frames(self, fxt_new_task: Task, quality: str, image_extension: str):
        fxt_new_task.get_jobs()[0].download_frames(
            [0],
            image_extension=image_extension,
            quality=quality,
            outdir=self.tmp_path,
            filename_pattern="frame-{frame_id}{frame_ext}",
        )

        if image_extension is not None:
            expected_frame_ext = image_extension
        else:
            if quality == "original":
                expected_frame_ext = "png"
            else:
                expected_frame_ext = "jpg"

        assert (self.tmp_path / f"frame-0.{expected_frame_ext}").is_file()
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("convert", [True, False])
    def test_can_convert_annotations_polygons_to_masks_param(
        self, fxt_new_task: Task, fxt_camvid_dataset: Path, convert: bool
    ):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        fxt_new_task.get_jobs()[0].import_annotations(
            format_name="CamVid 1.0",
            filename=fxt_camvid_dataset,
            pbar=pbar,
            conv_mask_to_poly=convert,
        )

        assert "uploaded" in self.logger_stream.getvalue()
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

        imported_annotations = fxt_new_task.get_jobs()[0].get_annotations()
        assert all(
            [s.type.value == "polygon" if convert else "mask" for s in imported_annotations.shapes]
        )

    def test_can_upload_annotations(self, fxt_new_task: Task, fxt_coco_file: Path):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        fxt_new_task.get_jobs()[0].import_annotations(
            format_name="COCO 1.0", filename=fxt_coco_file, pbar=pbar
        )

        assert "uploaded" in self.logger_stream.getvalue()
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    def test_can_get_meta(self, fxt_new_task: Task):
        meta = fxt_new_task.get_jobs()[0].get_meta()

        assert meta.image_quality == 80
        assert meta.size == 1
        assert not meta.deleted_frames
        assert self.stdout.getvalue() == ""

    def test_can_get_frame_info(self, fxt_new_task: Task):
        job = meta = fxt_new_task.get_jobs()[0]
        meta = job.get_meta()
        frames = job.get_frames_info()

        assert len(frames) == meta.size
        assert frames[0].name == "img.png"
        assert frames[0].width == 5
        assert frames[0].height == 10
        assert self.stdout.getvalue() == ""

    def test_can_remove_frames(self, fxt_new_task: Task):
        fxt_new_task.get_jobs()[0].remove_frames_by_ids([0])

        meta = fxt_new_task.get_jobs()[0].get_meta()
        assert meta.deleted_frames == [0]
        assert self.stdout.getvalue() == ""

    def test_can_get_issues(self, fxt_new_task: Task):
        issue = self.client.issues.create(
            models.IssueWriteRequest(
                frame=0,
                position=[2.0, 4.0],
                job=fxt_new_task.get_jobs()[0].id,
                message="hello",
            )
        )

        job_issue_ids = set(j.id for j in fxt_new_task.get_jobs()[0].get_issues())

        assert {issue.id} == job_issue_ids
        assert self.stdout.getvalue() == ""

    def test_can_get_annotations(self, fxt_task_with_shapes: Task):
        anns = fxt_task_with_shapes.get_jobs()[0].get_annotations()

        assert len(anns.shapes) == 1
        assert anns.shapes[0].type.value == "rectangle"
        assert self.stdout.getvalue() == ""

    def test_can_set_annotations(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.get_jobs()[0].set_annotations(
            models.LabeledDataRequest(
                tags=[models.LabeledImageRequest(frame=0, label_id=labels[0].id)],
            )
        )

        anns = fxt_new_task.get_jobs()[0].get_annotations()

        assert len(anns.tags) == 1
        assert self.stdout.getvalue() == ""

    def test_can_clear_annotations(self, fxt_task_with_shapes: Task):
        fxt_task_with_shapes.get_jobs()[0].remove_annotations()

        anns = fxt_task_with_shapes.get_jobs()[0].get_annotations()
        assert len(anns.tags) == 0
        assert len(anns.tracks) == 0
        assert len(anns.shapes) == 0
        assert self.stdout.getvalue() == ""

    def test_can_remove_annotations(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.get_jobs()[0].set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    ),
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[2, 2, 3, 3],
                    ),
                ],
            )
        )
        anns = fxt_new_task.get_jobs()[0].get_annotations()

        fxt_new_task.get_jobs()[0].remove_annotations(ids=[anns.shapes[0].id])

        anns = fxt_new_task.get_jobs()[0].get_annotations()
        assert len(anns.tags) == 0
        assert len(anns.tracks) == 0
        assert len(anns.shapes) == 1
        assert self.stdout.getvalue() == ""

    def test_can_update_annotations(self, fxt_task_with_shapes: Task):
        labels = fxt_task_with_shapes.get_labels()
        fxt_task_with_shapes.get_jobs()[0].update_annotations(
            models.PatchedLabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[0, 1, 2, 3],
                    ),
                ],
                tracks=[
                    models.LabeledTrackRequest(
                        frame=0,
                        label_id=labels[0].id,
                        shapes=[
                            models.TrackedShapeRequest(
                                frame=0, type="polygon", points=[3, 2, 2, 3, 3, 4]
                            ),
                        ],
                    )
                ],
                tags=[models.LabeledImageRequest(frame=0, label_id=labels[0].id)],
            )
        )

        anns = fxt_task_with_shapes.get_jobs()[0].get_annotations()
        assert len(anns.shapes) == 2
        assert len(anns.tracks) == 1
        assert len(anns.tags) == 1
        assert self.stdout.getvalue() == ""


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_masks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import pytest

try:
    import numpy as np
    from cvat_sdk.masks import encode_mask

except ModuleNotFoundError as e:
    if e.name.split(".")[0] != "numpy":
        raise

    encode_mask = None


@pytest.mark.skipif(encode_mask is None, reason="NumPy is not installed")
class TestMasks:
    def test_encode_mask(self):
        bitmap = np.array(
            [
                np.fromstring("0 0 1 1 1 0", sep=" "),
                np.fromstring("0 1 1 0 0 0", sep=" "),
            ],
            dtype=np.bool_,
        )
        bbox = [2.9, 0.9, 4.1, 1.1]  # will get rounded to [2, 0, 5, 2]

        # There's slightly different logic for when the cropped mask starts with
        # 0 and 1, so test both.
        # This one starts with 1:
        # 111
        # 100

        assert encode_mask(bitmap, bbox) == [0, 4, 2, 2, 0, 4, 1]

        bbox = [1, 0, 5, 2]

        # This one starts with 0:
        # 0111
        # 1100

        assert encode_mask(bitmap, bbox) == [1, 5, 2, 1, 0, 4, 1]

        # Edge case: full image
        bbox = [0, 0, 6, 2]
        assert encode_mask(bitmap, bbox) == [2, 3, 2, 2, 3, 0, 0, 5, 1]

    def test_encode_mask_invalid_dim(self):
        with pytest.raises(ValueError, match="bitmap must have 2 dimensions"):
            encode_mask([True], [0, 0, 1, 1])

    def test_encode_mask_invalid_dtype(self):
        with pytest.raises(ValueError, match="bitmap must have boolean items"):
            encode_mask([[1]], [0, 0, 1, 1])

    @pytest.mark.parametrize(
        "bbox",
        [
            [-0.1, 0, 1, 1],
            [0, -0.1, 1, 1],
            [0, 0, 1.1, 1],
            [0, 0, 1, 1.1],
            [1, 0, 0, 1],
            [0, 1, 1, 0],
        ],
    )
    def test_encode_mask_invalid_bbox(self, bbox):
        with pytest.raises(ValueError, match="bbox has invalid coordinates"):
            encode_mask([[True]], bbox)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_organizations.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger

import pytest
from cvat_sdk import Client, models
from cvat_sdk.api_client import exceptions
from cvat_sdk.core.proxies.organizations import Organization


class TestOrganizationUsecases:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
    ):
        logger, self.logger_stream = fxt_logger
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

        assert fxt_stdout.getvalue() == ""

    @pytest.fixture()
    def fxt_organization(self) -> Organization:
        org = self.client.organizations.create(
            models.OrganizationWriteRequest(
                slug="testorg",
                name="Test Organization",
                description="description",
                contact={"email": "nowhere@cvat.invalid"},
            )
        )

        try:
            yield org
        finally:
            # It's not allowed to create multiple orgs with the same slug,
            # so we have to remove the org at the end of each test.
            org.remove()

    def test_can_create_organization(self, fxt_organization: Organization):
        assert fxt_organization.slug == "testorg"
        assert fxt_organization.name == "Test Organization"
        assert fxt_organization.description == "description"
        assert fxt_organization.contact == {"email": "nowhere@cvat.invalid"}

    def test_can_retrieve_organization(self, fxt_organization: Organization):
        org = self.client.organizations.retrieve(fxt_organization.id)

        assert org.id == fxt_organization.id
        assert org.slug == fxt_organization.slug

    def test_can_list_organizations(self, fxt_organization: Organization):
        orgs = self.client.organizations.list()

        assert fxt_organization.slug in set(o.slug for o in orgs)

    def test_can_update_organization(self, fxt_organization: Organization):
        fxt_organization.update(
            models.PatchedOrganizationWriteRequest(description="new description")
        )
        assert fxt_organization.description == "new description"

        retrieved_org = self.client.organizations.retrieve(fxt_organization.id)
        assert retrieved_org.description == "new description"

    def test_can_remove_organization(self):
        org = self.client.organizations.create(models.OrganizationWriteRequest(slug="testorg2"))
        org.remove()

        with pytest.raises(exceptions.NotFoundException):
            org.fetch()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_progress.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import warnings
from typing import Optional

import tqdm
from cvat_sdk.core.helpers import DeferredTqdmProgressReporter, TqdmProgressReporter
from cvat_sdk.core.progress import NullProgressReporter, ProgressReporter


def _exercise_reporter(r: ProgressReporter) -> None:
    with r.task(total=5, desc="Test task", unit="parrots"):
        r.advance(1)
        r.report_status(4)

        for x in r.iter(["x"]):
            assert x == "x"


def test_null_reporter():
    _exercise_reporter(NullProgressReporter())
    # NPR doesn't do anything, so there's nothing to assert


def test_tqdm_reporter():
    f = io.StringIO()

    instance = tqdm.tqdm(file=f)

    with warnings.catch_warnings():
        r = TqdmProgressReporter(instance)

    _exercise_reporter(r)

    output = f.getvalue()

    assert "100%" in output
    assert "Test task" in output
    # TPR doesn't support parameters other than "total" and "desc",
    # so there won't be any parrots in the output.


def test_deferred_tqdm_reporter():
    f = io.StringIO()

    _exercise_reporter(DeferredTqdmProgressReporter({"file": f}))

    output = f.getvalue()

    assert "100%" in output
    assert "Test task" in output
    assert "parrots" in output


class _LegacyProgressReporter(ProgressReporter):
    # overriding start instead of start2
    def start(self, total: int, *, desc: Optional[str] = None) -> None:
        self.total = total
        self.desc = desc
        self.progress = 0

    def report_status(self, progress: int):
        self.progress = progress

    def advance(self, delta: int):
        self.progress += delta

    def finish(self):
        self.finished = True


def test_legacy_progress_reporter():
    r = _LegacyProgressReporter()

    _exercise_reporter(r)

    assert r.total == 5
    assert r.desc == "Test task"
    assert r.progress == 5


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_projects.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger
from pathlib import Path
from typing import Optional

import pytest
from cvat_sdk import Client, models
from cvat_sdk.api_client import exceptions
from cvat_sdk.core.proxies.projects import Project
from cvat_sdk.core.proxies.tasks import Task
from cvat_sdk.core.proxies.types import Location
from cvat_sdk.core.utils import filter_dict
from PIL import Image
from pytest_cases import fixture_ref, parametrize

from shared.fixtures.data import CloudStorageAssets
from shared.utils.config import IMPORT_EXPORT_BUCKET_ID

from .common import TestDatasetExport
from .util import make_pbar


class TestProjectUsecases(TestDatasetExport):
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
        restore_redis_ondisk_per_function,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

    @pytest.fixture
    def fxt_task_with_shapes(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    ),
                ],
            )
        )

        return fxt_new_task

    @pytest.fixture
    def fxt_new_project(self):
        project = self.client.projects.create(
            spec={
                "name": "test_project",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
        )

        return project

    @pytest.fixture
    def fxt_new_project_with_target_storage(self):
        project = self.client.projects.create(
            spec={
                "name": "test_project",
                "labels": [{"name": "car"}, {"name": "person"}],
                "target_storage": {
                    "location": Location.CLOUD_STORAGE,
                    "cloud_storage_id": IMPORT_EXPORT_BUCKET_ID,
                },
            },
        )

        return project

    @pytest.fixture
    def fxt_empty_project(self):
        return self.client.projects.create(spec={"name": "test_project"})

    @pytest.fixture
    def fxt_project_with_shapes(self, fxt_task_with_shapes: Task):
        project = self.client.projects.create(
            spec=models.ProjectWriteRequest(
                name="test_project",
                labels=[
                    models.PatchedLabelRequest(
                        **filter_dict(label.to_dict(), drop=["id", "has_parent"])
                    )
                    for label in fxt_task_with_shapes.get_labels()
                ],
            )
        )
        fxt_task_with_shapes.update(models.PatchedTaskWriteRequest(project_id=project.id))
        project.fetch()
        return project

    @pytest.fixture
    def fxt_backup_file(self, fxt_project_with_shapes: Project):
        backup_path = self.tmp_path / "backup.zip"

        fxt_project_with_shapes.download_backup(str(backup_path))

        yield backup_path

    def test_can_create_empty_project(self):
        project = self.client.projects.create(spec=models.ProjectWriteRequest(name="test project"))

        assert project.id != 0
        assert project.name == "test project"

    def test_can_create_project_with_attribute_with_blank_default(self):
        project = self.client.projects.create(
            spec=models.ProjectWriteRequest(
                name="test project",
                labels=[
                    models.PatchedLabelRequest(
                        name="text",
                        attributes=[
                            models.AttributeRequest(
                                name="text",
                                mutable=True,
                                input_type=models.InputTypeEnum("text"),
                                values=[],
                                default_value="",
                            )
                        ],
                    )
                ],
            )
        )

        labels = project.get_labels()
        assert labels[0].attributes[0].default_value == ""

    def test_can_create_project_from_dataset(self, fxt_coco_dataset: Path):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        project = self.client.projects.create_from_dataset(
            spec=models.ProjectWriteRequest(name="project with data"),
            dataset_path=fxt_coco_dataset,
            dataset_format="COCO 1.0",
            pbar=pbar,
        )

        assert project.get_tasks()[0].size == 1
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("convert", [True, False])
    def test_can_create_project_from_dataset_with_polygons_to_masks_param(
        self, fxt_camvid_dataset: Path, convert: bool
    ):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)
        project = self.client.projects.create_from_dataset(
            spec=models.ProjectWriteRequest(name="project with data"),
            dataset_path=fxt_camvid_dataset,
            dataset_format="CamVid 1.0",
            conv_mask_to_poly=convert,
            pbar=pbar,
        )

        assert project.get_tasks()[0].size == 1
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

        task = project.get_tasks()[0]
        imported_annotations = task.get_annotations()
        assert all(
            [s.type.value == "polygon" if convert else "mask" for s in imported_annotations.shapes]
        )

    def test_can_retrieve_project(self, fxt_new_project: Project):
        project_id = fxt_new_project.id

        project = self.client.projects.retrieve(project_id)

        assert project.id == project_id
        assert self.stdout.getvalue() == ""

    def test_can_list_projects(self, fxt_new_project: Project):
        project_id = fxt_new_project.id

        projects = self.client.projects.list()

        assert any(p.id == project_id for p in projects)
        assert self.stdout.getvalue() == ""

    def test_can_update_project(self, fxt_new_project: Project):
        fxt_new_project.update(models.PatchedProjectWriteRequest(name="foo"))

        retrieved_project = self.client.projects.retrieve(fxt_new_project.id)
        assert retrieved_project.name == "foo"
        assert fxt_new_project.name == retrieved_project.name
        assert self.stdout.getvalue() == ""

    def test_can_delete_project(self, fxt_new_project: Project):
        fxt_new_project.remove()

        with pytest.raises(exceptions.NotFoundException):
            fxt_new_project.fetch()
        assert self.stdout.getvalue() == ""

    def test_can_get_tasks(self, fxt_project_with_shapes: Project):
        tasks = fxt_project_with_shapes.get_tasks()

        assert len(tasks) == 1
        assert tasks[0].project_id == fxt_project_with_shapes.id

    def test_can_get_labels(self, fxt_project_with_shapes: Project):
        expected_labels = {"car", "person"}

        received_labels = fxt_project_with_shapes.get_labels()

        assert {obj.name for obj in received_labels} == expected_labels
        assert self.stdout.getvalue() == ""

    def test_can_download_backup(self, fxt_project_with_shapes: Project):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)
        backup_path = self.tmp_path / "backup.zip"

        fxt_project_with_shapes.download_backup(str(backup_path), pbar=pbar)

        assert backup_path.stat().st_size > 0
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    def test_can_create_from_backup(self, fxt_backup_file: Path):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        restored_project = self.client.projects.create_from_backup(fxt_backup_file, pbar=pbar)

        assert restored_project.get_tasks()[0].size == 1
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("format_name", ("CVAT for images 1.1",))
    @pytest.mark.parametrize("include_images", (True, False))
    @parametrize(
        "project, location",
        [
            (fixture_ref("fxt_new_project"), None),
            (fixture_ref("fxt_new_project"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_project"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (
                pytest.param(
                    fixture_ref("fxt_new_project_with_target_storage"),
                    None,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (fixture_ref("fxt_new_project_with_target_storage"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_project_with_target_storage"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
        ],
    )
    def test_can_export_dataset(
        self,
        format_name: str,
        include_images: bool,
        project: Project,
        location: Optional[Location],
        request: pytest.FixtureRequest,
        cloud_storages: CloudStorageAssets,
    ):
        file_path = self.tmp_path / f"project_{project.id}-{format_name.lower()}.zip"
        self._test_can_export_dataset(
            project,
            format_name=format_name,
            file_path=file_path,
            include_images=include_images,
            location=location,
            request=request,
            cloud_storages=cloud_storages,
        )

    def test_can_download_preview(self, fxt_project_with_shapes: Project):
        frame_encoded = fxt_project_with_shapes.get_preview()
        (width, height) = Image.open(frame_encoded).size

        assert width > 0 and height > 0
        assert self.stdout.getvalue() == ""


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_pytorch.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import itertools
import os
from logging import Logger
from pathlib import Path

import pytest
from cvat_sdk import Client, models
from cvat_sdk.core.proxies.tasks import ResourceType

try:
    import cvat_sdk.pytorch as cvatpt
    import PIL.Image
    import torch
    import torchvision.transforms
    import torchvision.transforms.functional as TF
    from torch.utils.data import DataLoader
except ModuleNotFoundError as e:
    if e.name.split(".")[0] not in {"torch", "torchvision"}:
        raise

    cvatpt = None

from shared.utils.helpers import generate_image_files

from .util import restrict_api_requests


@pytest.fixture(autouse=True)
def _common_setup(
    tmp_path: Path,
    fxt_login: tuple[Client, str],
    fxt_logger: tuple[Logger, io.StringIO],
    restore_redis_ondisk_per_function,
    restore_redis_inmem_per_function,
):
    logger = fxt_logger[0]
    client = fxt_login[0]
    client.logger = logger
    client.config.cache_dir = tmp_path / "cache"

    api_client = client.api_client
    for k in api_client.configuration.logger:
        api_client.configuration.logger[k] = logger


@pytest.mark.skipif(cvatpt is None, reason="PyTorch dependencies are not installed")
class TestTaskVisionDataset:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
    ):
        self.client = fxt_login[0]
        self.images = generate_image_files(10)

        image_dir = tmp_path / "images"
        image_dir.mkdir()

        image_paths = []
        for image in self.images:
            image_path = image_dir / image.name
            image_path.write_bytes(image.getbuffer())
            image_paths.append(image_path)

        self.task = self.client.tasks.create_from_data(
            models.TaskWriteRequest(
                "PyTorch integration test task",
                labels=[
                    models.PatchedLabelRequest(name="person"),
                    models.PatchedLabelRequest(name="car"),
                ],
            ),
            resource_type=ResourceType.LOCAL,
            resources=list(map(os.fspath, image_paths)),
            data_params={"chunk_size": 3},
        )

        self.label_ids = sorted(l.id for l in self.task.get_labels())

        self.task.update_annotations(
            models.PatchedLabeledDataRequest(
                tags=[
                    models.LabeledImageRequest(frame=5, label_id=self.label_ids[0]),
                    models.LabeledImageRequest(frame=6, label_id=self.label_ids[1]),
                    models.LabeledImageRequest(frame=8, label_id=self.label_ids[0]),
                    models.LabeledImageRequest(frame=8, label_id=self.label_ids[1]),
                ],
                shapes=[
                    models.LabeledShapeRequest(
                        frame=6,
                        label_id=self.label_ids[1],
                        type=models.ShapeType("rectangle"),
                        points=[1.0, 2.0, 3.0, 4.0],
                    ),
                    models.LabeledShapeRequest(
                        frame=7,
                        label_id=self.label_ids[0],
                        type=models.ShapeType("points"),
                        points=[1.1, 2.1, 3.1, 4.1],
                    ),
                ],
            )
        )

    def test_basic(self):
        dataset = cvatpt.TaskVisionDataset(self.client, self.task.id)

        # verify that the cache is not empty
        assert list(self.client.config.cache_dir.iterdir())

        assert len(dataset) == self.task.size

        for index, (sample_image, sample_target) in enumerate(dataset):
            sample_image_tensor = TF.pil_to_tensor(sample_image)
            reference_tensor = TF.pil_to_tensor(PIL.Image.open(self.images[index]))
            assert torch.equal(sample_image_tensor, reference_tensor)

            for index, label_id in enumerate(self.label_ids):
                assert sample_target.label_id_to_index[label_id] == index

        assert not dataset[0][1].annotations.tags
        assert not dataset[0][1].annotations.shapes

        assert len(dataset[5][1].annotations.tags) == 1
        assert dataset[5][1].annotations.tags[0].label_id == self.label_ids[0]
        assert not dataset[5][1].annotations.shapes

        assert len(dataset[6][1].annotations.tags) == 1
        assert dataset[6][1].annotations.tags[0].label_id == self.label_ids[1]
        assert len(dataset[6][1].annotations.shapes) == 1
        assert dataset[6][1].annotations.shapes[0].type.value == "rectangle"
        assert dataset[6][1].annotations.shapes[0].points == [1.0, 2.0, 3.0, 4.0]

        assert not dataset[7][1].annotations.tags
        assert len(dataset[7][1].annotations.shapes) == 1
        assert dataset[7][1].annotations.shapes[0].type.value == "points"
        assert dataset[7][1].annotations.shapes[0].points == [1.1, 2.1, 3.1, 4.1]

    def test_deleted_frame(self):
        self.task.remove_frames_by_ids([1])

        dataset = cvatpt.TaskVisionDataset(self.client, self.task.id)

        assert len(dataset) == self.task.size - 1

        # sample #0 is still frame #0
        assert torch.equal(
            TF.pil_to_tensor(dataset[0][0]), TF.pil_to_tensor(PIL.Image.open(self.images[0]))
        )

        # sample #1 is now frame #2
        assert torch.equal(
            TF.pil_to_tensor(dataset[1][0]), TF.pil_to_tensor(PIL.Image.open(self.images[2]))
        )

        # sample #4 is now frame #5
        assert len(dataset[4][1].annotations.tags) == 1
        assert dataset[4][1].annotations.tags[0].label_id == self.label_ids[0]
        assert not dataset[4][1].annotations.shapes

    def test_extract_single_label_index(self):
        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            transform=torchvision.transforms.PILToTensor(),
            target_transform=cvatpt.ExtractSingleLabelIndex(),
        )

        assert torch.equal(dataset[5][1], torch.tensor(0))
        assert torch.equal(dataset[6][1], torch.tensor(1))

        with pytest.raises(ValueError):
            # no tags
            _ = dataset[7]

        with pytest.raises(ValueError):
            # multiple tags
            _ = dataset[8]

        # make sure the samples can be batched with the default collater
        loader = DataLoader(dataset, batch_size=2, sampler=[5, 6])

        batch = next(iter(loader))
        assert torch.equal(batch[0][0], TF.pil_to_tensor(PIL.Image.open(self.images[5])))
        assert torch.equal(batch[0][1], TF.pil_to_tensor(PIL.Image.open(self.images[6])))
        assert torch.equal(batch[1], torch.tensor([0, 1]))

    def test_extract_bounding_boxes(self):
        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            transform=torchvision.transforms.PILToTensor(),
            target_transform=cvatpt.ExtractBoundingBoxes(include_shape_types={"rectangle"}),
        )

        assert torch.equal(dataset[0][1]["boxes"], torch.tensor([]))
        assert torch.equal(dataset[0][1]["labels"], torch.tensor([]))

        assert torch.equal(dataset[6][1]["boxes"], torch.tensor([(1.0, 2.0, 3.0, 4.0)]))
        assert torch.equal(dataset[6][1]["labels"], torch.tensor([1]))

        # points are filtered out
        assert torch.equal(dataset[7][1]["boxes"], torch.tensor([]))
        assert torch.equal(dataset[7][1]["labels"], torch.tensor([]))

    def test_transforms(self):
        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            transforms=lambda x, y: (y, x),
        )

        assert isinstance(dataset[0][0], cvatpt.Target)
        assert isinstance(dataset[0][1], PIL.Image.Image)

    def test_custom_label_mapping(self):
        label_name_to_id = {label.name: label.id for label in self.task.get_labels()}

        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            label_name_to_index={"person": 123, "car": 456},
        )

        _, target = dataset[5]
        assert target.label_id_to_index[label_name_to_id["person"]] == 123
        assert target.label_id_to_index[label_name_to_id["car"]] == 456

    def test_offline(self, monkeypatch: pytest.MonkeyPatch):
        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            update_policy=cvatpt.UpdatePolicy.IF_MISSING_OR_STALE,
        )

        fresh_samples = list(dataset)

        restrict_api_requests(monkeypatch)

        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
            update_policy=cvatpt.UpdatePolicy.NEVER,
        )

        cached_samples = list(dataset)

        assert fresh_samples == cached_samples

    def test_update(self, monkeypatch: pytest.MonkeyPatch):
        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
        )

        # Recreating the dataset should only result in minimal requests.
        restrict_api_requests(
            monkeypatch, allow_paths={f"/api/tasks/{self.task.id}", "/api/labels"}
        )

        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
        )

        assert dataset[5][1].annotations.tags[0].label_id == self.label_ids[0]

        # After an update, the annotations should be redownloaded.
        monkeypatch.undo()

        self.task.update_annotations(
            models.PatchedLabeledDataRequest(
                tags=[
                    models.LabeledImageRequest(
                        id=dataset[5][1].annotations.tags[0].id, frame=5, label_id=self.label_ids[1]
                    ),
                ]
            )
        )

        dataset = cvatpt.TaskVisionDataset(
            self.client,
            self.task.id,
        )

        assert dataset[5][1].annotations.tags[0].label_id == self.label_ids[1]


@pytest.mark.skipif(cvatpt is None, reason="PyTorch dependencies are not installed")
class TestProjectVisionDataset:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
    ):
        self.client = fxt_login[0]

        self.project = self.client.projects.create(
            models.ProjectWriteRequest(
                "PyTorch integration test project",
                labels=[
                    models.PatchedLabelRequest(name="person"),
                    models.PatchedLabelRequest(name="car"),
                ],
            )
        )
        self.label_ids = sorted(l.id for l in self.project.get_labels())

        subsets = ["Train", "Test", "Val"]
        num_images_per_task = 3

        all_images = generate_image_files(num_images_per_task * len(subsets))

        self.images_per_task = list(zip(*[iter(all_images)] * num_images_per_task))

        image_dir = tmp_path / "images"
        image_dir.mkdir()

        image_paths_per_task = []
        for images in self.images_per_task:
            image_paths = []
            for image in images:
                image_path = image_dir / image.name
                image_path.write_bytes(image.getbuffer())
                image_paths.append(image_path)
            image_paths_per_task.append(image_paths)

        self.tasks = [
            self.client.tasks.create_from_data(
                models.TaskWriteRequest(
                    "PyTorch integration test task",
                    project_id=self.project.id,
                    subset=subset,
                ),
                resource_type=ResourceType.LOCAL,
                resources=image_paths,
                data_params={"image_quality": 70},
            )
            for subset, image_paths in zip(subsets, image_paths_per_task)
        ]

        # sort both self.tasks and self.images_per_task in the order that ProjectVisionDataset uses
        self.tasks, self.images_per_task = zip(
            *sorted(zip(self.tasks, self.images_per_task), key=lambda t: t[0].id)
        )

        for task_id, label_index in ((0, 0), (1, 1), (2, 0)):
            self.tasks[task_id].update_annotations(
                models.PatchedLabeledDataRequest(
                    tags=[
                        models.LabeledImageRequest(
                            frame=task_id, label_id=self.label_ids[label_index]
                        ),
                    ],
                )
            )

    def test_basic(self):
        dataset = cvatpt.ProjectVisionDataset(self.client, self.project.id)

        assert len(dataset) == sum(task.size for task in self.tasks)

        for sample, image in zip(dataset, itertools.chain.from_iterable(self.images_per_task)):
            assert torch.equal(TF.pil_to_tensor(sample[0]), TF.pil_to_tensor(PIL.Image.open(image)))

        assert dataset[0][1].annotations.tags[0].label_id == self.label_ids[0]
        assert dataset[4][1].annotations.tags[0].label_id == self.label_ids[1]
        assert dataset[8][1].annotations.tags[0].label_id == self.label_ids[0]

    def _test_filtering(self, **kwargs):
        dataset = cvatpt.ProjectVisionDataset(self.client, self.project.id, **kwargs)

        assert len(dataset) == sum(task.size for task in self.tasks[1:])

        for sample, image in zip(dataset, itertools.chain.from_iterable(self.images_per_task[1:])):
            assert torch.equal(TF.pil_to_tensor(sample[0]), TF.pil_to_tensor(PIL.Image.open(image)))

        assert dataset[1][1].annotations.tags[0].label_id == self.label_ids[1]
        assert dataset[5][1].annotations.tags[0].label_id == self.label_ids[0]

    def test_task_filter(self):
        self._test_filtering(task_filter=lambda t: t.subset != self.tasks[0].subset)

    def test_include_subsets(self):
        self._test_filtering(include_subsets={self.tasks[1].subset, self.tasks[2].subset})

    def test_custom_label_mapping(self):
        label_name_to_id = {label.name: label.id for label in self.project.get_labels()}

        dataset = cvatpt.ProjectVisionDataset(
            self.client, self.project.id, label_name_to_index={"person": 123, "car": 456}
        )

        _, target = dataset[5]
        assert target.label_id_to_index[label_name_to_id["person"]] == 123
        assert target.label_id_to_index[label_name_to_id["car"]] == 456

    def test_separate_transforms(self):
        dataset = cvatpt.ProjectVisionDataset(
            self.client,
            self.project.id,
            transform=torchvision.transforms.ToTensor(),
            target_transform=cvatpt.ExtractSingleLabelIndex(),
        )

        assert torch.equal(
            dataset[0][0], TF.pil_to_tensor(PIL.Image.open(self.images_per_task[0][0]))
        )
        assert torch.equal(dataset[0][1], torch.tensor(0))

    def test_combined_transforms(self):
        dataset = cvatpt.ProjectVisionDataset(
            self.client,
            self.project.id,
            transforms=lambda x, y: (y, x),
        )

        assert isinstance(dataset[0][0], cvatpt.Target)
        assert isinstance(dataset[0][1], PIL.Image.Image)

    def test_offline(self, monkeypatch: pytest.MonkeyPatch):
        dataset = cvatpt.ProjectVisionDataset(
            self.client,
            self.project.id,
            update_policy=cvatpt.UpdatePolicy.IF_MISSING_OR_STALE,
        )

        fresh_samples = list(dataset)

        restrict_api_requests(monkeypatch)

        dataset = cvatpt.ProjectVisionDataset(
            self.client,
            self.project.id,
            update_policy=cvatpt.UpdatePolicy.NEVER,
        )

        cached_samples = list(dataset)

        assert fresh_samples == cached_samples


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_tasks.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import os.path as osp
import zipfile
from logging import Logger
from pathlib import Path
from typing import Optional

import pytest
from cvat_sdk import Client, models
from cvat_sdk.api_client import exceptions
from cvat_sdk.core.proxies.tasks import ResourceType, Task
from cvat_sdk.core.proxies.types import Location
from cvat_sdk.core.uploading import Uploader, _MyTusUploader
from PIL import Image
from pytest_cases import fixture_ref, parametrize

from shared.fixtures.data import CloudStorageAssets
from shared.utils.helpers import generate_image_files

from .common import TestDatasetExport
from .util import make_pbar


class TestTaskUsecases(TestDatasetExport):
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
        restore_redis_ondisk_per_function,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

    @pytest.fixture
    def fxt_backup_file(self, fxt_new_task: Task, fxt_coco_file: str):
        backup_path = self.tmp_path / "backup.zip"

        fxt_new_task.import_annotations("COCO 1.0", filename=fxt_coco_file)
        fxt_new_task.download_backup(backup_path)

        yield backup_path

    @pytest.fixture
    def fxt_new_task_without_data(self):
        task = self.client.tasks.create(
            spec={
                "name": "test_task",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
        )

        return task

    @pytest.fixture
    def fxt_task_with_shapes(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    ),
                ],
            )
        )

        return fxt_new_task

    def test_can_create_task_with_local_data(self):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        task_spec = {
            "name": f"test {self.user} to create a task with local data",
            "labels": [
                {
                    "name": "car",
                    "color": "#ff00ff",
                    "attributes": [
                        {
                            "name": "a",
                            "mutable": True,
                            "input_type": "number",
                            "default_value": "5",
                            "values": ["4", "5", "6"],
                        }
                    ],
                }
            ],
        }

        data_params = {
            "image_quality": 75,
        }

        task_files = generate_image_files(7)
        for i, f in enumerate(task_files):
            fname = self.tmp_path / f.name
            fname.write_bytes(f.getvalue())
            task_files[i] = fname

        task = self.client.tasks.create_from_data(
            spec=task_spec,
            data_params=data_params,
            resources=task_files,
            pbar=pbar,
        )

        assert task.size == 7
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    def test_can_create_task_with_local_data_and_predefined_sorting(
        self, fxt_new_task_without_data: Task
    ):
        task = fxt_new_task_without_data

        task_files = generate_image_files(6)
        task_filenames = []
        for f in task_files:
            fname = self.tmp_path / osp.basename(f.name)
            fname.write_bytes(f.getvalue())
            task_filenames.append(fname)

        task_filenames = [task_filenames[i] for i in [2, 4, 1, 5, 0, 3]]

        task.upload_data(
            resources=task_filenames,
            params={"sorting_method": "predefined"},
        )

        assert [f.name for f in task.get_frames_info()] == [f.name for f in task_filenames]

    def test_can_create_task_with_remote_data(self):
        task = self.client.tasks.create_from_data(
            spec={
                "name": "test_task",
                "labels": [{"name": "car"}, {"name": "person"}],
            },
            resource_type=ResourceType.SHARE,
            resources=["images/image_1.jpg", "images/image_2.jpg"],
            # make sure string fields are transferred correctly;
            # see https://github.com/cvat-ai/cvat/issues/4962
            data_params={"sorting_method": "lexicographical"},
        )

        assert task.size == 2
        assert task.get_frames_info()[0].name == "images/image_1.jpg"
        assert self.stdout.getvalue() == ""

    def test_cant_create_task_with_no_data(self):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        task_spec = {
            "name": f"test {self.user} to create a task with no data",
            "labels": [
                {
                    "name": "car",
                }
            ],
        }

        with pytest.raises(exceptions.ApiException) as capture:
            self.client.tasks.create_from_data(
                spec=task_spec,
                resource_type=ResourceType.LOCAL,
                resources=[],
                pbar=pbar,
            )

        assert capture.match("No media data found")
        assert self.stdout.getvalue() == ""

    def test_can_upload_data_to_empty_task(self):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        task = self.client.tasks.create(
            {
                "name": f"test task",
                "labels": [{"name": "car"}],
            }
        )

        data_params = {
            "image_quality": 75,
        }

        task_files = generate_image_files(7)
        for i, f in enumerate(task_files):
            fname = self.tmp_path / f.name
            fname.write_bytes(f.getvalue())
            task_files[i] = fname

        task.upload_data(
            resources=task_files,
            resource_type=ResourceType.LOCAL,
            params=data_params,
            pbar=pbar,
        )

        assert task.size == 7
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    def test_can_retrieve_task(self, fxt_new_task: Task):
        task_id = fxt_new_task.id

        task = self.client.tasks.retrieve(task_id)

        assert task.id == task_id
        assert self.stdout.getvalue() == ""

    def test_can_list_tasks(self, fxt_new_task: Task):
        task_id = fxt_new_task.id

        tasks = self.client.tasks.list()

        assert any(t.id == task_id for t in tasks)
        assert self.stdout.getvalue() == ""

    def test_can_update_task(self, fxt_new_task: Task):
        fxt_new_task.update(models.PatchedTaskWriteRequest(name="foo"))

        retrieved_task = self.client.tasks.retrieve(fxt_new_task.id)
        assert retrieved_task.name == "foo"
        assert fxt_new_task.name == retrieved_task.name
        assert self.stdout.getvalue() == ""

    def test_can_delete_task(self, fxt_new_task: Task):
        fxt_new_task.remove()

        with pytest.raises(exceptions.NotFoundException):
            fxt_new_task.fetch()
        assert self.stdout.getvalue() == ""

    def test_can_delete_tasks_by_ids(self, fxt_new_task: Task):
        task_id = fxt_new_task.id
        old_tasks = self.client.tasks.list()

        self.client.tasks.remove_by_ids([task_id])

        new_tasks = self.client.tasks.list()
        assert any(t.id == task_id for t in old_tasks)
        assert all(t.id != task_id for t in new_tasks)
        assert self.logger_stream.getvalue(), f".*Task ID {task_id} deleted.*"
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("format_name", ("CVAT for images 1.1",))
    @pytest.mark.parametrize("include_images", (True, False))
    @parametrize(
        "task, location",
        [
            (fixture_ref("fxt_new_task"), None),
            (fixture_ref("fxt_new_task"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_task"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (
                pytest.param(
                    fixture_ref("fxt_new_task_with_target_storage"),
                    None,
                    marks=pytest.mark.with_external_services,
                )
            ),
            (fixture_ref("fxt_new_task_with_target_storage"), Location.LOCAL),
            (
                pytest.param(
                    fixture_ref("fxt_new_task_with_target_storage"),
                    Location.CLOUD_STORAGE,
                    marks=pytest.mark.with_external_services,
                )
            ),
        ],
    )
    def test_can_export_dataset(
        self,
        format_name: str,
        include_images: bool,
        task: Task,
        location: Optional[Location],
        request: pytest.FixtureRequest,
        cloud_storages: CloudStorageAssets,
    ):
        file_path = self.tmp_path / f"task_{task.id}-{format_name.lower()}.zip"
        self._test_can_export_dataset(
            task,
            format_name=format_name,
            file_path=file_path,
            include_images=include_images,
            location=location,
            request=request,
            cloud_storages=cloud_storages,
        )

    def test_can_download_dataset_twice_in_a_row(self, fxt_new_task: Task):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        for i in range(2):
            path = self.tmp_path / f"dataset-{i}.zip"
            fxt_new_task.export_dataset(
                format_name="CVAT for images 1.1",
                filename=self.tmp_path / f"dataset-{i}.zip",
                include_images=False,
                pbar=pbar,
            )
            assert self.stdout.getvalue() == ""
            assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
            assert path.is_file()

    def test_can_download_backup(self, fxt_new_task: Task):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        task_id = fxt_new_task.id
        path = self.tmp_path / f"task_{task_id}-backup.zip"
        task = self.client.tasks.retrieve(task_id)
        task.download_backup(filename=path, pbar=pbar)

        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert path.is_file()
        assert self.stdout.getvalue() == ""

    def test_can_download_preview(self, fxt_new_task: Task):
        frame_encoded = fxt_new_task.get_preview()
        (width, height) = Image.open(frame_encoded).size

        assert width > 0 and height > 0
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    def test_can_download_frame(self, fxt_new_task: Task, quality: str):
        frame_encoded = fxt_new_task.get_frame(0, quality=quality)
        (width, height) = Image.open(frame_encoded).size

        assert width > 0 and height > 0
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    @pytest.mark.parametrize("image_extension", (None, "bmp"))
    def test_can_download_frames(
        self, fxt_new_task: Task, quality: str, image_extension: Optional[str]
    ):
        fxt_new_task.download_frames(
            [0],
            image_extension=image_extension,
            quality=quality,
            outdir=self.tmp_path,
            filename_pattern="frame-{frame_id}{frame_ext}",
        )

        if image_extension is not None:
            expected_frame_ext = image_extension
        else:
            if quality == "original":
                expected_frame_ext = "png"
            else:
                expected_frame_ext = "jpg"

        assert (self.tmp_path / f"frame-0.{expected_frame_ext}").is_file()
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("quality", ("compressed", "original"))
    def test_can_download_chunk(self, fxt_new_task: Task, quality: str):
        chunk_path = self.tmp_path / "chunk.zip"

        with open(chunk_path, "wb") as chunk_file:
            fxt_new_task.download_chunk(0, chunk_file, quality=quality)

        with zipfile.ZipFile(chunk_path, "r") as chunk_zip:
            assert chunk_zip.testzip() is None
            assert len(chunk_zip.infolist()) == 1
        assert self.stdout.getvalue() == ""

    @pytest.mark.parametrize("convert", [True, False])
    def test_can_upload_annotations(
        self, fxt_new_task: Task, fxt_camvid_dataset: Path, convert: bool
    ):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        fxt_new_task.import_annotations(
            format_name="CamVid 1.0",
            filename=fxt_camvid_dataset,
            conv_mask_to_poly=convert,
            pbar=pbar,
        )

        assert "uploaded" in self.logger_stream.getvalue()
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

        imported_annotations = fxt_new_task.get_jobs()[0].get_annotations()
        assert all(
            [s.type.value == "polygon" if convert else "mask" for s in imported_annotations.shapes]
        )

    def _test_can_create_from_backup(self, fxt_new_task: Task, fxt_backup_file: Path):
        pbar_out = io.StringIO()
        pbar = make_pbar(file=pbar_out)

        task = self.client.tasks.create_from_backup(fxt_backup_file, pbar=pbar)

        assert task.id
        assert task.id != fxt_new_task.id
        assert task.size == fxt_new_task.size
        assert "imported successfully" in self.logger_stream.getvalue()
        assert "100%" in pbar_out.getvalue().strip("\r").split("\r")[-1]
        assert self.stdout.getvalue() == ""

    def test_can_create_from_backup(self, fxt_new_task: Task, fxt_backup_file: Path):
        self._test_can_create_from_backup(fxt_new_task, fxt_backup_file)

    def test_can_create_from_backup_in_chunks(
        self, monkeypatch: pytest.MonkeyPatch, fxt_new_task: Task, fxt_backup_file: Path
    ):
        monkeypatch.setattr(Uploader, "_CHUNK_SIZE", 100)

        num_requests = 0
        original_do_request = _MyTusUploader._do_request

        def counting_do_request(uploader):
            nonlocal num_requests
            num_requests += 1
            original_do_request(uploader)

        monkeypatch.setattr(_MyTusUploader, "_do_request", counting_do_request)

        self._test_can_create_from_backup(fxt_new_task, fxt_backup_file)

        # make sure the upload was actually chunked
        assert num_requests > 1

    def test_can_get_labels(self, fxt_new_task: Task):
        expected_labels = {"car", "person"}

        received_labels = fxt_new_task.get_labels()

        assert {obj.name for obj in received_labels} == expected_labels
        assert self.stdout.getvalue() == ""

    def test_can_get_jobs(self, fxt_new_task: Task):
        jobs = fxt_new_task.get_jobs()

        assert len(jobs) != 0
        assert self.stdout.getvalue() == ""

    def test_can_get_meta(self, fxt_new_task: Task):
        meta = fxt_new_task.get_meta()

        assert meta.image_quality == 80
        assert meta.size == 1
        assert not meta.deleted_frames
        assert self.stdout.getvalue() == ""

    def test_can_get_frame_info(self, fxt_new_task: Task):
        meta = fxt_new_task.get_meta()
        frames = fxt_new_task.get_frames_info()

        assert len(frames) == meta.size
        assert frames[0].name == "img.png"
        assert frames[0].width == 5
        assert frames[0].height == 10
        assert self.stdout.getvalue() == ""

    def test_can_remove_frames(self, fxt_new_task: Task):
        fxt_new_task.remove_frames_by_ids([0])

        meta = fxt_new_task.get_meta()
        assert meta.deleted_frames == [0]
        assert self.stdout.getvalue() == ""

    def test_can_get_annotations(self, fxt_task_with_shapes: Task):
        anns = fxt_task_with_shapes.get_annotations()

        assert len(anns.shapes) == 1
        assert anns.shapes[0].type.value == "rectangle"
        assert self.stdout.getvalue() == ""

    def test_can_set_annotations(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.set_annotations(
            models.LabeledDataRequest(
                tags=[models.LabeledImageRequest(frame=0, label_id=labels[0].id)],
            )
        )

        anns = fxt_new_task.get_annotations()

        assert len(anns.tags) == 1
        assert self.stdout.getvalue() == ""

    def test_can_clear_annotations(self, fxt_task_with_shapes: Task):
        fxt_task_with_shapes.remove_annotations()

        anns = fxt_task_with_shapes.get_annotations()
        assert len(anns.tags) == 0
        assert len(anns.tracks) == 0
        assert len(anns.shapes) == 0
        assert self.stdout.getvalue() == ""

    def test_can_remove_annotations(self, fxt_new_task: Task):
        labels = fxt_new_task.get_labels()
        fxt_new_task.set_annotations(
            models.LabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[1, 1, 2, 2],
                    ),
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[2, 2, 3, 3],
                    ),
                ],
            )
        )
        anns = fxt_new_task.get_annotations()

        fxt_new_task.remove_annotations(ids=[anns.shapes[0].id])

        anns = fxt_new_task.get_annotations()
        assert len(anns.tags) == 0
        assert len(anns.tracks) == 0
        assert len(anns.shapes) == 1
        assert self.stdout.getvalue() == ""

    def test_can_update_annotations(self, fxt_task_with_shapes: Task):
        labels = fxt_task_with_shapes.get_labels()
        fxt_task_with_shapes.update_annotations(
            models.PatchedLabeledDataRequest(
                shapes=[
                    models.LabeledShapeRequest(
                        frame=0,
                        label_id=labels[0].id,
                        type="rectangle",
                        points=[0, 1, 2, 3],
                    ),
                ],
                tracks=[
                    models.LabeledTrackRequest(
                        frame=0,
                        label_id=labels[0].id,
                        shapes=[
                            models.TrackedShapeRequest(
                                frame=0, type="polygon", points=[3, 2, 2, 3, 3, 4]
                            ),
                        ],
                    )
                ],
                tags=[models.LabeledImageRequest(frame=0, label_id=labels[0].id)],
            )
        )

        anns = fxt_task_with_shapes.get_annotations()
        assert len(anns.shapes) == 2
        assert len(anns.tracks) == 1
        assert len(anns.tags) == 1
        assert self.stdout.getvalue() == ""


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\test_users.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
from logging import Logger
from pathlib import Path

import pytest
from cvat_sdk import Client, models
from cvat_sdk.api_client import exceptions


class TestUserUsecases:
    @pytest.fixture(autouse=True)
    def setup(
        self,
        tmp_path: Path,
        fxt_login: tuple[Client, str],
        fxt_logger: tuple[Logger, io.StringIO],
        fxt_stdout: io.StringIO,
    ):
        self.tmp_path = tmp_path
        logger, self.logger_stream = fxt_logger
        self.stdout = fxt_stdout
        self.client, self.user = fxt_login
        self.client.logger = logger

        api_client = self.client.api_client
        for k in api_client.configuration.logger:
            api_client.configuration.logger[k] = logger

        yield

    def test_can_retrieve_user(self):
        me = self.client.users.retrieve_current_user()

        user = self.client.users.retrieve(me.id)

        assert user.id == me.id
        assert user.username == self.user
        assert self.stdout.getvalue() == ""

    def test_can_list_users(self):
        users = self.client.users.list()

        assert self.user in set(u.username for u in users)
        assert self.stdout.getvalue() == ""

    def test_can_update_user(self):
        user = self.client.users.retrieve_current_user()

        user.update(models.PatchedUserRequest(first_name="foo", last_name="bar"))

        retrieved_user = self.client.users.retrieve(user.id)
        assert retrieved_user.first_name == "foo"
        assert retrieved_user.last_name == "bar"
        assert user.first_name == retrieved_user.first_name
        assert user.last_name == retrieved_user.last_name
        assert self.stdout.getvalue() == ""

    def test_can_remove_user(self):
        users = self.client.users.list()
        removed_user = next(u for u in users if u.username != self.user)

        removed_user.remove()

        with pytest.raises(exceptions.NotFoundException):
            removed_user.fetch()

        assert self.stdout.getvalue() == ""


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\util.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import textwrap
from collections.abc import Container
from pathlib import Path
from urllib.parse import urlparse

import pytest
from cvat_sdk.api_client.rest import RESTClientObject
from cvat_sdk.core.helpers import DeferredTqdmProgressReporter


def make_pbar(file, **kwargs):
    return DeferredTqdmProgressReporter({"file": file, "mininterval": 0, **kwargs})


def generate_coco_json(filename: Path, img_info: tuple[Path, int, int]):
    image_filename, image_width, image_height = img_info

    content = generate_coco_anno(
        image_filename.name,
        image_width=image_width,
        image_height=image_height,
    )
    with open(filename, "w") as coco:
        coco.write(content)


def generate_coco_anno(image_path: str, image_width: int, image_height: int) -> str:
    return (
        textwrap.dedent(
            """
    {
        "licenses": [],
        "info": {},
        "categories": [
            {
                "id": 1,
                "name": "car",
                "supercategory": ""
            },
            {
                "id": 2,
                "name": "person",
                "supercategory": ""
            }
        ],
        "images": [
            {
                "coco_url": "",
                "date_captured": "",
                "flickr_url": "",
                "license": 0,
                "id": 0,
                "file_name": "%(image_path)s",
                "height": %(image_height)d,
                "width": %(image_width)d
            }
        ],
        "annotations": [
            {
                "category_id": 1,
                "id": 1,
                "image_id": 0,
                "iscrowd": 0,
                "segmentation": [
                    []
                ],
                "area": 17702.0,
                "bbox": [
                    574.0,
                    407.0,
                    167.0,
                    106.0
                ]
            }
        ]
    }
    """
        )
        % {
            "image_path": image_path,
            "image_height": image_height,
            "image_width": image_width,
        }
    )


def restrict_api_requests(
    monkeypatch: pytest.MonkeyPatch, allow_paths: Container[str] = ()
) -> None:
    original_request = RESTClientObject.request

    def restricted_request(self, method, url, *args, **kwargs):
        parsed_url = urlparse(url)
        if parsed_url.path in allow_paths:
            return original_request(self, method, url, *args, **kwargs)
        raise RuntimeError("Disallowed!")

    monkeypatch.setattr(RESTClientObject, "request", restricted_request)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\sdk\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\fixtures\data.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import operator
from collections import defaultdict
from collections.abc import Iterable
from copy import deepcopy

import pytest

from shared.utils.config import ASSETS_DIR


class Container:
    def __init__(self, data, key="id"):
        self.raw_data = data
        self.map_data = {obj[key]: obj for obj in data}

    @property
    def raw(self):
        return self.raw_data

    @property
    def map(self):
        return self.map_data

    def __iter__(self):
        return iter(self.raw_data)

    def __len__(self):
        return len(self.raw_data)

    def __getitem__(self, key):
        if isinstance(key, slice):
            return self.raw_data[key]
        return self.map_data[key]


@pytest.fixture(scope="session")
def users():
    with open(ASSETS_DIR / "users.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def organizations():
    with open(ASSETS_DIR / "organizations.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def memberships():
    with open(ASSETS_DIR / "memberships.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def tasks():
    with open(ASSETS_DIR / "tasks.json") as f:
        return Container(json.load(f)["results"])


def filter_assets(resources: Iterable, **kwargs):
    filtered_resources = []
    exclude_prefix = "exclude_"

    for resource in resources:
        is_matched = True
        for key, value in kwargs.items():
            if not is_matched:
                break

            op = operator.eq
            if key.startswith(exclude_prefix):
                key = key[len(exclude_prefix) :]
                op = operator.ne

            cur_value, rest = resource, key
            while rest:
                field_and_rest = rest.split("__", maxsplit=1)
                if 2 == len(field_and_rest):
                    field, rest = field_and_rest
                else:
                    field, rest = field_and_rest[0], None
                cur_value = cur_value[field]
                # e.g. task has null target_storage
                if not cur_value:
                    break

            if not (not rest and op(cur_value, value) or rest and op == operator.ne):
                is_matched = False

        if is_matched:
            filtered_resources.append(resource)

    return filtered_resources


@pytest.fixture(scope="session")
def filter_projects(projects):
    def filter_(**kwargs):
        return filter_assets(projects, **kwargs)

    return filter_


@pytest.fixture(scope="session")
def filter_tasks(tasks):
    def filter_(**kwargs):
        return filter_assets(tasks, **kwargs)

    return filter_


@pytest.fixture(scope="session")
def tasks_wlc(labels, tasks):  # tasks with labels count
    tasks = deepcopy(tasks)
    tasks_by_project = defaultdict(list)
    for task in tasks:
        tasks_by_project[task["project_id"]].append(task)
        task["labels"]["count"] = 0

    for label in labels:
        task_id = label.get("task_id")
        project_id = label.get("project_id")
        if not label["parent_id"]:
            if task_id:
                tasks[task_id]["labels"]["count"] += 1
            elif project_id:
                for task in tasks_by_project[project_id]:
                    task["labels"]["count"] += 1

    return tasks


@pytest.fixture(scope="session")
def projects():
    with open(ASSETS_DIR / "projects.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def projects_wlc(projects, labels):  # projects with labels count
    projects = deepcopy(projects)
    for project in projects:
        project["labels"]["count"] = 0

    for label in labels:
        project_id = label.get("project_id")
        if not label["parent_id"] and project_id:
            projects[project_id]["labels"]["count"] += 1

    return projects


@pytest.fixture(scope="session")
def jobs():
    with open(ASSETS_DIR / "jobs.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def jobs_wlc(jobs, tasks_wlc):  # jobs with labels count
    jobs = deepcopy(jobs)
    for job in jobs:
        tid = job["task_id"]
        job["labels"]["count"] = tasks_wlc[tid]["labels"]["count"]
    return jobs


@pytest.fixture(scope="session")
def invitations():
    with open(ASSETS_DIR / "invitations.json") as f:
        return Container(json.load(f)["results"], key="key")


@pytest.fixture(scope="session")
def annotations():
    with open(ASSETS_DIR / "annotations.json") as f:
        return json.load(f)


CloudStorageAssets = Container


@pytest.fixture(scope="session")
def cloud_storages() -> CloudStorageAssets:
    with open(ASSETS_DIR / "cloudstorages.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def issues():
    with open(ASSETS_DIR / "issues.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def comments():
    with open(ASSETS_DIR / "comments.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def webhooks():
    with open(ASSETS_DIR / "webhooks.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def labels():
    with open(ASSETS_DIR / "labels.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def quality_reports():
    with open(ASSETS_DIR / "quality_reports.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def quality_conflicts():
    with open(ASSETS_DIR / "quality_conflicts.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def quality_settings():
    with open(ASSETS_DIR / "quality_settings.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def consensus_settings():
    with open(ASSETS_DIR / "consensus_settings.json") as f:
        return Container(json.load(f)["results"])


@pytest.fixture(scope="session")
def users_by_name(users):
    return {user["username"]: user for user in users}


@pytest.fixture(scope="session")
def jobs_by_org(tasks, jobs):
    data = {}
    for job in jobs:
        data.setdefault(tasks[job["task_id"]]["organization"], []).append(job)
    data[""] = data.pop(None, [])
    return data


@pytest.fixture(scope="session")
def projects_by_org(projects):
    data = {}
    for project in projects:
        data.setdefault(project["organization"], []).append(project)
    data[""] = data.pop(None, [])
    return data


@pytest.fixture(scope="session")
def tasks_by_org(tasks):
    data = {}
    for task in tasks:
        data.setdefault(task["organization"], []).append(task)
    data[""] = data.pop(None, [])
    return data


@pytest.fixture(scope="session")
def issues_by_org(tasks, jobs, issues):
    data = {}
    for issue in issues:
        data.setdefault(tasks[jobs[issue["job"]]["task_id"]]["organization"], []).append(issue)
    data[""] = data.pop(None, [])
    return data


@pytest.fixture(scope="session")
def assignee_id():
    def get_id(data):
        if data.get("assignee") is not None:
            return data["assignee"]["id"]

    return get_id


def ownership(func):
    def wrap(user_id, resource_id):
        if resource_id is None:
            return False
        return func(user_id, resource_id)

    return wrap


@pytest.fixture(scope="session")
def is_project_staff(projects, assignee_id):
    @ownership
    def check(user_id, pid):
        return user_id == projects[pid]["owner"]["id"] or user_id == assignee_id(projects[pid])

    return check


@pytest.fixture(scope="session")
def is_task_staff(tasks, is_project_staff, assignee_id):
    @ownership
    def check(user_id, tid):
        return (
            user_id == tasks[tid]["owner"]["id"]
            or user_id == assignee_id(tasks[tid])
            or is_project_staff(user_id, tasks[tid]["project_id"])
        )

    return check


@pytest.fixture(scope="session")
def is_job_staff(jobs, is_task_staff, assignee_id):
    @ownership
    def check(user_id, jid):
        return user_id == assignee_id(jobs[jid]) or is_task_staff(user_id, jobs[jid]["task_id"])

    return check


@pytest.fixture(scope="session")
def is_issue_staff(issues, jobs, assignee_id):
    @ownership
    def check(user_id, issue_id):
        return (
            user_id == issues[issue_id]["owner"]["id"]
            or user_id == assignee_id(issues[issue_id])
            or user_id == assignee_id(jobs[issues[issue_id]["job"]])
        )

    return check


@pytest.fixture(scope="session")
def is_issue_admin(issues, jobs, is_task_staff):
    @ownership
    def check(user_id, issue_id):
        return is_task_staff(user_id, jobs[issues[issue_id]["job"]]["task_id"])

    return check


@pytest.fixture(scope="session")
def find_users(test_db):
    def find(**kwargs):
        assert len(kwargs) > 0

        data = test_db
        for field, value in kwargs.items():
            if field.startswith("exclude_"):
                field = field.split("_", maxsplit=1)[1]
                exclude_rows = set(v["id"] for v in filter(lambda a: a[field] == value, test_db))
                data = list(filter(lambda a: a["id"] not in exclude_rows, data))
            else:
                data = list(filter(lambda a: a[field] == value, data))

        return data

    return find


@pytest.fixture(scope="session")
def test_db(users, users_by_name, memberships):
    data = []
    fields = [
        "username",
        "id",
        "privilege",
        "role",
        "org",
        "membership_id",
        "is_superuser",
        "has_analytics_access",
    ]

    def add_row(**kwargs):
        data.append({field: kwargs.get(field) for field in fields})

    for user in users:
        for group in user["groups"]:
            add_row(
                username=user["username"],
                id=user["id"],
                privilege=group,
                has_analytics_access=user["has_analytics_access"],
            )

    for membership in memberships:
        username = membership["user"]["username"]
        for group in users_by_name[username]["groups"]:
            add_row(
                username=username,
                role=membership["role"],
                privilege=group,
                id=membership["user"]["id"],
                org=membership["organization"],
                membership_id=membership["id"],
                has_analytics_access=users_by_name[username]["has_analytics_access"],
            )

    return data


@pytest.fixture(scope="session")
def org_staff(memberships):
    def find(org_id):
        if org_id in ["", None]:
            return set()
        else:
            return set(
                m["user"]["id"]
                for m in memberships
                if m["role"] in ["maintainer", "owner"]
                and m["user"] is not None
                and m["organization"] == org_id
            )

    return find


@pytest.fixture(scope="session")
def is_org_member(memberships):
    def check(user_id, org_id, *, role=None):
        if org_id in ["", None]:
            return True
        else:
            return user_id in set(
                m["user"]["id"]
                for m in memberships
                if m["user"] is not None
                if m["organization"] == org_id
                if not role or m["role"] == role
            )

    return check


@pytest.fixture(scope="session")
def find_job_staff_user(is_job_staff):
    def find(jobs, users, is_staff, wo_jobs=None):
        for job in jobs:
            if wo_jobs is not None and job["id"] in wo_jobs:
                continue
            for user in users:
                if is_staff == is_job_staff(user["id"], job["id"]):
                    return user["username"], job["id"]
        return None, None

    return find


@pytest.fixture(scope="session")
def find_task_staff_user(is_task_staff):
    def find(tasks, users, is_staff, wo_tasks=None):
        for task in tasks:
            if wo_tasks is not None and task["id"] in wo_tasks:
                continue
            for user in users:
                if is_staff == is_task_staff(user["id"], task["id"]):
                    return user["username"], task["id"]
        return None, None

    return find


@pytest.fixture(scope="session")
def find_issue_staff_user(is_issue_staff, is_issue_admin):
    def find(issues, users, is_staff, is_admin):
        for issue in issues:
            for user in users:
                i_admin, i_staff = (
                    is_issue_admin(user["id"], issue["id"]),
                    is_issue_staff(user["id"], issue["id"]),
                )
                if (is_admin is None and (i_staff or i_admin) == is_staff) or (
                    is_admin == i_admin and is_staff == i_staff
                ):
                    return user["username"], issue["id"]
        return None, None

    return find


@pytest.fixture(scope="session")
def filter_jobs_with_shapes(annotations):
    def find(jobs):
        return list(filter(lambda j: annotations["job"].get(str(j["id"]), {}).get("shapes"), jobs))

    return find


@pytest.fixture(scope="session")
def filter_tasks_with_shapes(annotations):
    def find(tasks):
        return list(
            filter(lambda t: annotations["task"].get(str(t["id"]), {}).get("shapes"), tasks)
        )

    return find


@pytest.fixture(scope="session")
def jobs_with_shapes(jobs, filter_jobs_with_shapes):
    return filter_jobs_with_shapes(jobs)


@pytest.fixture(scope="session")
def tasks_with_shapes(tasks, filter_tasks_with_shapes):
    return filter_tasks_with_shapes(tasks)


@pytest.fixture(scope="session")
def admin_user(users):
    for user in users:
        if user["is_superuser"] and user["is_active"]:
            return user["username"]
    raise Exception("Can't find any admin user in the test DB")


@pytest.fixture(scope="session")
def regular_user(users):
    for user in users:
        if not user["is_superuser"] and user["is_active"]:
            return user["username"]
    raise Exception("Can't find any regular user in the test DB")


@pytest.fixture(scope="session")
def regular_lonely_user(users):
    for user in users:
        if user["username"] == "lonely_user":
            return user["username"]
    raise Exception("Can't find the lonely user in the test DB")


@pytest.fixture(scope="session")
def job_has_annotations(annotations) -> bool:
    def check_has_annotations(job_id: int) -> bool:
        job_annotations = annotations["job"][str(job_id)]
        return bool(
            job_annotations["tags"] or job_annotations["shapes"] or job_annotations["tracks"]
        )

    return check_has_annotations


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\fixtures\init.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import logging
import os
import shlex
from enum import Enum
from http import HTTPStatus
from pathlib import Path
from subprocess import PIPE, CalledProcessError, run
from time import sleep
from typing import Union

import pytest
import requests
import yaml

from shared.utils.config import ASSETS_DIR, get_server_url

logger = logging.getLogger(__name__)

CVAT_ROOT_DIR = next(dir.parent for dir in Path(__file__).parents if dir.name == "tests")
CVAT_DB_DIR = ASSETS_DIR / "cvat_db"
PREFIX = "test"

CONTAINER_NAME_FILES = ["docker-compose.tests.yml"]

DC_FILES = CONTAINER_NAME_FILES + [
    "docker-compose.dev.yml",
    "tests/docker-compose.file_share.yml",
    "tests/docker-compose.minio.yml",
    "tests/docker-compose.test_servers.yml",
]


class Container(str, Enum):
    DB = "cvat_db"
    SERVER = "cvat_server"
    WORKER_ANNOTATION = "cvat_worker_annotation"
    WORKER_IMPORT = "cvat_worker_import"
    WORKER_EXPORT = "cvat_worker_export"
    WORKER_QUALITY_REPORTS = "cvat_worker_quality_reports"
    WORKER_WEBHOOKS = "cvat_worker_webhooks"
    WORKER_UTILS = "cvat_worker_utils"

    def __str__(self):
        return self.value

    @classmethod
    def covered(cls):
        return [item.value for item in cls if item != cls.DB]


def pytest_addoption(parser):
    group = parser.getgroup("CVAT REST API testing options")
    group._addoption(
        "--start-services",
        action="store_true",
        help="Start all necessary CVAT containers without running tests. (default: %(default)s)",
    )

    group._addoption(
        "--stop-services",
        action="store_true",
        help="Stop all testing containers without running tests. (default: %(default)s)",
    )

    group._addoption(
        "--rebuild",
        action="store_true",
        help="Rebuild CVAT images and then start containers. (default: %(default)s)",
    )

    group._addoption(
        "--cleanup",
        action="store_true",
        help="Delete files that was create by tests without running tests. (default: %(default)s)",
    )

    group._addoption(
        "--dumpdb",
        action="store_true",
        help="Update data.json without running tests. (default: %(default)s)",
    )

    group._addoption(
        "--platform",
        action="store",
        default="local",
        choices=("kube", "local"),
        help="Platform identifier - 'kube' or 'local'. (default: %(default)s)",
    )


def _run(command, capture_output=True):
    _command = command.split() if isinstance(command, str) else command
    try:
        logger.debug(f"Executing a command: {_command}")

        stdout, stderr = "", ""
        if capture_output:
            proc = run(_command, check=True, stdout=PIPE, stderr=PIPE)  # nosec
            stdout, stderr = proc.stdout.decode(), proc.stderr.decode()
        else:
            proc = run(_command)  # nosec

        if stdout:
            logger.debug(f"Output (stdout): {stdout}")
        if stderr:
            logger.debug(f"Output (stderr): {stderr}")

        return stdout, stderr
    except CalledProcessError as exc:
        message = f"Command failed: {' '.join(map(shlex.quote, _command))}."
        message += f"\nExit code: {exc.returncode}"
        if capture_output:
            message += f"\nStandard output:\n{exc.stdout.decode()}"
            message += f"\nStandard error:\n{exc.stderr.decode()}"

        pytest.exit(message)


def _kube_get_pod_name(label_filter):
    output, _ = _run(f"kubectl get pods -l {label_filter} -o jsonpath={{.items[0].metadata.name}}")
    return output


def _kube_get_server_pod_name():
    return _kube_get_pod_name("component=server")


def _kube_get_db_pod_name():
    return _kube_get_pod_name("app.kubernetes.io/name=postgresql")


def _kube_get_clichouse_pod_name():
    return _kube_get_pod_name("app.kubernetes.io/name=clickhouse")


def _kube_get_redis_inmem_pod_name():
    return _kube_get_pod_name("app.kubernetes.io/name=redis")


def _kube_get_redis_ondisk_pod_name():
    return _kube_get_pod_name("app.kubernetes.io/name=cvat,tier=kvrocks")


def docker_cp(source, target):
    _run(f"docker container cp {source} {target}")


def kube_cp(source, target):
    _run(f"kubectl cp {source} {target}")


def docker_exec(container, command, capture_output=True):
    return _run(f"docker exec -u root {PREFIX}_{container}_1 {command}", capture_output)


def docker_exec_cvat(command: Union[list[str], str]):
    base = f"docker exec {PREFIX}_cvat_server_1"
    _command = f"{base} {command}" if isinstance(command, str) else base.split() + command
    return _run(_command)


def kube_exec_cvat(command: Union[list[str], str]):
    pod_name = _kube_get_server_pod_name()
    base = f"kubectl exec {pod_name} --"
    _command = f"{base} {command}" if isinstance(command, str) else base.split() + command
    return _run(_command)


def container_exec_cvat(request: pytest.FixtureRequest, command: Union[list[str], str]):
    platform = request.config.getoption("--platform")
    if platform == "local":
        return docker_exec_cvat(command)
    elif platform == "kube":
        return kube_exec_cvat(command)
    else:
        assert False, "unknown platform"


def kube_exec_cvat_db(command):
    pod_name = _kube_get_db_pod_name()
    _run(["kubectl", "exec", pod_name, "--"] + command)


def docker_exec_clickhouse_db(command):
    _run(["docker", "exec", f"{PREFIX}_cvat_clickhouse_1"] + command)


def kube_exec_clickhouse_db(command):
    pod_name = _kube_get_clichouse_pod_name()
    _run(["kubectl", "exec", pod_name, "--"] + command)


def docker_exec_redis_inmem(command):
    return _run(["docker", "exec", f"{PREFIX}_cvat_redis_inmem_1"] + command)


def kube_exec_redis_inmem(command):
    pod_name = _kube_get_redis_inmem_pod_name()
    return _run(["kubectl", "exec", pod_name, "--"] + command)


def docker_exec_redis_ondisk(command):
    _run(["docker", "exec", f"{PREFIX}_cvat_redis_ondisk_1"] + command)


def kube_exec_redis_ondisk(command):
    pod_name = _kube_get_redis_ondisk_pod_name()
    _run(["kubectl", "exec", pod_name, "--"] + command)


def docker_restore_db():
    docker_exec(
        Container.DB, "psql -U root -d postgres -v from=test_db -v to=cvat -f /tmp/restore.sql"
    )


def kube_restore_db():
    kube_exec_cvat_db(
        [
            "/bin/sh",
            "-c",
            "PGPASSWORD=cvat_postgresql_postgres psql -U postgres -d postgres -v from=test_db -v to=cvat -f /tmp/restore.sql",
        ]
    )


def docker_restore_clickhouse_db():
    docker_exec_clickhouse_db(
        [
            "/bin/sh",
            "-c",
            'clickhouse-client --query "DROP TABLE IF EXISTS ${CLICKHOUSE_DB}.events;" && /docker-entrypoint-initdb.d/init.sh',
        ]
    )


def kube_restore_clickhouse_db():
    kube_exec_clickhouse_db(
        [
            "/bin/sh",
            "-c",
            'clickhouse-client --query "DROP TABLE IF EXISTS ${CLICKHOUSE_DB}.events;" && /bin/sh /docker-entrypoint-initdb.d/init.sh',
        ]
    )


def _get_redis_inmem_keys_to_keep():
    return (
        "rq:worker:",
        "rq:workers",
        "rq:scheduler_instance:",
        "rq:queues:",
        "cvat:applied_migrations",
        "cvat:applied_migration:",
    )


def docker_restore_redis_inmem():
    docker_exec_redis_inmem(
        [
            "sh",
            "-c",
            'redis-cli -e --scan --pattern "*" |'
            'grep -v "' + r"\|".join(_get_redis_inmem_keys_to_keep()) + '" |'
            "xargs -r redis-cli -e del",
        ]
    )


def kube_restore_redis_inmem():
    kube_exec_redis_inmem(
        [
            "sh",
            "-c",
            'export REDISCLI_AUTH="${REDIS_PASSWORD}" && '
            'redis-cli -e --scan --pattern "*" | '
            'grep -v "' + r"\|".join(_get_redis_inmem_keys_to_keep()) + '" | '
            "xargs -r redis-cli -e del",
        ]
    )


def docker_restore_redis_ondisk():
    docker_exec_redis_ondisk(["redis-cli", "-e", "-p", "6666", "flushall"])


def kube_restore_redis_ondisk():
    kube_exec_redis_ondisk(
        ["sh", "-c", 'REDISCLI_AUTH="${CVAT_REDIS_ONDISK_PASSWORD}" redis-cli -e -p 6666 flushall']
    )


def running_containers():
    return [cn for cn in _run("docker ps --format {{.Names}}")[0].split("\n") if cn]


def dump_db():
    if "test_cvat_server_1" not in running_containers():
        pytest.exit("CVAT is not running")
    with open(CVAT_DB_DIR / "data.json", "w") as f:
        try:
            run(  # nosec
                "docker exec test_cvat_server_1 \
                    python manage.py dumpdata \
                    --indent 2 --natural-foreign \
                    --exclude=auth.permission --exclude=contenttypes".split(),
                stdout=f,
                check=True,
            )
        except CalledProcessError:
            pytest.exit("Database dump failed.\n")


def create_compose_files(container_name_files):
    for filename in container_name_files:
        with (
            open(filename.with_name(filename.name.replace(".tests", "")), "r") as dcf,
            open(filename, "w") as ndcf,
        ):
            dc_config = yaml.safe_load(dcf)

            for service_name, service_config in dc_config["services"].items():
                service_config.pop("container_name", None)
                if service_name in (Container.SERVER, Container.WORKER_UTILS):
                    service_env = service_config["environment"]
                    service_env["DJANGO_SETTINGS_MODULE"] = "cvat.settings.testing_rest"

                if service_name in Container.covered():
                    service_env = service_config["environment"]
                    service_env["COVERAGE_PROCESS_START"] = ".coveragerc"
                    service_config["volumes"].append(
                        "./tests/python/.coveragerc:/home/django/.coveragerc"
                    )

            yaml.dump(dc_config, ndcf)


def delete_compose_files(container_name_files):
    for filename in container_name_files:
        filename.unlink(missing_ok=True)


def wait_for_services(num_secs: int = 300) -> None:
    for i in range(num_secs):
        logger.debug(f"waiting for the server to load ... ({i})")

        try:
            response = requests.get(get_server_url("api/server/health/", format="json"))

            statuses = response.json()
            logger.debug(f"server status: \n{statuses}")

            if response.status_code == HTTPStatus.OK:
                logger.debug("the server has finished loading!")
                return

        except Exception as e:
            logger.debug(f"an error occurred during the server status checking: {e}")

        sleep(1)

    raise Exception(
        f"Failed to reach the server during {num_secs} seconds. Please check the configuration."
    )


def docker_restore_data_volumes():
    docker_cp(
        CVAT_DB_DIR / "cvat_data.tar.bz2",
        f"{PREFIX}_cvat_server_1:/tmp/cvat_data.tar.bz2",
    )
    docker_exec_cvat("tar --strip 3 -xjf /tmp/cvat_data.tar.bz2 -C /home/django/data/")


def kube_restore_data_volumes():
    pod_name = _kube_get_server_pod_name()
    kube_cp(
        CVAT_DB_DIR / "cvat_data.tar.bz2",
        f"{pod_name}:/tmp/cvat_data.tar.bz2",
    )
    kube_exec_cvat("tar --strip 3 -xjf /tmp/cvat_data.tar.bz2 -C /home/django/data/")


def get_server_image_tag():
    return f"cvat/server:{os.environ.get('CVAT_VERSION', 'dev')}"


def docker_compose(dc_files, cvat_root_dir):
    return [
        "docker",
        "compose",
        f"--project-name={PREFIX}",
        # use compatibility mode to have fixed names for containers (with underscores)
        # https://github.com/docker/compose#about-update-and-backward-compatibility
        "--compatibility",
        f"--env-file={cvat_root_dir / 'tests/python/webhook_receiver/.env'}",
        *(f"--file={f}" for f in dc_files),
    ]


def start_services(dc_files, rebuild=False, cvat_root_dir=CVAT_ROOT_DIR):
    if any([cn in ["cvat_server", "cvat_db"] for cn in running_containers()]):
        pytest.exit(
            "It's looks like you already have running cvat containers. Stop them and try again. "
            f"List of running containers: {', '.join(running_containers())}"
        )

    _run(
        docker_compose(dc_files, cvat_root_dir) + ["up", "-d", *["--build"] * rebuild],
        capture_output=False,
    )


def stop_services(dc_files, cvat_root_dir=CVAT_ROOT_DIR):
    run(docker_compose(dc_files, cvat_root_dir) + ["down", "-v"], capture_output=False)


def session_start(
    session,
    cvat_root_dir=CVAT_ROOT_DIR,
    cvat_db_dir=CVAT_DB_DIR,
    extra_dc_files=None,
    waiting_time=300,
):
    stop = session.config.getoption("--stop-services")
    start = session.config.getoption("--start-services")
    rebuild = session.config.getoption("--rebuild")
    cleanup = session.config.getoption("--cleanup")
    dumpdb = session.config.getoption("--dumpdb")

    if session.config.getoption("--collect-only"):
        if any((stop, start, rebuild, cleanup, dumpdb)):
            raise Exception(
                """--collect-only is not compatible with any of the other options:
                --stop-services --start-services --rebuild --cleanup --dumpdb"""
            )
        return  # don't need to start the services to collect tests

    platform = session.config.getoption("--platform")

    if platform == "kube" and any((stop, start, rebuild, cleanup, dumpdb)):
        raise Exception(
            """--platform=kube is not compatible with any of the other options
            --stop-services --start-services --rebuild --cleanup --dumpdb"""
        )

    if platform == "local":
        local_start(
            start,
            stop,
            dumpdb,
            cleanup,
            rebuild,
            cvat_root_dir,
            cvat_db_dir,
            extra_dc_files,
            waiting_time,
        )

    elif platform == "kube":
        kube_start(cvat_db_dir)


def local_start(
    start, stop, dumpdb, cleanup, rebuild, cvat_root_dir, cvat_db_dir, extra_dc_files, waiting_time
):
    if start and stop:
        raise Exception("--start-services and --stop-services are incompatible")

    if dumpdb:
        dump_db()
        pytest.exit("data.json has been updated", returncode=0)

    dc_files = [cvat_root_dir / f for f in DC_FILES]
    if extra_dc_files is not None:
        dc_files += extra_dc_files

    container_name_files = [cvat_root_dir / f for f in CONTAINER_NAME_FILES]

    if cleanup:
        delete_compose_files(container_name_files)
        pytest.exit("All generated test files have been deleted", returncode=0)

    if not all([f.exists() for f in container_name_files]) or rebuild:
        delete_compose_files(container_name_files)
        create_compose_files(container_name_files)

    if stop:
        stop_services(dc_files, cvat_root_dir)
        pytest.exit("All testing containers are stopped", returncode=0)

    start_services(dc_files, rebuild, cvat_root_dir)

    docker_restore_data_volumes()
    docker_cp(cvat_db_dir / "restore.sql", f"{PREFIX}_cvat_db_1:/tmp/restore.sql")
    docker_cp(cvat_db_dir / "data.json", f"{PREFIX}_cvat_server_1:/tmp/data.json")

    wait_for_services(waiting_time)

    docker_exec_cvat("python manage.py loaddata /tmp/data.json")
    docker_exec(
        Container.DB, "psql -U root -d postgres -v from=cvat -v to=test_db -f /tmp/restore.sql"
    )

    if start:
        pytest.exit("All necessary containers have been created and started.", returncode=0)


def kube_start(cvat_db_dir):
    kube_restore_data_volumes()
    server_pod_name = _kube_get_server_pod_name()
    db_pod_name = _kube_get_db_pod_name()
    kube_cp(cvat_db_dir / "restore.sql", f"{db_pod_name}:/tmp/restore.sql")
    kube_cp(cvat_db_dir / "data.json", f"{server_pod_name}:/tmp/data.json")

    wait_for_services()

    kube_exec_cvat("python manage.py loaddata /tmp/data.json")

    kube_exec_cvat_db(
        [
            "/bin/sh",
            "-c",
            "PGPASSWORD=cvat_postgresql_postgres psql -U postgres -d postgres -v from=cvat -v to=test_db -f /tmp/restore.sql",
        ]
    )


def pytest_sessionstart(session: pytest.Session) -> None:
    session_start(session)


def pytest_sessionfinish(session: pytest.Session, exitstatus: int) -> None:
    session_finish(session)


def session_finish(session):
    if session.config.getoption("--collect-only"):
        return

    platform = session.config.getoption("--platform")

    if platform == "local":
        if os.environ.get("COVERAGE_PROCESS_START"):
            collect_code_coverage_from_containers()

        docker_restore_db()
        docker_exec(Container.DB, "dropdb test_db")

        docker_exec(Container.DB, "dropdb --if-exists cvat")
        docker_exec(Container.DB, "createdb cvat")
        docker_exec_cvat("python manage.py migrate")


def collect_code_coverage_from_containers():
    for container in Container.covered():
        process_command = "python3"

        # find process with code coverage
        pid, _ = docker_exec(container, f"pidof {process_command} -o 1")

        # stop process with code coverage
        docker_exec(container, f"kill -15 {pid}")
        sleep(3)

        # get code coverage report
        docker_exec(container, "coverage combine", capture_output=False)
        docker_exec(container, "coverage json", capture_output=False)
        docker_cp(
            f"{PREFIX}_{container}_1:home/django/coverage.json",
            f"coverage_{container}.json",
        )


@pytest.fixture(scope="function")
def restore_db_per_function(request):
    # Note that autouse fixtures are executed first within their scope, so be aware of the order
    # Pre-test DB setups (eg. with class-declared autouse setup() method) may be cleaned.
    # https://docs.pytest.org/en/stable/reference/fixtures.html#autouse-fixtures-are-executed-first-within-their-scope
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_db()
    else:
        kube_restore_db()


@pytest.fixture(scope="class")
def restore_db_per_class(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_db()
    else:
        kube_restore_db()


@pytest.fixture(scope="function")
def restore_cvat_data_per_function(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_data_volumes()
    else:
        kube_restore_data_volumes()


@pytest.fixture(scope="class")
def restore_cvat_data_per_class(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_data_volumes()
    else:
        kube_restore_data_volumes()


@pytest.fixture(scope="function")
def restore_clickhouse_db_per_function(request):
    # Note that autouse fixtures are executed first within their scope, so be aware of the order
    # Pre-test DB setups (eg. with class-declared autouse setup() method) may be cleaned.
    # https://docs.pytest.org/en/stable/reference/fixtures.html#autouse-fixtures-are-executed-first-within-their-scope
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_clickhouse_db()
    else:
        kube_restore_clickhouse_db()


@pytest.fixture(scope="class")
def restore_clickhouse_db_per_class(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_clickhouse_db()
    else:
        kube_restore_clickhouse_db()


@pytest.fixture(scope="function")
def restore_redis_inmem_per_function(request):
    # Note that autouse fixtures are executed first within their scope, so be aware of the order
    # Pre-test DB setups (eg. with class-declared autouse setup() method) may be cleaned.
    # https://docs.pytest.org/en/stable/reference/fixtures.html#autouse-fixtures-are-executed-first-within-their-scope
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_redis_inmem()
    else:
        kube_restore_redis_inmem()


@pytest.fixture(scope="class")
def restore_redis_inmem_per_class(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_redis_inmem()
    else:
        kube_restore_redis_inmem()


@pytest.fixture(scope="function")
def restore_redis_ondisk_per_function(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_redis_ondisk()
    else:
        kube_restore_redis_ondisk()


@pytest.fixture(scope="class")
def restore_redis_ondisk_per_class(request):
    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_redis_ondisk()
    else:
        kube_restore_redis_ondisk()


@pytest.fixture(scope="class")
def restore_redis_ondisk_after_class(request):
    yield

    platform = request.config.getoption("--platform")
    if platform == "local":
        docker_restore_redis_ondisk()
    else:
        kube_restore_redis_ondisk()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\fixtures\util.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import io
import logging

import pytest


@pytest.fixture
def fxt_stdout(capsys):
    class IoProxy(io.IOBase):
        def __init__(self, capsys):
            self.capsys = capsys

        def getvalue(self) -> str:
            capture = self.capsys.readouterr()
            return capture.out

    yield IoProxy(capsys)


@pytest.fixture
def fxt_logger():
    logger_stream = io.StringIO()
    logger = logging.Logger("test", level=logging.INFO)
    logger.propagate = False
    logger.addHandler(logging.StreamHandler(logger_stream))
    yield logger, logger_stream


@pytest.fixture
def fxt_test_name(request: pytest.FixtureRequest):
    name = request.node.name
    if request.fixturename:
        name += f"[{request.fixturename}]"

    yield name


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\fixtures\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\config.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from collections.abc import Generator
from contextlib import contextmanager
from pathlib import Path

import requests
from cvat_sdk.api_client import ApiClient, Configuration
from cvat_sdk.core.client import Client, Config

ROOT_DIR = next(dir.parent for dir in Path(__file__).parents if dir.name == "utils")
ASSETS_DIR = (ROOT_DIR / "assets").resolve()
# Suppress the warning from Bandit about hardcoded passwords
USER_PASS = "!Q@W#E$R"  # nosec
BASE_URL = "http://localhost:8080"
API_URL = BASE_URL + "/api/"

# MiniIO settings
MINIO_KEY = "minio_access_key"
MINIO_SECRET_KEY = "minio_secret_key"  # nosec
MINIO_ENDPOINT_URL = "http://localhost:9000"
IMPORT_EXPORT_BUCKET_ID = 3


def _to_query_params(**kwargs):
    return "&".join([f"{k}={v}" for k, v in kwargs.items()])


def get_server_url(endpoint, **kwargs):
    return BASE_URL + "/" + endpoint + "?" + _to_query_params(**kwargs)


def get_api_url(endpoint, **kwargs):
    return API_URL + endpoint + "?" + _to_query_params(**kwargs)


def get_method(username, endpoint, **kwargs):
    return requests.get(get_api_url(endpoint, **kwargs), auth=(username, USER_PASS))


def options_method(username, endpoint, **kwargs):
    return requests.options(get_api_url(endpoint, **kwargs), auth=(username, USER_PASS))


def delete_method(username, endpoint, **kwargs):
    return requests.delete(get_api_url(endpoint, **kwargs), auth=(username, USER_PASS))


def patch_method(username, endpoint, data, **kwargs):
    return requests.patch(get_api_url(endpoint, **kwargs), json=data, auth=(username, USER_PASS))


def post_method(username, endpoint, data, **kwargs):
    return requests.post(get_api_url(endpoint, **kwargs), json=data, auth=(username, USER_PASS))


def post_files_method(username, endpoint, data, files, **kwargs):
    return requests.post(
        get_api_url(endpoint, **kwargs), data=data, files=files, auth=(username, USER_PASS)
    )


def put_method(username, endpoint, data, **kwargs):
    return requests.put(get_api_url(endpoint, **kwargs), json=data, auth=(username, USER_PASS))


def server_get(username, endpoint, **kwargs):
    return requests.get(get_server_url(endpoint, **kwargs), auth=(username, USER_PASS))


def make_api_client(user: str, *, password: str = None) -> ApiClient:
    return ApiClient(
        configuration=Configuration(host=BASE_URL, username=user, password=password or USER_PASS)
    )


@contextmanager
def make_sdk_client(user: str, *, password: str = None) -> Generator[Client, None, None]:
    with Client(BASE_URL, config=Config(status_check_period=0.01)) as client:
        client.login((user, password or USER_PASS))
        yield client


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\dump_objects.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

"""
This script lists resources on the CVAT server
and saves them to JSON files in the `assets` directory.
Before running it, start the test instance by running:

    pytest tests/python --start-services

The script determines which endpoints to query by looking at the set of existing JSON files.
For example, if `tasks.json` exists, the script will overwrite it with output of `GET /api/tasks`.
Underscores in the file name are replaced with slashes in the URL path.
In addition, `annotations.json` is always saved.
"""

import argparse
import json
from datetime import timezone
from http import HTTPStatus
from pathlib import Path
from typing import Any

from config import ASSETS_DIR, get_method
from dateutil.parser import ParserError, parse


def clean_list_response(data: dict[str, Any]) -> dict[str, Any]:
    # truncate milliseconds to 3 digit precision to align with data.json
    # "2023-03-30T09:37:31.615123Z" ->
    # "2023-03-30T09:37:31.615000Z"

    for result in data["results"]:
        for k, v in result.items():
            if not isinstance(v, str):
                continue

            try:
                parsed_date = parse(v)
            except ParserError:
                continue

            parsed_date = parsed_date.replace(
                microsecond=parsed_date.microsecond - (parsed_date.microsecond % 1000)
            )
            result[k] = parsed_date.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

    return data


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--assets-dir", type=Path, default=ASSETS_DIR)
    args = parser.parse_args()

    assets_dir: Path = args.assets_dir

    annotations = {}

    for dump_path in assets_dir.glob("*.json"):
        endpoint = dump_path.stem.replace("_", "/")

        if endpoint == "annotations":
            continue  # this will be handled at the end

        response = get_method("admin1", endpoint, page_size="all")

        with open(dump_path, "w") as f:
            json.dump(clean_list_response(response.json()), f, indent=2, sort_keys=True)

        if endpoint in ["jobs", "tasks"]:
            obj = endpoint.removesuffix("s")
            annotations[obj] = {}
            for _obj in response.json()["results"]:
                oid = _obj["id"]

                response = get_method("admin1", f"{endpoint}/{oid}/annotations")
                if response.status_code == HTTPStatus.OK:
                    annotations[obj][oid] = response.json()

    with open(assets_dir / "annotations.json", "w") as f:
        json.dump(annotations, f, indent=2, sort_keys=True)


if __name__ == "__main__":
    main()


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\helpers.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import subprocess
from collections.abc import Generator
from contextlib import closing
from io import BytesIO
from typing import Optional

import av
import av.video.reformatter
from PIL import Image

from shared.fixtures.init import get_server_image_tag


def generate_image_file(filename="image.png", size=(100, 50), color=(0, 0, 0)):
    f = BytesIO()
    f.name = filename
    image = Image.new("RGB", size=size, color=color)
    image.save(f)
    f.seek(0)

    return f


def generate_image_files(
    count: int,
    *,
    prefixes: Optional[list[str]] = None,
    filenames: Optional[list[str]] = None,
    sizes: Optional[list[tuple[int, int]]] = None,
) -> list[BytesIO]:
    assert not (prefixes and filenames), "prefixes cannot be used together with filenames"
    assert not prefixes or len(prefixes) == count
    assert not filenames or len(filenames) == count

    images = []
    for i in range(count):
        prefix = prefixes[i] if prefixes else ""
        filename = f"{prefix}{i}.jpeg" if not filenames else filenames[i]
        image = generate_image_file(
            filename, color=(i, i, i), **({"size": sizes[i]}) if sizes else {}
        )
        images.append(image)

    return images


def generate_video_file(num_frames: int, size=(100, 50)) -> BytesIO:
    f = BytesIO()
    f.name = "video.avi"

    with av.open(f, "w") as container:
        stream = container.add_stream("mjpeg", rate=60)
        stream.width = size[0]
        stream.height = size[1]
        stream.color_range = av.video.reformatter.ColorRange.JPEG

        for i in range(num_frames):
            frame = av.VideoFrame.from_image(Image.new("RGB", size=size, color=(i, i, i)))
            for packet in stream.encode(frame):
                container.mux(packet)

    f.seek(0)

    return f


def read_video_file(file: BytesIO) -> Generator[Image.Image, None, None]:
    file.seek(0)

    with av.open(file) as container:
        video_stream = container.streams.video[0]

        with closing(video_stream.codec_context):  # pyav has a memory leak in stream.close()
            with closing(container.demux(video_stream)) as demux_iter:
                for packet in demux_iter:
                    for frame in packet.decode():
                        yield frame.to_image()


def generate_manifest(path: str) -> None:
    command = [
        "docker",
        "run",
        "--rm",
        "-u",
        "root:root",
        "-v",
        f"{path}:/local",
        "--entrypoint",
        "python3",
        get_server_image_tag(),
        "utils/dataset_manifest/create.py",
        "--output-dir",
        "/local",
        "/local",
    ]
    subprocess.check_output(command)


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\resource_import_export.py =====
import functools
import json
from abc import ABC, abstractmethod
from contextlib import ExitStack
from http import HTTPStatus
from time import sleep
from typing import Any, Optional, TypeVar

import pytest

T = TypeVar("T")

from shared.utils.config import get_method, post_method, put_method

FILENAME_TEMPLATE = "cvat/{}/{}.zip"
EXPORT_FORMAT = "CVAT for images 1.1"
IMPORT_FORMAT = "CVAT 1.1"


def _make_custom_resource_params(resource: str, obj: str, cloud_storage_id: int) -> dict[str, Any]:
    return {
        "filename": FILENAME_TEMPLATE.format(obj, resource),
        "location": "cloud_storage",
        "cloud_storage_id": cloud_storage_id,
    }


def _make_default_resource_params(resource: str, obj: str) -> dict[str, Any]:
    return {
        "filename": FILENAME_TEMPLATE.format(obj, resource),
    }


def _make_export_resource_params(
    resource: str, is_default: bool = True, **kwargs
) -> dict[str, Any]:
    func = _make_default_resource_params if is_default else _make_custom_resource_params
    params = func(resource, **kwargs)
    if resource != "backup":
        params["format"] = EXPORT_FORMAT
        params["save_images"] = resource == "dataset"

    return params


def _make_import_resource_params(
    resource: str, is_default: bool = True, **kwargs
) -> dict[str, Any]:
    func = _make_default_resource_params if is_default else _make_custom_resource_params
    params = func(resource, **kwargs)
    if resource != "backup":
        params["format"] = IMPORT_FORMAT
    return params


class _CloudStorageResourceTest(ABC):
    @staticmethod
    @abstractmethod
    def _make_client():
        pass

    @pytest.fixture(autouse=True)
    def setup(self, admin_user: str):
        self.user = admin_user
        self.client = self._make_client()
        self.exit_stack = ExitStack()
        with self.exit_stack:
            yield

    def _ensure_file_created(self, func: T, storage: dict[str, Any]) -> T:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            filename = kwargs["filename"]
            bucket = storage["resource"]

            # check that file doesn't exist on the bucket
            assert not self.client.file_exists(bucket=bucket, filename=filename)

            func(*args, **kwargs)

            # check that file exists on the bucket
            assert self.client.file_exists(bucket=bucket, filename=filename)

        return wrapper

    def _export_resource_to_cloud_storage(
        self,
        obj_id: int,
        obj: str,
        resource: str,
        *,
        user: str,
        _expect_status: Optional[int] = None,
        **kwargs,
    ):
        _expect_status = _expect_status or HTTPStatus.ACCEPTED

        sleep_interval = 0.1
        number_of_checks = 100

        # initialize the export process
        response = post_method(
            user,
            f"{obj}/{obj_id}/{resource if resource != 'annotations' else 'dataset'}/export",
            data=None,
            **kwargs,
        )
        assert response.status_code == _expect_status

        if _expect_status == HTTPStatus.FORBIDDEN:
            return

        rq_id = json.loads(response.content).get("rq_id")
        assert rq_id, "The rq_id was not found in server request"

        for _ in range(number_of_checks):
            sleep(sleep_interval)
            # use new requests API for checking the status of the operation
            response = get_method(user, f"requests/{rq_id}")
            assert response.status_code == HTTPStatus.OK

            request_details = json.loads(response.content)
            status = request_details["status"]
            assert status in {"started", "queued", "finished", "failed"}
            if status in {"finished", "failed"}:
                break

    def _import_resource_from_cloud_storage(
        self, url: str, *, user: str, _expect_status: Optional[int] = None, **kwargs
    ) -> None:
        _expect_status = _expect_status or HTTPStatus.ACCEPTED

        response = post_method(user, url, data=None, **kwargs)
        status = response.status_code

        assert status == _expect_status
        if status == HTTPStatus.FORBIDDEN:
            return

        rq_id = response.json().get("rq_id")
        assert rq_id, "The rq_id parameter was not found in the server response"

        number_of_checks = 100
        sleep_interval = 0.1

        for _ in range(number_of_checks):
            sleep(sleep_interval)
            # use new requests API for checking the status of the operation
            response = get_method(user, f"requests/{rq_id}")
            assert response.status_code == HTTPStatus.OK

            request_details = json.loads(response.content)
            status = request_details["status"]
            assert status in {"started", "queued", "finished", "failed"}
            if status in {"finished", "failed"}:
                break

    def _import_annotations_from_cloud_storage(
        self,
        obj_id,
        obj,
        *,
        user,
        _expect_status: Optional[int] = None,
        _check_uploaded: bool = True,
        **kwargs,
    ):
        _expect_status = _expect_status or HTTPStatus.CREATED

        url = f"{obj}/{obj_id}/annotations"
        response = post_method(user, url, data=None, **kwargs)
        status = response.status_code

        # Only the first POST request contains rq_id in response.
        # Exclude cases with 403 expected status.
        rq_id = None
        if status == HTTPStatus.ACCEPTED:
            rq_id = response.json().get("rq_id")
            assert rq_id, "The rq_id was not found in the response"

        while status != _expect_status:
            assert status == HTTPStatus.ACCEPTED
            response = put_method(user, url, data=None, rq_id=rq_id, **kwargs)
            status = response.status_code

        if _check_uploaded:
            response = get_method(user, url)
            assert response.status_code == HTTPStatus.OK

            annotations = response.json()

            assert len(annotations["shapes"])

    def _import_backup_from_cloud_storage(
        self, obj_id, obj, *, user, _expect_status: Optional[int] = None, **kwargs
    ):
        _expect_status = _expect_status or HTTPStatus.CREATED

        url = f"{obj}/backup"
        response = post_method(user, url, data=None, **kwargs)
        status = response.status_code

        while status != _expect_status:
            assert status == HTTPStatus.ACCEPTED
            data = json.loads(response.content.decode("utf8"))
            response = post_method(user, url, data=data, **kwargs)
            status = response.status_code

    def _import_dataset_from_cloud_storage(
        self, obj_id, obj, *, user, _expect_status: Optional[int] = None, **kwargs
    ):
        _expect_status = _expect_status or HTTPStatus.CREATED

        url = f"{obj}/{obj_id}/dataset"
        response = post_method(user, url, data=None, **kwargs)
        status = response.status_code

        # Only the first POST request contains rq_id in response.
        # Exclude cases with 403 expected status.
        rq_id = None
        if status == HTTPStatus.ACCEPTED:
            rq_id = response.json().get("rq_id")
            assert rq_id, "The rq_id was not found in the response"

        while status != _expect_status:
            assert status == HTTPStatus.ACCEPTED
            response = get_method(user, url, action="import_status", rq_id=rq_id)
            status = response.status_code

    def _import_resource(self, cloud_storage: dict[str, Any], resource_type: str, *args, **kwargs):
        methods = {
            "annotations": self._import_annotations_from_cloud_storage,
            "dataset": self._import_dataset_from_cloud_storage,
            "backup": self._import_backup_from_cloud_storage,
        }

        org_id = cloud_storage["organization"]
        if org_id:
            kwargs.setdefault("org_id", org_id)

        kwargs.setdefault("user", self.user)

        return methods[resource_type](*args, **kwargs)

    def _export_resource(self, cloud_storage: dict[str, Any], *args, **kwargs):
        org_id = cloud_storage["organization"]
        if org_id:
            kwargs.setdefault("org_id", org_id)

        kwargs.setdefault("user", self.user)

        export_callback = self._ensure_file_created(
            self._export_resource_to_cloud_storage, storage=cloud_storage
        )
        export_callback(*args, **kwargs)

        self.exit_stack.callback(
            self.client.remove_file,
            bucket=cloud_storage["resource"],
            filename=kwargs["filename"],
        )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\s3.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from io import BytesIO
from typing import Optional

import boto3
from botocore.exceptions import ClientError

from shared.utils.config import MINIO_ENDPOINT_URL, MINIO_KEY, MINIO_SECRET_KEY


class S3Client:
    def __init__(
        self, endpoint_url: str, *, access_key: str, secret_key: str, bucket: Optional[str] = None
    ) -> None:
        self.client = self._make_boto_client(
            endpoint_url=endpoint_url, access_key=access_key, secret_key=secret_key
        )
        self.bucket = bucket

    @staticmethod
    def _make_boto_client(endpoint_url: str, *, access_key: str, secret_key: str):
        s3 = boto3.resource(
            "s3",
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            endpoint_url=endpoint_url,
        )
        return s3.meta.client

    def create_file(self, filename: str, data: bytes = b"", *, bucket: Optional[str] = None):
        bucket = bucket or self.bucket
        assert bucket
        self.client.put_object(Body=data, Bucket=bucket, Key=filename)

    def remove_file(self, filename: str, *, bucket: Optional[str] = None):
        bucket = bucket or self.bucket
        assert bucket
        self.client.delete_object(Bucket=bucket, Key=filename)

    def file_exists(self, filename: str, *, bucket: Optional[str] = None) -> bool:
        bucket = bucket or self.bucket
        assert bucket
        try:
            self.client.head_object(Bucket=bucket, Key=filename)
            return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            else:
                raise

    def download_fileobj(self, key: str, *, bucket: Optional[str] = None) -> bytes:
        bucket = bucket or self.bucket
        assert bucket
        with BytesIO() as data:
            self.client.download_fileobj(Bucket=bucket, Key=key, Fileobj=data)
            return data.getvalue()


def make_client(*, bucket: Optional[str] = None) -> S3Client:
    return S3Client(
        endpoint_url=MINIO_ENDPOINT_URL,
        access_key=MINIO_KEY,
        secret_key=MINIO_SECRET_KEY,
        bucket=bucket,
    )


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\shared\utils\__init__.py =====


# ===== 文件: D:\wow_ai\docker\cvat\tests\python\webhook_receiver\server.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import os
import re
from http import HTTPStatus
from http.server import BaseHTTPRequestHandler, HTTPServer


class RequestHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        TARGET_URL_PATTERN = re.compile(r"/" + os.getenv("PAYLOAD_ENDPOINT"))
        if not re.search(TARGET_URL_PATTERN, self.path):
            return

        self.send_response(HTTPStatus.OK)
        self.end_headers()

        request_body = self.rfile.read(int(self.headers["content-length"]))
        self.wfile.write(request_body)


def main():
    TARGET_HOST = "0.0.0.0"
    TARGET_PORT = int(os.getenv("SERVER_PORT"))

    webhook_receiver = HTTPServer((TARGET_HOST, TARGET_PORT), RequestHandler)
    webhook_receiver.serve_forever()


if __name__ == "__main__":
    main()


# ===== 文件: D:\wow_ai\docker\cvat\utils\__init__.py =====
# Copyright (C) 2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\core.py =====
# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import json
import os
from abc import ABC, abstractmethod
from collections.abc import Iterator
from contextlib import closing
from enum import Enum
from inspect import isgenerator
from io import StringIO
from itertools import islice
from json.decoder import JSONDecodeError
from typing import Any, Callable, Optional, Union

import av
from PIL import Image

from .errors import InvalidManifestError, InvalidVideoError
from .types import NamedBytesIO
from .utils import SortingMethod, md5_hash, rotate_image, sort


class VideoStreamReader:
    def __init__(self, source_path, chunk_size, force):
        self._source_path = source_path
        self._frames_number = None
        self._force = force
        self._upper_bound = 3 * chunk_size + 1

        with closing(av.open(self.source_path, mode="r")) as container:
            video_stream = VideoStreamReader._get_video_stream(container)
            for packet in container.demux(video_stream):
                for frame in packet.decode():
                    # check type of first frame
                    if not frame.pict_type.name == "I":
                        raise InvalidVideoError("The first frame is not a key frame")

                    # get video resolution
                    if video_stream.metadata.get("rotate"):
                        frame = av.VideoFrame().from_ndarray(
                            rotate_image(
                                frame.to_ndarray(format="bgr24"),
                                360 - int(container.streams.video[0].metadata.get("rotate")),
                            ),
                            format="bgr24",
                        )
                    self.height, self.width = (frame.height, frame.width)

                    return

    @property
    def source_path(self):
        return self._source_path

    @staticmethod
    def _get_video_stream(container):
        video_stream = next(stream for stream in container.streams if stream.type == "video")
        video_stream.thread_type = "AUTO"
        return video_stream

    def __len__(self):
        assert (
            self._frames_number is not None
        ), "The length will not be available until the reader is iterated all the way through at least once"

        return self._frames_number

    @property
    def resolution(self):
        return (self.width, self.height)

    def validate_key_frame(self, container, video_stream, key_frame):
        for packet in container.demux(video_stream):
            for frame in packet.decode():
                if md5_hash(frame) != key_frame["md5"] or frame.pts != key_frame["pts"]:
                    return False
                return True

    def __iter__(self) -> Iterator[Union[int, tuple[int, int, str]]]:
        """
        Iterate over video frames and yield key frames or indexes.

        Yields:
            Union[Tuple[int, int, str], int]: (frame index, frame timestamp, frame MD5) or frame index.
        """
        # Open containers for reading frames and checking movement on them
        with (
            closing(av.open(self.source_path, mode="r")) as reading_container,
            closing(av.open(self.source_path, mode="r")) as checking_container,
        ):
            reading_v_stream = self._get_video_stream(reading_container)
            checking_v_stream = self._get_video_stream(checking_container)
            prev_pts: Optional[int] = None
            prev_dts: Optional[int] = None
            index, key_frame_count = 0, 0

            for packet in reading_container.demux(reading_v_stream):
                for frame in packet.decode():
                    # Check PTS and DTS sequences for validity
                    if None not in {frame.pts, prev_pts} and frame.pts <= prev_pts:
                        raise InvalidVideoError("Detected non-increasing PTS sequence in the video")
                    if None not in {frame.dts, prev_dts} and frame.dts <= prev_dts:
                        raise InvalidVideoError("Detected non-increasing DTS sequence in the video")
                    prev_pts, prev_dts = frame.pts, frame.dts

                    if frame.key_frame:
                        key_frame_data = {
                            "pts": frame.pts,
                            "md5": md5_hash(frame),
                        }

                        # Check that it is possible to seek to this key frame using frame.pts
                        checking_container.seek(
                            offset=key_frame_data["pts"],
                            stream=checking_v_stream,
                        )
                        is_valid_key_frame = self.validate_key_frame(
                            checking_container,
                            checking_v_stream,
                            key_frame_data,
                        )

                        if is_valid_key_frame:
                            key_frame_count += 1
                            yield (index, key_frame_data["pts"], key_frame_data["md5"])
                        else:
                            yield index
                    else:
                        yield index

                    index += 1
                    key_frame_ratio = index // (key_frame_count or 1)

                    # Check if the number of key frames meets the upper bound
                    if key_frame_ratio >= self._upper_bound and not self._force:
                        raise InvalidVideoError(
                            "The number of keyframes is not enough for smooth iteration over the video"
                        )

            # Update frames number if not already set
            if not self._frames_number:
                self._frames_number = index


class DatasetImagesReader:
    def __init__(
        self,
        sources: Union[list[str], Iterator[NamedBytesIO]],
        *,
        start: int = 0,
        step: int = 1,
        stop: Optional[int] = None,
        meta: Optional[dict[str, list[str]]] = None,
        sorting_method: SortingMethod = SortingMethod.PREDEFINED,
        use_image_hash: bool = False,
        **kwargs,
    ):
        self._is_generator_used = isgenerator(sources)

        if not self._is_generator_used:
            raw_data_used = not isinstance(sources[0], str)
            func: Optional[Callable[[NamedBytesIO], str]] = (
                (lambda x: x.filename) if raw_data_used else None
            )
            self._sources = sort(sources, sorting_method, func=func)
        else:
            if sorting_method != SortingMethod.PREDEFINED:
                raise ValueError("Only SortingMethod.PREDEFINED can be used with generator")
            self._sources = sources
        self._meta = meta
        self._data_dir = kwargs.get("data_dir", None)
        self._use_image_hash = use_image_hash
        self._start = start
        self._stop = stop if stop or self._is_generator_used else len(sources) - 1
        if self._stop is None:
            raise ValueError("The stop parameter should be passed when generator is used")
        self._step = step

    @property
    def start(self):
        return self._start

    @start.setter
    def start(self, value):
        self._start = int(value)

    @property
    def stop(self):
        return self._stop

    @stop.setter
    def stop(self, value):
        self._stop = int(value)

    @property
    def step(self):
        return self._step

    @step.setter
    def step(self, value):
        self._step = int(value)

    def _get_img_properties(self, image: Union[str, NamedBytesIO]) -> dict[str, Any]:
        img = Image.open(image, mode="r")
        if self._data_dir:
            img_name = os.path.relpath(image, self._data_dir)
        else:
            img_name = os.path.basename(image) if isinstance(image, str) else image.filename

        name, extension = os.path.splitext(img_name)
        image_properties = {
            "name": name.replace("\\", "/"),
            "extension": extension,
        }

        width, height = img.width, img.height
        orientation = img.getexif().get(274, 1)
        if orientation > 4:
            width, height = height, width
        image_properties["width"] = width
        image_properties["height"] = height

        if self._meta and img_name in self._meta:
            image_properties["meta"] = self._meta[img_name]

        if self._use_image_hash:
            image_properties["checksum"] = md5_hash(img)

        return image_properties

    def __iter__(self):
        sources = (
            self._sources
            if self._is_generator_used
            else islice(self._sources, self.start, self.stop + 1, self.step)
        )

        for idx in range(self.stop + 1):
            if idx in range(self.start, self.stop + 1, self.step):
                image = next(sources)
                yield self._get_img_properties(image)
            else:
                yield dict()

    @property
    def range_(self):
        return range(self._start, self._stop + 1, self._step)

    def __len__(self):
        return len(self.range_)


class Dataset3DImagesReader(DatasetImagesReader):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def __iter__(self):
        sources = (i for i in self._sources)
        for idx in range(self._stop + 1):
            if idx in self.range_:
                image = next(sources)
                img_name = (
                    os.path.relpath(image, self._data_dir)
                    if self._data_dir
                    else os.path.basename(image)
                )
                name, extension = os.path.splitext(img_name)
                image_properties = {
                    "name": name,
                    "extension": extension,
                }
                if self._meta and img_name in self._meta:
                    image_properties["meta"] = self._meta[img_name]
                yield image_properties
            else:
                yield dict()


class _Manifest:
    class SupportedVersion(str, Enum):
        V1 = "1.0"
        V1_1 = "1.1"

        @classmethod
        def choices(cls):
            return (x.value for x in cls)

        def __str__(self):
            return self.value

    FILE_NAME = "manifest.jsonl"
    VERSION = SupportedVersion.V1_1
    TYPE: str  # must be set externally

    def __init__(self, path, upload_dir=None):
        assert path, "A path to manifest file not found"
        self._path = os.path.join(path, self.FILE_NAME) if os.path.isdir(path) else path
        self._upload_dir = upload_dir

    @property
    def path(self):
        return self._path

    @property
    def name(self):
        return (
            os.path.basename(self._path)
            if not self._upload_dir
            else os.path.relpath(self._path, self._upload_dir)
        )

    def get_header_lines_count(self) -> int:
        if self.TYPE == "video":
            return 3
        elif self.TYPE == "images":
            return 2
        assert False, f"Unknown manifest type '{self.TYPE}'"


# Needed for faster iteration over the manifest file, will be generated to work inside CVAT
# and will not be generated when manually creating a manifest
class _Index:
    FILE_NAME = "index.json"

    def __init__(self, path):
        assert path and os.path.isdir(path), "No index directory path"
        self._path = os.path.join(path, self.FILE_NAME)
        self._index = {}

    @property
    def path(self):
        return self._path

    def dump(self):
        with open(self._path, "w") as index_file:
            json.dump(self._index, index_file, separators=(",", ":"))

    def load(self):
        with open(self._path, "r") as index_file:
            self._index = json.load(
                index_file, object_hook=lambda d: {int(k): v for k, v in d.items()}
            )

    def remove(self):
        os.remove(self._path)

    def create(self, manifest, *, skip):
        assert os.path.exists(manifest), "A manifest file not exists, index cannot be created"
        with open(manifest, "r+") as manifest_file:
            while skip:
                manifest_file.readline()
                skip -= 1
            image_number = 0
            position = manifest_file.tell()
            line = manifest_file.readline()
            while line:
                if line.strip():
                    self._index[image_number] = position
                    image_number += 1
                    position = manifest_file.tell()
                line = manifest_file.readline()

    def partial_update(self, manifest, number):
        assert os.path.exists(manifest), "A manifest file not exists, index cannot be updated"
        with open(manifest, "r+") as manifest_file:
            manifest_file.seek(self._index[number])
            line = manifest_file.readline()
            while line:
                if line.strip():
                    self._index[number] = manifest_file.tell()
                    number += 1
                line = manifest_file.readline()

    def __getitem__(self, number):
        if not 0 <= number < len(self):
            raise IndexError(
                "Invalid index number: {}, Maximum allowed index is {}".format(
                    number, len(self) - 1
                )
            )

        return self._index[number]

    def __len__(self):
        return len(self._index)

    def is_empty(self) -> bool:
        return not len(self)


class _ManifestManager(ABC):
    BASE_INFORMATION = {
        "version": 1,
        "type": 2,
    }

    def _json_item_is_valid(self, **state):
        for item in self._required_item_attributes:
            if state.get(item, None) is None:
                raise InvalidManifestError(
                    f"Invalid '{self.manifest.name}' file structure: "
                    f"'{item}' is required, but not found"
                )

    def __init__(self, path, create_index, upload_dir=None):
        self._manifest = _Manifest(path, upload_dir)
        self._index = _Index(os.path.dirname(self._manifest.path))
        self._reader = None
        self._create_index = create_index

    @property
    def reader(self):
        return self._reader

    def _parse_line(self, line):
        """Getting a random line from the manifest file"""
        with open(self._manifest.path, "r") as manifest_file:
            if isinstance(line, str):
                assert (
                    line in self.BASE_INFORMATION.keys()
                ), "An attempt to get non-existent information from the manifest"
                for _ in range(self.BASE_INFORMATION[line]):
                    fline = manifest_file.readline()
                return json.loads(fline)[line]
            else:
                assert self._index, "No prepared index"
                offset = self._index[line]
                manifest_file.seek(offset)
                properties = manifest_file.readline()
                parsed_properties = ImageProperties(json.loads(properties))
                self._json_item_is_valid(**parsed_properties)
                return parsed_properties

    def init_index(self):
        if os.path.exists(self._index.path):
            self._index.load()
        else:
            self._index.create(self._manifest.path, skip=self._manifest.get_header_lines_count())
            if self._create_index:
                self._index.dump()

    def reset_index(self):
        if self._create_index and os.path.exists(self._index.path):
            self._index.remove()

    def set_index(self):
        self.reset_index()
        self.init_index()

    def remove(self):
        self.reset_index()
        if os.path.exists(self.manifest.path):
            os.remove(self.manifest.path)

    @abstractmethod
    def create(self, content=None, _tqdm=None): ...

    @abstractmethod
    def partial_update(self, number, properties): ...

    def __iter__(self):
        self.set_index()

        with open(self._manifest.path, "r") as manifest_file:
            manifest_file.seek(self._index[0])
            for idx, line_start in enumerate(self._index):
                manifest_file.seek(line_start)
                line = manifest_file.readline()
                item = ImageProperties(json.loads(line))
                self._json_item_is_valid(**item)
                yield (idx, item)

    @property
    def manifest(self):
        return self._manifest

    def __len__(self):
        return len(self._index)

    def is_empty(self) -> bool:
        if self._index.is_empty():
            self._index.load()
        return self._index.is_empty()

    def __getitem__(self, item):
        if isinstance(item, slice):
            return [
                self._parse_line(i)
                for i in range(item.start or 0, item.stop or len(self), item.step or 1)
            ]
        return self._parse_line(item)

    @property
    def index(self):
        return self._index

    @property
    @abstractmethod
    def data(self): ...

    @abstractmethod
    def get_subset(self, subset_names): ...

    @property
    def exists(self):
        return os.path.exists(self._manifest.path)


class VideoManifestManager(_ManifestManager):
    _required_item_attributes = {"number", "pts"}

    def __init__(self, manifest_path, create_index=True):
        super().__init__(manifest_path, create_index)
        setattr(self._manifest, "TYPE", "video")
        self.BASE_INFORMATION["properties"] = 3

    def link(self, media_file, upload_dir=None, chunk_size=36, force=False, **kwargs):
        self._reader = VideoStreamReader(
            os.path.join(upload_dir, media_file) if upload_dir else media_file, chunk_size, force
        )

    def _write_base_information(self, file):
        base_info = {
            "version": self._manifest.VERSION,
            "type": self._manifest.TYPE,
            "properties": {
                "name": os.path.basename(self._reader.source_path),
                "resolution": self._reader.resolution,
                "length": len(self._reader),
            },
        }
        for key, value in base_info.items():
            json_item = json.dumps({key: value}, separators=(",", ":"))
            file.write(f"{json_item}\n")

    def _write_core_part(self, file, _tqdm):
        iterable_obj = (
            self._reader
            if _tqdm is None
            else _tqdm(self._reader, desc="Manifest creating", total=float("inf"))
        )
        for item in iterable_obj:
            if isinstance(item, tuple):
                json_item = json.dumps(
                    {"number": item[0], "pts": item[1], "checksum": item[2]}, separators=(",", ":")
                )
                file.write(f"{json_item}\n")

    def create(self, *, _tqdm=None):  # pylint: disable=arguments-differ
        """Creating and saving a manifest file"""
        tmp_file = StringIO()
        self._write_core_part(tmp_file, _tqdm)

        with open(self._manifest.path, "w") as manifest_file:
            self._write_base_information(manifest_file)
            manifest_file.write(tmp_file.getvalue())

        self.set_index()

        if self.is_empty() and not self._reader._force:
            raise InvalidManifestError("Empty manifest file has been created")

    def partial_update(self, number, properties):
        pass

    @property
    def video_name(self):
        return self["properties"]["name"]

    @property
    def video_resolution(self):
        return self["properties"]["resolution"]

    @property
    def video_length(self):
        return self["properties"]["length"]

    @property
    def data(self):
        return self.video_name

    def get_subset(self, subset_names):
        raise NotImplementedError()


class VideoManifestValidator(VideoManifestManager):
    def __init__(self, source_path, manifest_path):
        self._source_path = source_path
        super().__init__(manifest_path)

    @staticmethod
    def _get_video_stream(container):
        video_stream = next(stream for stream in container.streams if stream.type == "video")
        video_stream.thread_type = "AUTO"
        return video_stream

    def validate_key_frame(self, container, video_stream, key_frame):
        for packet in container.demux(video_stream):
            for frame in packet.decode():
                assert (
                    frame.pts == key_frame["pts"]
                ), "The uploaded manifest does not match the video"
                return

    def validate_seek_key_frames(self):
        with closing(av.open(self._source_path, mode="r")) as container:
            video_stream = self._get_video_stream(container)
            last_key_frame = None

            for _, key_frame in self:
                # check that key frames sequence sorted
                if last_key_frame and last_key_frame["number"] >= key_frame["number"]:
                    raise AssertionError("Invalid saved key frames sequence in manifest file")
                container.seek(offset=key_frame["pts"], stream=video_stream)
                self.validate_key_frame(container, video_stream, key_frame)
                last_key_frame = key_frame


class ImageProperties(dict):
    @property
    def full_name(self):
        return f"{self['name']}{self['extension']}"


class ImageManifestManager(_ManifestManager):
    _required_item_attributes = {"name", "extension"}

    def __init__(self, manifest_path, upload_dir=None, create_index=True):
        super().__init__(manifest_path, create_index, upload_dir)
        setattr(self._manifest, "TYPE", "images")

    def link(self, **kwargs):
        ReaderClass = (
            DatasetImagesReader if not kwargs.get("DIM_3D", None) else Dataset3DImagesReader
        )
        self._reader = ReaderClass(**kwargs)

    def _write_base_information(self, file):
        base_info = {
            "version": self._manifest.VERSION,
            "type": self._manifest.TYPE,
        }
        for key, value in base_info.items():
            json_line = json.dumps({key: value}, separators=(",", ":"))
            file.write(f"{json_line}\n")

    def _write_core_part(self, file, obj, _tqdm):
        iterable_obj = (
            obj
            if _tqdm is None
            else _tqdm(
                obj,
                desc="Manifest creating",
                total=None if not hasattr(obj, "__len__") else len(obj),
            )
        )
        for image_properties in iterable_obj:
            json_line = json.dumps(
                {key: value for key, value in image_properties.items()}, separators=(",", ":")
            )
            file.write(f"{json_line}\n")

    def create(self, content=None, _tqdm=None):
        """Creating and saving a manifest file for the specialized dataset"""
        with open(self._manifest.path, "w") as manifest_file:
            self._write_base_information(manifest_file)
            obj = content if content else self._reader
            self._write_core_part(manifest_file, obj, _tqdm)

        self.set_index()

    def partial_update(self, number, properties):
        pass

    @property
    def data(self):
        return (f"{image.full_name}" for _, image in self)

    def get_subset(self, subset_names):
        index_list = []
        subset = []
        for _, image in self:
            image_name = f"{image.full_name}"
            if image_name in subset_names:
                index_list.append(subset_names.index(image_name))
                properties = {
                    "name": f"{image['name']}",
                    "extension": f"{image['extension']}",
                    "width": image["width"],
                    "height": image["height"],
                }
                for optional_field in {"meta", "checksum"}:
                    value = image.get(optional_field)
                    if value:
                        properties[optional_field] = value
                subset.append(properties)
        return index_list, subset

    def emulate_hierarchical_structure(
        self,
        page_size: int,
        manifest_prefix: Optional[str] = None,
        prefix: str = "",
        default_prefix: Optional[str] = None,
        start_index: Optional[int] = None,
    ) -> dict:

        if (
            default_prefix
            and prefix
            and not (default_prefix.startswith(prefix) or prefix.startswith(default_prefix))
        ):
            return {
                "content": [],
                "next": None,
            }

        search_prefix = prefix
        if default_prefix and (len(prefix) < len(default_prefix)):
            if prefix and "/" in default_prefix[len(prefix) :]:
                next_layer_and_tail = default_prefix[prefix.find("/") + 1 :].split("/", maxsplit=1)
                if 2 == len(next_layer_and_tail):
                    directory = next_layer_and_tail[0]
                    return {
                        "content": [{"name": directory, "type": "DIR"}],
                        "next": None,
                    }
                else:
                    search_prefix = default_prefix
            else:
                search_prefix = default_prefix

        next_start_index = None
        # get part of manifest content
        # generally we cannot rely to slice with manifest content because it may not be sorted.
        # And then this can lead to incorrect index calculation.
        if manifest_prefix:
            content = [os.path.join(manifest_prefix, f[1].full_name) for f in self]
        else:
            content = [f[1].full_name for f in self]

        if search_prefix:
            content = list(filter(lambda x: x.startswith(search_prefix), content))
            if os.path.sep in search_prefix:
                last_slash = search_prefix.rindex(os.path.sep)
                content = [f[last_slash + 1 :] for f in content]

        files_in_root, files_in_directories = [], []

        for f in content:
            if os.path.sep in f:
                files_in_directories.append(f)
            else:
                files_in_root.append(f)

        directories = list(set([d.split(os.path.sep)[0] for d in files_in_directories]))
        level_in_hierarchical_structure = [
            {"name": d, "type": "DIR"} for d in sort(directories, SortingMethod.NATURAL)
        ]
        level_in_hierarchical_structure.extend(
            [{"name": f, "type": "REG"} for f in sort(files_in_root, SortingMethod.NATURAL)]
        )

        level_in_hierarchical_structure = level_in_hierarchical_structure[start_index:]
        if len(level_in_hierarchical_structure) > page_size:
            level_in_hierarchical_structure = level_in_hierarchical_structure[:page_size]
            next_start_index = start_index + page_size

        return {
            "content": level_in_hierarchical_structure,
            "next": next_start_index,
        }

    def reorder(self, reordered_images: list[str]) -> None:
        """
        The method takes a list of image names and reorders its content based on this new list.
        Due to the implementation of Honeypots, the reordered list of image names may contain duplicates.
        """
        unique_images: dict[str, Any] = {}
        for _, image_details in self:
            if image_details.full_name not in unique_images:
                unique_images[image_details.full_name] = image_details

        try:
            self.create(content=(unique_images[x] for x in reordered_images))
        except KeyError as ex:
            raise InvalidManifestError(f"Previous manifest does not contain {ex} image")


class _BaseManifestValidator(ABC):
    def __init__(self, full_manifest_path):
        self._manifest = _Manifest(full_manifest_path)

    def validate(self):
        try:
            # we cannot use index in general because manifest may be e.g. in share point with ro mode
            with open(self._manifest.path, "r") as manifest:
                for validator in self.validators:
                    line = json.loads(manifest.readline().strip())
                    validator(line)
            return True
        except (ValueError, KeyError, JSONDecodeError, InvalidManifestError):
            return False

    @staticmethod
    def _validate_version(_dict):
        if not _dict["version"] in _Manifest.SupportedVersion.choices():
            raise InvalidManifestError("Incorrect version field")

    def _validate_type(self, _dict):
        if not _dict["type"] == self.TYPE:
            raise InvalidManifestError("Incorrect type field")

    @property
    @abstractmethod
    def validators(self):
        pass

    @staticmethod
    @abstractmethod
    def _validate_first_item(_dict):
        pass


class _VideoManifestStructureValidator(_BaseManifestValidator):
    TYPE = "video"

    @property
    def validators(self):
        return (
            self._validate_version,
            self._validate_type,
            self._validate_properties,
            self._validate_first_item,
        )

    @staticmethod
    def _validate_properties(_dict):
        properties = _dict["properties"]
        if not isinstance(properties["name"], str):
            raise InvalidManifestError("Incorrect name field")
        if not isinstance(properties["resolution"], list):
            raise InvalidManifestError("Incorrect resolution field")
        if not isinstance(properties["length"], int) or properties["length"] == 0:
            raise InvalidManifestError("Incorrect length field")

    @staticmethod
    def _validate_first_item(_dict):
        if not isinstance(_dict["number"], int):
            raise InvalidManifestError("Incorrect number field")
        if not isinstance(_dict["pts"], int):
            raise InvalidManifestError("Incorrect pts field")


class _DatasetManifestStructureValidator(_BaseManifestValidator):
    TYPE = "images"

    @property
    def validators(self):
        return (
            self._validate_version,
            self._validate_type,
            self._validate_first_item,
        )

    @staticmethod
    def _validate_first_item(_dict):
        if not isinstance(_dict["name"], str):
            raise InvalidManifestError("Incorrect name field")
        if not isinstance(_dict["extension"], str):
            raise InvalidManifestError("Incorrect extension field")
        # FIXME
        # Width and height are required for 2D data, but
        # for 3D these parameters are not saved now.
        # It is necessary to uncomment these restrictions when manual preparation for 3D data is implemented.

        # if not isinstance(_dict['width'], int):
        #     raise InvalidManifestError('Incorrect width field')
        # if not isinstance(_dict['height'], int):
        #     raise InvalidManifestError('Incorrect height field')


def is_manifest(full_manifest_path):
    return is_video_manifest(full_manifest_path) or is_dataset_manifest(full_manifest_path)


def is_video_manifest(full_manifest_path):
    validator = _VideoManifestStructureValidator(full_manifest_path)
    return validator.validate()


def is_dataset_manifest(full_manifest_path):
    validator = _DatasetManifestStructureValidator(full_manifest_path)
    return validator.validate()


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\create.py =====
#!/usr/bin/env python3

# Copyright (C) 2021-2022 Intel Corporation
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

import argparse
import os
import re
import sys
from glob import glob

from tqdm import tqdm

from utils import SortingMethod, detect_related_images, is_image, is_video


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--force",
        action="store_true",
        help="Use this flag to prepare the manifest file for video data "
        "if by default the video does not meet the requirements and a manifest file is not prepared",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        help="Directory where the manifest file will be saved",
        default=os.getcwd(),
    )
    parser.add_argument(
        "--sorting",
        choices=[v[0] for v in SortingMethod.choices()],
        type=str,
        default=SortingMethod.LEXICOGRAPHICAL.value,
    )
    parser.add_argument("source", type=str, help="Source paths")
    return parser.parse_args()


def main():
    args = get_args()

    manifest_directory = os.path.abspath(args.output_dir)
    if not os.path.exists(manifest_directory):
        os.makedirs(manifest_directory)
    source = os.path.abspath(os.path.expanduser(args.source))

    sources = []
    if not os.path.isfile(source):  # directory/pattern with images
        data_dir = None
        if os.path.isdir(source):
            data_dir = source
            for root, _, files in os.walk(source):
                sources.extend([os.path.join(root, f) for f in files if is_image(f)])
        else:
            items = source.lstrip("/").split("/")
            position = 0
            try:
                for item in items:
                    if set(item) & {"*", "?", "[", "]"}:
                        break
                    position += 1
                else:
                    raise Exception("Wrong positional argument")
                assert position != 0, "Wrong pattern: there must be a common root"
                data_dir = source.split(items[position])[0]
            except Exception as ex:
                sys.exit(str(ex))
            sources = list(filter(is_image, glob(source, recursive=True)))

        sources = list(filter(lambda x: "related_images{}".format(os.sep) not in x, sources))

        # If the source is a glob expression, we need additional processing
        abs_root = source
        while abs_root and re.search(r"[*?\[\]]", abs_root):
            abs_root = os.path.split(abs_root)[0]

        related_images = detect_related_images(sources, abs_root)
        meta = {k: {"related_images": related_images[k]} for k in related_images}
        try:
            assert len(sources), "A images was not found"
            manifest = ImageManifestManager(manifest_path=manifest_directory)
            manifest.link(
                sources=sources,
                meta=meta,
                sorting_method=args.sorting,
                use_image_hash=True,
                data_dir=data_dir,
            )
            manifest.create(_tqdm=tqdm)
        except Exception as ex:
            sys.exit(str(ex))
    else:  # video
        try:
            assert is_video(
                source
            ), "You can specify a video path or a directory/pattern with images"
            manifest = VideoManifestManager(manifest_path=manifest_directory)
            manifest.link(media_file=source, force=args.force)
            try:
                manifest.create(_tqdm=tqdm)
            except AssertionError as ex:
                if str(ex) == "Too few keyframes":
                    msg = (
                        "NOTE: prepared manifest file contains too few key frames for smooth decoding.\n"
                        "Use --force flag if you still want to prepare a manifest file."
                    )
                    print(msg)
                    sys.exit(2)
                else:
                    raise
        except Exception as ex:
            sys.exit(str(ex))

    print("The manifest file has been prepared")


if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.append(base_dir)
    from dataset_manifest.core import ImageManifestManager, VideoManifestManager

    main()


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\errors.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT


class BasicError(Exception):
    """
    The basic exception type for all exceptions in the library
    """


class InvalidVideoError(BasicError):
    """
    Indicates an invalid video frame
    """


class InvalidManifestError(BasicError):
    """
    Indicates an invalid manifest
    """


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\types.py =====
# Copyright (C) CVAT.ai Corporation
#
# SPDX-License-Identifier: MIT

from io import BytesIO
from typing import Protocol


class Named(Protocol):
    filename: str


class NamedBytesIO(BytesIO, Named):
    pass


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\utils.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT

import hashlib
import mimetypes
import os
import re
from enum import Enum
from random import shuffle

import cv2 as cv
from av import VideoFrame
from natsort import os_sorted


def rotate_image(image, angle):
    height, width = image.shape[:2]
    image_center = (width / 2, height / 2)
    matrix = cv.getRotationMatrix2D(image_center, angle, 1.0)
    abs_cos = abs(matrix[0, 0])
    abs_sin = abs(matrix[0, 1])
    bound_w = int(height * abs_sin + width * abs_cos)
    bound_h = int(height * abs_cos + width * abs_sin)
    matrix[0, 2] += bound_w / 2 - image_center[0]
    matrix[1, 2] += bound_h / 2 - image_center[1]
    matrix = cv.warpAffine(image, matrix, (bound_w, bound_h))
    return matrix


def md5_hash(frame):
    if isinstance(frame, VideoFrame):
        frame = frame.to_image()
    return hashlib.md5(frame.tobytes()).hexdigest()  # nosec


def _define_data_type(media):
    return mimetypes.guess_type(media)[0]


def is_video(media_file):
    data_type = _define_data_type(media_file)
    return data_type is not None and data_type.startswith("video")


def is_image(media_file):
    data_type = _define_data_type(media_file)
    return (
        data_type is not None
        and data_type.startswith("image")
        and not data_type.startswith("image/svg")
    )


def _list_and_join(root):
    files = os.listdir(root)
    for f in files:
        yield os.path.join(root, f)


def _prepare_context_list(files, base_dir):
    return sorted(map(lambda x: os.path.relpath(x, base_dir), filter(is_image, files)))


# Expected 2D format is:
# data/
#   00001.png
#   related_images/
#     00001_png/
#       context_image_1.jpeg
#       context_image_2.png
def _detect_related_images_2D(image_paths, root_path):
    related_images = {}
    latest_dirname = ""
    related_images_exist = False

    for image_path in sorted(image_paths):
        rel_image_path = os.path.relpath(image_path, root_path)
        dirname = os.path.dirname(image_path)
        related_images_dirname = os.path.join(dirname, "related_images")
        related_images[rel_image_path] = []

        if latest_dirname == dirname and not related_images_exist:
            continue
        elif latest_dirname != dirname:
            # Update some data applicable for a subset of paths (within the current dirname)
            latest_dirname = dirname
            related_images_exist = os.path.isdir(related_images_dirname)

        if related_images_exist:
            related_images_dirname = os.path.join(
                related_images_dirname, "_".join(os.path.basename(image_path).rsplit(".", 1))
            )

            if os.path.isdir(related_images_dirname):
                related_images[rel_image_path] = _prepare_context_list(
                    _list_and_join(related_images_dirname), root_path
                )
    return related_images


# Possible 3D formats are:
# velodyne_points/
#     data/
#         image_01.bin
# IMAGE_00 # any number?
#     data/
#         image_01.png
#
# pointcloud/
#     00001.pcd
# related_images/
#     00001_pcd/
#         image_01.png # or other image
#
# Default formats
# Option 1
# data/
#     image.pcd
#     image.png
#
# Option 2
# data/
#    image_1/
#        image_1.pcd
#        context_1.png
#        context_2.jpg
def _detect_related_images_3D(image_paths, root_path):
    related_images = {}
    latest_dirname = ""
    dirname_files = []
    related_images_exist = False
    velodyne_context_images_dirs = []

    for image_path in sorted(image_paths):
        rel_image_path = os.path.relpath(image_path, root_path)
        name = os.path.splitext(os.path.basename(image_path))[0]
        dirname = os.path.dirname(image_path)
        related_images_dirname = os.path.normpath(os.path.join(dirname, "..", "related_images"))
        related_images[rel_image_path] = []

        if latest_dirname != dirname:
            # Update some data applicable for a subset of paths (within the current dirname)
            latest_dirname = dirname
            related_images_exist = os.path.isdir(related_images_dirname)
            dirname_files = list(_list_and_join(dirname))
            velodyne_context_images_dirs = [
                directory
                for directory in _list_and_join(os.path.normpath(os.path.join(dirname, "..", "..")))
                if os.path.isdir(os.path.join(directory, "data"))
                and re.search(r"image_\d.*", directory, re.IGNORECASE)
            ]

        filtered_dirname_files = list(filter(lambda x: x != image_path, dirname_files))
        if len(filtered_dirname_files) and os.path.basename(dirname) == name:
            # default format (option 2)
            related_images[rel_image_path].extend(
                _prepare_context_list(filtered_dirname_files, root_path)
            )
        else:
            filtered_dirname_files = list(
                filter(
                    lambda x: os.path.splitext(os.path.basename(x))[0] == name,
                    filtered_dirname_files,
                )
            )
            if len(filtered_dirname_files):
                # default format (option 1)
                related_images[rel_image_path].extend(
                    _prepare_context_list(filtered_dirname_files, root_path)
                )

        if related_images_exist:
            related_images_dirname = os.path.join(
                related_images_dirname, "_".join(os.path.basename(image_path).rsplit(".", 1))
            )
            if os.path.isdir(related_images_dirname):
                related_images[rel_image_path].extend(
                    _prepare_context_list(_list_and_join(related_images_dirname), root_path)
                )

        if dirname.endswith(os.path.join("velodyne_points", "data")):
            # velodynepoints format
            for context_images_dir in velodyne_context_images_dirs:
                context_files = _list_and_join(os.path.join(context_images_dir, "data"))
                context_files = list(
                    filter(
                        lambda x: os.path.splitext(os.path.basename(x))[0] == name, context_files
                    )
                )
                related_images[rel_image_path].extend(
                    _prepare_context_list(context_files, root_path)
                )

        related_images[rel_image_path].sort()
    return related_images


# This function is expected to be called only for images tasks
# image_path is expected to be a list of absolute path to images
# root_path is expected to be a string (dataset root)
def detect_related_images(image_paths, root_path):
    data_are_2d = False
    data_are_3d = False

    # First of all need to define data type we are working with
    for image_path in image_paths:
        # .bin files are expected to be converted to .pcd before this code
        if os.path.splitext(image_path)[1].lower() == ".pcd":
            data_are_3d = True
        else:
            data_are_2d = True
    assert not (data_are_3d and data_are_2d), "Combined data types 2D and 3D are not supported"

    if data_are_2d:
        return _detect_related_images_2D(image_paths, root_path)
    elif data_are_3d:
        return _detect_related_images_3D(image_paths, root_path)
    return {}


class SortingMethod(str, Enum):
    LEXICOGRAPHICAL = "lexicographical"
    NATURAL = "natural"
    PREDEFINED = "predefined"
    RANDOM = "random"

    @classmethod
    def choices(cls):
        return tuple((x.value, x.name) for x in cls)

    def __str__(self):
        return self.value


def sort(images, sorting_method=SortingMethod.LEXICOGRAPHICAL, func=None):
    if sorting_method == SortingMethod.LEXICOGRAPHICAL:
        return sorted(images, key=func)
    elif sorting_method == SortingMethod.NATURAL:
        return os_sorted(images, key=func)
    elif sorting_method == SortingMethod.PREDEFINED:
        return images
    elif sorting_method == SortingMethod.RANDOM:
        shuffle(images)
        return images
    else:
        raise NotImplementedError()


# ===== 文件: D:\wow_ai\docker\cvat\utils\dataset_manifest\__init__.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT
from .core import ImageManifestManager, VideoManifestManager, is_manifest


# ===== 文件: D:\wow_ai\docker\cvat\utils\dicom_converter\script.py =====
# Copyright (C) 2021-2022 Intel Corporation
#
# SPDX-License-Identifier: MIT


import argparse
import logging
import os
from glob import glob

import numpy as np
from PIL import Image
from pydicom import dcmread
from pydicom.pixel_data_handlers.util import convert_color_space
from tqdm import tqdm

# Script configuration
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")
parser = argparse.ArgumentParser(
    description="The script is used to convert some kinds of DICOM (.dcm) files to regular image files (.png)"
)
parser.add_argument(
    "input",
    type=str,
    help="A root directory with medical data files in DICOM format. The script finds all these files based on their extension",
)
parser.add_argument(
    "output",
    type=str,
    help="Where to save converted files. The script repeats internal directories structure of the input root directory",
)
args = parser.parse_args()


class Converter:
    def __init__(self, filename):
        with dcmread(filename) as ds:
            self._pixel_array = ds.pixel_array
            self._photometric_interpretation = ds.PhotometricInterpretation
            self._min_value = ds.pixel_array.min()
            self._max_value = ds.pixel_array.max()
            self._depth = ds.BitsStored

            logging.debug("File: {}".format(filename))
            logging.debug("Photometric interpretation: {}".format(self._photometric_interpretation))
            logging.debug("Min value: {}".format(self._min_value))
            logging.debug("Max value: {}".format(self._max_value))
            logging.debug("Depth: {}".format(self._depth))

            try:
                self._length = ds["NumberOfFrames"].value
            except KeyError:
                self._length = 1

    def __len__(self):
        return self._length

    def __iter__(self):
        if self._length == 1:
            self._pixel_array = np.expand_dims(self._pixel_array, axis=0)

        for pixel_array in self._pixel_array:
            # Normalization to an output range 0..255, 0..65535
            pixel_array = pixel_array - self._min_value
            pixel_array = pixel_array.astype(int) * (2**self._depth - 1)
            pixel_array = pixel_array // (self._max_value - self._min_value)

            # In some cases we need to convert colors additionally
            if "YBR" in self._photometric_interpretation:
                pixel_array = convert_color_space(
                    pixel_array, self._photometric_interpretation, "RGB"
                )

            if self._depth == 8:
                image = Image.fromarray(pixel_array.astype(np.uint8))
            elif self._depth == 16:
                image = Image.fromarray(pixel_array.astype(np.uint16))
            else:
                raise Exception("Not supported depth {}".format(self._depth))

            yield image


def main(root_dir, output_root_dir):
    dicom_files = glob(os.path.join(root_dir, "**", "*.dcm"), recursive=True)
    if not len(dicom_files):
        logging.info("DICOM files are not found under the specified path")
    else:
        logging.info("Number of found DICOM files: " + str(len(dicom_files)))

    pbar = tqdm(dicom_files)
    for input_filename in pbar:
        pbar.set_description("Conversion: " + input_filename)
        input_basename = os.path.basename(input_filename)

        output_subpath = os.path.relpath(os.path.dirname(input_filename), root_dir)
        output_path = os.path.join(output_root_dir, output_subpath)
        output_basename = "{}.png".format(os.path.splitext(input_basename)[0])
        output_filename = os.path.join(output_path, output_basename)

        if not os.path.exists(output_path):
            os.makedirs(output_path)

        try:
            iterated_converter = Converter(input_filename)
            length = len(iterated_converter)
            for i, image in enumerate(iterated_converter):
                if length == 1:
                    image.save(output_filename)
                else:
                    filename_index = str(i).zfill(len(str(length)))
                    list_output_filename = "{}_{}.png".format(
                        os.path.splitext(output_filename)[0], filename_index
                    )
                    image.save(list_output_filename)
        except Exception as ex:
            logging.error("Error while processing " + input_filename)
            logging.error(ex)


if __name__ == "__main__":
    input_root_path = os.path.abspath(args.input.rstrip(os.sep))
    output_root_path = os.path.abspath(args.output.rstrip(os.sep))

    logging.info("From: {}".format(input_root_path))
    logging.info("To: {}".format(output_root_path))
    main(input_root_path, output_root_path)


# ===== 文件: D:\wow_ai\scripts\check_env.py =====
import shutil, subprocess, sys, platform, json, torch
from pathlib import Path
def cmd(x): return subprocess.check_output(x, shell=True, text=True).strip()
report = {
    "python": sys.version,
    "torch": torch.__version__,
    "cuda_ok": torch.cuda.is_available(),
    "gpu": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
    "ffmpeg": shutil.which("ffmpeg") is not None,
    "docker": cmd("docker --version") if shutil.which("docker") else None,
}
Path("env_report.json").write_text(json.dumps(report, indent=2))
print(json.dumps(report, indent=2))


# ===== 文件: D:\wow_ai\scripts\check_my_env.py =====
# D:\wow_ai\scripts\check_my_env.py
import gymnasium as gym
from gymnasium.utils.env_checker import check_env
import sys
import os
import traceback # 导入 traceback

# --- 动态路径设置 ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'scripts' else current_dir
wow_rl_path = os.path.join(parent_dir, 'wow_rl')
if wow_rl_path not in sys.path:
    sys.path.append(parent_dir)
    print(f"Added {parent_dir} to sys.path")
# --- 路径设置结束 ---

try:
    from wow_rl.envs.wow_click_env import WowClickEnv
    print("Imported WowClickEnv successfully.")
except ImportError as e:
    print(f"ImportError: {e}")
    sys.exit(1)
except Exception as e: # 捕获其他可能的导入或初始化错误
    print(f"An unexpected error occurred during import or initialization:")
    traceback.print_exc()
    sys.exit(1)


print("Creating WowClickEnv instance for checking...")
# 创建环境实例，使用默认参数，不需要渲染
try:
    env_instance = WowClickEnv(render_mode=None)
    print("Environment instance created.")
except Exception as e:
    print(f"Error creating WowClickEnv instance:")
    traceback.print_exc()
    sys.exit(1)


# --- 运行环境检查 ---
print("\nRunning environment checker...")
try:
    # check_env 会自动调用 reset, step 等方法进行测试
    check_env(env_instance.unwrapped) # 使用 .unwrapped 获取原始环境实例
    print("\nEnvironment check passed successfully!")
except Exception as e:
    print("\nEnvironment check failed:")
    # 打印详细的错误信息
    traceback.print_exc()
finally:
    # 确保关闭环境
    try:
        env_instance.close()
    except Exception:
        pass # 忽略关闭时的错误

print("\nCheck finished.")

# ===== 文件: D:\wow_ai\scripts\collect_offline_data.py =====
# collect_offline_data.py
import traceback
import gymnasium as gym
import numpy as np
import sys
import os
import time
from tqdm import tqdm # 导入 tqdm 用于显示进度条

# --- 动态路径设置 (同 test_env.py) ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'scripts' else current_dir
wow_rl_path = os.path.join(parent_dir, 'wow_rl')
if wow_rl_path not in sys.path:
    sys.path.append(parent_dir)
    print(f"Added {parent_dir} to sys.path")
# --- 路径设置结束 ---

try:
    from wow_rl.envs.wow_click_env import WowClickEnv
    from wow_rl.buffers.replay_buffer import ReplayBuffer
    print("Imported WowClickEnv and ReplayBuffer successfully.")
except ImportError as e:
    print(f"ImportError: {e}")
    sys.exit(1)

# --- 配置参数 ---
BUFFER_CAPACITY = 30000 # 先收集 30k 步数据 (ChatGPT 建议值)
BUFFER_SAVE_DIR = 'offline_buffer_data_30k' # 数据保存目录名
NUM_EPISODES = 600 # 运行的回合数 (约 600 * 50 = 30k 步)
MAX_STEPS_PER_EPISODE = 50 # 每个回合的最大步数 (与环境默认值一致)

# 环境参数 (确保与创建环境时一致)
DETECTOR_W = r'D:\wow_ai\runs\detect\gwyx_detect_run_s_ep200_neg\weights\best.pt'
ROI_REWARD = (319, 35, 245, 64)
TEMPLATE_PATH = r'D:\wow_ai\data\target_frame_template.png'
MATCH_THRESHOLD = 0.85
# --- 配置结束 ---

print("--- Offline Data Collection Script ---")

# 1. 初始化环境 (不需要渲染)
print("Initializing environment...")
env = WowClickEnv(
    render_mode=None, # !! 收集数据时不需要渲染窗口 !!
    detector_w=DETECTOR_W,
    roi_reward=ROI_REWARD,
    template_path=TEMPLATE_PATH,
    match_threshold=MATCH_THRESHOLD,
    max_steps=MAX_STEPS_PER_EPISODE
)
print("Environment initialized.")

# 2. 初始化 Replay Buffer
# 获取观察空间和动作空间信息
obs_space = env.observation_space
act_space = env.action_space
print(f"Observation Space: {obs_space}, Action Space: {act_space}")

# 检查观察空间形状是否符合预期 (640, 640, 3)
if not (len(obs_space.shape) == 3 and obs_space.shape[2] == 3):
     print(f"Error: Unexpected observation space shape: {obs_space.shape}. Expected (H, W, 3).")
     env.close()
     sys.exit(1)

# 动作空间通常是 Discrete，我们需要其维度 n
action_dim = 1 if isinstance(act_space, gym.spaces.Discrete) else act_space.shape[0]
if isinstance(act_space, gym.spaces.Discrete):
    print(f"Action space is Discrete with {act_space.n} actions.")
else:
    print(f"Action space shape: {act_space.shape}")


print(f"Initializing replay buffer with capacity {BUFFER_CAPACITY}...")
buffer = ReplayBuffer(
    capacity=BUFFER_CAPACITY,
    obs_shape=obs_space.shape,
    action_dim=action_dim, # 对于 Discrete 动作空间，维度是 1
    save_dir=BUFFER_SAVE_DIR
)
# 尝试加载之前的进度
buffer.load()
print(f"Replay buffer initialized. Current size: {len(buffer)}")


# 3. 运行循环收集数据
print(f"Starting data collection for {NUM_EPISODES} episodes...")
collected_steps = 0
try:
    # 使用 tqdm 显示总的回合进度
    for episode in tqdm(range(NUM_EPISODES), desc="Collecting Episodes"):
        # 如果缓冲区满了，提前停止
        if len(buffer) >= BUFFER_CAPACITY:
            print("\nBuffer is full. Stopping collection.")
            break

        obs, info = env.reset()
        done = False
        truncated = False
        episode_reward = 0
        episode_steps = 0

        # 每个回合最多运行 max_steps 步
        while not done and not truncated:
            # a. 随机选择动作
            action = env.action_space.sample()

            # b. 与环境交互
            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated # done 表示回合结束

            # c. 添加到缓冲区 (确保数据类型和形状正确)
            # 我们存储的是原始观察状态 (640x640x3)
            # DreamerV3 后续会自己处理缩放
            buffer.add(obs, action, reward, next_obs, done)

            # d. 更新状态和统计
            obs = next_obs
            episode_reward += reward
            episode_steps += 1
            collected_steps += 1

            # 检查缓冲区是否已满
            if len(buffer) >= BUFFER_CAPACITY:
                 print("\nBuffer is full during episode. Stopping collection.")
                 break # 跳出内部循环

        print(f"\nEpisode {episode + 1} finished after {episode_steps} steps. Reward: {episode_reward:.2f}. Buffer size: {len(buffer)}/{BUFFER_CAPACITY}")
        # 每隔一定回合数保存一次进度
        if (episode + 1) % 50 == 0:
             print(f"Saving buffer progress at episode {episode + 1}...")
             buffer.save()

except KeyboardInterrupt:
    print("\nData collection interrupted by user.")
except Exception as e:
    print(f"\nAn error occurred during data collection: {e}")
    traceback.print_exc() # <-- 打印完整的错误堆栈信息
finally:
    # 4. 结束时保存缓冲区
    print("Finalizing data collection...")
    buffer.save()
    env.close()
    print(f"Data collection finished. Total steps collected in this run: {collected_steps}")
    print(f"Total buffer size: {len(buffer)}")
    print(f"Data saved in: {os.path.abspath(BUFFER_SAVE_DIR)}")

# ===== 文件: D:\wow_ai\scripts\littleboy_backup.py =====
import os

# 设置目标目录和输出文件路径
root_dir = r'D:\wow_ai'
output_file = r'D:\wow_ai\all_code_backup.txt'

with open(output_file, 'w', encoding='utf-8') as outfile:
    for foldername, subfolders, filenames in os.walk(root_dir):
        for filename in filenames:
            if filename.endswith('.py'):
                file_path = os.path.join(foldername, filename)
                try:
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        outfile.write(f"\n\n# ===== 文件: {file_path} =====\n")
                        outfile.write(infile.read())
                except Exception as e:
                    print(f"无法读取文件 {file_path}：{e}")

print(f"所有Python代码已备份到：{output_file}")


# ===== 文件: D:\wow_ai\scripts\measure_red_mean.py =====
import cv2
import numpy as np
import os # 导入 os 模块用于检查文件是否存在

# --- 配置参数 ---
# !! 非常重要：确保这里的 ROI 和你的 reward_sensor.py 中的一致 !!
ROI = (319, 35, 245, 64) # (x_left_top, y_left_top, width, height)

# !! 非常重要：确保这里的路径指向你保存的两张截图 !!
VISIBLE_IMAGE_PATH = r'D:\wow_ai\data\target_visible.png'
INVISIBLE_IMAGE_PATH = r'D:\wow_ai\data\no_target.png'
# --- 配置结束 ---

def calculate_roi_red_mean(image_path, roi):
    """加载图像，裁剪 ROI，并计算平均红色值"""
    # 检查文件是否存在
    if not os.path.exists(image_path):
        print(f"错误：找不到图像文件 '{image_path}'")
        return None

    # 加载图像
    img = cv2.imread(image_path)
    if img is None:
        print(f"错误：无法加载图像 '{image_path}'")
        return None

    # 提取 ROI 坐标和尺寸
    x, y, w, h = roi

    # 确保 ROI 不超出图像边界
    img_h, img_w = img.shape[:2]
    roi_x_end = min(x + w, img_w)
    roi_y_end = min(y + h, img_h)
    roi_x_start = max(0, x)
    roi_y_start = max(0, y)

    # 检查裁剪区域是否有效
    if roi_x_end <= roi_x_start or roi_y_end <= roi_y_start:
        print(f"警告：对于图像 '{image_path}'，计算出的 ROI 为空或无效。")
        return 0 # 返回 0 或其他默认值

    # 裁剪 ROI
    crop = img[roi_y_start:roi_y_end, roi_x_start:roi_x_end]

    # 检查裁剪区域是否为空
    if crop.size == 0:
        print(f"警告：对于图像 '{image_path}'，ROI 裁剪区域为空。")
        return 0 # 返回 0 或其他默认值

    # 计算红色通道 (BGR 索引为 2) 的平均值
    # 检查 crop 是否至少有一个维度大于0，避免 numpy 警告
    if crop.shape[0] > 0 and crop.shape[1] > 0:
        red_mean = np.mean(crop[:, :, 2])
        return red_mean
    else:
        print(f"警告：对于图像 '{image_path}'，裁剪区域维度无效。")
        return 0

# --- 主程序 ---
print(f"正在为 ROI {ROI} 计算平均红色值...")

# 计算“可见”状态的平均红色值
red_mean_visible = calculate_roi_red_mean(VISIBLE_IMAGE_PATH, ROI)
if red_mean_visible is not None:
    print(f"状态 '可见' (选中目标) 的平均红色值: {red_mean_visible:.2f}")

# 计算“不可见”状态的平均红色值
red_mean_invisible = calculate_roi_red_mean(INVISIBLE_IMAGE_PATH, ROI)
if red_mean_invisible is not None:
    print(f"状态 '不可见' (未选中目标) 的平均红色值: {red_mean_invisible:.2f}")

# 给出选择阈值的建议
if red_mean_visible is not None and red_mean_invisible is not None:
    suggested_threshold = (red_mean_visible + red_mean_invisible) / 2
    print(f"\n建议阈值 (Threshold) 可以选择两者之间的值，例如: {suggested_threshold:.2f}")
    print(f"请将选定的阈值填入 wow_rl/utils/reward_sensor.py 的 DEFAULT_RED_THRESHOLD")
else:
    print("\n无法计算建议阈值，请检查图像路径和 ROI 设置。")

# ===== 文件: D:\wow_ai\scripts\realtime_gwyx.py =====
import mss, cv2, numpy as np
from ultralytics import YOLO
import torch # 确保导入 torch

# --- 配置区 ---
# 【重要】修改为你实际的 best.pt 文件路径！
MODEL_PATH = r"D:/wow_ai/runs/detect/gwyx_detect_run_s_ep200_neg/weights/best.pt" 

# 屏幕截取区域 (根据你的游戏窗口调整)
# top: 距离屏幕顶部的像素
# left: 距离屏幕左侧的像素
# width: 截取宽度
# height: 截取高度
# 确保这个区域覆盖了你的游戏主要画面
MONITOR_AREA = {"top": 0, "left": 0, "width": 1920, "height": 1080} 

CONFIDENCE_THRESHOLD = 0.30  # 只显示置信度高于 30% 的检测结果 (可以调整)
# --- 配置区结束 ---

print("正在加载模型...")
# 明确指定使用 GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"使用的设备: {device}")
model = YOLO(MODEL_PATH)
model.to(device) # 将模型移动到 GPU
print("模型加载完毕.")

# 获取类别名称 (假设你的 yaml 文件里 names: [gwyx])
class_names = model.names
print(f"模型能识别的类别: {class_names}")

print("按 ESC 键退出实时检测...")

with mss.mss() as sct:
    while True:
        # 截取屏幕指定区域
        img_bgra = np.array(sct.grab(MONITOR_AREA))
        # 将 BGRA 转换为 BGR (OpenCV 通常使用 BGR)
        img_bgr = cv2.cvtColor(img_bgra, cv2.COLOR_BGRA2BGR)

        # 使用模型进行推理 (直接在 GPU 上推理)
        # stream=True 可能更节省内存，但对于单帧处理差异不大
        results = model.predict(source=img_bgr, device=device, conf=CONFIDENCE_THRESHOLD, verbose=False) 

        # 绘制检测结果
        # results[0].plot() 会返回一个带有绘制框的图像副本
        annotated_frame = results[0].plot()

        # 显示结果窗口
        cv2.imshow("WoW Giant Owl Detector (Press ESC to exit)", annotated_frame)

        # 检测按键，如果按下 ESC (ASCII 码 27) 则退出循环
        if cv2.waitKey(1) == 27:
            break

# 关闭所有 OpenCV 窗口
cv2.destroyAllWindows()
print("实时检测已退出。")

# ===== 文件: D:\wow_ai\scripts\test_env_key.py =====
# test_env_key.py (用于测试新的键盘环境)
import gymnasium as gym
import numpy as np
import cv2
import sys
import os
import time # 导入 time

# --- 动态路径设置 (同之前) ---
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir) if os.path.basename(current_dir) == 'scripts' else current_dir
wow_rl_path = os.path.join(parent_dir, 'wow_rl')
if wow_rl_path not in sys.path:
    sys.path.append(parent_dir)
    print(f"Added {parent_dir} to sys.path")
# --- 路径设置结束 ---

try:
    from wow_rl.envs.wow_key_env import WowKeyEnv # 导入新的环境类
    print("Imported WowKeyEnv successfully.")
except ImportError as e:
    print(f"ImportError: {e}")
    sys.exit(1)
except Exception as e:
    print(f"An unexpected error occurred during import: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("Creating WowKeyEnv instance...")
# 创建环境实例，使用 human 模式进行可视化测试
env = WowKeyEnv(render_mode="human")
print("Environment created.")

# --- 运行一个回合的随机策略测试 ---
print("Resetting environment...")
obs, info = env.reset()
print("Environment reset complete. Starting simulation loop...")
print("Observation structure:", obs.keys() if isinstance(obs, dict) else type(obs)) # 确认 obs 是字典

total_reward = 0
max_test_steps = 50

for step in range(max_test_steps):
    print(f"\n--- Step {step + 1}/{max_test_steps} ---")
    # 1. 选择随机动作 (0-4)
    action = env.action_space.sample()
    print(f"Action selected (randomly): {action}")

    # 2. 执行动作并获取反馈
    try:
        # !! 确保你的环境返回了字典 obs !!
        obs, reward, terminated, truncated, info = env.step(action)

        # 打印关键信息
        sel_flag = info.get('sel_flag', 'N/A')
        prob_target = info.get('yolo_prob_target', 'N/A')
        print(f"Step result: Reward={reward:.2f}, SelFlag={sel_flag}, P(Tgt)={prob_target:.4f}, Term={terminated}, Trunc={truncated}")
        total_reward += reward
    except Exception as e:
        print(f"Error during env.step({action}): {e}")
        import traceback
        traceback.print_exc() # 打印详细错误
        print("Stopping test.")
        break

    # 检查回合是否结束
    if terminated or truncated:
        print(f"Episode finished after {step + 1} steps.")
        break

    # 按 'q' 键退出 (需要 OpenCV 窗口激活)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        print("'q' pressed, stopping test loop.")
        break

    # 短暂暂停，方便观察
    # time.sleep(0.1) # 可以取消注释来看得更清楚

# --- 测试结束 ---
print(f"\n--- Test finished ---")
print(f"Total steps executed: {step + 1}")
print(f"Total reward collected: {total_reward:.2f}")

env.close()
print("Environment closed.")

# ===== 文件: D:\wow_ai\scripts\test_minimal_env.py =====
# D:\wow_ai\scripts\test_minimal_env.py
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class MinimalEnv(gym.Env):
    metadata = {"render_modes": ["human", None]}

    def __init__(self, render_mode=None):
        super().__init__()
        self.action_space = spaces.Discrete(2) # 简单动作空间
        self.observation_space = spaces.Box(0, 1, shape=(4,), dtype=np.float32) # 简单观察空间
        self.render_mode = render_mode
        self.current_step = 0
        print("MinimalEnv initialized.")

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.current_step = 0
        observation = self.observation_space.sample() # 返回随机观察
        info = {}
        print("MinimalEnv reset.")
        return observation, info

    def step(self, action):
        self.current_step += 1
        observation = self.observation_space.sample() # 返回随机观察
        reward = 0.0 # 简单奖励
        terminated = False
        truncated = self.current_step >= 10 # 最多 10 步
        info = {}
        print(f"MinimalEnv step {self.current_step}: action={action}, reward={reward}")
        return observation, reward, terminated, truncated, info

    def render(self):
        pass

    def close(self):
        print("MinimalEnv closed.")

# --- 测试代码 ---
if __name__ == "__main__":
    print("Testing MinimalEnv...")
    env = MinimalEnv()
    obs, info = env.reset()
    done = False
    truncated = False
    step = 0
    while not done and not truncated:
        step += 1
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
    print(f"MinimalEnv test finished after {step} steps.")
    env.close()

# ===== 文件: D:\wow_ai\scripts\test_reward_sensor_live.py =====
# D:\wow_ai\scripts\test_reward_sensor_live.py (完整修正版 v3 - 全屏显示)
import cv2
import mss
import numpy as np
import time
import sys
import os
import traceback
import pyautogui # 用于获取屏幕尺寸

# --- 更可靠的路径设置 ---
try:
    script_path = os.path.abspath(__file__)
    script_dir = os.path.dirname(script_path)
    project_root = os.path.dirname(script_dir)
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
        print(f"Added project root '{project_root}' to sys.path")
    else:
        print(f"Project root '{project_root}' already in sys.path")
except NameError:
    print("Warning: Could not determine project root automatically.")
    project_root = None
# --- 路径设置结束 ---

# --- 导入自定义模块 ---
try:
    from wow_rl.utils.error_sensor import ErrorMessageSensor
    print("Imported ErrorMessageSensor successfully.")
except ImportError as e:
    print(f"ImportError: {e}")
    sys.exit(1)
except Exception as e:
    print(f"An unexpected error occurred during import:")
    traceback.print_exc()
    sys.exit(1)
# --- 导入结束 ---

print("--- 错误信息传感器实时独立测试 (全屏显示版) ---")

# 1. 初始化 ErrorMessageSensor
try:
    error_sensor = ErrorMessageSensor() # 使用默认 ROI (800, 110, 330, 90)
except Exception as e:
    print(f"初始化 ErrorMessageSensor 时发生错误:")
    traceback.print_exc()
    sys.exit(1)

# 2. 初始化屏幕截图工具 和 获取屏幕尺寸
sct = mss.mss()
try:
    screen_width, screen_height = pyautogui.size()
    print(f"屏幕尺寸: {screen_width}x{screen_height}")
except Exception as e:
    print(f"无法获取屏幕尺寸，使用默认 1920x1080: {e}")
    screen_width, screen_height = 1920, 1080

FULL_SCREEN_REGION = {'left': 0, 'top': 0, 'width': screen_width, 'height': screen_height}
print("屏幕截图工具将截取全屏。")

# 3. 主循环
print("\n开始实时测试... 将焦点切换到此窗口按 ESC 键退出")
window_name = "Error Sensor Test - Live Feed"
cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
# 获取 ROI 的尺寸用于创建显示画布
roi_x, roi_y, roi_w, roi_h = error_sensor.x, error_sensor.y, error_sensor.w, error_sensor.h
display_width = max(300, roi_w) # 显示画布宽度
display_height = max(150, roi_h + 60) # 显示画布高度，留出空间给文字
cv2.resizeWindow(window_name, display_width, display_height)

frame_count = 0
start_loop_time = time.time()

while True:
    try:
        # a. 截取【全屏】图像
        frame_full = np.array(sct.grab(FULL_SCREEN_REGION))
        if frame_full is None or frame_full.size == 0: time.sleep(0.1); continue
        frame_bgr = cv2.cvtColor(frame_full, cv2.COLOR_BGRA2BGR)

        # b. 使用 ErrorMessageSensor 检测错误
        detected_errors = error_sensor.detect(frame_bgr)

        # c. 可视化准备
        # !! === 创建一个新的黑色背景用于显示 === !!
        display_canvas = np.zeros((display_height, display_width, 3), dtype=np.uint8)

        # 从全屏截图中【裁剪出 ROI】
        roi_x_end = min(roi_x + roi_w, screen_width)
        roi_y_end = min(roi_y + roi_h, screen_height)
        roi_x_start = max(0, roi_x)
        roi_y_start = max(0, roi_y)

        if roi_x_end > roi_x_start and roi_y_end > roi_y_start:
            roi_crop = frame_bgr[roi_y_start:roi_y_end, roi_x_start:roi_x_end]
            # 将裁剪出的 ROI 放到显示画布的某个位置 (例如左上角)
            # 确保 ROI 不会超出画布边界
            h_roi, w_roi = roi_crop.shape[:2]
            paste_h = min(h_roi, display_height - 60) # 留出上方空间给文字
            paste_w = min(w_roi, display_width)
            if paste_h > 0 and paste_w > 0:
                display_canvas[60:60+paste_h, 0:paste_w] = roi_crop[:paste_h, :paste_w]
        else:
            # ROI 无效时显示提示
            cv2.putText(display_canvas, "Invalid ROI", (10, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)

        # !! === 在【新的画布】上显示检测到的错误状态 === !!
        y_offset = 30 # 从顶部开始写文字
        for identifier, flag in detected_errors.items():
            text = f"{identifier}: {flag}"
            color = (0, 255, 0) if flag == 1 else (180, 180, 180)
            cv2.putText(display_canvas, text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            y_offset += 25

        # !! === 显示【新的画布】 === !!
        cv2.imshow(window_name, display_canvas)

        # d. 检测退出键 (ESC)
        key = cv2.waitKey(100) # 保持 100ms 延迟
        if key == 27:
            print("ESC key pressed, stopping test loop.")
            break

        frame_count += 1

    # --- 异常处理 ---
    except mss.ScreenShotError as ex:
        print(f"截图错误: {ex}. Waiting and retrying...")
        time.sleep(1)
    except KeyboardInterrupt:
        print("测试被用户中断 (Ctrl+C)")
        break
    except Exception as e:
        print("\n主循环发生错误:")
        traceback.print_exc()
        time.sleep(1) # 发生错误后暂停一下

# 清理
# ... (清理代码不变) ...
end_loop_time = time.time()
print(f"\n--- 测试结束 (运行了 {frame_count} 帧，耗时 {end_loop_time - start_loop_time:.2f} 秒) ---")
try:
    cv2.destroyAllWindows()
    for i in range(5): cv2.waitKey(1)
except Exception as e:
    print(f"Error closing OpenCV windows: {e}")
print("窗口已关闭。")

# ===== 文件: D:\wow_ai\scripts\test_template_matching.py =====
import cv2
import numpy as np
import mss # 直接使用 mss 来截屏，更简单
import time
import os

# --- 配置参数 ---
# !! 确认这是正确的 ROI !!
ROI = (319, 35, 245, 64) # (x_left_top, y_left_top, width, height)

# !! 确认这是正确的模板路径 !!
TEMPLATE_PATH = r'D:\wow_ai\data\target_frame_template.png'

# 截取屏幕的区域 (可以只截取包含 ROI 的部分以提高效率，或者截全屏)
# 这里我们截取屏幕左上角 800x600 的区域，应该包含了你的 ROI
# 如果你的 ROI 在屏幕其他位置，需要调整这里的 region
# 或者直接截全屏: SCREEN_REGION = {'left': 0, 'top': 0, 'width': 1920, 'height': 1080}
SCREEN_REGION = {'left': 0, 'top': 0, 'width': 800, 'height': 600}

# 模板匹配方法
MATCH_METHOD = cv2.TM_CCOEFF_NORMED
# --- 配置结束 ---

print("--- 模板匹配独立测试脚本 ---")
print(f"ROI: {ROI}")
print(f"模板路径: {TEMPLATE_PATH}")

# 1. 加载模板图像
if not os.path.exists(TEMPLATE_PATH):
    print(f"错误：找不到模板文件 '{TEMPLATE_PATH}'")
    exit()
template = cv2.imread(TEMPLATE_PATH, cv2.IMREAD_COLOR) # 以彩色加载
if template is None:
    print(f"错误：无法加载模板 '{TEMPLATE_PATH}'")
    exit()
# 获取模板的宽度和高度，用于后面绘制矩形
th, tw = template.shape[:2]
print(f"模板加载成功，尺寸 (高x宽): {th}x{tw}")

# (可选) 如果想测试灰度匹配，取消下面两行注释
# template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
# print("模板已转换为灰度图")

# 2. 初始化屏幕截图工具
sct = mss.mss()
print(f"屏幕截图区域设置为: {SCREEN_REGION}")

# 3. 主循环：截屏、裁剪、匹配、显示
print("\n开始实时匹配... 按 'q' 键退出")
window_name = "Template Matching Test - ROI View"
cv2.namedWindow(window_name, cv2.WINDOW_NORMAL) # 创建可调整窗口

while True:
    # 记录开始时间，用于计算 FPS (可选)
    start_time = time.time()

    # a. 截取屏幕指定区域
    frame_full = np.array(sct.grab(SCREEN_REGION))
    # 转换为 BGR 格式
    frame_bgr = cv2.cvtColor(frame_full, cv2.COLOR_BGRA2BGR)

    # b. 从截屏中裁剪出 ROI 区域
    x, y, w, h = ROI
    # !! 注意：这里的 x, y 是相对于 SCREEN_REGION 左上角的 !!
    # 如果 SCREEN_REGION 是 (0,0,...)，那么 x, y 不变
    # 如果 SCREEN_REGION 不是从 (0,0) 开始，需要调整 x, y:
    # roi_x_start = max(0, x - SCREEN_REGION['left'])
    # roi_y_start = max(0, y - SCREEN_REGION['top'])
    roi_x_start = max(0, x) # 假设 SCREEN_REGION 从 0,0 开始
    roi_y_start = max(0, y)
    roi_x_end = min(roi_x_start + w, frame_bgr.shape[1])
    roi_y_end = min(roi_y_start + h, frame_bgr.shape[0])

    if roi_x_end <= roi_x_start or roi_y_end <= roi_y_start:
        print("警告：ROI 区域无效或超出截图范围，跳过此帧")
        time.sleep(0.1) # 避免空跑 CPU 占用过高
        continue

    roi_crop = frame_bgr[roi_y_start:roi_y_end, roi_x_start:roi_x_end]

    # 检查 ROI 是否有效，以及模板是否能放进去
    if roi_crop.size == 0 or template.shape[0] > roi_crop.shape[0] or template.shape[1] > roi_crop.shape[1]:
        print(f"警告：ROI 裁剪区域为空或模板 ({th}x{tw}) 大于 ROI ({roi_crop.shape[0]}x{roi_crop.shape[1]})，跳过此帧")
        # 在窗口显示提示信息
        display_roi = np.zeros((100, 300, 3), dtype=np.uint8) # 创建黑色背景
        cv2.putText(display_roi, "ROI Invalid or Too Small", (10, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        cv2.imshow(window_name, display_roi)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
        continue # 跳到下一个循环

    # (可选) 如果测试灰度匹配，取消下面一行注释
    # roi_crop_display = cv2.cvtColor(roi_crop, cv2.COLOR_BGR2GRAY) # 转换用于匹配
    roi_crop_display = roi_crop # 保持彩色用于显示

    # c. 执行模板匹配
    result = cv2.matchTemplate(roi_crop_display, template, MATCH_METHOD)
    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)

    # 根据不同的匹配方法，最佳匹配点可能不同
    if MATCH_METHOD in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:
        match_loc = min_loc
        match_score = 1.0 - min_val # 将最小值转换为相似度 (值越小越好)
    else:
        match_loc = max_loc
        match_score = max_val # 值越大越好

    # d. 准备显示结果
    # 复制一份 ROI 图像用于绘制，避免修改原始数据
    display_roi = roi_crop.copy()

    # 在左上角显示匹配分数
    score_text = f"Score: {match_score:.4f}"
    cv2.putText(display_roi, score_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    # 在最佳匹配位置绘制一个矩形框
    # match_loc 是匹配到的模板左上角在 ROI 区域内的坐标
    bottom_right = (match_loc[0] + tw, match_loc[1] + th)
    cv2.rectangle(display_roi, match_loc, bottom_right, (0, 0, 255), 2) # 红色框

    # e. 显示带有分数的 ROI 图像
    cv2.imshow(window_name, display_roi)

    # f. 检测退出键 ('q')
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

    # (可选) 打印 FPS
    # end_time = time.time()
    # fps = 1 / (end_time - start_time)
    # print(f"FPS: {fps:.2f}")

# 清理
cv2.destroyAllWindows()
print("测试结束。")

# ===== 文件: D:\wow_ai\wow_rl\__init__.py =====


# ===== 文件: D:\wow_ai\wow_rl\buffers\replay_buffer.py =====
# wow_rl/buffers/replay_buffer.py
import numpy as np
import os
from pathlib import Path

class ReplayBuffer:
    def __init__(self, capacity, obs_shape, action_dim, save_dir='replay_buffer_data'):
        self.capacity = int(capacity)
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True) # 创建保存目录

        # 使用 NumPy 内存映射文件 (memmap) 来高效存储大数据，避免内存溢出
        # 注意：obs 存储为 uint8 (0-255)，reward 存储为 float32
        print(f"Initializing Replay Buffer with capacity {self.capacity}")
        print(f"Observation shape: {obs_shape}, Action dimension: {action_dim}")
        print(f"Data will be saved in: {self.save_dir.resolve()}")

        try:
            self.obs = np.memmap(self.save_dir / 'obs.memmap', dtype=np.uint8, mode='w+', shape=(self.capacity,) + obs_shape)
            self.next_obs = np.memmap(self.save_dir / 'next_obs.memmap', dtype=np.uint8, mode='w+', shape=(self.capacity,) + obs_shape)
            self.actions = np.memmap(self.save_dir / 'actions.memmap', dtype=np.int64, mode='w+', shape=(self.capacity, action_dim)) # 动作通常是整数索引
            self.rewards = np.memmap(self.save_dir / 'rewards.memmap', dtype=np.float32, mode='w+', shape=(self.capacity, 1))
            self.dones = np.memmap(self.save_dir / 'dones.memmap', dtype=bool, mode='w+', shape=(self.capacity, 1))
            print("Memory-mapped files created successfully.")
        except Exception as e:
            print(f"Error creating memory-mapped files: {e}")
            print("Please check disk space and permissions.")
            raise e

        self._idx = 0 # 当前写入位置
        self._size = 0 # 当前存储的数据量
        self.meta_file = self.save_dir / 'buffer_meta.npz' # 用于保存 _idx 和 _size

    def add(self, obs, action, reward, next_obs, done):
        """添加一条经验到缓冲区"""
        try:
            # 确保动作也是 (1,) 数组 (更稳妥)
            action_arr = np.array([action], dtype=np.int64)

            # 写入数据到内存映射文件
            self.obs[self._idx] = obs.astype(np.uint8)
            self.actions[self._idx] = action_arr # 赋值 (1,) 数组
            # !! 修正 reward 赋值 !!
            self.rewards[self._idx] = np.array([reward], dtype=np.float32) # 创建 (1,) 数组
            self.next_obs[self._idx] = next_obs.astype(np.uint8)
            # !! 修正 done 赋值 !!
            self.dones[self._idx] = np.array([done], dtype=bool) # 创建 (1,) 数组

            # 更新索引和大小
            self._idx = (self._idx + 1) % self.capacity
            self._size = min(self._size + 1, self.capacity)

        except Exception as e:
            print(f"Error adding data to replay buffer at index {self._idx}: {e}")
            print(f"obs type: {type(obs)}, shape: {getattr(obs, 'shape', 'N/A')}, dtype: {getattr(obs, 'dtype', 'N/A')}")
            print(f"action type: {type(action)}, value: {action}") # 打印原始 action
            print(f"reward type: {type(reward)}, value: {reward}") # 打印原始 reward
            print(f"next_obs type: {type(next_obs)}, shape: {getattr(next_obs, 'shape', 'N/A')}, dtype: {getattr(next_obs, 'dtype', 'N/A')}")
            print(f"done type: {type(done)}, value: {done}") # 打印原始 done
            # 如果需要，取消下面一行的注释以查看详细错误
            # traceback.print_exc()
            # 可以选择是否在这里重新抛出异常
            # raise e

    def save(self):
        """保存缓冲区元数据（大小和索引）"""
        try:
            print(f"Saving replay buffer metadata to {self.meta_file}...")
            # 确保 memmap 文件已刷新到磁盘
            self.obs.flush()
            self.next_obs.flush()
            self.actions.flush()
            self.rewards.flush()
            self.dones.flush()
            # 保存元数据
            np.savez(self.meta_file, idx=self._idx, size=self._size)
            print(f"Metadata saved: size={self._size}, idx={self._idx}")
        except Exception as e:
            print(f"Error saving replay buffer metadata: {e}")

    def load(self):
        """加载缓冲区元数据"""
        try:
            if self.meta_file.exists():
                meta = np.load(self.meta_file)
                self._idx = int(meta['idx'])
                self._size = int(meta['size'])
                print(f"Replay buffer metadata loaded: size={self._size}, idx={self._idx}")
            else:
                print(f"Metadata file {self.meta_file} not found. Starting with an empty buffer.")
                self._idx = 0
                self._size = 0
        except Exception as e:
            print(f"Error loading replay buffer metadata: {e}")
            self._idx = 0
            self._size = 0

    def __len__(self):
        return self._size

    # --- 用于 Dreamer 或其他库的数据集接口 ---
    def sample_batch(self, batch_size):
        """随机采样一个批次的数据"""
        if self._size < batch_size:
             print(f"Warning: Buffer size ({self._size}) is less than batch size ({batch_size}). Cannot sample.")
             return None # 或者抛出异常

        idxs = np.random.randint(0, self._size, size=batch_size)
        # 注意：需要从 memmap 读取数据
        batch = {
            'obs': self.obs[idxs].copy(),
            'action': self.actions[idxs].copy(),
            'reward': self.rewards[idxs].copy(),
            'next_obs': self.next_obs[idxs].copy(),
            'done': self.dones[idxs].copy()
        }
        return batch

    def as_tf_dataset(self, batch_size, sequence_length):
         """将缓冲区适配为 TensorFlow Dataset (需要 tensorflow)"""
         # 这个实现比较复杂，需要 tf.data.Dataset.from_generator
         # 暂时留空，DreamerV3 可能有自己的数据加载方式或需要特定格式
         print("Warning: as_tf_dataset is not fully implemented yet.")
         # 简单的示例生成器，需要适配 DreamerV3 的格式
         def _generator():
             while True:
                 idxs = np.random.randint(0, self._size - sequence_length + 1, size=batch_size)
                 # 需要根据序列长度来采样
                 # ... 实现序列采样逻辑 ...
                 # yield {'observation': seq_obs, 'action': seq_act, ...} # 调整键名以匹配 Dreamer
                 yield self.sample_batch(batch_size) # 临时返回批次

         # 需要指定输出签名 (output_signature)
         # output_signature = { 'observation': tf.TensorSpec(...), ... }
         # return tf.data.Dataset.from_generator(_generator, output_signature=...)
         return None

    def as_pytorch_dataloader(self, batch_size, sequence_length):
        """将缓冲区适配为 PyTorch DataLoader (需要 torch)"""
        # 这个实现相对直接，可以创建一个自定义的 Dataset 类
        # 暂时留空，DreamerV3 的 PyTorch 实现可能有自己的加载器
        print("Warning: as_pytorch_dataloader is not fully implemented yet.")
        from torch.utils.data import Dataset, DataLoader

        class BufferDataset(Dataset):
            def __init__(self, buffer, seq_len):
                self.buffer = buffer
                self.seq_len = seq_len

            def __len__(self):
                # 返回可以构成的完整序列的数量
                return max(0, len(self.buffer) - self.seq_len + 1)

            def __getitem__(self, index):
                # 返回一个序列
                end_idx = index + self.seq_len
                # 需要从 memmap 读取数据并转换为 torch tensor
                obs_seq = torch.tensor(self.buffer.obs[index:end_idx].copy(), dtype=torch.float32) / 255.0 # 归一化
                act_seq = torch.tensor(self.buffer.actions[index:end_idx].copy(), dtype=torch.long)
                rew_seq = torch.tensor(self.buffer.rewards[index:end_idx].copy(), dtype=torch.float32)
                done_seq = torch.tensor(self.buffer.dones[index:end_idx].copy(), dtype=torch.bool)
                # 可能还需要 next_obs，取决于 DreamerV3 实现
                # 需要调整字典键名以匹配 DreamerV3 的期望输入
                return {'observation': obs_seq, 'action': act_seq, 'reward': rew_seq, 'is_terminal': done_seq}

        dataset = BufferDataset(self, sequence_length)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0) # num_workers=0 简化处理
        return dataloader

# ===== 文件: D:\wow_ai\wow_rl\envs\wow_face_env.py =====


# ===== 文件: D:\wow_ai\wow_rl\envs\wow_key_env.py =====
# wow_rl/envs/wow_key_env.py (键盘交互版)
import gymnasium as gym
import numpy as np
import cv2
import time
from gymnasium import spaces
# 导入新的和需要的工具类
from wow_rl.utils.screen_grabber import ScreenGrabber
from wow_rl.utils.interactor_keys import send_key # 使用新的按键函数
from wow_rl.utils.reward_sensor import RewardSensor # 使用修正后的 RewardSensor
from wow_rl.utils.detector import Detector # Detector 用于获取 logits

class WowKeyEnv(gym.Env): # 类名改为 WowKeyEnv
    metadata = {"render_modes": ["human", None]}

    def __init__(self,
                 detector_w=r'D:\wow_ai\runs\detect\gwyx_detect_run_s_ep200_neg\weights\best.pt',
                 render_mode=None,
                 max_steps=50,
                 roi_reward=(319, 35, 245, 64),
                 template_path=r'D:\wow_ai\data\target_frame_template.png',
                 match_threshold=0.85, # 使用最终阈值
                 target_class_index=0): # 添加目标类别索引参数

        super().__init__()
        print("Initializing WowKeyEnv...")

        # --- 初始化工具类 ---
        try:
            self.grab = ScreenGrabber()
            print("ScreenGrabber initialized.")
            # 初始化修正后的 RewardSensor, 传入 YOLO 路径和目标索引
            self.rs = RewardSensor(roi=roi_reward, template_path=template_path,
                                   match_thresh=match_threshold, yolo_weight=detector_w,
                                   target_idx=target_class_index)
            print("RewardSensor initialized.")
            # 注意：不再需要 Interactor 实例，直接调用 send_key
            print("Interactor functions (send_key) ready.")
            # Detector 实例现在由 RewardSensor 内部管理，这里不再需要单独创建
            # self.det = Detector(detector_w)
            # print("Detector initialized.")
        except Exception as e:
            print(f"Error during utility initialization: {e}")
            raise e

        # --- 定义新的动作空间和观察空间 ---
        self.action_space = spaces.Discrete(5) # 0:Tab, 1:Shift+Tab, 2:G, 3:F(Attack), 4:None
        self.observation_space = spaces.Dict({
            "frame":   spaces.Box(low=0, high=255, shape=(96, 54, 3), dtype=np.uint8), # 缩小分辨率
            "sel_flag":spaces.Box(low=0, high=1, shape=(1,), dtype=np.uint8),
            "yolo":    spaces.Box(low=0.0, high=1.0, shape=(2,), dtype=np.float32)
        })

        # --- 回合变量 ---
        self.max_steps = max_steps
        self.current_step = 0 # 使用 current_step

        # --- 渲染模式 ---
        self.render_mode = render_mode
        self.window_name = "WowKeyEnv Feed"
        if self.render_mode == "human":
            cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)

        print("WowKeyEnv initialized successfully.")

    def _get_obs(self, full_frame):
        """获取符合新观察空间的字典状态"""
        frame_resized = cv2.resize(full_frame, (96, 54), interpolation=cv2.INTER_AREA)
        sel_flag, yolo_logits = self.rs.analyze(full_frame)
        obs_dict = {
            "frame": frame_resized.astype(np.uint8),
            "sel_flag": np.array([sel_flag], dtype=np.uint8),
            "yolo": yolo_logits.astype(np.float32)
        }
        return obs_dict

    def reset(self, seed=None, options=None):
        """重置环境"""
        super().reset(seed=seed)
        self.current_step = 0
        print("Environment reset.")
        full_frame = self.grab.grab()
        observation = self._get_obs(full_frame)
        info = {}
        if self.render_mode == "human":
            # 初始帧可以简单显示，或显示 ROI
            vis_reset = cv2.resize(full_frame,(480,270))
            cv2.imshow(self.window_name, vis_reset); cv2.waitKey(1)
        return observation, info

    def step(self, action: int):
        """执行键盘动作并计算奖励"""
        if not self.action_space.contains(action):
            # 对于 Discrete 空间，SB3 通常不会传无效动作，但最好还是检查
            print(f"Warning: Invalid action {action} received in step.")
            action = 4 # 默认为 DoNothing

        # 1. 执行键盘动作
        send_key(action, wait=0.05) # 使用 interactor_keys.py 中的函数

        # 2. 等待 UI 更新
        time.sleep(0.5) # 保持较长延迟

        # 3. 获取新状态
        full_frame = self.grab.grab()
        obs = self._get_obs(full_frame)

        # 4. 新的奖励计算逻辑
        sel_flag = int(obs["sel_flag"][0])
        yolo_prob_target = float(obs["yolo"][0])
        # yolo_prob_other = float(obs['yolo'][1]) # 如果需要用到

        reward = -0.05 # 默认小惩罚 (时间惩罚)

        if sel_flag == 1: # 如果选中了目标 (红框出现)
            # 检查是否是目标林精 (根据 YOLO 概率)
            # 0.8 是一个可调阈值，判断是否确信是林精
            is_target_monster = yolo_prob_target > 0.8

            if is_target_monster:
                reward = 1.0 # 基础奖励：成功选中了林精
                if action == 3: # 动作 3 是攻击键 (F)
                    # 强奖励：在选中林精的情况下按下了攻击键
                    reward = 2.0
                    # 未来可以加入ErrorMessageSensor判断距离太远给惩罚
            else:
                # 选中了目标，但不是林精 (或 YOLO 不确定)
                reward = -0.2 # 惩罚选中非目标

        # else: sel_flag == 0 (没选中目标)，奖励保持 -0.05

        # 5. 更新步数和结束判断
        self.current_step += 1
        terminated = False # 没有自然结束条件
        truncated = self.current_step >= self.max_steps

        # 6. 准备 info
        info = {
            'step': self.current_step,
            'raw_reward': reward,
            'sel_flag': sel_flag,
            'yolo_prob_target': yolo_prob_target,
            'action_taken': action
        }

        # 7. 渲染
        if self.render_mode=="human":
            vis = cv2.resize(full_frame,(480,270))
            text = f"Act:{action} Rew:{reward:.2f} Sel:{sel_flag} P(Tgt):{yolo_prob_target:.2f}"
            color = (0, 255, 0) if reward > 0 else ( (0,165,255) if reward > -0.1 else (0,0,255) ) # 正奖励绿色，选中非目标橙色，其他红色
            cv2.putText(vis, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.imshow(self.window_name, vis); cv2.waitKey(1)

        # 8. 返回
        return obs, reward, terminated, truncated, info

    def render(self):
         # human 模式的渲染在 step 中处理
         pass

    def close(self):
        """清理资源"""
        if self.render_mode == "human":
            print("Closing OpenCV window.")
            cv2.destroyWindow(self.window_name)
        print("WowKeyEnv closed.")

# --- 可以加一个简单的环境检查（如果需要） ---
if __name__ == '__main__':
    print("Running a simple check on WowKeyEnv...")
    try:
        env = WowKeyEnv(render_mode=None) # 用 None 模式检查更快
        from gymnasium.utils.env_checker import check_env
        # check_env(env.unwrapped) # 可能因为依赖特定窗口而失败，仅作参考
        print("WowKeyEnv basic initialization check passed.")
        env.close()
    except Exception as e:
        print(f"Error during WowKeyEnv check: {e}")

# ===== 文件: D:\wow_ai\wow_rl\utils\detector.py =====
# wow_rl/utils/detector.py
from ultralytics import YOLO
import torch
import numpy as np # 导入 numpy

class Detector:
    # 非常重要：将此路径更改为指向你自己的最佳模型！
    DEFAULT_WEIGHT_PATH = r'D:\wow_ai\runs\detect\gwyx_detect_run_s_ep200_neg\weights\best.pt'

    def __init__(self, weight_path=DEFAULT_WEIGHT_PATH, device='cuda:0'):
        try:
            self.model = YOLO(weight_path)
            self.device = device
            print(f"Detector 使用权重初始化: {weight_path} 在设备: {device}")
        except Exception as e:
            print(f"使用权重 {weight_path} 初始化 Detector 时出错: {e}")
            raise e # 如果模型加载失败，重新抛出异常以停止程序

    @torch.no_grad() # 对推理性能很重要 - 禁用梯度计算
    def infer(self, frame, conf=0.25):
        # verbose=False 禁用 YOLO 打印检测信息
        results = self.model.predict(frame, device=self.device,
                                     imgsz=640, conf=conf, verbose=False)
        # 检查是否有结果以及结果是否有 'boxes' 属性
        if results and results[0] and hasattr(results[0], 'boxes'):
            boxes = results[0].boxes.xyxy.cpu().numpy()  # [x1,y1,x2,y2]
            scores = results[0].boxes.conf.cpu().numpy()
            return boxes, scores
        else:
            # 如果没有检测到任何东西，返回空数组
            return np.array([]), np.array([])

# ===== 文件: D:\wow_ai\wow_rl\utils\error_sensor.py =====
# D:\wow_ai\wow_rl\utils\error_sensor.py (最终确认版 v2 - 补全方法和导入)
import cv2
import numpy as np
import pytesseract
import os
import traceback # <-- 添加导入

class ErrorMessageSensor:
    # --- 配置参数 ---
    DEFAULT_ERROR_ROI = (800, 110, 330, 90) # 确认这是你的 ROI
    DEFAULT_KEYWORDS = {
        "必须面对目标": "face",
        "距离太远": "range"
    }
    TESSERACT_CMD_PATH = '' # 确认 Tesseract 路径设置或环境变量
    OCR_LANG = 'chi_sim'
    # --- 配置结束 ---

    def __init__(self, roi=DEFAULT_ERROR_ROI, keywords=DEFAULT_KEYWORDS):
        self.x, self.y, self.w, self.h = roi
        self.keywords = keywords
        print(f"ErrorMessageSensor initialized. ROI: {roi}, Keywords: {keywords.keys()}")

        # --- Tesseract 检查 (保持不变) ---
        if self.TESSERACT_CMD_PATH and os.path.exists(self.TESSERACT_CMD_PATH):
            pytesseract.pytesseract.tesseract_cmd = self.TESSERACT_CMD_PATH
            print(f"Tesseract path set to: {self.TESSERACT_CMD_PATH}")
        else:
            try:
                 pytesseract.get_tesseract_version()
                 print("Tesseract found in system PATH.")
            except pytesseract.TesseractNotFoundError:
                 print("ERROR: Tesseract is not installed or not in your PATH.")
                 raise
            except Exception as e:
                 print(f"An unexpected error occurred while checking Tesseract: {e}")
                 raise
        # --- Tesseract 检查结束 ---

    # !! === 添加缺失的 _preprocess_for_ocr 方法 === !!
    def _preprocess_for_ocr(self, image):
        """对图像进行预处理以提高 OCR 准确率"""
        try:
            # 1. 转换为灰度图
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

            # 2. 阈值处理 (自适应阈值 + 反转二值化)
            thresh_img = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                               cv2.THRESH_BINARY_INV, 11, 2)

            # 3. (可选) 在这里可以根据需要添加更多预处理步骤，例如：
            #    - 形态学操作去噪:
            #      kernel = np.ones((1, 1), np.uint8)
            #      thresh_img = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, kernel)
            #    - 边缘保留滤波:
            #      thresh_img = cv2.bilateralFilter(thresh_img, 9, 75, 75)

            # cv2.imshow("OCR Preprocessed", thresh_img) # 用于调试
            # cv2.waitKey(1)
            return thresh_img
        except Exception as e:
            print(f"Error during OCR preprocessing: {e}")
            return np.zeros((10, 10), dtype=np.uint8) # 返回空图像
    # !! === 方法添加结束 === !!


    def detect(self, frame):
        """检测帧的 ROI 区域是否包含定义的错误关键词。"""
        error_flags = {identifier: 0 for identifier in self.keywords.values()}

        try:
            # --- 裁剪 ROI (保持不变) ---
            frame_h, frame_w = frame.shape[:2]
            roi_x_end = min(self.x + self.w, frame_w)
            roi_y_end = min(self.y + self.h, frame_h)
            roi_x_start = max(0, self.x)
            roi_y_start = max(0, self.y)

            if roi_x_end <= roi_x_start or roi_y_end <= roi_y_start:
                return error_flags

            roi_crop = frame[roi_y_start:roi_y_end, roi_x_start:roi_x_end]

            if roi_crop.size == 0:
                return error_flags
            # --- 裁剪结束 ---

            # --- 图像预处理和 OCR (调用现在已存在的方法) ---
            preprocessed_roi = self._preprocess_for_ocr(roi_crop) # !! 现在这个方法存在了 !!
            config = f'-l {self.OCR_LANG} --psm 6'
            detected_text = pytesseract.image_to_string(preprocessed_roi, config=config)
            # print(f"    [ErrorSensor Debug] Detected Text: '{detected_text.strip()}'")

            # --- 检查关键词 (保持不变) ---
            for keyword, identifier in self.keywords.items():
                if keyword in detected_text:
                    error_flags[identifier] = 1

        except pytesseract.TesseractNotFoundError:
            print("Error: Tesseract command not found during detect. Check initialization.")
        except Exception as e:
            print(f"Error in ErrorMessageSensor.detect:")
            traceback.print_exc() # !! 现在 traceback 已经导入 !!

        return error_flags

# !! 确保文件末尾没有 if __name__ == '__main__': 测试代码 !!

# ===== 文件: D:\wow_ai\wow_rl\utils\interactor.py =====
# wow_rl/utils/interactor.py
import pydirectinput
import time
import random

class Interactor:
    # 假设屏幕是 1920x1080，并将其划分为 32x18 的网格
    def __init__(self, origin=(0,0), grid=(32,18), size=(1920,1080)):
        self.ox, self.oy = origin # 游戏区域的左上角坐标（通常是 0,0）
        self.gw, self.gh = grid   # 网格的维度 (宽, 高)
        self.sw, self.sh = size   # 游戏区域的像素尺寸 (宽, 高)
        # 计算单个网格单元的尺寸
        self.cell_w = self.sw // self.gw
        self.cell_h = self.sh // self.gh
        # pyautogui.PAUSE = 0 # 禁用 pyautogui 的暂停（pydirectinput 默认没有）

    def click_action(self, a:int):
        """ 在与动作 'a' 对应的网格单元执行点击 """
        if not (0 <= a < self.gw * self.gh):
            print(f"警告: 动作 {a} 超出范围 (0-{self.gw * self.gh - 1}). 跳过点击。")
            return

        # 根据动作 'a' 计算网格坐标 (gx, gy)
        gx = a % self.gw
        gy = a // self.gw

        # 计算网格单元中心的像素坐标
        cell_center_x = self.ox + gx * self.cell_w + self.cell_w // 2
        cell_center_y = self.oy + gy * self.cell_h + self.cell_h // 2

        # 添加小的随机偏移，避免总是点击完全相同的像素
        # 将随机性限制在例如单元格尺寸的 +/- 1/4
        rand_x_offset = random.randint(-self.cell_w // 4, self.cell_w // 4)
        rand_y_offset = random.randint(-self.cell_h // 4, self.cell_h // 4)

        click_x = cell_center_x + rand_x_offset
        click_y = cell_center_y + rand_y_offset

        # 确保点击坐标在屏幕边界内
        click_x = max(self.ox, min(click_x, self.ox + self.sw - 1))
        click_y = max(self.oy, min(click_y, self.oy + self.sh - 1))

        # print(f"动作: {a} -> 网格({gx},{gy}) -> 点击坐标 ({click_x},{click_y})") # 用于调试

        # 使用 pydirectinput 移动和点击
        try:
            pydirectinput.moveTo(click_x, click_y)
            pydirectinput.click()
            time.sleep(0.05)  # 点击后短暂暂停
        except Exception as e:
            print(f"pydirectinput 交互期间出错: {e}")

# ===== 文件: D:\wow_ai\wow_rl\utils\interactor_keys.py =====
# wow_rl/utils/interactor_keys.py
import pydirectinput
import time

# --- 按键映射字典 ---
# 动作编号 -> lambda 函数来执行按键
# 使用 pydirectinput 来发送指令
KEY_FUNCTIONS = {
    0: lambda: pydirectinput.press('tab'),          # 动作 0: 按 Tab (选中最近敌人)
    1: lambda: (pydirectinput.keyDown('shift'),     # 动作 1: 按 Shift+Tab
                time.sleep(0.02),                 # 短暂等待确保 shift 按下
                pydirectinput.press('tab'),
                time.sleep(0.02),
                pydirectinput.keyUp('shift')),
    2: lambda: pydirectinput.press('g'),            # 动作 2: 按 G (上一个敌对)
    3: lambda: pydirectinput.press('f'),            # 动作 3: 按 F (攻击目标 - 假设 F 是攻击键)
    4: lambda: None                                 # 动作 4: 什么都不做
}
# --- 按键映射结束 ---

def send_key(action: int, wait: float = 0.05):
    """
    根据动作编号执行对应的键盘指令。
    Args:
        action (int): 动作编号 (0-4)。
        wait (float): 执行按键后等待的时间 (秒)。
    """
    if action in KEY_FUNCTIONS:
        try:
            # print(f"Sending Key Action: {action}") # 用于调试
            action_func = KEY_FUNCTIONS[action]
            action_func() # 执行对应的 lambda 函数
            if action_func is not None: # 如果不是空动作，才等待
                time.sleep(wait)
        except Exception as e:
            print(f"Error sending key for action {action}: {e}")
    else:
        print(f"Warning: Invalid action number {action} received in send_key.")

# 可以添加一个简单的测试函数
if __name__ == '__main__':
    print("Testing Interactor Keys...")
    print("Pressing Tab (Action 0) in 3 seconds...")
    time.sleep(3)
    send_key(0)
    print("Tab sent.")
    time.sleep(1)
    print("Pressing Shift+Tab (Action 1) in 3 seconds...")
    time.sleep(3)
    send_key(1)
    print("Shift+Tab sent.")
    time.sleep(1)
    print("Pressing G (Action 2) in 3 seconds...")
    time.sleep(3)
    send_key(2)
    print("G sent.")
    time.sleep(1)
    print("Pressing F (Action 3) in 3 seconds...")
    time.sleep(3)
    send_key(3)
    print("F sent.")
    time.sleep(1)
    print("Doing Nothing (Action 4) in 3 seconds...")
    time.sleep(3)
    send_key(4)
    print("None sent.")
    print("Test finished.")

# ===== 文件: D:\wow_ai\wow_rl\utils\reward_sensor.py =====
# wow_rl/utils/reward_sensor.py (修正版 v3 - 结合模板匹配和YOLO)
import cv2
import numpy as np
import os
import torch # 导入 torch
from ultralytics import YOLO # 需要导入 YOLO

class RewardSensor:
    # --- 配置参数 (确保这些值是你最终验证有效的) ---
    DEFAULT_ROI = (319, 35, 245, 64)
    DEFAULT_TEMPLATE_PATH = r'D:\wow_ai\data\target_frame_template.png'
    DEFAULT_MATCH_THRESHOLD = 0.85 # 使用我们验证过的可靠阈值
    DEFAULT_YOLO_WEIGHT = r'D:\wow_ai\runs\detect\gwyx_detect_run_s_ep200_neg\weights\best.pt'
    TARGET_CLASS_INDEX = 0 # 假设林精是第 0 类
    # --- 配置结束 ---

    def __init__(self, roi=DEFAULT_ROI, template_path=DEFAULT_TEMPLATE_PATH,
                 match_thresh=DEFAULT_MATCH_THRESHOLD, yolo_weight=DEFAULT_YOLO_WEIGHT,
                 target_idx=TARGET_CLASS_INDEX, device='cuda:0'):

        self.x, self.y, self.w, self.h = roi
        self.match_thresh = match_thresh
        self.target_idx = target_idx
        self.device = device

        self.template = self._load_template(template_path)
        if self.template is None:
            raise ValueError(f"无法加载模板图像: {template_path}")

        try:
            self.det = YOLO(yolo_weight)
            print(f"RewardSensor: YOLO detector loaded from {yolo_weight}")
        except Exception as e:
            print(f"RewardSensor: Error loading YOLO model {yolo_weight}: {e}")
            raise e

        print(f"RewardSensor 使用 ROI {roi}, 模板 {template_path}, 匹配阈值 {match_thresh}, 目标类别 {target_idx} 初始化")

    def _load_template(self, template_path):
        if not os.path.exists(template_path):
            print(f"错误: 模板文件未找到 '{template_path}'")
            return None
        template_img = cv2.imread(template_path, cv2.IMREAD_COLOR)
        if template_img is None:
            print(f"错误: 无法加载模板 '{template_path}'")
            return None
        return template_img

    @torch.no_grad()
    def analyze(self, frame):
        sel_flag = 0
        yolo_logits = np.zeros(2, dtype=np.float32)
        match_score = -1.0

        if self.template is None:
            return sel_flag, yolo_logits

        try:
            frame_h, frame_w = frame.shape[:2]
            roi_x_end = min(self.x + self.w, frame_w)
            roi_y_end = min(self.y + self.h, frame_h)
            roi_x_start = max(0, self.x)
            roi_y_start = max(0, self.y)

            if roi_x_end <= roi_x_start or roi_y_end <= roi_y_start:
                return sel_flag, yolo_logits

            roi_crop = frame[roi_y_start:roi_y_end, roi_x_start:roi_x_end]

            if roi_crop.size == 0 or self.template.shape[0] > roi_crop.shape[0] or self.template.shape[1] > roi_crop.shape[1]:
                return sel_flag, yolo_logits

            result = cv2.matchTemplate(roi_crop, self.template, cv2.TM_CCOEFF_NORMED)
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)
            match_score = max_val
            if match_score >= self.match_thresh:
                sel_flag = 1

            if sel_flag == 1:
                res = self.det.predict(roi_crop, device=self.device, verbose=False, imgsz=224, conf=0.1)[0]
                if len(res.boxes):
                    best_box_idx = res.boxes.conf.argmax()
                    cls = int(res.boxes.cls[best_box_idx])
                    prob = float(res.boxes.conf[best_box_idx])
                    if cls == self.target_idx:
                        yolo_logits[0] = prob
                    else:
                        yolo_logits[1] = prob

            return sel_flag, yolo_logits.astype(np.float32)

        except Exception as e:
            print(f"Error in RewardSensor.analyze: {e}")
            return sel_flag, yolo_logits.astype(np.float32)

# ===== 文件: D:\wow_ai\wow_rl\utils\screen_grabber.py =====
# wow_rl/utils/screen_grabber.py
import mss, numpy as np, cv2

class ScreenGrabber:
    def __init__(self, region=(0, 0, 1920, 1080)): # 默认截取整个 1920x1080 屏幕
        self.mon = {'left':region[0], 'top':region[1],
                    'width':region[2], 'height':region[3]}
        self.sct = mss.mss()

    def grab(self):
        frame = np.array(self.sct.grab(self.mon))
        # 移除 alpha 通道（如果存在）并将 BGRA 转换为 BGR
        return cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)